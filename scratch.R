

fruit <- c("apple", "banana", "pear", "pineapple")

str_subset(fruit, "a")
str_subset(fruit, "^a")
str_subset(fruit, "a$")
str_subset(fruit, "b")
str_subset(fruit, "[aeiou]")

# Elements that don't match
str_subset(fruit, "^p", negate = TRUE)

# Missings never match
str_subset(c("a", NA, "b"), ".")



art %>%  separate(title, into = c("title", "year"), sep = "\\(")

    


page %>% html_nodes(".record-title a")


titles <- page %>%
    html_nodes(".iteminfo") %>%
    html_node("h3") %>%
    html_node("a") %>%
    html_text2()

titles



gss16$polviews[gss16$polviews == "Extrmly conservative"] <- "Extremely conservative"

save(gss16, file = '~/github/ds201/ds201/hw/hw-08/data/gss16.rda')
save(gss16, file = '~/github/ds201/ds201/starters/hw/hw-08-exploring-gss/data/gss16.rda')
save(gss16, file = '~/github/ds201/ds201/solns/data/gss16.rda')



## Scrape the list of top 250 movies from https://www.imdb.com/chart/top

# Load packages ---------------------------------------------------------------

library(tidyverse)
library(rvest)

# Read html page ---------------------------------------------------------------

page <- read_html("https://www.imdb.com/chart/top")

titles <- page %>% html_nodes(".ipc-title__text") %>% html_text()

titles



# Titles -----------------------------------------------------------------------

titles <- page %>%
  html_nodes(".title a") %>%
    html_text()

titles <- gsub('^[0-9]*\\. *', '', titles[-(1:2)][1:250])



# Years-------------------------------------------------------------------------



years <- page %>% html_nodes(".cli-title-metadata span:nth-child(1)") %>% html_text() %>% as.numeric()


years <- page %>%
  html_nodes(".secondaryInfo") %>%
  html_text() %>%
  str_remove("\\(") %>%
  str_remove("\\)") %>%
  as.numeric()

# Scores -----------------------------------------------------------------------

scores <- page %>% html_nodes(".ipc-rating-star--imdb") %>% html_attr("aria-label") %>% str_remove("IMDb rating: ") %>% as.numeric()

scores <- page %>%
  html_nodes("strong") %>%
  html_text() %>%
  as.numeric()

# Put it all in a data frame ---------------------------------------------------

imdb_top_250 <- tibble(
  title = titles,
  rating = scores,
  year = years
)

# Add rank ---------------------------------------------------------------------

imdb_top_250 <- imdb_top_250 %>%
  mutate(rank = 1:nrow(imdb_top_250)) %>%
  relocate(rank)





df <- data.frame(year = c(1,1,2,2), amount = c(1,2, 0,10), party = c('a','b','a','b'))
df

df %>% ggplot(aes(x=year, y=amount, color=party)) + geom_line()
p  %>% ggplot(aes(x=year, y=amount, color=party)) + geom_line()


p %>% ggplot(aes(x=year, y=amount, color=party)) +
    geom_line()

df <- data.frame(aaa = c('a1b', 'AA1B1B', '1CCC'))

lapply(strsplit(df$aaa, '1'), length)

df %>% separate_wider_delim(aaa, '1', names = c('name1', 'name2'))
df %>% separate_wider_delim(aaa, '1', names = c('name1', 'name2'), too_many = "merge")


library(nimble)
library(testthat)
source('~/github/nimble/nimble/packages/nimble/tests/testthat/test_utils.R')

        ##
        cat('=============================\n')
        a <- trialResults
        print(paste0('length trialResults: ', length(a)))
        cat('=============================\n')
        tmp <- paste0(1:length(a), ': ~', a, '~\n'); cat(tmp)
        cat('=============================\n')
        a <- correctResults
        print(paste0('length correctResults: ', length(a)))
        cat('=============================\n')
        tmp <- paste0(1:length(a), ': ~', a, '~\n'); cat(tmp)
        cat('=============================\n')
##




a <- c('sdfsd', 'bbbbb', 'cccccc')
tmp <- paste0(1:length(a), ': ~', a, '~\n')
print(tmp)
cat(tmp)

print(paste0('length trialResults: ', length(a)))

lego_sales <- lego_sales %>%
  mutate(age_group = case_when(
    age <= 18            ~ "18 and under",
    between(age, 19, 25) ~ "19 - 25",
    between(age, 26, 35) ~ "26 - 35",
    between(age, 36, 51) ~ "36 - 51",
    age >= 51            ~ "51 and over"))

as.data.frame(lego_sales %>% select(age, age_group))




lego_sales %>%
  filter(theme == "Star Wars")
count(subtheme)



  arrange(desc(n)) %>%
      slice_max(n, n = 3)




l <- c('a', 'f', 'c', 'z', 'y')

f <- factor(l, levels = l)


f
fct_relevel(f, unique(sort(l)))
fct_relevel(f, letters)
day.name

library(tidyverse)
hotels <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv")

hotels






library(nimble)

code <- nimbleCode({
    rate ~ dunif(0, 100)
    beta ~ dunif(0, 100)
    for(i in 1:N) {
        shape[i] <- exp(beta*x[i]) * rate
        y[i] ~ dgamma(shape[i], rate)
    }
})

shape <- .3
rate <- 4
N <- 10000
set.seed(0)
y <- rgamma(N, shape=shape, rate=rate)
x <- rnorm(N, 0, 0.2)

constants <- list(N = N, x = x)
data <- list(y = y)
inits <- list(rate = 1, beta = 0.4)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000, 1000)
dim(samples)

library(basicMCMCplots)
samplesPlot(samples)

library(coda)
effectiveSize(samples)

colnames(samples)
samplesSummary(samples)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()

stochVars <- unique(nimble:::removeIndexing(Rmodel$getNodeNames(stochOnly = TRUE)))
for(v in stochVars)   cat(v, ': ', Rmodel$calculate(v), '\n')





names(college_recent_grads)

?college_recent_grads


college_recent_grads %>%
    mutate(prop_women = percent(women / total)) %>%
    arrange(desc(prop_women)) %>%
    select(major, total, prop_women) %>%
    top_n(3)




states$abbreviation

dennys %>%
    filter(!state %in% states$abbreviation)

dennys %>%
    filter(!(state %in% states$abbreviation))




laquinta %>%
    filter(!(state %in% states$abbreviation))



lq2 <- laquinta

state.abb <- states$abbreviation



laquinta <- laquinta %>%
  mutate(country = case_when(
    state %in% state.abb     ~ "United States",
    state %in% c("ON", "BC") ~ "Canada",
    state == "ANT"           ~ "Colombia",
    state %in% c("AG", "QR", "CH", "NL", "VE", "PU", "SL") ~ "Mexico",
    state == "FM"            ~ "Honduras"
  ))



dennys %>%
    group_by(state) %>%
    count() %>%
    arrange(n) %>%
    head(1)

dennys %>%
    group_by(state) %>%
    count() %>%
    arrange(n) %>%
    tail(1)

laquinta %>%
    group_by(state) %>%
    count() %>%
    arrange(n) %>%
    tail(1)

laquinta %>%
    group_by(state) %>%
    count() %>%
    arrange(n) %>%
    head(1)


dennys %>%
  count(state) %>%
  inner_join(states, by = c("state" = "abbreviation")) %>%
  mutate(density = n/area) %>%
  arrange(desc(density)) %>%
  head(5)  

laquinta %>%
  count(state) %>%
  inner_join(states, by = c("state" = "abbreviation")) %>%
  mutate(density = n/area) %>%
  arrange(desc(density)) %>%
  head(5)  


dennys <- dennys %>%
  mutate(establishment = "Denny's")
laquinta <- laquinta %>%
  mutate(establishment = "La Quinta")
dn_lq <- bind_rows(dennys, laquinta)


dennys %>%
    count(state)

library(nimble)

nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {},
    methods = list(
        myMin = function(x = double(1)) {
            returnType(double())
            return(min(x))
        },
        myMax = function(x = double(1)) {
            returnType(double())
            return(max(x))
        },
        myMean = function(x = double(1)) {
            returnType(double())
            return(mean(x))
        },
        myProd = function(x = double(1)) {
            returnType(double())
            return(prod(x))
        },
        mySD = function(x = double(1)) {
            returnType(double())
            return(sd(x))
        }
    )
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)

x <- 1:5
Rnf$myMin(x);  Cnf$myMin(x)
Rnf$myMax(x);  Cnf$myMax(x)
Rnf$myMean(x); Cnf$myMean(x)
Rnf$myProd(x); Cnf$myProd(x)
Rnf$mySD(x);   Cnf$mySD(x)

x <- c(1, 2, NA, 4, 5)
Rnf$myMin(x);  Cnf$myMin(x)
Rnf$myMax(x);  Cnf$myMax(x)
Rnf$myMean(x); Cnf$myMean(x)
Rnf$myProd(x); Cnf$myProd(x)
Rnf$mySD(x);   Cnf$mySD(x)




6*pi


system("mkdir ~/temp/shiny")
system("mkdir ~/temp/shiny/app-2")

runApp('~/temp/shiny/app-1', display.mode = 'showcase')

runApp('~/temp/shiny/app-2', display.mode = 'showcase')

runExample("01_hello")      # a histogram
runExample("02_text")       # tables and data frames
runExample("03_reactivity") # a reactive expression
runExample("04_mpg")        # global variables
runExample("05_sliders")    # slider bars
runExample("06_tabsets")    # tabbed panels
runExample("07_widgets")    # help text and submit buttons
runExample("08_html")       # Shiny app built from HTML
runExample("09_upload")     # file upload wizard
runExample("10_download")   # file download wizard
runExample("11_timer")      # an automated timer




system("osascript -e 'tell application \"Acrobat\" to quit'")
setwd('~/github/nimble/nimbleHMC/joss/paper')
f <- 'paper.md'
library(rmarkdown)
rmarkdown::render(f, output_format = 'pdf_document')
system('open paper.pdf')







library(nimble)
library(nimbleEcology)     ## NEW

##lion <- read_csv("simbaS.csv")
##y <- lion %>% select(Jan_2016:Apr_2019) %>% as.matrix()


##y <- matrix(c(1,1,1,1,0,0), nrow=2, byrow=TRUE)
y <- matrix(c(1,0), nrow=1, byrow=TRUE)



#' ## CJS models
#' 
#' We start the session by fitting models with or without a time effect on survival 
#' and recapture probabilities. 
#' 
#' A model with constant parameters.
## ---------------------------------------------------------------------------------------------------------
hmm.phip <- nimbleCode({
    phi ~ dunif(0, 1) # prior survival
    p ~ dunif(0, 1) # prior detection
    # likelihood
    gamma[1,1] <- phi      # Pr(alive t -> alive t+1)
    gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)
    gamma[2,1] <- 0        # Pr(dead t -> alive t+1)
    gamma[2,2] <- 1        # Pr(dead t -> dead t+1)
    delta[1] <- 1          # Pr(alive t = 1) = 1
    delta[2] <- 0          # Pr(dead t = 1) = 0
    omega[1,1] <- 1 - p    # Pr(alive t -> non-detected t)
    omega[1,2] <- p        # Pr(alive t -> detected t)
    omega[2,1] <- 1        # Pr(dead t -> non-detected t)
    omega[2,2] <- 0        # Pr(dead t -> detected t)
    for (i in 1:N){
        z[i,first[i]] ~ dcat(delta[1:2])
        for (j in (first[i]+1):T){
            z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])
            y[i,j] ~ dcat(omega[z[i,j], 1:2])
        }
    }
})

hmm.phip_dHMM <- nimbleCode({                 ## NEW model code
    phi ~ dunif(0, 1) # prior survival
    p ~ dunif(0, 1) # prior detection
    # likelihood
    gamma[1,1] <- phi      # Pr(alive t -> alive t+1)
    gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)
    gamma[2,1] <- 0        # Pr(dead t -> alive t+1)
    gamma[2,2] <- 1        # Pr(dead t -> dead t+1)
    delta[1] <- 1          # Pr(alive t = 1) = 1
    delta[2] <- 0          # Pr(dead t = 1) = 0
    omega[1,1] <- 1 - p    # Pr(alive t -> non-detected t)
    omega[1,2] <- p        # Pr(alive t -> detected t)
    omega[2,1] <- 1        # Pr(dead t -> non-detected t)
    omega[2,2] <- 0        # Pr(dead t -> detected t)
    for (i in 1:N){
        y[i, first[i]:T] ~ dHMM(delta[1:2], omega[1:2,1:2], gamma[1:2,1:2], len[i], 1)    ## NEW: uses dHMM distribution
    }
})


#' 
#' Get the occasion of first capture for all individuals.
## ---------------------------------------------------------------------------------------------------------
first <- apply(y, 1, function(x) min(which(x !=0)))

## we have to discard individuals which are seen for the first time
## on the T=14 sampling occastion
indToKeep <- first < ncol(y)    ## NEW
first <- first[indToKeep]       ## NEW
y <- y[indToKeep, , drop = FALSE]             ## NEW

#' 
#' A list with constants.
## ---------------------------------------------------------------------------------------------------------
my.constants <- list(N = nrow(y), 
                     T = ncol(y), 
                     ##first = first,
                     ##len = ncol(y)-first)    ## NEW: length of each observation history (from t=first to t=T)
                     first = c(first, 0),
                     len = c(ncol(y)-first+1, 0))    ## NEW: length of each observation history (from t=first to t=T)
my.constants

#' 
#' Now the data in a list. Note that we add 1 to the data to have 
#' 1 for non-detections and 2 for detections. 
#' You may use the coding you prefer of course, 
#' you will just need to adjust the $\Omega$ and $\Gamma$ matrices in the model above.  
## ---------------------------------------------------------------------------------------------------------
my.data <- list(y = y + 1)

#' 
#' Specify initial values. For the latent states, 
#' we go for the easy way, and say that all individuals are alive through the study period. 
## ---------------------------------------------------------------------------------------------------------
zinits <- y + 1 # non-detection -> alive                       ## NEW: no longer need zinits
zinits[zinits == 2] <- 1 # dead -> alive                       ## NEW: no longer need zinits
initial.values <- function() list(phi = runif(1,0,1),
                                  p = runif(1,0,1),
                                  z = zinits)                  ## NEW: no longer need zinits
initial.values()



#' 
#' Some information that we now pass as initial value info (observations of alive) 
#' are actually known states, and could also be passed as data â€“ 
#' in which case the initial values have to be 0.
#' 
#' Specify the parameters we wish to monitor. 
## ---------------------------------------------------------------------------------------------------------
parameters.to.save <- c("phi", "p")
parameters.to.save

#' 
#' MCMC details. 
## ---------------------------------------------------------------------------------------------------------
n.iter <- 50000
n.burnin <- 20000
n.chains <- 1

#' 
#' At last, let's run nimble.
## ---------------------------------------------------------------------------------------------------------
samples <- nimbleMCMC(code = hmm.phip,          ## NEW: using new model code (hmm.phip_dHMM)
                      constants = my.constants,
                      data = my.data,              
                      inits = initial.values,
                      monitors = parameters.to.save,
                      niter = n.iter,
                      nburnin = n.burnin, 
                      nchains = n.chains)




samples_dHMM <- nimbleMCMC(code = hmm.phip_dHMM,          ## NEW: using new model code (hmm.phip_dHMM)
                           constants = my.constants,
                           data = my.data,              
                           inits = initial.values,
                           monitors = parameters.to.save,
                           niter = n.iter,
                           nburnin = n.burnin, 
                           nchains = n.chains)


samplesSummary(samples)
samplesSummary(samples_dHMM)




set.seed(0)
inits <- initial.values()
Rmodel <- nimbleModel(hmm.phip_dHMM, my.constants, my.data, inits)


Rmodel$calculate('y[1, 1:2]')

phi <- Rmodel$phi
p <- Rmodel$p
log(1 - phi * p)
log(1-phi + phi*(1-p))

log(p * (1 - phi*p))


gamma <- Rmodel$gamma
omega <- Rmodel$omega

x <- my.data$y[1,]

dHMM(x[1:2], c(1,0), omega, gamma, 2, 1, log = TRUE)

undebug(dHMM)
debug(dHMM)






















#---------------------------------------------- SIMULATE DATA -----------------------------------------#

library(nimble)
library(mvtnorm)

# Simulate data from an inner product network model
set.seed(1234)

# Data (and latent space) structure
V <- 100 # Number of vertices
H <- 10 # Dimension of latent space

# Hyperparameters
a1 <- 2.5 # Gamma shrinkage parameter for factor 1
a2 <- 2.5 # Gamma shrinkage parameters for factors 2:H
sd.e <- 0.1 # Gaussian noise added to linear predictor
meanP <- 0.25 # Moderately sparse network
mu0 <- probit(meanP) # Prior mean for intercept
sd.mu <- 0.1 # Prior sd for intercept: consider probit scale

# Simulate multiplicative gamma shrinkage process
U <- rep(NA,H)
U[1] <- rgamma(1, a1, 1)
U[2:H] <- rgamma(H-1, a2, 1)

Tau <- rep(NA, H)
for(h in 1:H){
    Tau[h] <- prod(U[1:h])
}

# Simulate latent factors
X <- matrix(NA, nrow = V, ncol = H)
for(h in 1:H){
    X[,h] <- rmvnorm(n = 1, mean = rep(0, V), sigma = diag(Tau[h]^(-1), nrow = V))
}

# Simulate intercept
mu <- rnorm(1,mean = mu0, sd = sd.mu) # Normal prior for baseline connection score

# Compute the linear predictor and Z latent connection score
M.Z <- mu + X %*% t(X)
vecE <- rnorm(V*(V-1)/2, 0, sd.e)
E <- matrix(NA, V,V)
E[upper.tri(E)] <- vecE
E[lower.tri(E)] <- t(E)[lower.tri(E)]
diag(E) <- 0
Z <- M.Z + E # Data augmentation approach

# Simulate edges and non-edges
Y <- ifelse(Z > 0, 1, 0)
diag(Y) <- diag(Z) <- 0 # No self-edges

#----------------------------------------- DEFINE NIMBLE MODEL---------------------------------------------#
fmCode <- nimbleCode({
    
    # Intercept
    mu ~ dnorm(mu0, sd = sd.mu)
    
    # Shrinkage process
    U[1] ~ dgamma(a1, 1)
    for(h in 2:H){
        U[h] ~ dgamma(a2, 1)
    }

    for(h in 1:H){
        Tau[h] <- prod(U[1:h])
    }

    # Latent factors
    for(h in 1:H){
        for(v in 1:V){
            X[v,h] ~ dnorm(M[v] , sd = sqrt(Tau[h]^(-1)))
        }
    }
    
    # Compute linear predictor
    M.Z[1:V,1:V] <- mu + X[,] %*% t(X[,]) # Recall multivariate nodes must be used with []

    # Model
    for (i in 2:V){
        for (j in 1:(i-1)){ # Self-edges not allowed
            E[i,j] ~ dnorm(mean = 0, sd = sd.e)
            Z[i,j] <- M.Z[i,j] + E[i,j]
            indZ[i,j] <- getIndicator(Z[i,j]) # Function required to pass indices to if statement
            Y[i,j] ~ dbin(size = 1, prob = indZ[i,j]) 
        }
    }

})


# Function to get indicator
getIndicator <- nimbleFunction(
    run = function(Zij = double(0)) {  
        if(Zij > 0){
            Yij <- 1
        }else{
            Yij <- 0
        }
        return(Yij)
        returnType(double(0))
    })


# Define the constants
mu0 = probit(mean(Y, na.rm = TRUE)) # Reasonable guess for intercept prior mean

fmConsts <- list(V = V,
                 H = H,
                 a1 = a1, a2 = a2,
                 mu0 = mu0, sd.mu = sd.mu,
                 M = rep(0, V), sd.e = sd.e)

# Define the data
fmData <- list(Y = Y)

# Set initialization parameters
fmInits <- list(X = matrix(0, V, H),
                U = 1:H,
                mu = mu0,
                E = matrix(0,V,V),
                Z = ifelse(Y==1, 1, -1),
                indZ = Y)

fmDims <- list(Tau = H, X = c(V, H), mvCov = c(V,V,H),
               E = c(V,V))

# Create NIMBLE model
fm <- nimbleModel(code = fmCode, name = "fm", constants = fmConsts, data = fmData,
                  dimensions = fmDims, inits = fmInits)

Rmodel <- fm
Rmodel$calculate()
Rmodel$initializeInfo(TRUE)
Rmodel$logProb_Y

Ynodes <- Rmodel$expandNodeNames('Y')
ylp <- sapply(Ynodes, function(n) Rmodel$calculate(n))

head(ylp, 20)

Rmodel$expandNodeNames('Y[18, 1]')
Rmodel$calculate('Y[18, 1]')
Rmodel$Y[18,1]
Rmodel$indZ[18,1]
Rmodel$Z[18,1]
Rmodel$M.Z[18,1]
Rmodel$E[18,1]


# Check conjugacy
configureMCMC(fm, print = TRUE) # no conjugacy

# ------------------------------------ FIT MODEL AND EVALUATE----------------------------------------#
niter <- 200##2000
nchains <- 1#2

mcmc.out <- nimbleMCMC(code = fmCode, constants = fmConsts,
                       data = fmData, inits = fmInits,
                       nchains = nchains, niter = niter,
                       summary = TRUE, WAIC = TRUE,
                       monitors = c('mu', 'Z', 'Tau')) 

# Check recovery of latent probabilities
fm.samples <- do.call(rbind, mcmc.out$samples)
p.samples <- fm.samples[, grepl("P", colnames(fm.samples))]

p.post <- matrix(data = colMeans(p.samples), byrow = FALSE, nrow = V, ncol = V)
diag(p.post) <- 0
plot(c(p.post), c(P))





get.surv.riv <- nimbleFunction(
    ##N, F, M, ,error, area, nseas
    run = function(N=double(0),Mv=double(0),eps=double(0),Ff=double(1),sel=double(1),nfl=integer(0)) {
        returnType(double(0)) 
        Fv<-nimNumeric(nfl)
        for(j in 1:nfl){
            Fv[j]<-exp(Ff[j])*sel[j]  #river non RiverFishingbuilding
        }
        stmp<-exp(-(Mv+sum(Fv[1:nfl])))
        survtmp<-N*stmp*eps 
        return(survtmp)
    }
)


a <- 3
N <- 30
Mv <- .3
eps <- 234.3
Ff <- (1:a) / 100
sel <- rnorm(a, 3)^2 + 1

get.surv.riv(N, Mv, eps, Ff, sel, a)

N * eps * exp(-(Mv + sum(sel[1:a] * exp(Ff[1:a]))))






expr <- quote(
    ##AA <- get.surv.sea(N, MV, EPS, FF[1:10], PHI[1:10], SEL[1:10], 2) ## orig
    T.surv.imm[i,1,1,st,a]<-get.surv.sea(T.lik[i,1,1,st,a],Mpsa[relage[i,st],relyr[i,st],st],kE.Mps[relyr[i,st],aseas[1],relage[i,st],a],LF[relyr[i,st],1:2,st,a],phi[relyr[i,st],aseas[1],1:2,st,a],Fsel[relage[i,st],1:2],2)
)
print(expr)

## function(N=double(0),Mv=double(0),eps=double(0),Ff=double(1),phi=double(1),sel=double(1),nfl=integer(0))
##
## N * EPS * exp(-MV - sum(exp(FF) * PHI * SEL))
stopifnot(expr[[1]] == '<-')
lhs <- expr[[2]]
rhs <- expr[[3]]
stopifnot(rhs[[1]] == 'get.surv.sea')
stopifnot(length(rhs) == 8)
Nexpr <- rhs[[2]]
MVexpr <- rhs[[3]]
epsexpr <- rhs[[4]]
Ffexpr <- rhs[[5]]
stopifnot(length(strsplit(deparse(Ffexpr), ':')[[1]]) == 2)
phiexpr <- rhs[[6]]
stopifnot(length(strsplit(deparse(phiexpr), ':')[[1]]) == 2)
selexpr <- rhs[[7]]
stopifnot(length(strsplit(deparse(selexpr), ':')[[1]]) == 2)

exprOut <- substitute(
    LHS <- N * EPS * exp(-MV - sum(exp(FF) * PHI * SEL)) + get.surv.sea(),
    list(LHS = lhs, N = Nexpr, EPS = epsexpr, MV = MVexpr, FF = Ffexpr, PHI = phiexpr, SEL = selexpr)
)
strOut <- paste0(gsub(' \\+ get\\.surv\\.sea\\(\\)$', '    ## get.surv.sea()', gsub(', ', ',', gsub('^ *','', deparse(exprOut)))), collapse = '')
cat('\n\n', strOut, '\n\n')









Rmodel$calculate('rec.obs')

Rmodel$initializeInfo(TRUE)

rolp <- Rmodel$logProb_rec.obs
dim(rolp)
sum(rolp)

rolp[1:47,96,]

Rmodel$logProb_rec.obs[1,96,1]

node <- 'rec.obs[1, 96, 1]'
Rmodel$expandNodeNames(node)

parr <- Rmodel$getParents(node, returnScalarComponents = TRUE)
length(parr)


parr_val_orig <- sapply(parr, function(s) eval(parse(text = paste0('Rmodel$', s))[[1]]))
rm(parr_val)
parr_val
load('~/temp/parr.RData')

parVars <- unique(nimble:::removeIndexing(parr))

v <- 1
parVars[1]
thisParVal_orig <- eval(parse(text = paste0('Rmodel$', parVars[1]))[[1]])
load('~/temp/thisparval.RData')
dim(thisParVal_orig)
dim(thisParVal)
all(thisParVal_orig - thisParVal == 0)
head(which(thisParVal_orig - thisParVal != 0, arr.ind = TRUE))

Rmodel$T.surv.mat[1,3,1,1,1]







###########################################

node <- 'T.surv.mat[1, 3, 1, 1, 1]'
Rmodel$expandNodeNames(node)
parr <- Rmodel$getParents(node)

parVars <- unique(nimble:::removeIndexing(parr))
parVars




length(parr_val)
length(parr_val_orig)

parr_diff <- parr_val - parr_val_orig

names(data)

Rmodel$T.lik.riv[2,4,1,2,2]
i <- 2
jj <- 4
k <- 1
st <- 2
Rmodel$T.lik.riv[i,jj,k,st,2]

nareas

Rmodel$T.surv.imm[i,(jj-1),12,st,1:nareas]
Rmodel$T.surv.mat[i,(jj-1),12,st,1:nareas]  ### VERY DIFFERENT

Rmodel$T.surv.mat[2,3,12,2,1:nareas]  ### VERY DIFFERENT

i <- 2
jj <- 3
k <- 12
st <- 2

Rmodel$T.surv.mat[i,jj,k,st,]
Rmodel$T.surv.mat[2,3,12,2,]

Rmodel$T.lik.mat[2,3,2,2,]   ## VERY DIFFERENT

i <- 2
jj <- 3
k <- 2
st <- 2


Rmodel$T.lik.mat[i,jj,k,st,1:nareas]


Rmodel$T.surv.mat[i,jj,(k-1),st,1:nareas]

get.lik14(
    T.surv.mat[i,jj,(k-1),st,1:nareas],
    0,
    psi.imm[sblock[k],1:nareas,1:nareas,st],
    nareas)


i <- 2
jj <- 2
k <- 
Rmodel$T.surv.mat[2,3,1,2,1:nareas]
Rmodel$T.surv.mat[i,jj,(k-1),st,1:nareas]


i <- 2
jj <- 3
k <- 1

Rmodel$T.surv.mat[i,jj,k,st,]


Rmodel$T.lik.mat[i,jj,k,st,]
Rmodel$T.lik.mat[2,3,1,st,]

get.surv.sea(
    T.lik.mat[i,jj,k,st,a],
    Mseas,
    kE[(relyr[i,st]+jj-1),aseas[k],(relage[i,st]+jj-1),a],
    LF[(relyr[i,st]+jj-1),1:2,st,a],
    phi[(relyr[i,st]+jj-1),aseas[k],1:2,st,a],
    Fsel[(relage[i,st]+jj-1),1:2],
    2
)



Rmodel$T.lik.mat[i,jj,k,st,]


Rmodel$T.surv.mat[i,(jj-1),12,st,]

get.lik14(
    T.surv.mat[i,(jj-1),12,st,1:nareas],
    p.rep.spawn[(relage[i,st]+jj-1)],
    psi.imm[sblock[k],1:nareas,1:nareas,st],
    nareas
)


Rmodel$T.surv.mat[2,2,7,st,]


i <- 2
jj <- 2
k <- 7
st <- 2

Rmodel$T.lik.mat[i,jj,k,st,]





Rmodel$T.lik.mat[i,jj,k,st,]


get.mov.Nov(
    Rmodel$T.surv.spawn[i,jj,(k-1),st,1:(nareas+1)],
    Rmodel$p.surv.spawn[st],
    Rmodel$psi.mat[sblock[k],1:(nareas+1),1:(nareas+1),st],
    (nareas+1),
    nareas
)


N <- Rmodel$T.surv.spawn[i,jj,(k-1),st,1:(nareas+1)]
psurv <-Rmodel$p.surv.spawn[st]
psi <- Rmodel$psi.mat[sblock[k],1:(nareas+1),1:(nareas+1),st]
Nar1 <- nareas + 1
Nar2 <- nareas
    
function (N, psurv, psi, Nar1, Nar2) 

    movsum <- nimNumeric(length = Nar2)
    psim <- nimArray(dim = nimC(Nar1, Nar2))
    for (ii in 1:Nar1) {
        for (jjj in 1:Nar2) {
            psim[ii, jjj] <- psi[ii, jjj]/sum(psi[ii, 1:Nar2])
            movsum[jjj] <- movsum[jjj] + N[ii] * psurv * psim[ii, jjj]
        }
    }
    ##return(movsum)





sum( Rmodel$T.surv.spawn[i,jj,(k-1),st,1:(nareas+1)], Rmodel$zz_psi_mat_Norm[st,1:(nareas+1),z] ) * Rmodel$p.surv.spawn[st]    ## get.mov.Nov()


i <- 2
jj <- 3
k <- 2
st <- 2

z <- 1

sum( Rmodel$T.surv.mat[i,jj,(k-1),st,1:nareas] * Rmodel$psi.imm[sblock[k],1:nareas,z,st] )    ## get.mov()

Rmodel$T.surv.mat[i,jj,(k-1),st,1:nareas]



get.surv.sea(
    T.lik.mat[i,jj,k,st,a],
    Mseas,
    kE[(relyr[i,st]+jj),aseas[k],(relage[i,st]+jj-1),a],
    LF[(relyr[i,st]+jj),1:2,st,a],
    phi[(relyr[i,st]+jj),aseas[k],1:2,st,a],
    Fsel[(relage[i,st]+jj-1),1:2],
    2
)





T.lik.riv[i,jj,k,st,1:(nareas+1)]<-
    get.lik5(
        T.surv.imm[i,(jj-1),12,st,1:nareas],
        T.surv.mat[i,(jj-1),12,st,1:nareas],
        pmat[(relage[i,st]+jj-1),(relyr[i,st]+jj-1),st],
        p.rep.spawn[(relage[i,st]+jj-1)],psi.mat[sblock[k],1:nareas,1:(nareas+1),st],(nareas+1)
    )
                    ##


########################################################

tlik_orig <- Rmodel$T.lik
load('~/temp/tlik.RData')
dim(tlik_orig)
dim(tlik)

max(abs(tlik_orig - tlik))

tlikriv_orig <- Rmodel$T.lik.riv
load('~/temp/tlikriv.RData')
dim(tlikriv_orig)
dim(tlikriv)

abs(tlikriv_orig - tlikriv)[1,1,1,,]
max(abs(tlikriv_orig - tlikriv))

which(abs(tlikriv_orig - tlikriv) > 20, arr.ind = TRUE)

Rmodel$T.lik.riv[2,4,1,2,2]

T.lik.riv

library(scales)
library(MCMCvis)
library(nimble)
library(MASS)
library(writexl)
library(readxl)
library(grid)
library(dplyr)
library(data.table)

Q=data.frame(matrix(ncol = 7,nrow = 30))
Q[1,]=c(1,0,0,0,0,0,0)
Q[2,]=c(0,1,0,0,0,0,0)
Q[3,]=c(0,0,1,0,0,0,0)
Q[4,]=c(0,0,0,1,0,0,0)
Q[5,]=c(0,0,0,0,1,0,0)
Q[6,]=c(0,0,0,0,0,1,0)
Q[7,]=c(0,0,0,0,0,0,1)
Q[8,]=c(1,0,0,0,0,0,0)
Q[9,]=c(0,1,0,0,0,0,0)
Q[10,]=c(0,0,1,0,0,0,0)
Q[11,]=c(0,0,0,1,0,0,0)
Q[12,]=c(0,0,0,0,1,0,0)
Q[13,]=c(0,0,0,0,0,1,0)
Q[14,]=c(0,0,0,0,0,0,1)
Q[15,]=c(1,1,0,0,0,0,0)
Q[16,]=c(0,0,1,1,0,0,0)
Q[17,]=c(0,0,0,1,1,0,0)
Q[18,]=c(0,0,1,0,0,1,1)
Q[19,]=c(0,0,0,1,1,1,0)
Q[20,]=c(0,1,0,0,1,1,0)
Q[21,]=c(0,0,1,1,0,0,1)
Q[22,]=c(1,0,1,0,0,0,1)
Q[23,]=c(0,1,1,1,0,1,0)
Q[24,]=c(1,1,0,0,1,1,0)
Q[25,]=c(1,1,1,1,0,0,0)
Q[26,]=c(0,0,0,1,1,1,1)
Q[27,]=c(1,1,0,0,1,0,1)
Q[28,]=c(1,1,1,0,0,1,1)
Q[29,]=c(1,0,1,0,1,1,1)
Q[30,]=c(1,1,0,1,1,0,1)



y_NA=fread("~/Downloads/missing_response.csv")



y_NA=fread("missing_response.csv")
numNonNAs <- apply(y_NA, 1, function(x) sum(!is.na(x)))

apply(y_NA, 1, function(x) which(!is.na(x)))

nonNAindices <- array(NA, dim(y_NA))
for(i in 1:nrow(y_NA))   nonNAindices[i,1:numNonNAs[i]] <- which(!is.na(y_NA[i,]))

## include numNonNAs and nonNAindices in your constants list:
constants$numNonNAs <- numNonNAs
constants$nonNAindices <- nonNAindices

## include y_NA in your data list:
data$y_NA <- y_NA



#model we fit
code=nimbleCode({
    for (n in 1:1000) {
        ##for (i in 1:vec1[n+1]) {
        ##    add=vec1[n]
        ##    index=vec2[add+i]
        ##    for (k in 1:7) {w[n, index, k] <- pow(attribute[n, k], Q[index, k])}
        ##    logit(prob[1+(i-1)+vec1[n]]) <- beta[index] + delta[index] * prod(w[n,index, 1:7])
        ##    y[1+(i-1)+vec1[n]] ~ dbern(prob[1+(i-1)+vec1[n]])
        ##}}
        for(i in 1:numNonNAs[n]) {
            y[n, nonNAindices[n,i]] ~ dbern(....)
        }
    for (n in 1:1000) {
        for (k in 1:7) {
            logit(att_prob[n, k]) <- gamma[k] * theta[n] - lambda[k]
            attribute[n, k] ~ dbern(att_prob[n, k])
        }}
    for (n in 1:1000) {theta[n]~ dnorm(0,1)}
    for (i in 1:30) {
        item_parameter[i, 1:2] ~ dmnorm(item_mu[1:2], item_den[1:2, 1:2])
        beta[i] <- item_parameter[i, 1]
        delta[i] <- item_parameter[i, 2]
    }
    for (k in 1:7) {
        lambda[k] ~ dnorm(0, 1)
        gamma[k] ~ T(dnorm(0, 1),0,)
    }
    item_mu[1] ~ dnorm(0, 1)
    item_mu[2] ~ T(dnorm(0, 1),0,)
    R[1, 1] <- 1
    R[2, 2] <- 1
    R[1, 2] <- 0
    R[2, 1] <- 0
    item_den[1:2, 1:2] ~ dwish(R[1:2, 1:2], 2)
})

data <- list(y=y,Q=Q,vec1=vec1,vec2=vec2)
inits <- list(item_den = diag(2))
monitors=c("gamma","lambda","beta","delta")
model<-nimbleModel(code,data = data,inits=inits)

any(is.na(data$y))


cModel_DINA <-compileNimble(model)
conf_DINA <- configureMCMC(model, monitors = monitors)
model_DINAMCMC <- buildMCMC(conf_DINA)
cModel_DINAMCMC <- compileNimble(model_DINAMCMC, project = model)
system.time(samples_DINA<-runMCMC(cModel_DINAMCMC, nchains=2,niter = 100, nburnin = 10))




library(nimble)

sumLogPostDens <- nimbleFunction(
    name = 'sumLogPostDens',
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        stochNodes <- setdiff(model$getNodeNames(stochOnly = TRUE), target)
    },
    run = function() {
        model[[target]] <<- model$getLogProb(stochNodes)
    },
    methods = list( reset = function() {} )
)


code <- nimbleCode({
    a ~ dnorm(0, sd = 1)
    b ~ dnorm(0, sd = 2)
    y ~ dnorm(a + b, sd = 3)
    logDens ~ dnorm(0, 1)    ## this distribution does not matter
})
constants <- list()
data <- list(y = 3)
inits <- list(a = 0, b = 1)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$removeSamplers('logDens')
conf$addSampler(target = 'logDens', type = 'sumLogPostDens')   ## remove sampler assigned to 'logDens'
conf$printSamplers()

Rmcmc <- buildMCMC(conf)


Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
##samples <- runMCMC(Rmcmc, 10)
samples <- runMCMC(Cmcmc, 10)


a <- samples[, 'a']
b <- samples[, 'b']
cbind(
    dnorm(a,0,1,log=TRUE) + dnorm(b,0,2,log=TRUE) + dnorm(y,a+b,3,log=TRUE),
    samples[, 'logDens']
)



dnorm(samples[1], 1, log = TRUE)

colnames(samples)
samplesSummary(samples)
library(basicMCMCplots)
samplesPlot(samples)
library(coda)
effectiveSize(samples)

plot(simulated_returns)

a <- 4

if(a > 3) {
    print('yes')
    skip
    print('here')
}

continue
next
break

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(0, 1)
    for(i in 1:10) {
        rr[i] <- r[i]^2
        x[i] ~ dnorm(rr[i] + a, s[i] + b)
    }
})

inits <- list(a=1, b=1, x=rep(0,10))

Rmodel <- nimbleModel(code, inits = inits)

Rmodel$getNodeType(c('a', 'x','r'))




##n<-length(as.numeric(simulated_returns$simulated_returns))

# Use R nimble library
library(nimble)
## BUGS code
# Model and prior


myCode <- nimbleCode({
    # Likelihood
    for (t in 1:n) {   ## changed 1:1 to 1:n
        y[t] ~ dnorm(0, sigma_h[t]^2)
        sigma_h[t] <- exp(h[t])
    }
    ##Priors  
    omega ~ dgamma(omega_star, 0.25)
    alpha_beta[ 1: 3] ~ ddirch(alpha_beta_star[1:3])
    h[1] <- omega/(1-alpha+beta) ### change the [1] after omega should be removed
    for(t in 2:n) {
        h[t] <- sqrt(omega + alpha* pow(y[t-1], 2)  ### change the [1] after omega should be removed
                     + beta * pow(h[t-1], 2))
    }
    alpha <- alpha_beta[1]  ##[1,1]   ## change
    beta <- alpha_beta[2]   ##[1,2]   ## change
    alpha_beta_star[1:3]<-c(alpha_star[1],beta_star[1],phi[1])
})

n <- 1510   ## added this line

# Constants, Data, Initial values for MCMC
myConstants <- list( n = n )
myData      <- list( y = rnorm(n)) ##as.numeric(simulated_returns$simulated_returns))   ## changed the data
myInits     <- list( h = rnorm(n+1, 0, 1), alpha_star = runif(1),beta_star=runif(1),phi=1, omega = 1 , alpha_beta =c(0,0,0),alpha=0,beta=0,omega_star=rgamma(1,2,2))

##dimensions <- list(
##    h<-c(1510),
##    y<-c(1509,1),
##    alpha_star<-c(1),
##    beta_star<-c(1),
##    omega<-c(1),
##    phi<-c(1),
##    alpha_beta<-c(3),
##    alpha<-c(1),
##    beta<-c(1),
##    mu<-c(1)
##)

dimensions <- list(   ## change.  Need to use "=" inside list, rather than "<-"
    h = c(1510),
    y = 1510, ##c(1509,1),   ## change here
    ##alpha_star = c(1), 
    ##beta_star = c(1),   ## don't include dimensions for scalars, all lines below
    ##omega = c(1),
    ##phi = c(1),
    alpha_beta = c(3)
    ##alpha = c(1),
    ##beta = c(1),
    ##mu = c(1)
)

model <- nimbleModel(myCode, data=myData, inits=myInits, dimensions = dimensions)





library(nimble)

code <- nimbleCode({
    omega ~ dgamma(omega_star, 0.25)
    alpha_beta[ 1: 3] ~ ddirch(alpha_beta_star[1:3])
    for(i in 1:3) {
        x[i] <- 1
    }
    yy[1:3] <- c(x[1], x[2], x[3])
})

Rmodel <- nimbleModel(code)

conf <- configureMCMC(Rmodel)


library(nimble)

code <- nimbleCode({
    p[1] <- 0.1
    p[2] <- 0.2
    p[3] <- 0.3
    p[4] <- 0.4
    x[1:4] ~ dmulti(size = 1, prob = p[1:4])
})

Rmodel <- nimbleModel(code, inits = list(x = c(1,0,0,0)))

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('x', 'RW_multinomial')
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 1000000)

apply(samples, 2, mean)
##    x[1]    x[2] 
## 0.17789 0.82211



code <- nimbleCode({...})
constants <- list(...)
data <- list(...)
inits <- list(...)
Rmodel <- nimbleModel(code, constants, data, inits)

llFunction_definition <- nimbleFunction(
    setup = function(model) { },
    run = function() {
        returnType(double())
        return(model$Likelihoods)
    }
)
my_llFunction <- llFunction_definition(Rmodel)

conf <- configureMCMC(Rmodel)
conf$addSampler(type = "RW_llFunction", target = "mu", llFunction = my_llFunction)

Rmcmc <- buildMCMC(conf)
## compile, run MCMC, etc...



(7442 - 1000) / 24


setwd('~/github/courses/ds201/dsb/course-materials/_slides')

library(devtools)
devtools::install_github("hadley/emo")
library(rmarkdown)

render('u1-d01-welcome/u1-d01-welcome.Rmd')

getwd()

install.packages('emo')


library(xaringan)
library(xaringanExtra)
library(emo)


xaringan::inf_mr('u1-d01-welcome/u1-d01-welcome.Rmd', cast_from = '..')

servr::daemon_stop()



library(nimble)


code <- nimbleCode({
    y[1:10] ~ dmnorm(mu[1:10], pr[1:10,1:10])
})

constants <- list(mu = rep(0, 10), pr = diag(10))
data <- list(y = c(rep(0,5), rep(NA,5)))
inits <- list(y = c(rep(NA,5), rep(0,5)))


Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()
Rmodel$isData('y')
Rmodel$isDataFromGraphID
Rmodel$isDataEnv$y

conf <- configureMCMC(Rmodel)





library(nimble)

set.seed(1)
p <- 15    # number of explanatory variables
n <- 100   # number of observations
X <- matrix(rnorm(p*n), nrow = n, ncol = p) # explanatory variables
true_betas <- c(c(0.1, 0.2, 0.3, 0.4, 0.5), rep(0, p-5)) # coefficients
sigma <- 1
y <- rnorm(n, X %*% true_betas, sigma)

code <- nimbleCode({
    beta0 ~ dnorm(0, sd = 100)
    for(k in 2:p)
        beta[k] ~ dnorm(0, sd = 100)
    sigma ~ dunif(0, 100)  # prior for variance components based on Gelman (2006)
    beta[1] <- 1
    for(i in 1:n) {
        y[i] ~ dnorm(beta0 + inprod(beta[1:p], x[i, 1:p]), sd = sigma)
    }
})

X <- sweep(X, 2, colMeans(X))  # center for better MCMC performance

constants <- list(n = n, p = p, x = X)
data <- list(y = y)
inits <- list(beta0 = mean(y), beta = rep(0, p), sigma = 0.5)
Rmodel <- mod <- nimbleModel(code, constants = constants, data = data, inits = inits)
Rmodel$calculate()
model <- compileNimble(mod)

conf <- configureMCMC(mod)
Rmcmc <- buildMCMC(conf)
Cmcmc <- compileNimble(Rmcmc, project = mod)
conf$printSamplers()

conf$removeSampler("beta")

conf
conf$printSamplers()

conf$addSampler("beta[2:15]", "AF_slice") # throws warning
conf$addSampler("beta", "AF_slice") # throws warning
conf$printSamplers()

mcmc <- buildMCMC(conf) # another warning


Cmcmc <- cmcmc <- compileNimble(mcmc, project = model)
res <- runMCMC(cmcmc,  niter=20, nburnin = 1, thin=1, 
               nchains = 1, samplesAsCodaMCMC = TRUE) # runs while all beta nodes lack a sampler






library(nimble)




nfDef <- nimbleFunction(
    setup = function(x) {},
    run = function() {
        print(x)
        if(x == "a") print(1)
        f(1)
    },
    methods = list(
        f = function(arg1 = double()) {
            print('----')
            print(arg1)
            print(x)
            print('----')
            print(arg1, x, 3, 'xx')
            stop('stsop message')
            print('----')
        }
    )
)

Rnf <- nfDef('a')
##Rnf <- nfDef('b')

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()

stochVars <- unique(nimble:::removeIndexing(Rmodel$getNodeNames(stochOnly = TRUE)))
for(v in stochVars)   cat(v, ': ', Rmodel$calculate(v), '\n')




x <- c(1.9, 1.99, 1.999, 2.001, 2.01, 2.1)
(x^3 - 8) / (x^2 - 4)


x <- c(9.9, 9.99, 9.999, 10.001, 10.01, 10.1)
(x-5) / (x-10)^2


xs <- seq(-2, 2, by = 0.01)
ys <- xs ^ 1.5
plot(xs, ys, type = 'l')

(-2) ^ 2.1


github/nimble/nimble/packages/nimble/R/s


library(nimbleHMC)

code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:10) {
        x[i] ~ dnorm(mu, sd = sigma)
    }
})
data <- list(x = c(2, 5, 3, 4, 1, 0, 1, 3, 5, 3))
inits <- function() list(mu = rnorm(1,0,1), sigma = runif(1,0,10))

debug(nimbleHMC)

out <- nimbleHMC(code, data = data, inits = inits,
                 monitors = c("mu", "sigma"), thin = 10,
                 niter = 20000, nburnin = 1000, nchains = 3,
                 summary = TRUE, WAIC = TRUE)

code <- nimbleCode({
    a[1] ~ dnorm(0, 1)
    a[2] ~ dnorm(a[1]+1, 1)
    a[3] ~ dnorm(a[2]+1, 1)
    b ~ dbern(0.5)
    d ~ dnorm(sum(a[1:3]) + b, sd=2)
})

constants <- list()
data <- list(d = 5)
inits <- list(a = rep(0, 3), b = 0)
Rmodel <- nimbleModel(code, constants, data, inits, buildDerivs = TRUE)

Rmcmc <- buildHMC(Rmodel)

conf <- configureHMC(Rmodel)

conf$removeSampler('b')
conf
Rmcmc <- buildHMC(conf)


library(nimble)
Rmodel <- nimbleModel(quote({ x ~ dnorm(0, 1) }))
Cmodel <- compileNimble(Rmodel)


library(nimble)
library(testthat)


code <- nimbleCode({
    x ~ dcat(prob = a[1:3])
    y[1:3] ~ ddirch(alpha = A[x,1:3])
})
constants <- list(a = c(1, 1, 0))
data <- list(y = c(0, 1/2, 1/2))
inits <- list(x = 2, A = array(1/3, c(3,3)))
Rmodel <- nimbleModel(code, constants, data, inits)


Rmodel$calculate()

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('x', 'slice')
conf$printSamplers()

##expect_true(length(conf$getSamplers()) == 1)
##expect_true(conf$getSamplers()[[1]]$name == 'categorical')

Rmcmc <- buildMCMC(conf)




samples <- runMCMC(Rmcmc, 10)


expect_output(samples <- runMCMC(Rmcmc, 10), 'encountered an invalid model density, and sampling results are likely invalid')





code <- nimbleCode({
    x ~ dcat(prob = a[1:3])
    y ~ dnorm(x, -1)
})

constants <- list(a = c(1, 1, 0))
data <- list(y = 0)
inits <- list(x = 2)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('x', 'slice')
conf$printSamplers()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

samples <- runMCMC(Cmcmc, 100)



set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

table(samples)



samples <- runMCMC(Rmcmc, 10)
debug(Rmcmc$samplerFunctions[[1]]$run)
samples


expect_output(samples <- runMCMC(Rmcmc, 10), 'encountered an invalid model density, and sampling results are likely invalid')
expect_output(samples <- runMCMC(Rmcmc, 10), 'encountered an invalid model density, and sampling results are likely invalid')
expect_true(all(samples == 1))


expect_identical(Rmodel$calculate(), Inf)
conf <- configureMCMC(Rmodel)





reGroup1 <- model$getDependencies(paramPairs[[1]], self = FALSE, stochOnly = TRUE)
reGroup2 <- model$getDependencies(paramPairs[[2]], self = FALSE, stochOnly = TRUE)
reGroup1Deps <- model$getDependencies(reGroup1, self = FALSE, stochOnly = TRUE)
reGroup2Deps <- model$getDependencies(reGroup2, self = FALSE, stochOnly = TRUE)
intersect(reGroup1Deps, reGroup2Deps) ## Not empty with lots of z's
## Another example
reGroup5 <- model$getDependencies(paramPairs[[5]], self = FALSE, stochOnly = TRUE)
reGroup9 <- model$getDependencies(paramPairs[[9]], self = FALSE, stochOnly = TRUE)
reGroup5Deps <- model$getDependencies(reGroup5, self = FALSE, stochOnly = TRUE)
reGroup9Deps <- model$getDependencies(reGroup9, self = FALSE, stochOnly = TRUE)
intersect(reGroup5Deps, reGroup9Deps) ## Not empty with lots of y's 


pairInd <- c(1:4, 7,8)
pairInd <- c(5,6, 9,10)
dep <- lapply(paramPairs[pairInd], function(x) model$getDependencies(x, self = FALSE, stochOnly = TRUE))
depdep <- lapply(dep, function(x) model$getDependencies(x, self = FALSE, stochOnly = TRUE))

str(dep)
str(depdep)

g <- expand.grid(pairInd, pairInd)
nr <- nrow(g)
idList <- rep(NA, nr)
for(i in 1:nr) idList[i] <- identical(depdep[[g[i,1]]], depdep[[g[i,2]]])
idList

identical(depdep[[1]], depdep[[2]])

all(depdep[[4]] %in% depdep[[1]])
6554 + 6090





library(nimble)
library(nimbleHMC)

code <- nimbleCode({
    p ~ dnorm(0,1)
    re1 ~ dnorm(p, 1)
    re2 ~ dnorm(re1, 1)
    re3 ~ dnorm(re2, 1)
    y ~ dnorm(re3, 1)
})

Rmodel <- nimbleModel(code, data = list(y = 1), buildDerivs = TRUE)

conf <- configureHMC(Rmodel)
class(conf)

a <- addHMC(conf, 'p')
a
class(a)

Rmcmc <- buildHMC(Rmodel)
class(Rmcmc)

Rmcmc <- buildMCMC(conf)
mvSaved <- Rmcmc$mvSaved

conf$samplerConfs[[1]]$mvSaved

samp <- sampler_HMC(Rmodel, mvSaved, 'p', list())
class(samp)

## this works fine:
setupMargNodes(Rmodel, 'p')
## $randomEffectsNodes
## [1] "re1" "re2" "re3"

code <- nimbleCode({
    p ~ dnorm(0,1)
    re1 ~ dnorm(p, 1)
    re2 ~ dbern(re1)    ## discrete node here
    re3 ~ dnorm(re2, 1)
    y ~ dnorm(re3, 1)
})

Rmodel <- nimbleModel(code, data = list(y = 1))

## this includes 're3' as a RE
setupMargNodes(Rmodel, 'p')
## $randomEffectsNodes
## [1] "re1" "re3"





Rmodel$getNodeNames()
Rmodel$getNodeNames(stochOnly = TRUE)
Rmodel$getNodeNames(determOnly = TRUE)
Rmodel$getNodeNames(latentOnly = TRUE)
Rmodel$getNodeNames(includePredictive = FALSE, stochOnly = TRUE)
Rmodel$getNodeNames(predictiveOnly = TRUE, stochOnly = TRUE)





library(nimble)

mc <- nimbleCode({
    P ~ dnorm(0,1)
    RE1 ~ dnorm(P, 1)
    RE2 ~ dnorm(P, 1)
    Z1 ~ dnorm(RE1, 1)
    Z2 ~ dnorm(RE2, 1)
    Y ~ dnorm(Z1 + Z2, 1)
    predictive <- Z1 + Z2
})

m <- nimbleModel(mc, data = list(Y = 1))

setupMargNodes(m) # Gives RE1 and RE2 as randomEffectsNodes. Wrongly puts Z1 and Z2 in givenNodes

m$getNodeNames(latentOnly = TRUE, stochOnly = TRUE)



library(nimble)

code <- nimbleCode({
    z[1:N] ~ dCRP(alpha, size = N)
    alpha ~ dgamma(1, 1)
    for(i in 1:M) {
        thetatilde[i] ~ dnorm(0, var = 100)
        s2tilde[i] ~ dinvgamma(1, 1)
    }
    for(i in 1:N)
        y[i] ~ dnorm(thetatilde[z[i]], var = s2tilde[z[i]])
})

set.seed(1)
constants <- list(N = 100, M = 50)
data <- list(y = c(rnorm(50, -5, sqrt(3)), rnorm(50, 5, sqrt(4))))
inits <- list(thetatilde = rnorm(constants$M, 0, 10),
              s2tilde = rinvgamma(constants$M, 1, 1),
              z = sample(1:10, size = constants$N, replace = TRUE),
              alpha = 1)

model <- nimbleModel(code, constants, data, inits)

model$calculate()

foldFunction <- function(i){
    foldNodes_i <- paste0('y[i]')  # will return 'y[1]' for i = 1 e.g.
    return(foldNodes_i)
}

dyesMCMCconfiguration <- configureMCMC(model)


debug(buildMCMC)

debug(sampler_CRP)

crossValOutput <- runCrossValidate(MCMCconfiguration = dyesMCMCconfiguration,
                                   k = 2,
                                   foldFunction = foldFunction,
                                   lossFunction = "MSE",
                                   MCMCcontrol = list(niter = 5000,
                                                      nburnin = 500), silent = TRUE)




debug(`%|>%`)
undebug(`%|>%`)



## another example of unknownsAsGiven and getConditionallyIndependentSets

library(nimble)

code <- nimbleCode({
    ## parameters:
    p ~ dnorm(0, 1)
    ## random effect:
    for(i in 1:2) {
        re[i] ~ dnorm(p, 1)
        z[i]  ~ dnorm(re[i], 1)
    }
    ## data:
    y ~ dnorm(z[1] + z[2], 1)
})

Rmodel <- nimbleModel(code, data = list(y=0, z = c(1,1)))

Rmodel$getNodeNames(latentOnly = TRUE)

Rlaplace <- buildLaplace(Rmodel, 'p')   ## uses default split = TRUE
Rlaplace <- buildLaplace(Rmodel, 'p', c('re'))   ## uses default split = TRUE
Rlaplace <- buildLaplace(Rmodel, 'p', c('re', 'z'))   ## uses default split = TRUE

Rlaplace$AGHQuad_nfl[[1]]$randomEffectsNodes
Rlaplace$AGHQuad_nfl[[2]]$randomEffectsNodes
Rlaplace$AGHQuad_nfl[[1]]$calcNodes
Rlaplace$AGHQuad_nfl[[2]]$calcNodes
## "p2"  "re1"


debug(setupMargNodes)
undebug(setupMargNodes)


## example of unknownsAsGiven and getConditionallyIndependentSets

reSets <- MargNodes$randomEffectsSets
## remove any nodes within reSets which are not among scalarRENodes:
reSets <- lapply(reSets, function(set) intersect(set, scalarRENodes))
## remove any elements of reSets which are zero-length:
reSets <- reSets[sapply(reSets,length) > 0]


l <- list(1, numeric(), 3)
l[sapply(l, function(set) length(set)>0)]

l[sapply(l,length) > 0]

                 
library(nimble)

code <- nimbleCode({
    ## parameters:
    p1 ~ dnorm(0, 1)
    p2 ~ dnorm(0, 1)
    ## random effect:
    re1 ~ dnorm(p1, 1)
    ## data:
    y ~ dnorm(re1 + p2, 1)
})

Rmodel <- nimbleModel(code, data = list(y=0))


Rlaplace <- buildLaplace(Rmodel, 'p1', 're1', control = list(split = FALSE))
Rlaplace$AGHQuad_nfl[[1]]$randomEffectsNodes
## "re1"

Rlaplace <- buildLaplace(Rmodel, 'p1', 're1')   ## uses default split = TRUE
Rlaplace$AGHQuad_nfl[[1]]$randomEffectsNodes
## "p2"  "re1"




debug(buildLaplace)
undebug(buildLaplace)
debug(nimble:::buildAGHQuad)
undebug(nimble:::buildAGHQuad)
debug(setupMargNodes)
undebug(setupMargNodes)





code <- nimbleCode({
    theta1 ~ dnorm(0, 1)
    x1 ~ dnorm(theta1, 1)
    theta2 ~ dnorm(0, 1)
    y ~ dnorm(x1 + theta2, 1)
})
model <- nimbleModel(code)
model$getConditionallyIndependentSets(nodes = 'x1', givenNodes = c('theta1', 'y'))


## figuring out how SetupMargNodes works


code <- nimbleCode({
    for(i in 1:4) th[i] ~ dnorm(0, 1)
    for(i in 1:3) thx[i] ~ dnorm(0, 1)
    for(i in 1:4) re[i] ~ dnorm(th[1] + th[2], 1)
    for(i in 5:8) re[i] ~ dnorm(th[3] + th[4], 1)
    z ~ dnorm(thx[1] + sum(re[1:4]), 1)
    y[1] ~ dnorm(z + thx[2] + sum(re[5:8]), 1)
    y[2] ~ dnorm(thx[3] + sum(re[5:8]), 1)
})

model <- nimbleModel(code, data = list(y = 1:2))

confnimble <- configureMCMC(model)

confnimble$getSamplers()
args(confnimble$getSamplers)
args(confnimble$printSamplers)

samplerConfs <- confnimble$getSamplers()
RWbool <- sapply(samplerConfs, `[[`, 'name') == 'RW'
RWtargets <- sapply(samplerConfs[RWbool], `[[`, 'target')
confnimble$replaceSamplers(RWtargets, type = 'slice', expandTarget = TRUE)
                           

typeInd <- unique(unname(unlist(lapply(type, grep, x = lapply(samplerConfs, `[[`, 'name')))))

targetslist <- sapply(confnimble$getSamplers(), function(xx)xx$target)
nameslist <- sapply(confnimble$getSamplers(), function(xx)xx$name)
for(asampler in targetslist[nameslist == 'RW']){
    confnimble$removeSamplers(asampler)
    confnimble$addSampler(target=asampler, type='slice')
}

conf <- configureMCMC(model, onlySlice = TRUE)
conf
pppp

##nodes <- Rmodel$getNodeNames(stochOnly = TRUE)
##l <- lapply(nodes, Rmodel$getDependencies)
##names(l) <- nodes
##l

setupMargNodes(model, 'th[1]')

## input: paramNodes
(paramNodes <- 'th[1]')
(paramNodes <- 'th[2]')
(paramNodes <- 'th[3]')
(paramNodes <- 'th[4]')
(paramNodes <- 'thx[1]')
(paramNodes <- 're[1]')
(paramNodes <- 're[5]')
(paramStochDeps <- model$getDependencies(paramNodes, stochOnly = TRUE, self = FALSE))
(randomEffectsNodes <- intersect(paramStochDeps, model$getNodeNames(latentOnly = TRUE)))
(calcNodes <- model$getDependencies(randomEffectsNodes))
(calcNodesOther <- setdiff(paramStochDeps, calcNodes))
(givenNodes <- setdiff(c(paramNodes, calcNodes), c(randomEffectsNodes, model$getDependencies(randomEffectsNodes, determOnly = TRUE))))
(reSets <- model$getConditionallyIndependentSets(nodes = randomEffectsNodes, givenNodes = givenNodes))

getConditionallyIndependentSets
model$modelDef$maps$nimbleGraph$getConditionallyIndependentSets

## error in linear_predictor example from nimble-demos:

1

library(nimble)

##build(new_devel)
##build(devel_old)

set.seed(1)
p <- 15    # number of explanatory variables
n <- 2   # number of observations
X <- matrix(rnorm(p*n), nrow = n, ncol = p) # explanatory variables
true_betas <- c(c(0.1, 0.2, 0.3, 0.4, 0.5), rep(0, p-5)) # coefficients
y <- rnorm(n, X %*% true_betas, 1)

code <- nimbleCode({
    beta0 ~ dnorm(0, 1)
    beta[1:p] ~ dmnorm(zeros[1:p], omega[1:p, 1:p])
    linpred[1:n] <- (x[1:n, 1:p] %*% beta[1:p])[1:n,1]
    for(i in 1:n) {
        y[i] ~ dnorm(beta0 + linpred[i], 1)
    }
})

constants <- list(n = n, p = p, x = X, zeros = rep(0, p), omega = 0.0001 * diag(p))
data <- list(y = y)
inits <- list(beta = rep(0, p))

Rmodel <- nimbleModel(code, constants = constants, data = data, inits = inits)

##debug(Rmodel$getUnrolledIndicesList)
##debug(nimble:::cc_expandDetermNodesInExpr)

##options(error = recover)

conf <- configureMCMC(Rmodel)





## error with nimbleSMC finding sampler_BASE default methods
## after extension of the system:



library(nimble)
library(nimbleSMC)

## RW sampler from 'nimble' package:
## class generator includes 'before_chain' and 'after_chain' methods,
## which were added automatically:
environment(sampler_RW)$nfRefClass


## RW_PF sampler from 'nimbleSMC' package:
## class generator *does not include* 'before_chain' or 'after_chain' methods,
## which were *not* added:
environment(sampler_RW_PF)$nfRefClass

My 

## helping with scalar / vector case for a user on nimble-users list

library(nimble)

code <- nimbleCode({
    for(i in 1:N) {
        if(N == 1) x_mean[i] <- mu
        if(N  > 1) x_mean[i] <- mu[i]
        x[i] ~ dnorm(x_mean[i], 1)
    }
})

N <- 1
constants <- list(N = N, mu = rep(0,N))
data <- list(x = rep(0,N))
inits <- list()

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()
Rmodel$getNodeNames()

Cmodel <- compileNimble(Rmodel)
Cmodel$calculate()
Cmodel$getNodeNames()



## bug in Laplace with different numbers of conditionally indepedent RE sets

library(nimble)

nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)

code <- nimbleCode({
    for(i in 1:2) {
        param[i] ~ dnorm(0, 1)
        for(j in 1:num_re[i]) {
            re[i,j] ~ dnorm(param[i], 1)
        }
        y[i] ~ dnorm(sum(re[i,1:num_re[i]]), 1)
    }
})

num_re <- c(3,7)   ## different numbers of REs in two conditionally independent sets
constants <- list(num_re = num_re)
data <- list(y = c(0,0))

Rmodel <- nimbleModel(code, constants, data, buildDerivs = TRUE)
Rlaplace <- buildLaplace(Rmodel, 'param', 're', control = list(split = FALSE))

Cmodel <- compileNimble(Rmodel)
Claplace <- compileNimble(Rlaplace, project = Rmodel)

Rlaplace$AGHQuad_nfl
Rlaplace$AGHQuad_nfl[[1]]
Rlaplace$AGHQuad_nfl[[1]]$optimControl$ndeps      ## not set in uncompiled
Rlaplace$AGHQuad_nfl[[1]]$optimControl$parscale   ## not set in uncompiled

Claplace$AGHQuad_nfl
Claplace$AGHQuad_nfl[[2]]
Claplace$AGHQuad_nfl[[1]]$optimControl$ndeps      ## not set in compiled, either
Claplace$AGHQuad_nfl[[1]]$optimControl$parscale   ## not set in compiled, either

Claplace$findMLE(c(0,0))

## Error in Claplace$findMLE(c(0, 0)) : 
##   In compiled optim (aka nimOptim) call: lengths for control parameters parscale and ndeps must equal length(par).





library(nimble)

code <- nimbleCode({
    x[1:3] ~ ddirch(a[1:3])
})

Rmodel <- nimbleModel(code)

Rmodel$a
Rmodel$a <- c(2,2,2)/2
Rmodel$a


Rmodel$x
newX <- c(1,1,0)
Rmodel$x <- newX / sum(newX)
Rmodel$x


Rmodel$calculate('x')

ddirichlet(Rmodel$x, Rmodel$a, log = TRUE)







https://us02web.zoom.us/j/7341328146?pwd=Z3JRd1JjQ0ZTZnR1WkdISXRrUk1KQT09
https://us02web.zoom.us/j/7341328146?pwd=Z3JRd1JjQ0ZTZnR1WkdISXRrUk1KQT09





library(nimble)


prepbirds <- nimbleModel(code = nimIBM, constants = nc,
                         data = nd, inits = ni, calculate = T, check = T)
prepbirds$phi.p[,1] <- 1
prepbirds$initializeInfo()
prepbirds$calculate()
mcmcbirds <- configureMCMC(prepbirds, monitors = pars, print = T, control = list(adaptInterval = adaptInterval, scale=.1))
mcmcbirds$removeSampler(paste0("s[",1:M,", 1:2, 1]"))

for(i in 1:M){
    mcmcbirds$addSampler(target = paste0("s[",i,", 1:2, 1]"),
                         type = 'RW_block', silent=TRUE,
                         control = list(adaptInterval = adaptInterval, propCov=diag(2), scale=SamplerScale, adaptScaleOnly=TRUE))
    for(tt in 2:occ){
        mcmcbirds$addSampler(target = paste0("s[",i,", 1:2, ", tt, "]"),
                             type = 'myRWtrajectoryPotentialSampler',
                             control = list(ind=i, scale=SamplerScale, npix=npix, pxw = pxw,xmin = xmin, ymin = ymin, xmax = xmax, ymax = ymax, gcoords= gcoords, pixMat = pixels))
    }
}

birdsMCMC <- buildMCMC(mcmcbirds)
Cmodel <- compileNimble(prepbirds) #takes a long time (sometimes)
Compbirds <- compileNimble(birdsMCMC, project = prepbirds)
Compbirds$run(niter = 2, nburnin = 0, thin = 1, reset = F)





code <- nimbleCode({
    beta0 ~ dnorm(0, sd = 100)
    beta1 ~ dnorm(0, sd = 100)
    beta2 ~ dnorm(0, sd = 100)
    sigma ~ dinvgamma(1, 1)
})

Rmodel <- nimbleModel(
    code,
    data = list(),
    inits = initial values for the 4 parameters,
    constants = list()
)

llFun <- nimbleFunction(
    setup = function(model,
                     ## all the other constants you need here: covs, sim_covs, knots_dist, the other dist arrays,
                     ## note also that you may not need all of n_uof, n_sim, n_knots for indexing the arrays, 
                     ## since arrays do not need to be "fully indexed" in nimbleFunction code.  I'll give an example below.
                     ) { },
    run = function() {
        ## calculate XB
        ## calculate XB_int
        ## calculate knots_cov
        ## calclate observed_pts_vs_knots_cov
        ## calculate integration_pts_vs_knots_cov
        ## *simulate* the GP gp_on_knots:
        knots_cov_chol <- chol(knots_cov)
        gp_on_knots <- rmnorm_chol(1, knots_mean, knots_cov_chol, prec_param = 0)
        ## the above lines first calculate the Cholesky factorization of the covariance matrix,
        ## since nimble's underlying implementation of the multivaraite normal operate
        ## using that.  The argument prec_param = 0 indicates that the third argument (knots_cov_chol)
        ## is the cholesky factor of the covariance matrix (the 0 means "not of the precision matrix")
        ## notice also how we do *not* need indexing such as [1:n_knots] or [1:n_knots, 1:n_knots]
        ## on all the vectors or arrays; omitting the indexing means "use the entire vector or array"
        ## calculate gp_on_observed_pts, again omitting indexing:
        gp_on_observed_pts <- observed_pts_vs_knots_cov %*% inverse(knots_cov) %*% asCol(gp_on_knots)
        ## calculate gp_on_integration_pts
        ## calculate integ
        ll <- sum(XB + gp_on_observed_pts) - integ
        returnType(double())
        return(ll)
    }
)

RllFun <- llFun(Rmodel, all the other constants needed)

mcmcConf <- configureMCMC(Rmodel, nodes = NULL)  ## monitors will be correct by default

mcmcConf$addSampler(target = c("beta0", "beta1", "beta2", "sigma"), 
                    type = "RW_llFunction_block",
                    control = list(llFunction = RllFun, includesTarget = F))

## etc....





## helping 'Zhihan Yang' via nimble-users
## question on: LGCP on simulated example

set.seed(42)

library(nimble)
library(ggplot2)
library(fields)  # for rdist
library(glue)
library(dplyr)
library(sf)
library(sp)
library(spatstat)
library(reshape2)
#install RandomFieldsUtils if necessary, it has been removed from CRAN
#install.packages(("https://cran.r-project.org/src/contrib/Archive/RandomFieldsUtils/RandomFieldsUtils_1.2.5.tar.gz"), repos=NULL, type="source")
library(RandomFieldsUtils)
#install RandomFields if needed
#install.packages(("https://cran.r-project.org/src/contrib/Archive/RandomFields/RandomFields_3.3.14.tar.gz"), repos=NULL, type="source")
library(RandomFields)

## Generate a simulated dataset:

sampled_process <- rLGCP("exp", function(x, y){0.1 * x - 0.1 * y + 1},
                         var=1, scale=4,
                         win = owin(c(-10, 10), c(-10, 10)))

## plot(sampled_process)

## Iâ€™m using an exponential covariance function:

expcov <- nimbleFunction(run = function(dists = double(2), phi = double(0), sigma = double(0)) {
    returnType(double(2))
    n <- dim(dists)[1]
    m <- dim(dists)[2]
    sigma2 <- sigma*sigma
    result <- sigma2*exp(-dists/phi)
    return(result)
})

## Defining observed points (obs_pts), knots and integration points (int_pts): 

obs_pts <- as.data.frame(sampled_process)
knots_xs <- seq(from = -10, to = 10, length.out = 10)
knots_ys <- seq(from = -10, to = 10, length.out = 10)
knots <- expand.grid(knots_xs, knots_ys)
colnames(knots) <- c("x", "y")
int_xs <- runif(500, min=-10, max=10)
int_ys <- runif(500, min=-10, max=10)
int_pts <- data.frame(cbind(int_xs, int_ys))
colnames(int_pts) <- c("x", "y")
n_uof = nrow(obs_pts)
n_sim = nrow(int_pts)
n_knots <- nrow(knots)

# defining covariate values for regression
# in this case, the x-y coordinates themselves are the covariates
covs <- as.matrix(obs_pts)
sim_covs <- as.matrix(int_pts)  # sim stands for â€œsimulationâ€

# distance matrices
knots_dists <- rdist(knots, knots)
observed_pts_vs_knots_dists <- rdist(obs_pts, knots)
integration_pts_vs_knots_dists <- rdist(int_pts, knots)

## Iâ€™m using knots to reduce the computational cost (following this paper https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3712798/). 
##  
## Basically, instead of directly sampling the value of the GP on the observed data points, I first sample the value of the GP on the knots (which are much fewer than the number of data points) and then interpolate the value of the GP on the observed data points using Gaussian conditioning formulas.

code <- nimbleCode({
    beta0 ~ dnorm(0, sd = 100)
    beta1 ~ dnorm(0, sd = 100)
    beta2 ~ dnorm(0, sd = 100)
    sigma ~ dinvgamma(1, 1)
    XB[1:n_uof] <- beta0 + beta1*covs[,1] + beta2*covs[,2]
    XB_int[1:n_sim] <- beta0 + beta1*sim_covs[,1] + beta2*sim_covs[,2]
    knots_cov[1:n_knots, 1:n_knots] <- expcov(knots_dists[1:n_knots, 1:n_knots], phi=4, sigma=sigma)
    observed_pts_vs_knots_cov[1:n_uof, 1:n_knots] <- expcov(observed_pts_vs_knots_dists[1:n_uof, 1:n_knots], phi=4, sigma=sigma)
    integration_pts_vs_knots_cov[1:n_sim, 1:n_knots] <- expcov(integration_pts_vs_knots_dists[1:n_sim, 1:n_knots], phi=4, sigma=sigma)
    gp_on_knots[1:n_knots] ~ dmnorm(knots_mean[1:n_knots], cov=knots_cov[1:n_knots, 1:n_knots])
    gp_on_observed_pts[1:n_uof] <- observed_pts_vs_knots_cov[1:n_uof, 1:n_knots] %*% inverse(knots_cov[1:n_knots, 1:n_knots]) %*% asCol(gp_on_knots[1:n_knots])
    gp_on_integration_pts[1:n_sim] <- integration_pts_vs_knots_cov[1:n_sim, 1:n_knots] %*% inverse(knots_cov[1:n_knots, 1:n_knots]) %*% asCol(gp_on_knots[1:n_knots])
    integ <- (area / n_sim) * sum(exp(XB_int[1:n_sim] + gp_on_integration_pts[1:n_sim]))
})

## Constructing nimbleModel:

Rmodel <- nimbleModel(
    code, 
    data = list(),
    inits = list(
        beta0 = 0,
        beta1 = 0,
        beta2 = 0,
        sigma = 1, 
        gp_on_knots = numeric(n_knots)
    ),
    constants = list(
        sim_covs = sim_covs, 
        covs = covs, 
        area = 400,  # 20 * 20
        n_uof = n_uof,
        n_sim = n_sim,
        knots_mean = numeric(n_knots),
        knots_dists = knots_dists, 
        observed_pts_vs_knots_dists = observed_pts_vs_knots_dists,
        integration_pts_vs_knots_dists = integration_pts_vs_knots_dists,
        n_knots = n_knots
    )
)

Rmodel$initializeInfo()
Rmodel$initializeInfo(TRUE)

Rmodel$calculate()

## Defining custom log likelihood:
##  
## Iâ€™m using a block sampler for all parameters instead of adding a separate sampler for each parameter - the second approach doesnâ€™t seem to work (for example, the posterior samples for sigma look like they are from the prior).

llFun <- nimbleFunction(
    setup = function(model) { },
    run = function() {
        ll <- sum(model$XB + model$gp_on_observed_pts) - model$integ
        returnType(double())
        return(ll[1])
    }
)

RllFun <- llFun(Rmodel)

mcmcConf <- configureMCMC(Rmodel, nodes = NULL, monitors = c("beta0", "beta1", "beta2", "sigma", "gp_on_knots"))

# this ensures that gp_on_knots is monitored alongside of other nodes, but also makes sure that no default samplers are assigned (we are adding a custom sampler just below)
# adding custom samplers

mcmcConf$addSampler(target = c("beta0", "beta1", "beta2", "sigma", "gp_on_knots"), 
                    type = "RW_llFunction_block",
                    control = list(llFunction = RllFun, includesTarget = F))

# mcmcConf$addSampler(target = "beta1", type = "RW_llFunction",
#                     control = list(llFunction = RllFun, includesTarget = FALSE))
# 
# mcmcConf$addSampler(target = "beta2", type = "RW_llFunction",
#                     control = list(llFunction = RllFun, includesTarget = FALSE))
# 
# mcmcConf$addSampler(target = "sigma", type = "RW_llFunction",
#                     control = list(llFunction = RllFun, includesTarget = FALSE))
# 
# mcmcConf$addSampler(target = "gp_on_knots", type = "RW_llFunction_block",
#                     control = list(llFunction = RllFun, includesTarget = FALSE))

mcmcConf$printSamplers() 

Rmcmc <- buildMCMC(mcmcConf)

comp_model <- compileNimble(Rmodel)
comp_MCMC <- compileNimble(Rmcmc)

mcmcConf$getUnsampledNodes()

comp_MCMC$run(niter = 10000)

samples <- as.matrix(comp_MCMC$mvSamples)


## Hereâ€™s the code Iâ€™ve written for plotting the trace plots:

samples <- as.matrix(comp_MCMC$mvSamples) %>% as.data.frame()
n_samples <- nrow(samples)

# trace plot for beta0, beta1 and beta2

ggplot() + 
    geom_line(aes(x=1:n_samples, y=samples[, 1]), color="red") +   # beta0
    geom_line(aes(x=1:n_samples, y=samples[, 2]), color="green") +   # beta1
    geom_line(aes(x=1:n_samples, y=samples[, 3]), color="blue") +   # beta2
    xlab("Number of samples") + 
    ylab("Value")

# trace plot for the value of GP on a few knots (1st to 10th)

melted_samples <- melt(cbind(1:n_samples, samples[, 4:14]), id.vars="1:n_samples")
colnames(melted_samples)[1] <- "index"

ggplot() +
    geom_line(data=melted_samples, aes(x=index, y=value, color=variable)) +
    xlab("Number of samples") +
    ylab("Value") +
    theme(legend.position="none")

# trace plot for sigma

ggplot() + geom_line(aes(x=1:n_samples, y=samples[, ncol(samples)]))





## trying to debug crossvalidation CV of dyes model


library(nimble)

nimbleOptions('CV_buggy')
nimbleOptions(CV_buggy = TRUE)
nimbleOptions(CV_buggy = FALSE)
nimbleOptions('CV_buggy')

nimbleOptions('debug_CV')
nimbleOptions(debug_CV = TRUE)
nimbleOptions(debug_CV = FALSE)
nimbleOptions('debug_CV')




code <- nimbleCode({
    for (i in 1:BATCHES) {
        for (j in 1:SAMPLES) {
            y[i,j] ~ dnorm(mu[i], tau.within)
        }
        mu[i] ~ dnorm(theta, tau.between)
    }
    theta ~ dnorm(0.0, 1.0E-10)
    tau.within ~ dgamma(0.001, 0.001)
    sigma2.within <- 1/tau.within
    tau.between ~ dgamma(0.001, 0.001)
    sigma2.between <- 1/tau.between
})
data <- list(y = matrix(c(1545, 1540, 1595, 1445, 1595,
                          1520, 1440, 1555, 1550, 1440,
                          1630, 1455, 1440, 1490, 1605,
                          1595, 1515, 1450, 1520, 1560,
                          1510, 1465, 1635, 1480, 1580,
                          1495, 1560, 1545, 1625, 1445),
                        nrow = 6, ncol = 5))
constants <- list(BATCHES = 6, SAMPLES = 5)
inits <- list(theta = 1500, tau.within = 1, tau.between =  1)
foldFun <- function(i){
    foldNodes_i <- paste0('y[', i, ', ]')  # will return 'y[1,]' for i = 1 e.g.
    return(foldNodes_i)
}
RMSElossFunction <- function(simulatedDataValues, actualDataValues){
    dataLength <- length(simulatedDataValues) # simulatedDataValues is a vector
    SSE <- 0
    for(i in 1:dataLength){
        SSE <- SSE + (simulatedDataValues[i] - actualDataValues[i])^2
    }
    MSE <- SSE / dataLength
    RMSE <- sqrt(MSE)
    return(RMSE)
}
Rmodel <- nimbleModel(code, constants, data, inits)


conf <- configureMCMC(Rmodel)


set.seed(0)

crossValOutput <- runCrossValidate(MCMCconfiguration = conf,
                                   k = 6,
                                   foldFunction = foldFun,
                                   lossFunction = RMSElossFunction,
                                   MCMCcontrol = list(niter = 5000, nburnin = 500))



## lapply(1:k, calcCrossVal,
##        MCMCconfiguration,
##        foldFunction,
##        lossFunction,
##        niter,
##        nburnin,
##        returnSamples,
##        nBootReps,
##        FALSE,
##        silent)

calcCrossVal(1, MCMCconfiguration, foldFunction, lossFunction, niter, nburnin, returnSamples, nBootReps, FALSE, silent)


## buggy (old): 63.872534
## new: 97.89058
crossValOutput$CVvalue

## buggy (old): 0.0023289839
## new: 1.042394
crossValOutput$CVstandardError

## buggy (old): c(56.525316, 41.326957, 73.621281, 61.466634, 109.806489, 40.488530)
## new: c(100.56460, 79.60581, 92.88915, 105.16338, 112.89869, 96.22186)
sapply(crossValOutput$foldCVinfo, function(x) x['foldCVvalue'])

## buggy (old): c(0.0052012467, 0.0058034916, 0.0060221136, 0.0059338851, 0.0045952414, 0.0064763733))
## new: c(2.768838, 1.402500, 2.177210, 4.491977, 1.136694, 1.809252))
sapply(crossValOutput$foldCVinfo, function(x) x['foldCVstandardError'])


crossValOutput$foldCVinfo
##
## NEW:
##         foldCVvalue foldCVstandardError 
## 1         100.564595            2.768838 
## 2           79.60581             1.40250 
## 3           92.88915             2.17721 
## 4         105.163379            4.491977 
## 5         112.898689            1.136694 
## 6          96.221865            1.809252 
##
## buggy (old):
##         foldCVvalue foldCVstandardError 
## 1       56.525316158         0.005201247 
## 2       41.326956792         0.005803492 
## 3       73.621280571         0.006022114 
## 4       61.466634332         0.005933885 
## 5      109.806488561         0.004595241 
## 6       40.488529767         0.006476373 
##
##




##
##
##
##


code <- nimbleCode({
    x[1, 1:2] <- nimC(1, 0)
    x[2, 1:3] <- nimC(1, 0, 0)
})

Rmodel <- nimbleModel(code)

Rmodel$x

Rmodel$getNodeNames()
Rmodel$expandNodeNames('x')

code <- nimbleCode({
    for (i in 1:ns){
        truex[i] ~ dcat(p[1:4])
        biopsies[i,] ~ dmulti(error[truex[i], 1:XX[i]], nbiops[i])
    }
    error[1, 1:4] <- c(1, 0, 0, 0)
    error[2, 1:2] ~ ddirch(prior[2, 1:2])
    error[3, 1:3] ~ ddirch(prior[3, 1:3])
    error[4, 1:4] ~ ddirch(prior[4, 1:4])
    p[] ~ ddirch(prior[4,1:4])
})
ns <- 157
p <- c(0.25, 0.25, 0.25, 0.25)
truex <- rep(4, ns)
prior <- structure(c(1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1), .Dim = as.integer(c(4,4)))
error <- matrix(c(1,0,0,0,
                  .5,.5,NA,NA,
                  .1,.1,.8,NA,
                  .1,.1,.1,.7), nrow=4, byrow=TRUE)
inits <- list(p=p, truex=truex, error = error)
biopsies <-structure(c(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 
                       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 
                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
                       3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 
                       1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 1, 1, 
                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 
                       1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 
                       2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 
                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 
                       3, 3, 0, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 
                       1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 
                       3, 3), .Dim = as.integer(c(157, 4)))
nbiops <- c(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
            2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
            2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 
            3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
            3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
            3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
            3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
            3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3)
constants <- list(ns=ns, prior=prior, nbiops = nbiops, XX=c(4,2,3,4))
data <- list(biopsies=biopsies)

Rmodel <- nimbleModel(code, constants, data, inits)





readBUGSmodel(file.path(tempdir(), "biops.bug"))

readLines("/var/folders/5s/5tb6_7ln4q32j4rh8kfc0bh40000gn/T//RtmpGYn4mI/biops.bug")

options(error = recover)
test_mcmc(model = file.path(tempdir(), "biops.bug"), name = 'biops', inits = file.path(tempdir(), "biops-inits.R"), data = system.file('classic-bugs', 'vol2', 'biops','biops-data.R', package = 'nimble'), numItsC = 1000, avoidNestedTest = TRUE)




1

library(nimble)
##build(devel)
source('~/github/nimble/nimble/packages/nimble/tests/testthat/test_utils.R')

library(testthat)




system.in.dir(paste("echo 'var\n
    biopsies[ns,4], #  grades observed in ith session (multinomial)\n
    nbiops[ns],     # total number of biopsies in ith session\n
    truex[ns],       # true state in ith session\n
    error[4,4],     # error matrix in taking biopsies\n
    prior[4,4],     # prior parameters for rows of error[,]\n
    p[4];           # underlying   incidence of true  states\n
model {\n
   for (i in 1:ns){\n
      truex[i]       ~ dcat(p[]);\n
      biopsies[i,]  ~ dmulti(error[truex[i],],nbiops[i]); \n
   }\n
   error[1, 1:4] <- c(1, 0, 0, 0)\n
    error[2, 1:2] ~ ddirch(prior[2, 1:2])\n
    error[3, 1:3] ~ ddirch(prior[3, 1:3])\n
    error[4, 1:4] ~ ddirch(prior[4, 1:4])\n
   p[]       ~ ddirch(prior[4,]);     # prior for p\n
   }' >> ", file.path(tempdir(), "biops.bug")), dir = system.file('classic-bugs','vol2','biops', package = 'nimble'))

system.in.dir(paste("sed 's/true/truex/g' biops-inits.R > ", file.path(tempdir(), "biops-inits.R")), dir = system.file('classic-bugs','vol2','biops', package = 'nimble'))

system.in.dir(paste("echo 'error <- matrix(c(1,0,0,0, .5, .5, 0, 0, 1/3,1/3,1/3,0,1/4,1/4,1/4,1/4), 4,4, byrow=T)'  >> ", file.path(tempdir(), "biops-inits.R")), dir = system.file('classic-bugs','vol2','cervix', package = 'nimble'))

testBUGSmodel('biops', dir = "", model = file.path(tempdir(), "biops.bug"), data = system.file('classic-bugs','vol2','biops','biops-data.R', package = 'nimble'),  inits = file.path(tempdir(), "biops-inits.R"),  useInits = TRUE)

test_mcmc(model = file.path(tempdir(), "biops.bug"), name = 'biops', inits = file.path(tempdir(), "biops-inits.R"), data = system.file('classic-bugs', 'vol2', 'biops','biops-data.R', package = 'nimble'), numItsC = 1000, avoidNestedTest = TRUE)



code <- nimbleCode({
    y[1:3] ~ dmnorm(mu[1:3], pr[1:3,1:3])
    mu[1:2] ~ dmnorm(mu0[1:2],pr[1:2,1:2])
    mu[3] <- 0
})


Rmodel <- nimbleModel(code, data = list(y = rnorm(3)),
                      inits = list(mu = rnorm(3)))

Rmodel <- m

conf <- configureMCMC(Rmodel)

debug(Rmodel$checkConjugacy)
undebug(nimble:::conjugacyRelationshipsObject$checkConjugacy)
undebug(nimble:::conjugacyRelationshipsObject$checkConjugacy_new)


conjugacyObj <-nimble:::conjugacyRelationshipsObject$conjugacys[['dnorm']]
debug(conjugacyObj$checkConjugacyOneDep)

debug(cc_checkLinearity)
undebug(cc_checkLinearity)

Rmodel$checkConjugacy('mu[1:4]')
Rmodel$checkConjugacy('beta[1]')


linearityCheckExprRaw
linearityCheckExpr
targetNode

cc_expandDetermNodesInExpr(model, linearityCheckExprRaw, targetNode = targetNode)
cc_checkLinearity(linearityCheckExpr, targetNode)

Rmodel$getDependencies('beta[1]')

expect_true(conf$samplerConfs[[8]]$name == 'CAR_normal')
pppp
class(Rmcmc$samplerFunctions$contentsList[[1]])

class(Rmcmc$samplerFunctions$contentsList[[8]]$componentSamplerFunctions$contentsList[[1]])
expect_true(class(Rmcmc$samplerFunctions$contentsList[[8]]$componentSamplerFunctions$contentsList[[1]]) == 'CAR_scalar_conjugate')



code <- nimbleCode({
    for(i in 1:n) 
        y[i] ~ dnorm(b0 + inprod(beta[1:p], X[i, 1:p]), 1)
    for(i in 1:p) 
        beta[i] ~ dnorm(0, 1)
    b0 ~ dnorm(0, 1)
})
constants <- list(n = 5, p = 3)
data <- list(y = rnorm(constants$n),
             X = matrix(rnorm(constants$n * constants$p), constants$n))
inits <- list(b0 = 1, beta = rnorm(constants$p))
Rmodel <- nimbleModel(code, data = data, constants = constants)

conf <- configureMCMC(Rmodel)
pppp



code <- nimbleCode({
    S[1:N] ~ dcar_normal(adj[1:L], weights[1:L], numneighbours[1:N], 1)
    for(i in 1:K) {
        beta[i] ~ dnorm(0, 1)
    }
    for(i in 1:N){
        eta[i] <- inprod(beta[1:K]^2, x[1:K])
        mu[i] <- S[i] + eta[i]
        y[i] ~ dnorm(mu[i], 1)
    }
})


nimble:::cc_expandDetermNodesInExpr(Rmodel, quote(eta[1]))

Rmodel$checkConjugacy('beta[1]')



K = 4
# nimble code
require("nimble")
require("evaluate")

# Create indices of main, interaction effects as nimbleList: la_index
library("utils")

la_index_mat = la_index_list = vector("list",K)

v = character(K)

for (k in 1:K){
    la_index_mat[[k]] = combn(1:K,k)
    v[k] = paste("v",as.character(k),sep="")
    la_index_list[[k]] = nimbleType(name = v[k], type = 'double', dim = 2)
}

la_index_list_def <- nimbleList(la_index_list)

txt = character()
for (k in 1:K){
    if (k==1){point = ""}else{point = ", "}
    txt = paste(txt, point, v[k]," = la_index_mat[[",as.character(k),"]]", sep="")
}
txt = paste("la_index = la_index_list_def$new(",txt,")",sep="")





a <- eval(parse(text=txt))

a

la_index


K <- 4
la_index_list <- lapply(1:K, function(k)
    nimbleType(name = paste0('v',k), type = 'double', dim = 2))
la_index_list_def <- nimbleList(la_index_list)
argList <- lapply(1:K, function(k) combn(1:K,k))
names(argList) <- paste0('v', 1:K)
do.call(la_index_list_def$new, argList)







constants$n.occasions_rel[1]   ## 47 47

## st goes from 1:2
##  i goes from 1:47


data$rec.obs

constants$FL

rec.obs[1:47, (7*4*96+(j-1)*96+(jj-1)*12+k), 1:2]

aa <- numeric()
for(jj in 2:8) {
    for(k in 7:12) {
        for(j in 1:2) {
            aa <- c(aa, (7*4*96+(j-1)*96+(jj-1)*12+k))
        }
    }
}



ind <- which(!is.na(data$rec.obs[1:47, aa, 1:2]) & data$rec.obs[1:47, aa, 1:2]>0, arr.ind = TRUE)

dim(ind)
ind_aa <- ind
ind_aa[,2] <- aa[ind[,2]]

ind_aa
check <- apply(ind_aa, 1, function(x) data$rec.obs[x[1],x[2],x[3]])
class(check)
length(check)
any(is.na(check))
all(check > 0)
check

ind_aa


MRConf <- configureMCMC(Rmodel, print=TRUE, useConjugacy = FALSE, nodes = NULL)

MRConf
MRConf$printSamplers()

MRConf$removeSamplers(c("LF", "LFR"))
MRConf$addSampler(target = "LF", type = "RW")
MRConf$addSampler(target = "LFR", type = "RW", scalarComponents = TRUE)

Rmcmc <- buildMCMC(MRConf)

## segfault in nimbleHMC


build(ADoak)
library(nimbleHMC)

code <- nimbleCode({
    for (i in seq(1, n - 1)) {
        for (j in seq(i + 1, n)) {
            p[i, j] <- 1 / (1 + (angular_distance(theta[i], theta[j], b, pi_) * radius_div_mu / kappa[i] / kappa[j])^beta)
            adjacency[i, j] ~ dbern(p[i, j])
        }
    }
    for (i in 1:n) {
        theta[i] ~ dunif(-pi_, pi_)
        kappa[i] ~ dunif(kappa_min, kappa_max)
    }
})

model <- nimbleModel(
    model_code,
    constants = list(n = n, beta = beta, radius_div_mu = radius_div_mu,
                     kappa_min = kappa_min, kappa_max = kappa_max,
                     gamma = gamma, b = b, pi_ = pi),
    data = list(adjacency = adjacency),
    inits = list(theta = theta, kappa = kappa),
    buildDerivs = TRUE
)

hmc <- buildHMC(model)
compiled_model <- compileNimble(model)
compiled_hmc <- compileNimble(hmc, project = model)
samples <- runMCMC(compiled_hmc, nchain = chains, niter = 1000, nburnin = 500)


n <- 10
x <- array(NA, c(n,n))
for (i in seq(1, n - 1)) {
    for (j in seq(i + 1, n)) {
        x[i,j] <- 0
    }
}

x


## testing how lists work inside of constants list


library(nimble)

code <- nimbleCode({
    for(i in 1:N) {
        for(j in cls[[i]]) {
            x[i,j] <- 0
        }
    }
})
N <- 3
cls <- lapply(1:N, function(x) 2^(1:x))
## [[1]]
## [1] 1
##  
## [[2]]
## [1] 1 2
##  
## [[3]]
## [1] 1 2 3
constants <- list(N = N, cls = cls)

Rmodel <- nimbleModel(code, constants)
Rmodel$calculate()
## 0

Rmodel$getNodeNames()
## [1] "x[1, 1]" "x[2, 1]" "x[2, 2]" "x[3, 1]" "x[3, 2]" "x[3, 3]"

Cmodel <- compileNimble(Rmodel)

Cmodel$calculate()
## 0

Cmodel$getNodeNames()

Rmodel$x
Cmodel$x





library(nimble)

code <- nimbleCode({
    for(i in cls) {
        x[i] <- 0
    }
})
(cls <- 1:5)

constants <- list(cls = cls)

Rmodel <- nimbleModel(code, constants)
Rmodel$calculate()
## 0

Rmodel$getNodeNames()
## [1] "x[1, 1]" "x[2, 1]" "x[2, 2]" "x[3, 1]" "x[3, 2]" "x[3, 3]"



rrr
load('~/github/rebecca_whitlock/save.RData')
ls()
> #Nrep=Nseq,obs_rep=r.obs 
> #omega=diag(nstocks)
> 
> source("make_inits_mark_recap_spatial_simple_move.r")
Error in file(filename, "r", encoding = encoding) : 
  cannot open the connection
Calls: source -> file
In addition: Warning message:
In file(filename, "r", encoding = encoding) :
  cannot open file 'make_inits_mark_recap_spatial_simple_move.r': No such file or directory
Execution halted





library(nimbleHMC)
library(mra) 
data(dipper.data) 
y <- dipper.data[,1:7]

code <- nimbleCode({
    phi[1] ~ dunif(0, 1)
    phi[2] ~ dunif(0, 1)
    p ~ dunif(0, 1)
    for(i in 1:N) {
        x[i, first[i]] <- 1
        for(t in (first[i]+1):T) {
            x[i,t] ~ dbern(phi[f[t]] * x[i,t-1])
            y[i,t] ~ dbern(p * x[i,t])
        }
    }
})

Rmodel <- nimbleModel(
    code,
    constants = list(N = nrow(y),
                     T = ncol(y),
                     first = apply(y, 1, which.max),
                     f = c(1,2,2,1,1,1,1)),
    data = list(y = y),
    inits = list(phi = c(0.5, 0.5),
              p = 0.5,
              x = array(1, dim(y))),
    buildDerivs = TRUE
)

conf <- configureMCMC(Rmodel, nodes = NULL)
addHMC(conf)
addHMC(conf, replace = TRUE)
conf

conf <- configureMCMC(Rmodel)
addHMC(conf, nodes = c('phi'))
addHMC(conf, nodes = c('phi', 'p'))
addHMC(conf, nodes = c('p'))
addHMC(conf, nodes = c('p', 'x'))
addHMC(conf, nodes = c('x'))


conf

Rmodel$getNodeNames

conf <- configureMCMC(Rmodel)

## RW sampler (3)
##   - phi[]  (2 elements)
##   - p
## binary sampler (848)
##   - x[]  (848 elements)

conf$replaceSamplers(target = c("phi", "p"), type = "HMC")
conf$printSamplers(byType = TRUE)

## HMC sampler (1)
##   - phi, p 
## binary sampler (848)
##   - x[]  (848 elements)

Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, niter = 20000, nburnin = 10000)

samplesSummary( samples)
samplesSummary(samples, round = 2)
##            Mean    Median    St.Dev. 95%CI_low 95%CI_upp
##p      0.8950736 0.8975031 0.02884604 0.8322880 0.9443211
##phi[1] 0.5770015 0.5776852 0.02767979 0.5221968 0.6302397
##phi[2] 0.4988479 0.4985191 0.05509090 0.3918466 0.6057851


basicMCMCplots::samplesPlot(samples,
                            legend.location = 'topleft',
                            width=4.5, height=2,
                            file='~/github/nimble/nimbleHMC/joss/paper/samplesPlot.pdf')





system("osascript -e 'tell application \"Acrobat\" to quit'")
setwd('~/github/nimble/nimbleHMC/joss/paper')
f <- 'paper.md'
library(rmarkdown)
rmarkdown::render(f, output_format = 'pdf_document')
system('open paper.pdf')





## making nimbleHMC example for JOSS paper

library(nimbleHMC)

code <- nimbleCode({
    phi[1] ~ dunif(0, 1)
    phi[2] ~ dunif(0, 1)
    p ~ dunif(0, 1)
    for(i in 1:N) {
        for(t in (first[i]+1):T) {
            x[i,t] ~ dbern(phi[flood[t]] * x[i,t-1])
            y[i,t] ~ dbern(p * x[i,t])
        }
    }
})

library(mra)
data(dipper.data)
y <- dipper.data[,1:7]

Rmodel <- nimbleModel(
    code,
    constants = list(N = nrow(y),
                     T = ncol(y),
                     first = apply(y, 1, which.max),
                     flood = c(1,2,2,1,1,1,1)),
    data = list(y = y),
    inits = list(phi = c(0.5, 0.5),
              p = 0.5,
              x = array(1, dim(y))),
    buildDerivs = TRUE
)

conf <- configureMCMC(Rmodel)
##RW sampler (3)
##  - phi[]  (2 elements)
##  - p
##binary sampler (848)
##  - x[]  (848 elements)
conf$replaceSamplers(target = c('phi','p'), type = 'HMC')
conf$printSamplers(byType = TRUE)
##HMC sampler (1)
##  - phi, p 
##binary sampler (848)
##  - x[]  (848 elements)

Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, niter = 20000, nburnin = 10000)

samplesSummary(samples)
##            Mean    Median    St.Dev. 95%CI_low 95%CI_upp
##p      0.8950736 0.8975031 0.02884604 0.8322880 0.9443211
##phi[1] 0.5770015 0.5776852 0.02767979 0.5221968 0.6302397
##phi[2] 0.4988479 0.4985191 0.05509090 0.3918466 0.6057851








## I don't know what this was from (below here):


library(nimble)


dMyDist <- nimbleFunction(
   run = function(x = double(), theta = double(), log = integer(default = 1)) {    ## "y" will take the place of "x", and "a+b" will take the place of "theta"
      lp <- dnorm(theta, 0, 1)    ## you could use any distribution here, in your case it looks like dpois()      
      returnType(double())
      if(log) return(lp) else return(exp(lp))
   }
)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(0, 1)
    y ~ dMyDist(theta = a + b)
})

## definitely specify a data value for y, so the model does not try to simulate it
data <- list(y = 0)    ## the data value you provide for y does not matter
inits <- list (a=0, b=0)
constants <- list()


Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)


1
setwd('~/github/nimble/nimbleHMC/joss/paper')
f <- 'paper.md'
library(rmarkdown)
rmarkdown::render(f, output_format = 'pdf_document')
system('open paper.pdf')




library(nimbleHMC)
library(testthat)
        



rm(list=ls())

resultFiles <- list.files('~/github/RWishart_paper/sims/results', '^d', full.names = TRUE)

f2 <- grep('d2_rho0.5[N_]', resultFiles, value = TRUE)

load(f2[1])
load(f2[2])

ls()

essAr2 <- essAr
essAr2
time

essAr
time

    




combinedFile <- '~/github/RWishart_paper/sims/results/allCombined.RData'
load(combinedFile)
library(ggplot2)
library(dplyr)






x <- seq(0.01, 5, by=0.1)

plot(-1,-1, xlim = range(x), ylim = c(0,1))
lines(x, exp(-x), col = 'red')
lines(x, x^(-1/2), col = 'blue')
lines(x, x^(-3), col = 'green')
lines(x, x^(-2.7), col = 'black')
lines(x, x^(1/3), col = 'green')
lines(x, x^(1/9), col = 'blue')


for(n in 1:10) {
    cat(paste0('================= n = ', n, '\n'))
    x <- 1:n
    ind <- x*(x-1)/2 + 1
    print(ind)
}


a <- c(1,2,-1.5)
(A <- array(c(a[1],a[3],a[3],a[2]), c(2,2)))
chol(A)

solve(A)


{
    dd <- FALSE
    if(dd) print('yes')
    else print('no')
}


p
## testing non-conjugate Wishart distribution in jags
## and in stan


library(nimble)
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
nimbleOptions('buildDerivs')

code <- nimbleCode({
    Q[1:3,1:3] ~ dwish(I[1:3,1:3], 5)
    y[1:3] ~ dmnorm(mu0[1:3], Q[1:3,1:3])
    a <- y2  + 1
    y2 ~ dexp(Q[1,1])
})

constants <- list(mu0 = rep(0,3), I = diag(3))
##data <- list(y = c(1,2,3))
data <- list(y = c(1,2,3), y2=1)
inits <- list(Q = diag(3))

Rmodel <- nimbleModel(code, constants, data, inits, buildDerivs = TRUE)
Rmodel$calculate()



conf <- configureMCMC(Rmodel, nodes = NULL)
addHMC(conf)
addHMC(conf, replace = TRUE)
conf


conf <- configureMCMC(Rmodel)
conf$printSamplers()
######## conjuate case:
## [1] conjugate_dwish_dmnorm_identity sampler: Q[1:3, 1:3]
######## non-conjuate case:
## [1] RW_wishart sampler: Q[1:3, 1:3]

conf$addSampler(c('y','a'), 'HMC')
conf$addSampler('Q', 'HMC')

conf$addSampler('y2', 'RW')
conf$addSampler('Q', 'RW_block')
pppp

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

##
mcmc <- Cmcmc
mcmc$samplerFunctions$contentsList[[3]]$setScale(2)
mcmc$samplerFunctions$contentsList[[3]]$setPropCov(3*diag(9))
mcmc$samplerFunctions$contentsList[[3]]$scale
mcmc$samplerFunctions$contentsList[[3]]$propCov
mcmc$samplerFunctions$contentsList[[3]]$chol_propCov
mcmc$samplerFunctions$contentsList[[3]]$chol_propCov_scale
#
mcmc$samplerFunctions$contentsList[[2]]$setScale(23)
mcmc$samplerFunctions$contentsList[[2]]$setPropCov(3*diag(9))
mcmc$samplerFunctions$contentsList[[2]]$reset()
mcmc$samplerFunctions$contentsList[[2]]$scale
mcmc$samplerFunctions$contentsList[[2]]$scaleOriginal


constsAndData <- c(constants, data)
modelfile <- file.path('~/temp/model.txt')
writeLines(paste0('model\n', paste0(deparse(code, width.cutoff=500L), collapse='\n')), con=modelfile)

##install.packages('rjags')
library(rjags)
packageVersion('rjags')

jags_mod <- jags.model(file=modelfile, data=constsAndData, inits=inits, n.chains=1, quiet=FALSE)
list.samplers(jags_mod)
######## conjugate case:
## $`bugs::ConjugateWishart`
## [1] "Q"
######## non-conjugate case:
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 2
##    Unobserved stochastic nodes: 1
##    Total graph size: 19
##  
## Initializing model
## Deleting model
##  
## Error in jags.model(file = modelfile, data = constsAndData, inits = inits,  : 
##   Error in node Q
## Unable to find appropriate sampler


## below here, using non-conjugate wishart in stan:

library(rstan)
packageVersion('rstan')

suppressWarnings(rm('ppp', 'pppp', 'qqq', 'rrr'))



constants <- list(
    mu0 = rep(0,3),
    I = diag(3)
)

data <- list(
    y = c(1, 2, 3),
    y2 = 1
)

inits <- list(
    Q = diag(3)
)

monitors <- c('Q')

stan_code <- '
data {
  vector[3] mu0;
  cov_matrix[3] I;
  vector[3] y;
  real y2;
}

parameters {
  cov_matrix[3] Q;
}

model {
  Q ~ wishart(5, I);
  y ~ multi_normal(mu0, Q);
  y2 ~ exponential(Q[1,1]*Q[2,2] + Q[3,1]^2 + 1);
}

'

stan_mod <- rstan::stan_model(model_code = stan_code)

stan_data <- c(constants, data)

## niter and warmup notes:
## - if you omit 'warmup' argument, then half of niter is used as warmup
## - if you specify 'warmup' and 'niter', then the value for 'niter'
##   also includes the warmup iterations

stan_out <- rstan::sampling(
                       stan_mod,
                       data = stan_data,
                       chains = 1,
                       warmup = 5000,
                       iter = 10000,
                       pars = monitors,
                       init = list(inits)  ## or a list of length 'chains'
                   )

rstan::get_elapsed_time(stan_out)    ## timings

samples_stan <- as.matrix(stan_out)  ## samples

head(samples_stan)

















###################################################
##                     NimblefunctionR           ##
###################################################
etl_codeco_nim <- nimbleFunction(
  run = function(theta = double(1), z.init = integer(1), num.days = integer(), tau = double(0, default = 1)){
    #State transition matrix (S,I_observed,I_hidden,Bacteria,New observed infection)
    nu <- matrix(c(0,0,-1,-1,0,0,
                    0,0,1,0,-1,0,
                    0,0,0,1,0,-1,
                    1,-1,0,0,0,0,
                    0,0,1,0,0,0),ncol = 5)
      valueForSeq <- tau
    time <- seq(0,num.days,valueForSeq)
    ##Monte Carlo Trajectory
    traj <- matrix(nrow = length(time), ncol = length(z.init))
    traj[1,] <- z.init
    for(i in 2:length(time)){
      z <- traj[(i-1),]
      #Event rates
      R <- c((z[2]+z[3])*theta[2],                                            # Bacteria Birth  
             z[4]*0.1,                                                              # Bacteria death
             theta[1]  * (z[4] / (z[4] + 10**6)) * z[1] * theta[4],         # Observed Infection
             theta[1]  * (z[4] / (z[4] + 10**6)) * z[1] * (1-theta[4]),     # Hidden Infection
             theta[3]  *  z[2],                                                  # Observed Infective recovery
             theta[3]  *  z[3])                                                  # Hidden Infective recovery
      lambda <- tau * R
      k <- rpois(length(lambda),lambda)
      knu <- k*nu
      changes <- numeric(length = length(z.init))
      for(j in 1:length(changes)){
        changes[j] <- sum(knu[,j])
      }
      traj[i,] <- traj[(i-1),] +  changes
      traj[i,][traj[i,] < 0] <- 0
    }
      returnType(double(1))
    return(traj[-1,])
  })

###################################################
##                  SET UP                       ##
###################################################
##Parameter values
theta <- c(
  a  = 0.015,
  e  = 100,
  r  = 0.05,
  p = 0.3
)
num.days <- 500
tau <- 1
##Initial conditions (Susceptible, Infected, Bacteria, New Infections)
N        = 10000
Io0       = 3
Ih0       = 7
B0       = 1000
S0       = N - Io0 - Ih0
z.init   = c(S = S0, Io = Io0, Ih = Ih0, B = B0, NI = 0)

###################################################
##                        RUN                    ##
###################################################

R_test <- etl_codeco_nim(theta,z.init,num.days) #Runs fine
C_etl <- compileNimble(etl_codeco_nim, showCompilerOutput = TRUE) #Error
#Error in sizeSeq(<environment>, new("symbolTable", .xData = <environment>),  :
#object 'thisSizeExpr' not found






library(nimble)
##install.packages('nimbleSMC')
library(nimbleSMC)

timeModelCode <- nimbleCode({
    x[1] ~ dnorm(mu_0, 1)
    y[1] ~ dnorm(x[1], 1)
    for(i in 2:t){
        x[i] ~ dnorm(x[i-1] * a + b, 1)
        y[i] ~ dnorm(x[i] * c, 1)
    }
    a ~ dunif(0, 1)
    b ~ dnorm(0, 1)
    c ~ dnorm(1,1)
    mu_0 ~ dnorm(0, 1)
})

# simulate some data
set.seed(0)
t <- 25; mu_0 <- 1
x <- rnorm(1 ,mu_0, 1)
y <- rnorm(1, x, 1)
a <- 0.5; b <- 1; c <- 1
for(i in 2:t){
    x[i] <- rnorm(1, x[i-1] * a + b, 1)
    y[i] <- rnorm(1, x[i] * c, 1)
}

# build the model
inits <- list(a = 0.5, b = 1, c = 1, mu_0 = 1, x=y)
Rmodel <- rTimeModel <- nimbleModel(timeModelCode, constants = list(t = t),
                                    data <- list(y = y), inits = inits)

Rmodel$calculate()
stochVars <- unique(nimble:::removeIndexing(Rmodel$getNodeNames(stochOnly = TRUE)))
for(v in stochVars)   cat(v, ': ', Rmodel$calculate(v), '\n')

##cTimeModel <- compileNimble(rTimeModel)
timeConf <- configureMCMC(rTimeModel, nodes = NULL)

timeConf$removeSamplers(c("a", "b", "c", "mu_0"))
timeConf

timeConf$addSampler(target = c("a", "b", "c", "mu_0"), type = "RW_PF_block",
                    control = list(propCov= diag(4), adaptScaleOnly = FALSE,
                                   latents = "x[2:24]", pfOptimizeNparticles = TRUE))


timeConf$addSampler(target = c("a", "b", "c"), type = "RW_PF_block",
                    control = list(propCov= diag(4), adaptScaleOnly = FALSE,
                                   latents = "x[1:4]", pfOptimizeNparticles = TRUE))

timeConf$printSamplers()

timeConf$getUnsampledNodes()


timeConfMCMC <- buildMCMC(timeConf)
compiled_model <- compileNimble(rTimeModel)
compiled_model_MCMC <- compileNimble(timeConfMCMC, project = rTimeModel)

set.seed(0)
resultsLinear <- runMCMC(compiled_model_MCMC,
                         niter = 1000,
                         nburnin = 0)

head(resultsLinear, 1000)

library(basicMCMCplots)
samplesPlot(resultsLinear[1:1000,])





createCodeAndConstants <- function(N, listOfBlockIndexes=list(), rhoVector=numeric(), expDecay=FALSE, gammaScalars=FALSE) {
    code <- quote({})
    constants <- list()
    if(length(listOfBlockIndexes) != length(rhoVector)) stop()
    for(i in seq_along(listOfBlockIndexes)) {
        blockIndexes <- listOfBlockIndexes[[i]]
        rho <- rhoVector[i]
        numNodes <- as.numeric(length(blockIndexes))
        if(numNodes == 1) {
            code[[length(code)+1]] <-
                if(gammaScalars) substitute(x[IND] ~ dgamma(1.1, 1.1), list(IND=as.numeric(blockIndexes)))
                else             substitute(x[IND] ~ dnorm(0, 1),      list(IND=as.numeric(blockIndexes)))
        } else {
            muText <- paste0('mu', i)
            sigmaText <- paste0('Sigma', i)
            indMin <- as.numeric(min(blockIndexes))
            indMax <- as.numeric(max(blockIndexes))
            code[[length(code)+1]] <- substitute(x[MIN:MAX] ~ dmnorm(MU[1:NUM], cov = SIGMA[1:NUM,1:NUM]), list(MIN=indMin, MAX=indMax, NUM=numNodes, MU=as.name(muText), SIGMA=as.name(sigmaText)))
            constants[[muText]] <- rep(0, numNodes)
            constants[[sigmaText]] <- createCov(N=numNodes, rho=rho, expDecay=expDecay)
        }
    }
    allInd <- 1:N
    leftoverInd <- setdiff(allInd, unlist(listOfBlockIndexes))
    for(ind in leftoverInd) {
        code[[length(code)+1]] <-
            if(gammaScalars) substitute(x[IND] ~ dgamma(1.1, 1.1), list(IND=as.numeric(ind)))
            else             substitute(x[IND] ~ dnorm(0, 1),      list(IND=as.numeric(ind)))
    }
    codeAndConstantsList <- list(code=code, constants=constants)
    return(codeAndConstantsList)
}

createCov <- function(N, indList=list(1:N), rho=0.8, indList2=list(), rho2=0.3, indList3=list(), rho3=0.5, expDecay=FALSE) {
    Sigma <- diag(N)
    for(gp in indList)  { for(i1 in gp) for(i2 in gp) Sigma[i1,i2] <- Sigma[i2,i1] <- if(expDecay) rho^ abs(i1-i2) else rho  }
    for(gp in indList2) { for(i1 in gp) for(i2 in gp) Sigma[i1,i2] <- Sigma[i2,i1] <- if(expDecay) rho2^abs(i1-i2) else rho2 }
    for(gp in indList3) { for(i1 in gp) for(i2 in gp) Sigma[i1,i2] <- Sigma[i2,i1] <- if(expDecay) rho2^abs(i1-i2) else rho3 }
    diag(Sigma) <- 1
    Sigma
}


createCodeAndConstants(10, listOfBlockIndexes = list(1:5, 4:6), rhoVector = c(3,5))



n <- 5; p <- 2

data <- matrix(rbinom(n * p, 1, .5), n, p)

data




library(gdistance)
costDistance
showMethods(costDistance)
gdistance::costDistance

?standardGeneric

?GenericFunctions


isGeneric('costDistance')
isGroup('costDistance')
findFunction('costDistance')

findMethods('costDistance')

gdistance:::.cd
igraph::shortest.paths


showMethods(costDistance)

showMethods(transition)
findMethods(transition)

gdistance:::.TfromR

dumpMethod('costDistance', c("TransitionLayer", "Coords", "Coords"), file = '~/temp/sig')

findFunction('costDistance')
findMethods('geoCorrection')


library(nimble)

code <- nimbleCode({
    for(j in 1:p){
        w[1:n, j] ~ dmnorm(zeroes[1:n], cov = cov[1:n, 1:n])
    }
    for(i in 1:n){
        y[i, 1:p] ~ dmnorm(w[i, 1:p], cov = Omega[1:p, 1:p])
    }
})
##
# constants
cov <- structure(c(1, 0.291439908378029, 0.944925896806776, 0.291439908378029,
                   1, 0.278127103103443, 0.944925896806776, 0.278127103103443, 1
                   ), dim = c(3L, 3L))
n <- 3; p <- 2
##
constants <- list(Omega = diag(1,p),
                  zeroes = rep(0, n),
                  p = p,
                  n = n,
                  cov = cov)
##
jsdmModel <- nimbleModel(code, constants = constants)
jsdmModel$setData(list(y = matrix(rnorm(n * p), n, p)))
##
jsdmModel$calculate()
##
##
Rmodel <- jsdmModel
Rmodel$calculate()

undebug(nimble:::conjugacyRelationshipsObject$checkConjugacy_new)
debug(nimble:::conjugacyRelationshipsObject$checkConjugacy_new)

undebug(nimble:::cc_expandDetermNodesInExpr)
debug(nimble:::cc_expandDetermNodesInExpr)

undebug(nimble:::cc_checkLinearity)
debug(nimble:::cc_checkLinearity)


cc_expandDetermNodesInExpr(model, linearityCheckExprRaw, targetNode = targetNode)

conf <- configureMCMC(Rmodel)





##stochVars <- unique(nimble:::removeIndexing(Rmodel$getNodeNames(stochOnly = TRUE)))
##for(v in stochVars)   cat(v, ': ', Rmodel$calculate(v), '\n')
##
##jsdmcompiled <- compileNimble(jsdmModel, resetFunctions = T)
jsdmConf <- configureMCMC(jsdmModel, print = TRUE, enableWAIC = T, useConjugacy = FALSE)

jsdmConf <- configureMCMC(jsdmModel, print = TRUE, enableWAIC = T)

jsdmConf$removeSampler('w')
jsdmConf$addSampler('w', default = TRUE, useConjugacy = FALSE)

jsdmConf




jsdmConf$printSamplers()

jsdmMCMC <- buildMCMC(jsdmConf)
CDNAMCMC <- compileNimble(jsdmMCMC)
MCMCsamples <- runMCMC(CDNAMCMC, niter = 10000, WAIC = T)





## Binomial pmf: dbinom()


## Cumulative Distribution Function: pbinom()


## x passengers show up:
x <- 105:95

## probability of x passengers showing up:

probs <- dbinom(x, size=105, prob=0.97)
profits <- 105*300 - pmax(x-100,0)*800

for(p in probs) {cat('\n'); cat(p)}; cat('\n\n')

plot(x, probs*100, type = 'b', ylab = 'Probability (%)', xlab='Passengers', xlim = c(95,107), xaxt = 'n')
axis(1, at = 95:105, las=2, cex = 0.5)
text(x+0.7, probs*100, labels = profits, cex = 0.6)


## profits from ticket sales:


## total profits:




## plot passengers, probability, and profits

plot(x, probs, type = 'b', xlim = c(x[1]-1, 110))
text(x+1, probs, labels = profits, cex = 0.6)


## is this the plot we want?
## we want to study the *distribution of profits*

x <- 100:105
x

profits <- ..

probs <- ..

probs[1] <- ..

sum(probs)   ## double-check

probs
profits
plot(profits, probs, type = 'b')


## mean profit:





profits


## expected earnings?

sum(probs*profits)




##
y <- c(5, 6, 6.4)
xmin <- 1
xs <- seq(xmin, 8, 0.01)

plot(-100, -100, xlim = c(xmin, 8), ylim = c(0, 1), cex.axis = 1)
ys <- dnorm(xs, 5, 1)
lines(xs, ys, col = 'red', lwd = 2)
ys <- sapply(xs, function(x) prod(dnorm(y, x,1))) * 20
lines(xs, ys, col = 'black', lwd = 2)
ys <- dnorm(xs, 5.5, 0.4)
##lines(xs, ys, col = 'blue', lwd=2)
set.seed(0)
ns <- 10000
ysamp <- rnorm(ns, 5.5, .4)
hist(ysamp, add = TRUE, freq = FALSE, breaks = ifelse(ns>9999, 100, 20), col = 'lightblue', border = 'grey')
##lines(xs, ys, col = 'blue', lwd=1)

##lines(xs, ysamp, col = 'blue', lwd=1)
ts.plot(ysamp)


39 + 41 + 42 + 40 + 17 + 42 + 40 + 29 + 39 + 17 + 6 + 11 + 20 + 38 + 39 + 40 + 19 + 39 + 29 + 30 + 40 + 25 + 11 + 2 + 3


## calculate logProbs in the run code

## This works         

# logProbsVec[1] <<- getLogProb(model = model, nodes = depsList[[1]]$dependencies)             

# logProbsVec[2] <<- getLogProb(model = model, nodes = depsList[[2]]$dependencies)



## This does not

for(j in seq_along(depsNimbleFunctionList)) {
    logProbsVec[j] <<- depsNimbleFunctionList[[j]]$getLogProbMethod()
}



library(nimble)

parameterTransform
## basic example of using Rcpp to create a cpp function

library(Rcpp)

Rcpp::cppFunction(
          'int fibonacci(const int x) {
              if (x == 0) return(0);
              if (x == 1) return(1);
              return (fibonacci(x - 1)) + fibonacci(x - 2);
          }')

fibonacci

fibonacci(7)





## helping alireza beheshty and Hossein Baghishani
## parallelize their code.


library(parallel)

this_cluster <- makeCluster(4)
set.seed(1)






myFunction <- function(coords, D, Beta, Gama , data) {

    k1 <- knn2nb(knearneigh(coords))
    critical.threshold <- max(unlist(nbdists(k1,coords)))
    nb.dist.band <- dnearneigh(coords, 0, critical.threshold)
    distances <- nbdists(nb.dist.band,coords)
    invd1 <- lapply(distances, function(x) (1/x))
    invd1a <- lapply(distances, function(x) (1/(x/100)))
    invd.weights <- nb2listw(nb.dist.band,glist = invd1a,style = "B")
    W.mat <- listw2mat(invd.weights)
    W.mat <- W.mat + 0.01 * diag(rep(1, 100))
    W.mat <- t(apply(W.mat ,1, function(x) x/sum(x)))

    e <- eigen(W.mat)
    e.values<- e$values
    rho.max <- 1 / max(e.values)
    rho.min <- 1 / min(e.values)
    rho <- 0.5
    data$group <- c(rep(1,D[1]),rep(2,D[2]) )

    data$COV1 <- runif(N, -3, 3)
    XX <-as.matrix(data$COV1)

    coef_cov <- matrix(c(rep(B1[1],D[1]),rep(B1[2],D[2])), ncol = 1)
    coef_gam <- matrix(c(rep(G1[1],D[1]),rep(G1[2],D[2])), ncol = 1)

    beta.star <- matrix(c(coef_cov,coef_gam), ncol = 2)
    beta.star <- t(beta.star)

    WX<- (W.mat %*% XX)

    X.star <- matrix(c(XX,WX), ncol = 2)
    x.star_b.star <- matrix(0, N, 1 )
    for(i in 1:N){
        x.star_b.star[i,1]<-X.star[i,] %*% beta.star[,i]
    }

    lambda <- rho
    A <- Diagonal(N) - rho * W.mat
    X.Beta <- XX * coef_cov
    X.Beta.l <- A %*% X.Beta
    I.x.star_b.star <- A %*%x.star_b.star
    #set.seed(2)
    sd_err <- 1
    err <- as.matrix(rnorm(N, sd = sd_err))

    x.star_b.star_err <- x.star_b.star + err
    data$PRICE_SDM <- as.numeric(solve(A, x.star_b.star_err))

    data$coef_cov <- as.vector(coef_cov)
    data$coef_gam <- as.vector(coef_gam)
    data_new <- data

}
#####################################################
#####################################################
#### Determination of beta, gamma coefficients,######
#### number of clusters and geographic coordinates###
library(spdep)
library(spatialreg)
library(nimble)
library(flexclust) # randIndex
library(clusterCrit) #extCriteria(part1, part2, crit)

#Load data
data("baltimore")
data <- baltimore

set.seed(1)
d <- data[sample(nrow(data), 100), 16:17]

coords <- cbind(d$X,d$Y)

k1 <- knn2nb(knearneigh(coords))
critical.threshold <- max(unlist(nbdists(k1,coords)))
nb.dist.band <- dnearneigh(coords, 0, critical.threshold)
distances <- nbdists(nb.dist.band,coords)
invd1 <- lapply(distances, function(x) (1/x))
invd1a <- lapply(distances, function(x) (1/(x/100)))
invd.weights <- nb2listw(nb.dist.band,glist = invd1a,style = "B")
W.mat <- listw2mat(invd.weights)
W.mat <- W.mat + 0.01 * diag(rep(1, 100))
W.mat <- t(apply(W.mat ,1, function(x) x/sum(x)))

e <- eigen(W.mat)
e.values<- e$values
rho.max <- 1 / max(e.values)
rho.min <- 1 / min(e.values)
rho <- round(mean(c(rho.min, rho.max)),2)
D <- c(55,45)
B1 <- c(-1 , 5)
G1 <- c(1 ,3)
N <- S <- 100
##############################################################
##############################################################
### Running mcmc using the nimble package (100 iterations) ###
#########set.seed(1361)

dati <- myFunction(coords, D=D, Beta=B1,Gama =G1,data=d)
X.mati = matrix(c(dati$COV1), nrow = N)
WXi <- W.mat %*% X.mati
X.stari = matrix(c(X.mati,WXi) , nrow = N )

data_SDMi <- list(yi = dati$PRICE_SDM,
                  Dist = W.mat, X.stari = X.stari)



run_MCMC_allcode <- function(seed, dataArg) {
    library(nimble)
    ##
    code_SDMi <- nimbleCode({
        rhoi ~ dunif(-2.11 , 1)
        I.rhoW[1:S, 1:S] <- diag(S) - (rhoi * Dist[1:S, 1:S])
        inv[1:S, 1:S] <- inverse(I.rhoW[1:S, 1:S])
        inv.X.stari[1:S, 1:2] <- inv[1:S, 1:S] %*% X.stari[1:S, 1:2]
        yi[1:S] ~ dmnorm(mu_yi[1:S], prec = tau_y_SDMi[1:S, 1:S])
        for (j in 1:S) {
            mu_yi[j] <- bi[j, 1] * inv.X.stari[j,1] + bi[j, 2] * inv.X.stari[j,2]
            bi[j, 1:2] <- bmi[latenti[j], 1:2]
            latenti[j] ~ dcat(zlatenti[1:M])
        }
        tau_y_SDMi[1:S, 1:S] <- tau_yi * inv[1:S, 1:S]%*%t(inv[1:S, 1:S])
        for (k in 1:M) {
            for(j in 1:2) {
                bmi[k, j] ~ dnorm(mu_bmi[j], tau = tau_bmi)
            }
        }
        tau_bmi ~ dgamma(1, 1)
        for (j in 1:2) {
            mu_bmi[j] ~ dnorm(0, 1)
        }
        zlatenti[1:M] <- stick_breaking(vlatenti[1:(M - 1)])
        for (j in 1:(M - 1)) {
            vlatenti[j] ~ dbeta(1, alphai)
        }
        alphai ~ dgamma(1, 1)
        tau_yi ~ dgamma(1, 1)
    })
    ##
    constants <- list(S = 100, M = 50)
    set.seed(seed)   ## different initial values for each chain
    inits_SDMi <- list(tau_yi = 1, latenti = rep(1, constants$S), alphai = 2,
                       bmi = array(0, c(constants$M, 2)), tau_bmi = 1,
                       mu_bmi = rnorm(2), rhoi = -0.56,
                       vlatenti = rbeta(constants$M - 1, 1, 1))
    Rmodel_SDMi <- nimbleModel(code_SDMi, constants, dataArg, inits_SDMi)
    Rmodel_SDMi$calculate()
    monitorsi = c("bmi", "bi", "rhoi", "alphai",
                  "latenti", "tau_yi", "tau_bmi",
                  "mu_yi", "mu_bmi" )
    conf_SDMi <- configureMCMC(Rmodel_SDMi, monitors = monitorsi)
    Rmcmc_SDMi <- buildMCMC(conf_SDMi)
    Cmodel_SDMi <- compileNimble(Rmodel_SDMi)
    Cmcmc_SDMi <- compileNimble(Rmcmc_SDMi, project = Rmodel_SDMi)
    #####set.seed(1357)
    samples_SDM <- runMCMC(Cmcmc_SDMi, niter = 30000, nburnin = 15000,
                           thin = 5, setSeed = seed)
    return(samples_SDM)
}

all_output <- parLapply(cl = this_cluster, X = 1:100, 
                        fun = run_MCMC_allcode, 
                        dataArg = data_SDMi)


stopCluster(this_cluster)



str(all_output)

i <- 20
cbind(all_output[[1]][, i], all_output[[2]][, i])








library(nimble)

ndata <- 10
data <- list( mydata=matrix(rnorm(ndata*2), nrow=ndata, ncol=2) )
constants <- list(ndata=ndata)

initsFun <-function(){list(
                          probvector = rep(1/3,3), # prior probabilities for index variable
                          scalevector = c(0.5, 1, 2), # possible scale values
                          scaleindex = c(1, 1),
                          variance = c(1, 1)
                      )}



examplecode <- nimbleCode({
    ##
    for(variate in 1:2){
        scaleindex[variate] ~ dcat(prob=probvector[1:3])
        variance[variate] ~ dinvgamma(shape=1, scale=scalevector[scaleindex[variate]])
    }
    ##
    for(i in 1:ndata){
        for(variate in 1:2){
            mydata[i,variate] ~ dnorm(mean=0, var=variance[variate])
        }
    }
})

dimensions <- list(
    scaleindex = 2,
    variance = 2
)

set.seed(0)
inits <- initsFun()
inits <- list(scalevector = c(0.5, 1, 2) )

examplemodel <- nimbleModel(code=examplecode, name='test',
                            constants=constants,
                            inits=inits,
                            data=data,
                            dimensions=dimensions)


examplemodel$probvector <- rep(NA, 3)
examplemodel$variance <- c(-1, 1)
examplemodel$variance
examplemodel$variate

examplemodel$calculate('mydata')

examplemodel$initializeInfo()
examplemodel$initializeInfo(TRUE)

examplemodel$mydata
examplemodel$variance
examplemodel$scaleindex
examplemodel$
examplemodel$
examplemodel$



library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a^2 + 1, 1)
    y ~ dexp(a^2 + 10)
    y2 ~ dexp(b^2 + 10)
})
constants <- list()
data <- list(y = 5, y2 = 10)
inits <- list(a = 0, b=0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmcmc <- buildMCMC(Rmodel)
compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

nimble:::clearCompiled(Cmodel)
nimble:::clearCompiled(Cmcmc)



Rmodel <- nimbleModel(code, constants, data, inits)

samples <- nimbleMCMC(Rmodel, niter=100)
samples <- nimbleMCMC(Rmodel, niter=100, monitors = 'b')
##samples <- nimbleMCMC(Rmodel, niter=100, monitors = c('a','b'))

Rmcmc <- buildMCMC(Rmodel)
Rmcmc <- buildMCMC(Rmodel, monitors = 'b')
Rmcmc <- buildMCMC(Rmodel, monitors = c('a','b'))

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
Cmcmc$run(niter = 100)
samples <- as.matrix(Cmcmc$mvSamples)



dim(samples)
head(samples)
samplesSummary(samples)
samples


Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf <- configureMCMC(Rmodel, thin = 0.5)
conf <- configureMCMC(Rmodel, thin = -3)
conf <- configureMCMC(Rmodel, thin = 1.5)
conf <- configureMCMC(Rmodel, thin2 = -3)
conf <- configureMCMC(Rmodel, thin2 = 0.5)
conf <- configureMCMC(Rmodel, thin2 = 90)



Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

nimble:::clearCompiled
getNimbleProject(Cmcmc)
getNimbleProject(Cmcmc)$clearCompiled()
nimble:::clearCompiled(Cmodel)

mcmc <- Rmcmc
mcmc <- Cmcmc

samp <- runMCMC(mcmc, niter = 10, thin = -4)
samp <- runMCMC(mcmc, niter = 10, thin = -0)
samp <- runMCMC(mcmc, niter = 10, thin = 0.5)
samp <- runMCMC(mcmc, niter = 10, thin = 1.4)
samp <- runMCMC(mcmc, niter = 10, thin = 95.3)
samp <- runMCMC(mcmc, niter = 10, thin = 4)

samp <- runMCMC(mcmc, niter = 10, thin2 = -4)
samp <- runMCMC(mcmc, niter = 10, thin2 = -0)
samp <- runMCMC(mcmc, niter = 10, thin2 = 0.5)
samp <- runMCMC(mcmc, niter = 10, thin2 = 1.4)
samp <- runMCMC(mcmc, niter = 10, thin2 = 95.3)
samp <- runMCMC(mcmc, niter = 10, thin2 = 4)


mcmc <- Rmcmc
mcmc <- Cmcmc


mcmc$run(10, thin = 10.2)
mcmc$run(10, thin = 2.2)
mcmc$run(10, thin = 0.4)
mcmc$run(10, thin = 0)
mcmc$run(10, thin = -2)

mcmc$run(10, thin2 = 10.2)
mcmc$run(10, thin2 = 2.2)
mcmc$run(10, thin2 = 0.4)
mcmc$run(10, thin2 = 0)
mcmc$run(10, thin2 = -2)


niter <- 1000
thin <- 10.5



code <- nimbleCode({
    y ~ dnorm(mu, sd = sigma)
    mu ~ dnorm(0,1)
    sigma ~ dunif(0,1)
})

m <- nimbleModel(code, data = list(y = 0), inits = list(mu = 0, sigma = 1))

mcmc <- buildMCMC(m, thin = 2)

cm <- compileNimble(m)

cmcmc <- compileNimble(mcmc, project = m)

cmcmc$run(20, thin = 1)
cmcmc$run(20, thin = 1.1)

as.matrix(cmcmc$mvSamples)

## notation:
## TP = true positive   (model predicts success, property is successful)
## TN = true negative   (model predicts failure, property fails)
## FP = false positive  (model predicts success, property fails)
## FN = false negative  (model predicts failure, property succeeds)

TP <- 5387
TN <- 2011
FP <- 2105
FN <- 497

## total number of properties:
(n <- TP + TN + FP + FN)

## marginal probrobabilty that a property is successful
(TP + FN) / n

## overall accuracy of the model:
(TP + TN) / n

## "sensitivity", or "recall" = 
## Pr(model predicts success | property is successful)
TP / (TP + FN)

## "specificity" = 
## Pr(model predicts failure | property fails)
TN / (TN + FP)

## "positive predictive value", or "precision" =
## Pr(property is successful | model predicts success)
TP / (TP + FP)

## "negative predictive value" =
## Pr(property fails | model predicts failure)
TN / (TN + FN)

## gain and loss from properties invested in (in millions USD)
gain <- 1
loss <- -3

## if we consider "feedback on this model's performance" as how we faired over the past 5 years in terms of average profit or loss, after following the model's investment strategy on these 10000 properties:
TP * gain + FP * loss  ## loss of 928M over the past 5 years, on average

## average gain/loss from following this strategy on a single property:
TP/n * gain + FP/n * loss

## average loss of 0.0928 million dollars, or $92,800 loss per property.


## variance of this:
m <- TP/n * gain + FP/n * loss
v <- TP/n * (gain-m)^2 + FP/n * (loss-m)^2

## standard deviation of average gain/loss, per property:
sqrt(v)

## verify mean and sd by simulation:
nSim <- 1000000
sims <- sample(c(0, 0, -3, 1),
               size = nSim,
               replace = TRUE,
               prob = c(TN, FN, FP, TP)/n)
mean(sims)
sd(sims)

## Discussion





df_main <- read.csv('~/Downloads/Property Level Data.csv')
df_census <- read.csv('~/Downloads/Census Level Data.csv')

df <- merge(x = df_main,
            y = df_census,
            by.x = 'StateCode',
            by.y = 'STATECODE')


##hist(df$SuccesssProb)

##logit <- function(x) log(x/(1-x))
##hist(logit(df$SuccesssProb))


## remove 'PropertyType' variable, which only has a single level (Multifamily)
unique(df$PropertyType)
##df <- subset(df, select = -PropertyType)
df <- df[, -which(names(df) == 'PropertyType')]


###### make 'YearLastRenovated' variable binary,
###### so -1's become 0 (never renovated),
###### and any else becomes 1 (renovated)
####df$YearLastRenovated <- ifelse(df$YearLastRenovated > 0, 1, 0)

## standardize 'PropertySubType' variable,
## specifically the 'student house', and 'coop' levels
table(df$PropertySubType)
df$PropertySubType[grepl('tudent', df$PropertySubType)] <- 'StudentHousing'
table(df$PropertySubType)
df$PropertySubType[grepl('^[cC]', df$PropertySubType)] <- 'COOP'
table(df$PropertySubType)
df$PropertySubType <- factor(df$PropertySubType)

df$ParkingRatio <- factor(df$ParkingRatio)

## drop 'StateCode' and 'PropertyId' variables,
## we won't use them for prediction
##df <- subset(df, select = -c(StateCode, PropertyId))
df <- df[, -which(colnames(df) %in% c('StateCode', 'PropertyId'))]

## Classification Tree




numNA <- apply(df, 1, function(x) sum(is.na(x)))
indKeep <- which(numNA == 0)
df <- df[indKeep, ]
dim(df)


library(rpart)
##library(rpart.plot)





##df$Success <- factor(ifelse(df$SuccesssProb > 0.5, 'Y', 'N'))
##ct <- rpart(Success ~ . -SuccesssProb, method = 'class', data = df, cp = 0.0001)
##printcp(ct)
##plot(ct$cptable[,'CP'], ct$cptable[,'xerror'], type = 'l')
##(i <- which(ct$cptable[,'xerror'] == min(ct$cptable[,'xerror'])))
##ct$cptable[i,]
##names(ct)
##ct$variable.importance

## Regression Tree

## remove binary 'Success' factor
##df <- subset(df, select = -Success)

rt <- rpart(SuccesssProb ~ ., data = df, cp = 0.0001)





printcp(rt)


### Bagging

library(ipred)

(bg <- bagging(SuccesssProb ~ ., data = df, coob = TRUE))


library(ipred)
library(caret)

set.seed(0)
bg <- ipred::bagging(SuccesssProb ~ ., data = df, nbagg = 100, coob = TRUE, control = rpart.control(cp = 0))




vi <- varImp(bg)

vi

vi <- imp[order(i$Overall, decreasing = TRUE), , drop = FALSE]

ov <- vi$Overall

vi$cumulativeRed <- cumsum(ov / sum(ov))


vi





hist(df$SuccesssProb)


###df$Success <- ifelse(df$SuccesssProb > 0.5, 1, 0)

logit <- function(x) log(x/(1-x))
logitSP <- logit(df$SuccesssProb)

hist(logitSP)

max(logitSP)   ## about 9

hist(9 - logitSP)
hist(log(9 - logitSP))   ## approximately normal


df$y <- log(9 - logitSP)




fracNA <- apply(df, 2, function(x) mean(is.na(x)))
sort(fracNA, decreasing = TRUE)

varsOmit <- names(which(fracNA > 0.1))
varsOmit <- c(varsOmit, 'SuccesssProb', 'StateCode', 'PropertyId')

varsToKeep <- setdiff(names(df), varsOmit)


X <- df[, varsToKeep]
apply(X, 2, function(x) mean(is.na(x)))

##m <- glm(Success ~ ., data = X, family = 'binomial')
m <- lm(y ~ ., data = X)
summary(m)




varsOmit <- c(varsOmit, 'RAINDAYS', 'SNOWDAYS', 'ANNULRAIN', 'ANNULSNOW')
varsToKeep <- setdiff(names(df), varsOmit)
X <- df[, varsToKeep]



m <- lm(y ~ ., data = X)
summary(m)

plot(df$y, predict(m, X))

pred <- predict(m, newdata = X)

predProb <- 1 / (1 + exp(-pred))

plot(x = df$SuccesssProb, y = predProb, type = 'p')




plot(df$BuildingCount, df$SuccesssProb)   ## not useful






data <- data.frame(y = c(0, 0, 0, 1, 1), x1 = c(0, .1, .1, 0, .05), x2 = c(-1, -2, -1, 0, .2))
data

m <- glm(y ~ x1 + x2, data = data, family = 'binomial')
summary(m)

predict(m, newdata = data)








head(df)
tail(df)

names(df_main)


head(df_main)

ilogit <- function(x) log(x / (1-x))
ilogit(0.5)
ilogit(.99)

df_main$eta <- ilogit(df_main$SuccesssProb)


hist(df_main$SuccesssProb)
hist(df_main$eta)

dfHigh <- df_main[df_main$SuccesssProb > 0.9, ]

dfHigh$SuccesssProb
df_main$SuccessBinary <- ifelse(df_main$SuccesssProb > 0.5, 1, 0)

head(df_main)


lm(SuccesssProb ~ BuildingCount + StoryCount, data = df_main)
lm(eta ~ BuildingCount + StoryCount + PropertyValue, data = df_main)









library(nimble)
library(rdist)
library(raster)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(raster)
library(readr)
library(nimble)
library(rdist)

N <- 64
S <- 64

mybeta <- c(2,2,0, 4, 2)
set.seed(1)
x1 <- rnorm(64);
x2 <- rnorm(64);
x3 <- rnorm(64);
x4 <- rnorm(64);
x5 <- rnorm(64);
y  <- cbind(x1, x2, x3, x4, x5) %*% mybeta + rnorm(64)
y <-  matrix(y, nrow = length(y), ncol = length(y), byrow = FALSE)

dnorm_vec4 <- nimbleFunction( ## Define the distribution
    run = function(x = double(1), mean = double(1), sd = double(1), log = integer(0, default = 0)) {
        returnType(double(0))
        logProb <- sum(dnorm(x, mean, sd, log = TRUE))
        if(log) return(logProb)
        else return(exp(logProb))
    })

registerDistributions('dnorm_vec4')

library(nimble)

code <- SLMMCode <- nimbleCode({
    for (i in 1:S){
        y[1:N,i] ~ dnorm_vec4(b[i, 1] * x1[1:N] + b[i, 2] * x2[1:N] + b[i, 3] *
                              x3[1:N] + b[i, 4] * x4[1:N]+ b[i, 5] * x5[1:N], 1/(psi_y[i] * exp(-Dist[1:N, i]/lambda)))
        psi_y[i] ~ dgamma(1, 1)
        b[i, 1:5] <- bm[latent[i], 1:5]
        latent[i] ~ dcat(zlatent[1:M])
    }
    for (k in 1:M) {
        bm[k, 1:5] ~ dmnorm(mu_bm[1:5], cov = var_bm[1:5, 1:5])
    }
    var_bm[1:5, 1:5] <- 1/tau_bm * diag(rep(1, 5))
    tau_bm ~ dgamma(1, 1)
    for (j in 1:5) {
        mu_bm[j] ~ dnorm(0, 1)
    }
    zlatent[1:M] <- stick_breaking(vlatent[1:(M - 1)])
    for (j in 1:(M - 1)) {
        vlatent[j] ~ dbeta(1, alpha)
    }
    ##alpha ~ dgamma(1, 1)
    ##alpha ~ dgamma(3, 0.1)
    lambda ~ dunif(0, D)
})


netdist <- read.csv('~/Downloads/country_distance.csv')
my_points <- matrix(c(netdist$Longitude,    # Create longitude/latitude matrix
                      netdist$Latitude),ncol=2)

colnames(my_points) <- c("longitude", "latitude")

my_points
head(my_points)

df <- my_points
dist_mat <- pdist(df)
dist_1 <- pdist(df[, 1])
dist_2 <- pdist(df[, 2])

dist_prod <- product_metric(dist_1, dist_2)


data <- GWRData <- list(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4, x5 = x5,
                        Dist = dist_prod )

constants <- GWRConst <- list(S = S, M = 64, N = N, D = 50)

inits <- GWRInits <- list(psi_y = rep(1, GWRConst$S), lambda = 10,alpha=10,
                          latent = rep(1,GWRConst$S), tau_bm = 1,  mu_bm = rnorm(5),
                          vlatent = rbeta(GWRConst$M - 1, 1, 1),
                          bm = array(0, c(64,5)))


##mcmc.out <- nimbleMCMC(code = SLMMCode, data = GWRData, constants = GWRConst,
##                       inits = GWRInits,
##                       monitors = c("b","latent",'bm', "lambda"),
##                       niter = 50000,
##                       thin = 10, nchains = 1)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printSamplers(byType = TRUE)

conf$addMonitors('latent')
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 1000)


samplesPlot(samples[,3:50], ind = 1:100)

colnames(samples)
samplesSummary(samples)
library(basicMCMCplots)
samplesPlot(samples, scale = TRUE)
library(coda)
apply(samples, 2, effectiveSize)

samples[,20:60]

samples[990:1000,]



stochVars <- unique(nimble:::removeIndexing(Rmodel$getNodeNames(stochOnly = TRUE)))
for(v in stochVars) {
    lp <- Rmodel$calculate(v)
    cat(v, ': ', lp, '\n')
}








## helping Ciar with log-log linear model on nimble users group

library(pacman)
p_load(nimble, betafunctions, MCMCvis, metafor)

#Generate masses corresponding to each sample
set.seed(0)
n_samples <- 25
min_mass <- 10
max_mass <- 20000

masses <- ceiling(rBeta.4P(n = (n_samples-2),
                           l = min_mass, u = max_mass,  
                           alpha = 1, beta = 4))
masses <- c(min_mass, masses, max_mass)
masses <- masses[order(masses)]

#Define true model parameters
b <- 0.3
a <- 2.5
exp_Value <- exp(a + log(masses) * b)

#Simulate an estimation process for the dependent variable (point estimates +- SD)
Est_Value <- c()
Est_ValueSD <- runif(n_samples, 0.1, 2)

for(i in 1:n_samples){
    Est_Value[i] <- rnorm(1, exp_Value[i], 15)
}

##plot(log(masses), log(Est_Value))


##mod_constants <- list(n = n_samples, mass = masses, psd = Est_ValueSD)
mod_constants <- list(n = n_samples,
                      logmass_centered = log(masses) - mean(log(masses)),
                      psd = Est_ValueSD)
mod_data <- list(N = Est_Value)

loglog_model <- nimbleCode({
    #Priors
    a ~ dnorm(0, 0.0001) #Intercept Prior
    b ~ dnorm(0, 0.0001) #slope prior
    sd.resids ~ dunif(0, 10) #SD of Residuals
    tau.resids <- pow(sd.resids, -2) #Precision of residuals
    #Likelihood
    for(p in 1:n){
        tau.psd[p] <- pow(psd[p], -2) #Known residual component - aka measurement error
        eps[p] ~ dnorm(0, tau.resids) #Standard residual component
        ##muN[p] <- exp(a + log(mass[p]) * b) + eps[p] #Add residual error to model
        muN[p] <- exp(a + logmass_centered[p] * b) + eps[p] #Add residual error to model
        N[p] ~ dnorm(muN[p], tau.psd[p]) #Measurement error for estimates
    }
})

outs <- c('a', 'b') #Monitors
inits <- list(a = rnorm(1), b = rnorm(1)) #Initial values

#Run Model
model <- nimbleModel(loglog_model, constants = mod_constants, data = mod_data, inits = inits)
MCMCconf <- configureMCMC(model, monitors = outs)
MCMC <- buildMCMC(MCMCconf)
compModel <- compileNimble(model)
compMCMC <- compileNimble(MCMC, project = compModel)

##outputs <- runMCMC(compMCMC, niter = 50000, nburnin = 10000,
##                   thin = 10, nchains = 4, samplesAsCodaMCMC = T)
set.seed(0)
samples <- runMCMC(compMCMC, niter = 100000, nburnin = 50000)

library(basicMCMCplots)
samplesPlot(samples)
samplesPlot(samples, scale = TRUE)
cor(samples)





## investigating testing failure in nimbleLists
## (after merging in posterior_predictive_revamp

library(nimble)
library(testthat)

source('~/github/nimble/nimble/packages/nimble/tests/testthat/test_utils.R')

nlTestFunc27 <- nimbleFunction(
    setup = function(){
        testListDef27 <- nimbleList(testSvd = svdNimbleList())
        testList27 <- testListDef27$new()
        testMat <- diag(2)
    },
    run = function(){
        svdOut <- svd(testMat)
        testList27$testSvd <<- svdOut
        returnType(svdNimbleList())
        return(testList27$testSvd)
    }
)

testInst <- nlTestFunc27()
RnimbleList <- testInst$run()
CtestInst <- compileNimble(testInst)
CnimbleList <- CtestInst$run()

expect_equal(diag(2), RnimbleList$u%*%diag(RnimbleList$d)%*%solve(RnimbleList$v))
expect_equal(RnimbleList$u, CnimbleList$u)
expect_equal(RnimbleList$d, CnimbleList$d)
expect_equal(RnimbleList$v, CnimbleList$v)

CtestInst$testList27$testSvd
ls(CtestInst$testList27$testSvd)
CtestInst$testList27$testSvd$d
CtestInst$testList27$testSvd$u
CtestInst$testList27$testSvd$v

expect_equal(testInst$testList27$testSvd$d, CtestInst$testList27$testSvd$d)
expect_equal(testInst$testList27$testSvd$u, CtestInst$testList27$testSvd$u)
expect_equal(testInst$testList27$testSvd$v, CtestInst$testList27$testSvd$v)

expect_identical(is.nl(CnimbleList), TRUE)


## testing the categorial_general sampler, for nimbleSCR package, for Cyril

library(nimble)


code <- nimbleCode({
    x ~ dcat(p[1:5])
    y ~ dcat(p[1:5])
})
constants <- list()
data <- list()
inits <- list(p = 1:5, x = 1, y = 2)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel, nodes = NULL)

conf$addSampler(target = 'x', type = 'categorical')
conf$addSampler(target = 'y', type = 'categorical_general', numCategories = 5)

conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)
samplesPlot(samples)
apply(samples, 2, effectiveSize)


## timing tests of basic predictive and posterior predictive node models
## for the posterior_predictive_revamp merge


library(nimble)


n <- 10000
n <- 100

code <- nimbleCode({
    for(i in 1:n) {
        z[i] ~ dnorm(y[i], 1)
        y[i] ~ dnorm(mu, 1)
    }
    mu ~ dnorm(0, 1)
})

system.time({Rmodel <- nimbleModel(code, data=list(y=rnorm(n)), constants=list(n=n))})
## devel: 4.2  4.5  4

debug(configureMCMC)
conf <- configureMCMC(Rmodel)

system.time({conf <- configureMCMC(Rmodel)})
## devel: 8.8  9.1  9.6



Rmodel$isEndNode(IDs)




## testing assignment of posterior_predictive_samplers
## for the posterior_predictive_revamp merge


library(nimble)

code <- nimbleCode({
    mu0 ~ dnorm(0,1)
    mu ~ T(dnorm(mu0,1),-5,5)
    y~dnorm(mu,1)
    z~dnorm(y,1)
    z0~dnorm(z,1)
    z1~dnorm(z,1)
})

m=nimbleModel(code,data=list(y=1))

conf <- configureMCMC(m)
conf <- configureMCMC(m, nodes = NULL)
conf <- configureMCMC(m, nodes = c('mu0', 'mu', 'z', 'z0', 'z1'))
conf <- configureMCMC(m, nodes = c('mu0', 'mu', 'z', 'z0'))
conf <- configureMCMC(m, nodes = c('mu0', 'mu', 'z', 'z0'))
conf <- configureMCMC(m, nodes = c('mu0', 'mu', 'z'))
conf <- configureMCMC(m, nodes = c('mu0', 'mu', 'z0', 'z1'))

conf$getUnsampledNodes()
conf

conf$addSampler('mu0', type='RW',control=list(adaptive=F))
conf$addSampler('z', type='posterior_predictive')
conf$addSampler('mu', type='RW',control=list(adaptive=F))

conf$printSamplers()
# [1] RW sampler: mu0,  adaptive: FALSE
# [2] posterior_predictive sampler: z
# [3] RW sampler: mu,  adaptive: FALSE

conf$samplerExecutionOrder
# [1] 1 2 3

Rmcmc <- buildMCMC(conf)


https://us02web.zoom.us/j/7341328146?pwd=Z1NQeVVwU01lNEdGaTYzS2RNZVg1Zz09
https://us02web.zoom.us/j/7341328146?pwd=N0lPZGMrSVd6RzlmU2hyQVBOV2JKZz09




## helping Ehsan (Norway) look into RJMCMC error message


##build('v11-1')
##build('v12-0')
build('v12-1')
build('v12-2')

code <- nimbleCode({
    tau ~ dgamma(0.1, 0.1)
    omega ~ dbeta(2,8)     ##-- Prior on inclusion probability
    for (i in 1:N) {
        betaHabCovs[i] ~ dnorm(0, tau)
        w[i] ~ dbern(omega)  ##-- w: Indicator variable for each coefficient
    }
    y ~ dnorm(sum(w[1:N] * betaHabCovs[1:N]), 1)
})

N <- 10
constants <- list(N = N)
data <- list(y = 0)
inits <- list(tau = 1, omega = 0.5, betaHabCovs = rep(0, N), w = rep(0, N))

Rmodel <- nimbleModel(code, constants, data, inits, buildDerivs = TRUE)

Rmodel$calculate()   ## -19.97234    ## with y=0: -20.89128

conf <- configureMCMC(Rmodel)
conf$addSampler(c('tau', 'omega'), 'HMC')
conf$addSampler(c('tau', 'omega', 'w'), 'HMC')
Rmcmc <- buildMCMC(conf)
pppp

##configureRJ(conf, targetNodes = c('betaHabCovs'))

configureRJ(conf, targetNodes = c('betaHabCovs'), indicatorNodes = c('w'))

pppp

targetNodes <- c('betaHabCovs')
indicatorNodes <- c('w')
model <- conf$model
nNodes <- length(targetNodes)
control <- list(mean = NULL, scale = NULL, fixedValue = NULL)
fixedValue <- if(!is.null(control$fixedValue)) control$fixedValue else 0
mean       <- if(!is.null(control$mean))       control$mean       else 0
scale      <- if(!is.null(control$scale))      control$scale      else 1
priorProb <- NULL
indicatorFlag <- !is.null(indicatorNodes)
priorFlag     <- !is.null(priorProb)



##if(length(model$getParents(targetNodes, stochOnly = TRUE, includeData = FALSE)) > 0) {


targetNodesAsScalars <- model$expandNodeNames(targetNodes, returnScalarComponents = TRUE)


i <- 1
##for(i in seq_along(targetNodesAsScalars)) {


if(length(model$getParents(targetNodesAsScalars[i], stochOnly = TRUE, includeData = FALSE)) > 0) {
    stop('Reversible jump target node \"', targetNodesAsScalars[i], '\" appears to have a non-constant hyper-parameter, which is not currently supported for reversible jump MCMC.')
}


##}


##}



## update a new MCMC testing gold file
## to remove the "Test passed" messages from test_that testthat package
## to compare gold file
1

path <- '~/github/nimble/nimble/packages/nimble/tests/testthat/'
infile <- 'mcmcTestLog_Correct_NEW.Rout'
outfile <- 'mcmcTestLog_Correct_NEW2.Rout'

t <- readLines(paste0(path, infile))
##t <- t[1:800]
##t[570:577]

i <- 1
tot <- length(t)
while(i <= tot) {
    if(grepl('Test passed', t[i])) {
        t[i] <- gsub('Test passed .', '', t[i])
        if(i < tot) {
            t[i] <- paste0(t[i], t[i+1])
            t <- t[-(i+1)]
        } else {
            if(t[i] == '') t <- t[-i]
        }
        tot <- length(t)
    } else {
        i <- i+1
    }
}

grep('Test passed', t, value = TRUE)

writeLines(t, con = paste0(path, outfile))




## working on posterior_predictive_revamp ppb branch nodes


nimbleOptions('determinePredictiveNodesInModel')
nimbleOptions(determinePredictiveNodesInModel = TRUE)
nimbleOptions(determinePredictiveNodesInModel = FALSE)
nimbleOptions('determinePredictiveNodesInModel')

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    for(i in 1:8) {
        b[i] ~ dnorm(sqrt(a), 1)
        c[i] ~ dnorm(b[i]^2, 1)
        d[i] ~ dnorm(c[i]^3, 1)
    }
})

Rmodel <- nimbleModel(code)

Rmodel$resetData()

Rmodel$setData(list(b = 1:8))

Rmodel$getPredictiveNodeIDs()
Rmodel$getPredictiveBranchPointNodeIDs()

nimbleOptions('MCMCusePosteriorPredictiveSampler')
nimbleOptions(MCMCusePosteriorPredictiveSampler = FALSE)

conf <- configureMCMC(Rmodel)
conf$setSamplers(c(8, 9, 1:9))
pppp
conf$removeSampler('c')



Rmcmc <- buildMCMC(conf)

Rmcmc$samplerFunctions$contentsList[[1]]
Rmcmc$samplerFunctions$contentsList[[1]]$calcNodesNoSelf
Rmcmc$samplerFunctions$contentsList[[1]]$copyNodesDeterm
Rmcmc$samplerFunctions$contentsList[[1]]$copyNodesStoch



conf <- configureMCMC(Rmodel)
conf <- configureMCMC(Rmodel, nodes = NULL)
conf <- configureMCMC(Rmodel, nodes = NULL, print = FALSE)

conf$unsampledNodes
conf$getUnsampledNodes()

conf$addSampler('a')
conf$addSampler('b')
conf$addSampler('d[1]', 'posterior_predictive')
conf$addSampler(c('d[1]', 'd[3]'), 'posterior_predictive')
conf$addSampler('a', 'posterior_predictive')
conf$addSampler('c', 'posterior_predictive')
conf$addSampler('d', 'posterior_predictive')
conf$addSampler('d[1:1]', 'posterior_predictive')
conf$addSampler('d[1:3]', 'posterior_predictive')
conf$addSampler(c('d[1:3]', 'd'), 'posterior_predictive')
conf$addSampler(c('d[1:3]', 'b'), 'posterior_predictive')
conf$addSampler('b', 'posterior_predictive')
conf$addSampler('a', 'posterior_predictive')


conf$addSampler('d', 'RW_block')
conf$addSampler(c('d[1]', 'd[3]'), 'RW_block')
conf$addSampler('d[1]', 'RW_block')

conf$printSamplers(byType = TRUE)
conf

undebug(conf$printSamplersByType)
debug(conf$printSamplersByType)
conf$printSamplers(byType = TRUE)


## orig:
sampleVals = list(x = c(3.950556165467749, 1.556947815895538, 1.371834934033851, 2.036442813764752, 2.247416118159410, 2.537131924778210, 2.382184991769738, 2.653737836857812, 2.934255734970981, 3.007873553270551),
                  c = c(0.010341199485849559, 0.010341199485849559, 0.003846483017887228, 0.003846483017887228, 0.003846483017887228, 0.006269117826484087, 0.009183580181658716, 0.009183580181658716, 0.006361841408434201, 0.006361841408434201))


## new:
sampleVals = list(x = c(3.9505561655, 2.0475815875, 1.6434823847, 1.3639509750, 1.2026040395, 0.7809965060, 1.0238211270, 0.9245529610, 0.7592556898, 0.4670729702),
                  c = c(0.015855585146, 0.010426702596, 0.010426702596, 0.014013991556, 0.001477657554, 0.001477657554, 0.001477657554, 0.001477657554, 0.001477657554, 0.006195021298))




c(0.015855585146, 0.010426702596, 0.010426702596, 0.014013991556, 0.001477657554, 0.001477657554, 0.001477657554, 0.001477657554, 0.001477657554, 0.006195021298)



exactSample <- sampleVals

exactSample



varName <- 'x'
varName <- 'c'

expect_equal(round(C_samples[seq_along(exactSample[[varName]]), varName], 8), round(exactSample[[varName]], 8))



C_samples[seq_along(exactSample[[varName]]), varName]

round(exactSample[[varName]], 8)

modelDef$nodeName2GraphIDs



##> buildMCMC(mConf)
##Error in samplerFunction(model = model, mvSaved = mvSaved, target = target,  : 
##  sampler_CRP: Detected that the CRP variable is used in some way not as an index: y[i] ~ dnorm(mean = xi[i], sd = 1). NIMBLE's CRP sampling not set up to handle this case.



1

library(nimble)
library(testthat)

##build(devel)
##build(ppr)

source('~/github/nimble/nimble/packages/nimble/tests/testthat/test_utils.R')


context("Testing of default MCMC")

RwarnLevel <- options('warn')$warn
options(warn = 1)



t <- readLines('~/github/nimble/nimble/packages/nimble/tests/testthat/mcmcTestLog_Correct_NEW.Rout')

t[1:100]

t[93]

grep('^Test passed ....$', t[93], value = TRUE)
grep('^Test passed ', t, value = TRUE)



library(nimble)
library(testthat)


test_that('mcmc_determineCalcAndCopyNodes works correctly in different situations', {
    nimbleOptions(MCMCusePosteriorPredictiveSampler = TRUE)
    code <- nimbleCode({
        x[1] ~ dnorm(0, 1)
        for(i in 2:10) x[i] ~ dnorm(x[i-1], 1)
    })
    Rmodel <- nimbleModel(code)
    conf <- configureMCMC(Rmodel)
    expect_identical(conf$getUnsampledNodes(), character())
    conf$removeSamplers()
    expect_identical(conf$getUnsampledNodes(), paste0('x[', 1:10, ']'))
    conf$addSampler('x[3]', 'posterior_predictive')
    expect_identical(conf$getUnsampledNodes(), paste0('x[', 1:2, ']'))
    conf$addSampler('x[1]', 'RW')
    expect_identical(conf$getUnsampledNodes(), 'x[2]')
    conf$addSampler('x[1]', 'posterior_predictive')
    expect_identical(conf$getUnsampledNodes(), character())
    conf$removeSamplers()
    Rmodel$setData(x = c(rep(NA,5), rep(0,5)))
    expect_identical(conf$getUnsampledNodes(), paste0('x[', 1:5, ']'))
    conf$addSampler(c('x[1]', 'x[3]'), 'RW_block')
    expect_identical(conf$getUnsampledNodes(), paste0('x[', c(2,4,5), ']'))
})






##save(code, data, const, inits, file = '~/temp/tempp.RData')
load('~/temp/tempp.RData')

m <- nimbleModel(code, data = data, constants = const, inits = inits)

m <- nimbleModel(code, data = c(data, list(z = 0)), constants = const, inits = inits)

conf <- configureMCMC(m)

##debug(sampler_CRP)

getNimbleOption('getDependenciesIncludesPredictiveNodes')






codeTest <- nimbleCode ({
    X[1:nGroups] ~ dmultinom(size=N, prob=pVecX[1:nGroups])
    Y[1:nGroups] ~ dmultinom(size=N, prob=pVecY[1:nGroups])
    for (ii in 1:nGroups) {
        Z[ii] ~ dbeta(1 + X[ii], 1 + Y[ii])
    }
})
set.seed(0)
nGroups   <- 5
N         <- 1E6
pVecX     <- rdirch(1, rep(1, nGroups))
pVecY     <- rdirch(1, rep(1, nGroups))
X         <- rmultinom(1, N, pVecX)[,1]
Y         <- rmultinom(1, N, pVecY)[,1]
Z         <- rbeta(nGroups, 1+X, 1+Y)
## Hard code in the results of sample() since output from sample
## changed as of R 3.6.0 to fix a long-standing bug in R.
smpX <- pVecX[c(2,1,4,3,5)]
smpY <- pVecY[c(1,4,2,3,5)]
fakeSample <- sample(pVecX)  # to keep random number stream as before
Xini      <- rmultinom(1, N, smpX)[,1]
fakeSample <- sample(pVecY)  # to keep random number stream as before
Yini      <- rmultinom(1, N, smpY)[,1]
Constants <- list(nGroups=nGroups)
Inits     <- list(X=Xini, Y=Yini, pVecX=pVecX, pVecY=pVecY, N=N)
Data      <- list(Z=Z)
modelTest <- nimbleModel(codeTest, constants=Constants, inits=Inits, data=Data, check=TRUE)
cModelTest <- compileNimble(modelTest)
mcmcTestConfig <- configureMCMC(cModelTest, print = nimbleOptions('verbose'))
samplers <- mcmcTestConfig$getSamplers()
test_that('assign RW_multinomial sampler', expect_equal(samplers[[1]]$name, 'RW_multinomial'))
test_that('assign RW_multinomial sampler', expect_equal(samplers[[2]]$name, 'RW_multinomial'))
mcmcTest  <- buildMCMC(mcmcTestConfig)
cMcmcTest <- compileNimble(mcmcTest, project=modelTest)
## Optionally resample data
cModelTest$N      <- N <- 1E3
(cModelTest$pVecX <- sort(rdirch(1, rep(1, nGroups))))
(cModelTest$pVecY <- sort(rdirch(1, rep(1, nGroups))))
simulate(cModelTest, "X", includeData=TRUE); (X <- cModelTest$X)
simulate(cModelTest, "Y", includeData=TRUE); (Y <- cModelTest$Y)
simulate(cModelTest, "Z", includeData=TRUE); (Z <- cModelTest$Z)
niter  <- 1E4
cMcmcTest$run(niter)
samples <- as.matrix(cMcmcTest$mvSamples)

as.numeric(samples[10000,])


mcmcTest$samplerFunctions[[1]]$my_setAndCalculateDiff$targetNodesAsScalar
mcmcTest$samplerFunctions[[1]]$my_setAndCalculateDiff$calcNodes


mcmcTest$samplerFunctions[[2]]$my_setAndCalculateDiff$targetNodesAsScalar
mcmcTest$samplerFunctions[[2]]$my_setAndCalculateDiff$calcNodes


expect_identical(as.numeric(samples[10000,]), c(8, 25, 31, 115, 821, 25,19, 84, 510, 362), info = 'exact results of RW_multinomial sampler')




buildMCMC(conf)

## on devel:
##Error in samplerFunction(model = model, mvSaved = mvSaved, target = target,  : 
##  sampler_CRP: Only the variables being clustered can depend on the cluster parameters.

expect_error(mcmc <- buildMCMC(conf), "Only the variables being clustered")

  

library(testthat)





set.seed(1)
code <- nimbleCode({
    xi[1:6] ~ dCRP(conc0, 6)
    conc0 ~ dgamma(1, 1)
    for(i in 1:6){
        y[i] ~ dnorm(xi[i], 1)
    }
})
Inits <- list(xi = c(1,1,2,1,1,2),  conc0 = 1)
Data <- list( y =  rnorm(6))
m <- nimbleModel(code, data=Data, inits=Inits)
mConf <- configureMCMC(m)

debug(buildMCMC)

buildMCMC(mConf)

expect_error(buildMCMC(mConf), 'sampler_CRP: Detected that the CRP variable is used in some way not as an index')





library(nimble); library(testthat)


source('~/github/nimble/nimble/packages/nimble/tests/testthat/test_utils.R')

RwarnLevel <- options('warn')$warn
options(warn = 1)
## verbose: set to FALSE
nimbleVerboseSetting <- nimbleOptions('verbose')
###########nimbleOptions(verbose = FALSE)
## MCMC progress bar: set to FALSE
nimbleProgressBarSetting <- nimbleOptions('MCMCprogressBar')
nimbleOptions(MCMCprogressBar = FALSE)
## MCMC orderSamplersPosteriorPredictiveLast - save current setting
nimbleReorderPPsamplersSetting <- getNimbleOption('MCMCorderPosteriorPredictiveSamplersLast')
## MCMC use usePosteriorPredictiveSampler - save current setting
nimbleUsePosteriorPredictiveSamplerSetting <- getNimbleOption('MCMCusePosteriorPredictiveSampler')
## MCMC calculation include predictive dependencies - save current setting
nimbleUsePredictiveDependenciesSetting <- nimbleOptions('MCMCusePredictiveDependenciesInCalculations')
## MCMC warn about unsampled nodes - save current setting
nimbleWarnUnsampledNodesSetting <- nimbleOptions('MCMCwarnUnsampledStochasticNodes')



nimbleOptions('MCMCusePredictiveDependenciesInCalculations')
nimbleOptions(MCMCusePredictiveDependenciesInCalculations = TRUE)
nimbleOptions(MCMCusePredictiveDependenciesInCalculations = FALSE)

nimbleOptions('getDependenciesIncludesPredictiveNodes')
nimbleOptions(getDependenciesIncludesPredictiveNodes = TRUE)
nimbleOptions(getDependenciesIncludesPredictiveNodes = FALSE)



####source('~/github/nimble/nimble/packages/nimble/tests/testthat/test_utils.R')
##RwarnLevel <- options('warn')$warn
##options(warn = 1)
#### verbose: set to FALSE
##nimbleVerboseSetting <- nimbleOptions('verbose')
##nimbleOptions(verbose = FALSE)
#### MCMC progress bar: set to FALSE
##nimbleProgressBarSetting <- nimbleOptions('MCMCprogressBar')
##nimbleOptions(MCMCprogressBar = FALSE)
#### MCMC orderSamplersPosteriorPredictiveLast - save current setting
##nimbleReorderPPsamplersSetting <- getNimbleOption('MCMCorderPosteriorPredictiveSamplersLast')
#### MCMC use posteriorPredictiveBranch sampler - save current setting
##nimblePPBranchSamplerSetting <- getNimbleOption('MCMCjointlySamplePredictiveBranches')
#### MCMC calculation include predictive dependencies - save current setting
##nimbleUsePredictiveDependenciesSetting <- nimbleOptions('MCMCusePredictiveDependenciesInCalculations')
##nimbleWarnUnsampledNodesSetting <- nimbleOptions('MCMCwarnUnsampledStochasticNodes')


samplerConfs <- conf$samplerConfs
model <- Rmodel


node <- 'a'
node <- 'y'
node <- 'b'
node <- 'c'


set.seed(0)
Rmcmc$run(10)
node <- 'b'


Rmodel$getLogProb(node)
Rmodel$calculate(node)

Cmodel$getLogProb(node)
Cmodel$calculate(node)


Rmcmc$samplerFunctions[[2]]$calcNodes













message("  [Note] safeDeparse: truncating deparse output to ", nlines, " lines.")

nlines <- 1

message("  [Note] safeDeparse: truncating deparse output to ", nlines, " line", if(nlines>1) "s" else "")





requireNamespace("sf", quietly = TRUE)
requireNamespace("spdep", quietly = TRUE)
library(spdep)

nc.sids <- sf::st_read(system.file("shapes/sids.shp", package="spData")[1])
nc.sids <- as(nc.sids, "Spatial")
proj4string(nc.sids) <- CRS("+proj=longlat +ellps=clrk66")
row.names(nc.sids) <- as.character(nc.sids$FIPS)
rn <- row.names(nc.sids)
ncCC89_nb <- read.gal(system.file("weights/ncCC89.gal", package="spData")[1],
                      region.id=rn)
ncCR85_nb <- read.gal(system.file("weights/ncCR85.gal", package="spData")[1],
                      region.id=rn)

plot(nc.sids, border="grey")
plot(ncCR85_nb, coordinates(nc.sids), add=TRUE, col="blue")
plot(nc.sids, border="grey")
plot(ncCC89_nb, coordinates(nc.sids), add=TRUE, col="blue")

library(nimble)


nc.sids.code = nimbleCode({
    for (i in 1:k){
      theta[i] ~ dnorm(mu, tau)
      y[i] ~ dpois(n[i] *exp(theta[i]))
    }
    #prior for hyperparameters
    mu ~ dnorm(-6, 0.001)
    tau ~ dgamma(1, 0.001)
  }
)
 
k= 100
n= nc.sids$BIR79
y= nc.sids$SID79
 
nc.sids.Consts = list(k = k, n = n)
nc.sids.Data= list(y = y)
nc.sids.Inits = list(mu = -2, tau = 12, theta = rep(0.002, k))
 
nc.sids.Model = nimbleModel( nc.sids.code,
                             data= nc.sids.Data,
                             constants = nc.sids.Consts,
                             inits= nc.sids.Inits)
 
 
compile.nc.sids.Model = compileNimble(nc.sids.Model)

nc.sids.Model$getNodeNames()

str(nc.sids.Model)
str(compile.nc.sids.Model)
 
 
nc.sids.conf = configureMCMC(nc.sids.Model, print= T)
nc.sids.conf$addMonitors(c("mu", "tau", "theta"))
 
 
nc.sids.MCMC = buildMCMC(nc.sids.conf)
compile.nc.sids.MCMC = compileNimble(nc.sids.MCMC, project= nc.sids.Model)
 
 
niter = 5500
nburn = 0
 
set.seed(1)
 
inits = list(mu = -2, tau =12, theta = rep(0.002, k))
start.time = proc.time()
samples = runMCMC( compile.nc.sids.MCMC, niter = niter, nburnin = nburn,
                   inits = inits, nchains = 1, samplesAsCodaMCMC = TRUE)
 
stop.time = proc.time()

time.elapsed = stop.time - start.time

 

print(time.elapsed)






library(nimble)

mcmc_determineCalcAndCopyNodes_NEW <- function(model, target, includePPDeps) {
    if(missing(includePPDeps)) stop('need to provide includePPDeps')
    optionIncludePPDeps <- includePPDeps
    ## if this sampler is operating on a posterior-predictive node,
    ## the we're doing non-standard sampler assignment here, so
    ## skip over the option of 'MCMCincludePredictiveDependencies':
    if(!length(model$getDependencies(target, includePosteriorPred = optionIncludePPDeps)))   optionIncludePPDeps <- TRUE
    calcNodes <- model$getDependencies(target, includePosteriorPred = optionIncludePPDeps)
    calcNodesNoSelf <- model$getDependencies(target, self = FALSE, includePosteriorPred = optionIncludePPDeps)
    calcNodesPPskipped <- if(optionIncludePPDeps) character() else model$getDependencies(target, posteriorPredOnly = TRUE)
    copyNodes <- model$getDependencies(target, self = FALSE)
    isStochCopyNodes <- model$isStoch(copyNodes)
    copyNodesDeterm <- copyNodes[!isStochCopyNodes]
    copyNodesStoch <- copyNodes[isStochCopyNodes]
    ccLst <- list(
        calcNodes = calcNodes,
        calcNodesNoSelf = calcNodesNoSelf,
        calcNodesPPskipped = calcNodesPPskipped,
        copyNodesDeterm = copyNodesDeterm,
        copyNodesStoch = copyNodesStoch
    )
    return(ccLst)
}

sampler_RW_NEW <- nimbleFunction(
    name = 'sampler_RW_NEW',
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        ## control list extraction
        adaptive            <- extractControlElement(control, 'adaptive',            TRUE)
        adaptInterval       <- extractControlElement(control, 'adaptInterval',       200)
        adaptFactorExponent <- extractControlElement(control, 'adaptFactorExponent', 0.8)
        scale               <- extractControlElement(control, 'scale',               1)
        includePP <- extractControlElement(control, 'includePP', error = 'must specify includePP')  ## NEW
        ## node list generation
        targetAsScalar <- model$expandNodeNames(target, returnScalarComponents = TRUE)
        ccLst <- mcmc_determineCalcAndCopyNodes_NEW(model, target, includePPDeps = includePP)
        calcNodesNoSelf <- ccLst$calcNodesNoSelf; calcNodesPPskipped <- ccLst$calcNodesPPskipped; copyNodesDeterm <- ccLst$copyNodesDeterm; copyNodesStoch <- ccLst$copyNodesStoch   # not used: calcNodes
        ## numeric value generation
        scaleOriginal <- scale
        timesRan      <- 0
        timesAccepted <- 0
        timesAdapted  <- 0
        optimalAR     <- 0.44
        gamma1        <- 0
    },
    run = function() {
        currentValue <- model[[target]]
        propValue <- rnorm(1, mean = currentValue,  sd = scale)
        model[[target]] <<- propValue
        logMHR <- model$calculateDiff(target)
        if(logMHR == -Inf) {
            jump <- FALSE
            nimCopy(from = mvSaved, to = model, row = 1, nodes = target, logProb = TRUE)
        } else {
            logMHR <- logMHR + model$calculateDiff(calcNodesNoSelf)
            jump <- decide(logMHR)
            if(jump) {
                model$calculate(calcNodesPPskipped)
                nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
                nimCopy(from = model, to = mvSaved, row = 1, nodes = copyNodesDeterm, logProb = FALSE)
                nimCopy(from = model, to = mvSaved, row = 1, nodes = copyNodesStoch, logProbOnly = TRUE)
            } else {
                nimCopy(from = mvSaved, to = model, row = 1, nodes = target, logProb = TRUE)
                nimCopy(from = mvSaved, to = model, row = 1, nodes = copyNodesDeterm, logProb = FALSE)
                nimCopy(from = mvSaved, to = model, row = 1, nodes = copyNodesStoch, logProbOnly = TRUE)
            }
        }
        if(adaptive)     adaptiveProcedure(jump)
    },
    methods = list(
        adaptiveProcedure = function(jump = logical()) {
            timesRan <<- timesRan + 1
            if(jump)     timesAccepted <<- timesAccepted + 1
            if(timesRan %% adaptInterval == 0) {
                acceptanceRate <- timesAccepted / timesRan
                timesAdapted <<- timesAdapted + 1
                gamma1 <<- 1/((timesAdapted + 3)^adaptFactorExponent)
                gamma2 <- 10 * gamma1
                adaptFactor <- exp(gamma2 * (acceptanceRate - optimalAR))
                scale <<- scale * adaptFactor
                timesRan <<- 0
                timesAccepted <<- 0
            }
        },
        reset = function() {
            scale <<- scaleOriginal
            timesRan      <<- 0
            timesAccepted <<- 0
            timesAdapted  <<- 0
            gamma1 <<- 0
        }
    )
)



thetaPP <- TRUE
xPP     <- TRUE

thetaPP <- FALSE
xPP     <- FALSE



code <- nimbleCode({
    theta ~ dnorm(0, sd = 100)
    x ~ dexp(scale = theta^3 + 1)
    y ~ dpois(x)
    pp ~ dnorm(theta + x, sd = 1)
    ##for(i in 1:10) {
    ##    y[i] ~ dpois(x)
    ##    pp[i] ~ dnorm(theta, sd = x)
    ##}
})
constants <- list()
data <- list(y = 30)  ##y = 21:30)
inits <- list(theta = 0, x = 1, pp = 20)  ##, pp = 1:10)
niter <- 300000
nburnin <- 200000

runOptions <- function(thetaPP, xPP) {
    Rmodel <- nimbleModel(code, constants, data, inits)
    print(Rmodel$calculate())
    conf <- configureMCMC(Rmodel, monitors = c('theta', 'x'))
    conf$removeSampler('theta')
    conf$addSampler(target = 'theta', type = 'RW_NEW', includePP = thetaPP)
    conf$removeSampler('x')
    conf$addSampler(target = 'x', type = 'RW_NEW', includePP = xPP)
    conf$printSamplers()
    Rmcmc <- buildMCMC(conf)
    ##print(Rmcmc$samplerFunctions[[1]]$target)
    ##print(Rmcmc$samplerFunctions[[1]]$calcNodesNoSelf)
    ##print(Rmcmc$samplerFunctions[[1]]$calcNodesPPskipped)
    ##print(Rmcmc$samplerFunctions[[1]]$copyNodesDeterm)
    ##print(Rmcmc$samplerFunctions[[1]]$copyNodesStoch)
    ##print(Rmcmc$samplerFunctions[[2]]$target)
    ##print(Rmcmc$samplerFunctions[[2]]$calcNodesNoSelf)
    ##print(Rmcmc$samplerFunctions[[2]]$calcNodesPPskipped)
    ##print(Rmcmc$samplerFunctions[[2]]$copyNodesDeterm)
    ##print(Rmcmc$samplerFunctions[[2]]$copyNodesStoch)
    compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
    Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
    set.seed(0)
    summary <- runMCMC(Cmcmc, niter = niter, nburnin = nburnin, samples = FALSE, summary = TRUE, nchains = 3)
    summary <- runMCMC(Cmcmc, niter = niter, nburnin = nburnin, samples = TRUE, summary = TRUE, nchains = 3)
    
    return(summary)
}

summary$summary
##           Mean    Median  St.Dev. 95%CI_low 95%CI_upp
## theta  5.64403  4.241991 4.597069  2.048039   18.6041
## x     30.29612 29.921270 5.528604 20.504477   42.2030
##  
## $chain2
##            Mean    Median  St.Dev. 95%CI_low 95%CI_upp
## theta  5.908568  4.227502 6.511099  2.032014  20.18764
## x     30.259251 29.932926 5.473758 20.536729  41.87943
##  
## $chain3
##            Mean    Median  St.Dev. 95%CI_low 95%CI_upp
## theta  5.869351  4.246819 5.817485  2.041714  20.35873
## x     30.284271 29.984159 5.482468 20.500284  42.06938
##  
## $all.chains
##            Mean    Median  St.Dev. 95%CI_low 95%CI_upp
## theta  5.807316  4.239139 5.698266  2.040487  19.67218
## x     30.279882 29.946955 5.494999 20.511994  42.05092

> 

library(basicMCMCplots)

samplesPlot(summary$samples[[2]], var = 'theta')
samplesPlot(summary$samples, var = 'x')

summaryTT <- runOptions(thetaPP = TRUE,  xPP = TRUE)
summaryFF <- runOptions(thetaPP = FALSE, xPP = FALSE)
summaryTF <- runOptions(thetaPP = TRUE,  xPP = FALSE)

summaryTT
summaryFF
summaryTF



## > summary0
##           Mean   Median   St.Dev. 95%CI_low 95%CI_upp
## theta 66.15734 53.35958 48.948089   9.72623 192.93780
## x     25.55575 25.52742  1.603924  22.48439  28.79097
## > summary1
##           Mean   Median   St.Dev. 95%CI_low 95%CI_upp
## theta 65.68603 52.48918 48.535005   9.45394 188.91579
## x     25.52053 25.47460  1.613703  22.50450  28.75823




library(nimble)

code <- nimbleCode({
    theta ~ dnorm(0, sd = 100)
    x ~ dexp(scale = theta + 1)
    for(i in 1:10) {
        y[i] ~ dpois(x)
        pp1[i] ~ dnorm(x, sd = 10)
        pp2[i] ~ dnorm(theta, sd = 10)
        pp12[i] ~ dnorm(theta, x)
    }
})

constants <- list()
data <- list(y = 21:30)
inits <- list(theta = 0, x = 1, pp1 = 1:10, pp2 = 1:10, pp12 = 1:10)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel, monitors = c('theta', 'x'))
Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

set.seed(0)
summary0 <- runMCMC(Cmcmc, niter = 200000, nburnin = 100000, samples = FALSE, summary = TRUE)
summary1 <- runMCMC(Cmcmc, niter = 200000, nburnin = 100000, samples = FALSE, summary = TRUE)
summary2 <- runMCMC(Cmcmc, niter = 300000, nburnin = 200000, samples = FALSE, summary = TRUE)
summary12 <- runMCMC(Cmcmc, niter = 300000, nburnin = 200000, samples = FALSE, summary = TRUE)
summary12 <- runMCMC(Cmcmc, niter = 300000, nburnin = 200000, samples = TRUE, summary = TRUE)
summary12 <- runMCMC(Cmcmc, niter = 300000, nburnin = 0, samples = TRUE, summary = TRUE)
summary1212 <- runMCMC(Cmcmc, niter = 300000, nburnin = 200000, samples = FALSE, summary = TRUE)

summary0
summary1
summary2
summary12
summary1212

Cmodel$pp12
Cmodel$theta
Cmodel$x

summaryFF
summaryTT

dim(summary0)
library(basicMCMCplots)
samplesPlot(summary0, 'theta')

samplesPlot(summary12, 'theta', densityplot = FALSE)
samplesPlot(summary12, 'x', densityplot = FALSE)

## > summary0
##           Mean   Median   St.Dev. 95%CI_low 95%CI_upp
## theta 66.15734 53.35958 48.948089   9.72623 192.93780
## x     25.55575 25.52742  1.603924  22.48439  28.79097
## > summary1
##           Mean   Median   St.Dev. 95%CI_low 95%CI_upp
## theta 65.68603 52.48918 48.535005   9.45394 188.91579
## x     25.52053 25.47460  1.613703  22.50450  28.75823
## > summary2
##           Mean   Median   St.Dev. 95%CI_low 95%CI_upp
## theta 74.55041 59.89222 54.540043  10.26444 213.55068
## x     25.54929 25.52478  1.613239  22.49440  28.78259






numPoints <- 10
lowerObsCoords <- matrix(c(0, 0, 1, 0, 0, 1, 1, 1), nrow = 4, byrow = TRUE)
upperObsCoords <- matrix(c(1, 1, 2, 1, 1, 2, 2, 2), nrow = 4, byrow = TRUE)
s <- c(1, 1)
windowIntensities <- c(1:4)
sd <- 0.1

set.seed(0)
stratRejectionSampler_normal(numPoints, lowerObsCoords, upperObsCoords, s, windowIntensities, sd)



library(nimbleSCR)
library(knitr)
library(rmarkdown)

setwd('~/github/nimble/nimbleSCR/nimbleSCR/vignettes')
f <- list.files()
f2 <- grep('\\.[Rr]md$', f, value = TRUE)
times <- numeric(length(f2))
names(times) <- f2

for(ixi in 1:length(f2)) {
    message(f2[ixi], ':')
    tm <- system.time(render(f2[ixi]))
    times[ixi] <- tm[3]
}

times



i <- 3
f2[i]
tm <- system.time(render(f2[i]))
tm


times
n
length(times)






f <- list.files('~/github/nimble/nimbleSCR/nimbleSCR/R')
f2 <- gsub('\\.R$', '', f)
f3 <- f2[!(f2 == 'zzz')]

##n <- 'aaaa'
n <- length(f3)
n  ## 37

i <- 37
(name <- f3[i])
cl <- parse(text = paste0('system.time(example(', name, '))'))[[1]]; message(n, ':'); eval(cl)




1
remove.packages('R6')
library(R6)





remove.packages('nimble')
remove.packages('nimbleHMC')

1


library(nimble)   ## no version of nimble installed
## Error in library(nimble) : there is no package called â€˜nimbleâ€™
nimbleOptions('buildDerivs')

library(devtools)
install_github("nimble-dev/nimble", ref = "AD-rc0", subdir = "packages/nimble")

nimble:::mcmc_createRmodelObject

library(nimbleHMC)

install_github("nimble-dev/nimbleHMC", subdir = "nimbleHMC")

library(nimbleHMC)





code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:10) {
        x[i] ~ dnorm(mu, sd = sigma)
    }
})

data <- list(x = c(2, 5, 3, 4, 1, 0, 1, 3, 5, 3))

inits <- function() list(mu = rnorm(1,0,1), sigma = runif(1,0,10))

##debug(nimbleHMC)

mcmc.output <- nimbleHMC(code, data = data, inits = inits,
                         monitors = c("mu", "sigma"), thin = 10,
                         niter = 20000, nburnin = 1000, nchains = 3,
                         summary = TRUE, WAIC = TRUE)

nimbleHMC
buildHMC
configureHMC



constants <- list()

set.seed(0)
initsList <- inits()
Rmodel <- nimbleModel(code, constants, data, initsList, buildDerivs = TRUE)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
addHMC
nodes <- c('mu', 'sigma')

##undebug(addHMC)
addHMC(conf, nodes)

conf$addSampler(target = nodes, type = "HMC", print = TRUE)


conf$printSamplers()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)
samplesPlot(samples)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()

stochVars <- unique(nimble:::removeIndexing(Rmodel$getNodeNames(stochOnly = TRUE)))
for(v in stochVars) {
    lp <- Rmodel$calculate(v)
    cat(v, ': ', lp, '\n')
}





my_sampler <- nimbleFunction(
    contains = sampler_BASE,
    setup = function(...) {
        ...
        matrixInitializedFlag <- 0
        matrix <- array(0, c(N, M))
    }
    run = function() {
        ## when the matrix is first used:
        if(matrixInitializedFlag == 0) {
            matrixInitializedFlag <<- 1     ## use double assignment operator <<-
            ## initialize matrix here       ## also use <<- here to assign values into the matrix
            ...
        }
        ...
    }
    methods = list(
        reset = funtion() {
            ...
            matrixInitializedFlag <<- 0     ## use <<- here
        }
    )
)


a <- 1
b <- quote({print(1); 4})
b
eval(b)

if(a && eval(b)) {
    print('yes')
} else {
    print('no')
}


library(nimble)
library(testthat)
##source('~/github/nimble/nimble/packages/nimble/tests/testthat/test_utils.R')

## verbose: set to FALSE
nimbleVerboseSetting <- nimbleOptions('verbose')
nimbleOptions(verbose = FALSE)

## MCMC progress bar: set to FALSE
nimbleProgressBarSetting <- nimbleOptions('MCMCprogressBar')
nimbleOptions(MCMCprogressBar = FALSE)
## MCMC use MCMCreorderSamplersPosteriorPredLast sampler - save current setting
nimbleReorderPPsamplersSetting <- getNimbleOption('MCMCreorderSamplersPosteriorPredLast')
## MCMC use posteriorPredictiveBranch sampler - save current setting
nimblePPBranchSamplerSetting <- getNimbleOption('MCMCjointlySamplePredictiveBranches')
## MCMC calculation include predictive dependencies - save current setting
nimbleIncludePredDependenciesSetting <- nimbleOptions('MCMCincludePredictiveDependencies')




nimbleOptions(MCMCjointlySamplePredictiveBranches = FALSE)

nimbleOptions(MCMCjointlySamplePredictiveBranches = nimblePPBranchSamplerSetting)




RwarnLevel <- options('warn')$warn
options(warn = 1)
nimbleVerboseSetting <- nimbleOptions('verbose')
nimbleOptions(verbose = FALSE)
nimblePPBranchSamplerSetting <- getNimbleOption('MCMCjointlySamplePredictiveBranches')
nimbleOptions(MCMCjointlySamplePredictiveBranches = FALSE)
nimbleIncludePredDependenciesSetting <- nimbleOptions('MCMCincludePredictiveDependencies')



code <- nimbleCode({
    mu ~ dnorm(0, 1)
    pp ~ dnorm(mu, 1)
    for(i in 1:5)
        x[i] ~ dexp(mu^2+1)
    for(i in 1:4)
        y[i] ~ dnorm(x[i], 1)
})
constants <- list()
data <- list(y = c(1, 2, 3, NA))
inits <- list(mu = 0, x = rep(1, 5), y = c(NA, NA, NA, 4), pp = 0)
Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)

pppp

## configureMCMC should already have reordered PP samplers last (expect_true):
samplerNames <- sapply(conf$samplerConfs, `[[`, 'name')
ppSamplerInd <- which(grepl('^posterior_predictive', samplerNames))
otherSamplerInd <- which(!grepl('^posterior_predictive', samplerNames))
expect_true(all(sapply(ppSamplerInd, function(ind) all(ind > otherSamplerInd))))
sapply(ppSamplerInd, function(ind) all(ind > otherSamplerInd))
ppSamplerInd
otherSamplerInd

[1] RW sampler: mu
[2] RW sampler: x[1]
[3] RW sampler: x[2]
[4] RW sampler: x[3]
[5] posterior_predictive sampler: pp
[6] RW sampler: x[4]
[7] posterior_predictive sampler: x[5]
[8] posterior_predictive sampler: y[4]



model <- Rmodel
m <- m
m <- Rmodel

m$getNodeNames(stochOnly = TRUE)
m$getNodeNames(stochOnly = TRUE, includeData = FALSE)
m$getNodeNames(posteriorPredOnly = TRUE)
m$getNodeNames(posteriorPredBranchOnly = TRUE)

nodes <- m$getNodeNames(stochOnly = TRUE, includeData = FALSE)
nodes
m$topologicallySortNodes(nodes)
nodes == m$topologicallySortNodes(nodes)

ppNodes <- m$getNodeNames(posteriorPredOnly = TRUE)
ppNodes

nodes %in% ppNodes


    expect_true(all(sapply(conf$getSamplers(), function(x) x$name) ==
                    c('conjugate_dgamma_dnorm_identity',
                      rep('conjugate_dnorm_dnorm_identity_dnorm_multiplicative', 8),
                      'posterior_predictive',
                      'posterior_predictive_branch')))


code <- nimbleCode({
    mu ~ dnorm(0, 1)
    pp ~ dnorm(mu, 1)
    for(i in 1:5)
        x[i] ~ dexp(mu^2+1)
    for(i in 1:4)
        y[i] ~ dnorm(x[i], 1)
})
constants <- list()
data <- list(y = c(1, 2, 3, NA))
inits <- list(mu = 0, x = rep(1, 5), y = c(NA, NA, NA, 4), pp = 0)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()

conf <- configureMCMC(Rmodel)
pppp

conf$addSampler('mu', 'slice')
conf$addSampler('x[1]', 'slice')
conf$addSampler(c('x[1]', 'mu'), 'RW_block')

conf$addSampler(c('x[1]', 'mu'), 'posterior_predictive_branch')
conf$addSampler(c('x[2]', 'mu'), 'sampler_posterior_predictive_branch')


unlist(lapply(conf$samplerConfs, `[[`, 'target'))

Rmcmc <- buildMCMC(conf)

samplerNames <- sapply(conf$samplerConfs, `[[`, 'name')
ppSamplerInd <- which(grepl('^posterior_predictive', samplerNames))
otherSamplerInd <- which(!grepl('^posterior_predictive', samplerNames))



samplerNames <- sapply(conf$samplerConfs, `[[`, 'name')
ppBranchSamplerInd <- which(samplerNames == 'posterior_predictive_branch')
unlist(lapply(conf$samplerConfs[ppBranchSamplerInd], `[[`, 'target'))


nfDef <- nimbleFunction(
    setup = function() {
        a <- 1
        text <- 'abc'
    },
    run = function() {
        print(a)
    },
    methods = list(
        printText = function() {
            print(text)
        },
        stopText = function() {
            stop(text)
        },
        setText = function(b = double()) {
            if(b==1) text <<- 'a'
            if(b==2) text <<- 'b'
            if(b==3) text <<- 'c'
            if(b==4) text <<- 'd'
            if(b==5) text <<- 'e'
        }
    )
)

Rnf <- nfDef()
Rnf$run()
Rnf$printText()
Rnf$setText()
Rnf$printText()
Rnf$stopText()

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)
Cnf$run()
Cnf$printText()

Cnf$setText(3.3)
Cnf$setText(4)
Cnf$printText()
Cnf$stopText()






conf$samplerExecutionOrder == 1:max(conf$samplerExecutionOrder)

oo <- c(3,4,5,6)
oo == 1:max(oo)
all(oo == min(oo):max(oo))


exOrder <- numeric()

min(exOrder) == 1
exOrder == 1:max(exOrder)
all(exOrder == 1:max(exOrder))
(min(exOrder) == 1) && all(exOrder == 1:max(exOrder))

library(nimble)

code <- nimbleCode({
    mu[1] <- 10
    mu[2] <- 20
    mu[3] <- 30
    x[1:3] ~ dmnorm(mu[1:3], prec = Q[1:3,1:3])
})

Q = matrix(c(1.0,0.2,-1.0,0.2,4.04,1.6,-1.0,1.6,10.81), nrow=3)
data = list(Q = Q)
inits = list(x = c(10, 20, 30))
constants <- list()

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel, nodes = NULL)

Rmcmc <- buildMCMC(conf)
Rmcmc$samplerFunctions

conf$addSampler('x[1:3]', 'RW_block')

model <- Rmodel
target <- 'x[1:3]'

## node list generation
(targetAsScalar <- model$expandNodeNames(target, returnScalarComponents = TRUE))
ccLst <- nimble:::mcmc_determineCalcAndCopyNodes(model, target)
ccLst
calcNodes <- ccLst$calcNodes; calcNodesPPskipped <- ccLst$calcNodesPPskipped; copyNodesDeterm <- ccLst$copyNodesDeterm; copyNodesStoch <- ccLst$copyNodesStoch   # not used: calcNodesNoSelf
finalTargetIndex <- max(match(model$expandNodeNames(target), calcNodes))
if(!is.integer(finalTargetIndex) | length(finalTargetIndex) != 1 | is.na(finalTargetIndex[1]))   stop('problem with target node in RW_block sampler')
calcNodesProposalStage <- calcNodes[1:finalTargetIndex]
calcNodesDepStage <- calcNodes[-(1:finalTargetIndex)]
## numeric value generation


Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)


test_mcmc(model = code, name = 'block sampler on multivariate node', data = data, seed = 0, numItsC = 10000,
          results = list(mean = list(x = c(10,20,30)),
                         var = list(x = diag(solve(Q)))),
          resultsTolerance = list(mean = list(x = rep(1,3)),
                                  var = list(x = c(.1, .03, .01))),
          samplers = list(
              list(type = 'RW_block', target = 'x[1:3]')), avoidNestedTest = TRUE)
# caution: setting targetNodes='x' works but the initial end sampler is not removed because x[1:3] in targetNode in default sampler != 'x' in targetNodes passed in




library(nimble)
library(testthat)

includePredDepOptionSave <- nimbleOptions('MCMCincludePredictiveDependencies')

niter <- 200000
nburnin <- 50000

code <- nimbleCode({
    ## binary sampler:
    x[1] ~ dbern(0.4)
    ## categorical sampler:
    x[2] ~ dcat(p[1:5])
    ## RW sampler:
    x[3] ~ dgamma(1, 1)
    ## RW_block sampler:
    x[4:5] ~ dmnorm(mu[1:2], Sigma[1:2,1:2])
    ## slice sampler:
    x[6] ~ dbinom(size = 4, prob = 0.5)
    ## ess sampler:
    x[7:8] ~ dmnorm(mu[1:2], Sigma[1:2,1:2])
    ## AF_slice sampler:
    x[9:10] ~ dmnorm(mu[1:2], Sigma[1:2,1:2])
    ## RW_dirichlet sampler:
    x[11:13] ~ ddirch(a[1:3])
    ## RW_wishart sampler:
    w[1:2,1:2] ~ dwish(Sigma[1:2,1:2], 2)
    ## data and predictive nodes:
    for(i in 1:N) {
        y[i] ~ dnorm(x[i], 1)
        yp[i] ~ dnorm(x[i], 1)
    }
    for(i in 1:2) {
        w_data[i] ~ dnorm(w[1,i], 1)
        wp_data[i] ~ dnorm(w[1,i], 1)
    }
})

N <- 13
constants <- list(
    N = N,
    p = rep(0.2, 5),
    mu = c(0,0),
    Sigma = diag(2),
    a = c(1,2,3)
)
data <- list(
    y = rep(0, N),
    w_data = rep(0,2)
)
inits <- list(
    x = c(0,1,1, 0,0, 0, 0,0, 0,0, 1/3,1/3,1/3),
    w = diag(2),
    yp = rep(0, N),
    wp_data = rep(0,2)
)


## include PP nodes in all sampler calculations:
nimbleOptions(MCMCincludePredictiveDependencies = TRUE)

Rmodel <- nimbleModel(code, constants, data, inits)
lp <- Rmodel$calculate()
expect_equal(lp, -45.04049)
##
conf <- configureMCMC(Rmodel)
conf$removeSampler('x[7:8]')
conf$addSampler('x[7:8]', 'ess')
conf$removeSampler('x[9:10]')
conf$addSampler('x[9:10]', 'AF_slice')
##
Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

set.seed(0)
samplesT <- runMCMC(Cmcmc, niter, nburnin)



## now exclude all PP nodes:
nimbleOptions(MCMCincludePredictiveDependencies = FALSE)

Rmodel <- nimbleModel(code, constants, data, inits)
lp <- Rmodel$calculate()
expect_equal(lp, -45.04049)
##
conf <- configureMCMC(Rmodel)
conf$removeSampler('x[7:8]')
conf$addSampler('x[7:8]', 'ess')
conf$removeSampler('x[9:10]')
conf$addSampler('x[9:10]', 'AF_slice')
##
Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

set.seed(0)
samplesF <- runMCMC(Cmcmc, niter, nburnin)

expect_true(all(abs(as.numeric(apply(samplesT, 2, mean)) - as.numeric(apply(samplesF, 2, mean))) < 0.03))
expect_true(all(abs(as.numeric(apply(samplesT, 2, sd  )) - as.numeric(apply(samplesF, 2, sd  ))) < 0.05))



nimbleOptions(MCMCincludePredictiveDependencies = includePredDepOptionSave)





i <- 1
conf$samplerConfs[[i]]$name
conf$samplerConfs[[i]]$target
##
Rmcmc$samplerFunctions[[i]]$calcNodes
Rmcmc$samplerFunctions[[i]]$calcNodesNoSelf
Rmcmc$samplerFunctions[[i]]$calcNodesPPskipped
Rmcmc$samplerFunctions[[i]]$copyNodesDeterm
Rmcmc$samplerFunctions[[i]]$copyNodesStoch
##
Rmcmc$samplerFunctions[[i]]$calcNodesProposalStage
Rmcmc$samplerFunctions[[i]]$calcNodesDepStage
Rmcmc$samplerFunctions[[i]]$calcNodesPPskipped
Rmcmc$samplerFunctions[[i]]$copyNodesDeterm
Rmcmc$samplerFunctions[[i]]$copyNodesStoch




includePredDepOptionSave <- nimbleOptions('MCMCincludePredictiveDependencies')
nimbleOptions(MCMCincludePredictiveDependencies = TRUE)
nimbleOptions(MCMCincludePredictiveDependencies = includePredDepOptionSave)





library(nimble)



nimbleOptions('MCMCincludePredictiveDependencies')
nimbleOptions(MCMCincludePredictiveDependencies = FALSE)
nimbleOptions(MCMCincludePredictiveDependencies = TRUE)
nimbleOptions('MCMCincludePredictiveDependencies')


code <- nimbleCode({
    mu ~ dbern(0.4)
    muPlus1 <- mu + 1
    for(i in 1:2) {
        y[i] ~ dnorm(muPlus1, sd=10)
    }
    pp ~ dnorm(muPlus1, sd=10)
})

constants <- list()
data <- list(y = c(1,2))
inits <- list(mu = 0, pp = 0)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()

conf <- configureMCMC(Rmodel, monitors = c('mu', 'pp'))
conf$printSamplers()

Rmcmc <- buildMCMC(conf)

i <- 1
Rmcmc$samplerFunctions[[1]]$calcNodes
Rmcmc$samplerFunctions[[1]]$calcNodesNoSelf
Rmcmc$samplerFunctions[[1]]$calcNodesPPskipped
Rmcmc$samplerFunctions[[1]]$copyNodesDeterm
Rmcmc$samplerFunctions[[1]]$copyNodesStoch


compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc


if(FALSE) {
    model <- Rmodel
    mcmc <- Rmcmc
    model <- Cmodel
    mcmc <- Cmcmc
}

if(FALSE) {
    debug(Rmcmc$samplerFunctions[[1]]$run)
    undebug(Rmcmc$samplerFunctions[[1]]$run)
}


set.seed(0)
samples <- runMCMC(Cmcmc, 1000)

samplesRT <- samples
samplesRF <- samples
samplesCF <- samples
samplesCT <- samples

all(samplesRT - samplesCT == 0)
all(samplesRF - samplesCF == 0)

samplesT <- samplesCT
samplesF <- samplesCF

samplesT[, 'mu'] - samplesF[, 'mu']
table(samplesT[, 'mu'] - samplesF[, 'mu'])

samplesT[, 'pp'] - samplesF[, 'pp']
samplesT[, 'pp']
samplesF[, 'pp']
table(samplesT[, 'pp'] - samplesF[, 'pp'])



set.seed(0)
mcmc$run(1)

mcmc$run(1, reset = FALSE)

(mvSaved <- mcmc$mvSaved)
(samples <- as.matrix(mcmc$mvSamples))

cbind(model$mu - as.numeric(mvSaved[['mu']]))

lp1 <- model$logProb_mu
lp2 <- as.numeric(mvSaved[['logProb_mu']])
lp3 <- dbinom(model$mu, size = 1, prob = 0.4, log = TRUE)
cbind(lp1 - lp2, lp1 - lp3)

cbind(model$y - if(is.list(mvSaved[['y']])) mvSaved[['y']][[1]] else mvSaved[['y']])

lp1 <- model$logProb_y
lp2 <- if(is.list(mvSaved[['logProb_y']])) mvSaved[['logProb_y']][[1]] else mvSaved[['logProb_y']]
lp3 <- dnorm(model$y, model$muPlus1, 10, log = TRUE)
cbind(lp1 - lp2, lp1 - lp3)
cbind(model$pp - as.numeric(mvSaved[['pp']]))

lp1 <- model$logProb_pp
lp2 <- as.numeric(mvSaved[['logProb_pp']])
lp3 <- dnorm(model$pp, model$muPlus1, sd=10, log = TRUE)
cbind(lp1 - lp2, lp1 - lp3)






model$logProb_pp
as.numeric(mvSaved[['logProb_pp']])
dnorm(model$pp, model$muPlus1, sd=10, log = TRUE)




code <- nimbleCode({
    a ~ dnorm(0, 1)
    for(i in 1:8) {
        b[i] ~ dnorm(a, 1)
        c[i] ~ dnorm(b[i], 1)
        d[i] ~ dnorm(c[i], 1)
    }
})

Rmodel <- nimbleModel(code)

expect_identical(Rmodel$getPostPredNodeIDs(), as.numeric(1:length(Rmodel$getNodeNames())))
expect_identical(Rmodel$getPostPredBranchNodeIDs(), 1)

expect_identical(Rmodel$getNodeNames(includePosteriorPred = FALSE), character())
expect_identical(Rmodel$getNodeNames(includePosteriorPredBranch = FALSE), Rmodel$expandNodeNames(c('b', 'c', 'd')))
expect_identical(Rmodel$getNodeNames(posteriorPredOnly = TRUE), Rmodel$getNodeNames())
expect_identical(Rmodel$getNodeNames(posteriorPredBranchOnly = TRUE), 'a')

Rmodel$resetData()
Rmodel$setData(list(b=1:8, c=1:8, d=1:8))

expect_identical(Rmodel$getPostPredNodeIDs(), numeric())
expect_identical(Rmodel$getPostPredBranchNodeIDs(), numeric())

expect_identical(Rmodel$getNodeNames(includePosteriorPred = FALSE), Rmodel$getNodeNames())
expect_identical(Rmodel$getNodeNames(includePosteriorPredBranch = FALSE), Rmodel$getNodeNames())
expect_identical(Rmodel$getNodeNames(posteriorPredOnly = TRUE), character())
expect_identical(Rmodel$getNodeNames(posteriorPredBranchOnly = TRUE), character())

Rmodel$resetData()
Rmodel$setData(list(
           b = rep(c(0, NA), each = 4),
           c = rep(c(0, NA, 0, NA), each = 2),
           d = rep(c(0, NA), 4)))

expect_identical(Rmodel$getPostPredNodeIDs(), c(9, 13, 17, 19, 21, 23, 25))
expect_identical(Rmodel$getPostPredBranchNodeIDs(), c(9, 13))

expect_identical(Rmodel$getNodeNames(includePosteriorPred = FALSE),
                 setdiff(Rmodel$getNodeNames(), Rmodel$modelDef$maps$graphID_2_nodeName[Rmodel$getPostPredNodeIDs()]))
expect_identical(Rmodel$getNodeNames(includePosteriorPredBranch = FALSE),
                 setdiff(Rmodel$getNodeNames(), Rmodel$modelDef$maps$graphID_2_nodeName[Rmodel$getPostPredBranchNodeIDs()]))
expect_identical(Rmodel$getNodeNames(posteriorPredOnly = TRUE),
                 Rmodel$modelDef$maps$graphID_2_nodeName[Rmodel$getPostPredNodeIDs()])
expect_identical(Rmodel$getNodeNames(posteriorPredBranchOnly = TRUE),
                 Rmodel$modelDef$maps$graphID_2_nodeName[Rmodel$getPostPredBranchNodeIDs()])

expect_identical(Rmodel$getDependencies('a', includePosteriorPred = FALSE),
                 Rmodel$expandNodeNames(c('a', 'b[1:7]')))
expect_identical(Rmodel$getDependencies('a', includePosteriorPredBranch = FALSE),
                 Rmodel$expandNodeNames(c('a', 'b[1:7]')))
expect_identical(Rmodel$getDependencies('a', posteriorPredOnly = TRUE),
                 'b[8]')
expect_identical(Rmodel$getDependencies('a', posteriorPredBranchOnly = TRUE),
                 'b[8]')

expect_identical(Rmodel$getDependencies('b', includePosteriorPred = FALSE),
                 Rmodel$expandNodeNames(c('b[1:7]', 'c[1:3]', 'c[5:7]')))
expect_identical(Rmodel$getDependencies('b', includePosteriorPredBranch = FALSE),
                 Rmodel$expandNodeNames(c('b[1:7]', 'c[1:3]', 'c[5:8]')))
expect_identical(Rmodel$getDependencies('b', posteriorPredOnly = TRUE),
                 c('b[8]', 'c[4]', 'c[8]'))
expect_identical(Rmodel$getDependencies('b', posteriorPredBranchOnly = TRUE),
                 c('b[8]', 'c[4]'))

expect_identical(Rmodel$getDependencies('c', includePosteriorPred = FALSE),
                 Rmodel$expandNodeNames(c('c[1:3]', 'c[5:7]', 'd[1]', 'd[3]', 'd[5]', 'd[7]')))
expect_identical(Rmodel$getDependencies('c', includePosteriorPredBranch = FALSE),
                 Rmodel$expandNodeNames(c('c[1:3]', 'c[5:8]', 'd[1:8]')))
expect_identical(Rmodel$getDependencies('c', posteriorPredOnly = TRUE),
                 c('c[4]', 'c[8]', 'd[2]', 'd[4]', 'd[6]', 'd[8]'))
expect_identical(Rmodel$getDependencies('c', posteriorPredBranchOnly = TRUE),
                 c('c[4]'))

expect_identical(Rmodel$getDependencies('d', includePosteriorPred = FALSE),
                 c('d[1]', 'd[3]', 'd[5]', 'd[7]'))
expect_identical(Rmodel$getDependencies('d', includePosteriorPredBranch = FALSE),
                 Rmodel$expandNodeNames('d[1:8]'))
expect_identical(Rmodel$getDependencies('d', posteriorPredOnly = TRUE),
                 c('d[2]', 'd[4]', 'd[6]', 'd[8]'))
expect_identical(Rmodel$getDependencies('d', posteriorPredBranchOnly = TRUE),
                 character())

node <- 'a'
node <- 'c[2]'
node <- 'b[8]'
Rmodel$getDependencies(node)
Rmodel$getDependencies(node, includePosteriorPred = FALSE)
Rmodel$getDependencies(node, includePosteriorPredBranch = FALSE)


Rmodel$isData('a')
Rmodel$isData('b')
Rmodel$isData('c')
Rmodel$isData('d')


Rmodel$getNodeNames(includePosteriorPred = FALSE)

f <- function(x) Rmodel$modelDef$maps$graphID_2_nodeName[x]
Rmodel$modelDef$maps$graphID_2_nodeName[Rmodel$getPostPredNodeIDs()]
Rmodel$modelDef$maps$graphID_2_nodeName[Rmodel$getPostPredBranchNodeIDs()]


Rmodel$modelDef$maps$graphID_2_nodeName[thisCandNodeID]
Rmodel$modelDef$maps$graphID_2_nodeName[stochDownstreamNoSelfIDs]
posteriorPredictiveBranchNodeIDs

posteriorPredictiveNodeIDs <- numeric()
posteriorPredictiveBranchNodeIDs <- numeric()
stochNonDataIDs <- Rmodel$getNodeNames(stochOnly = TRUE, includeData = FALSE, returnType = 'ids')
anyPPnodes <- any(Rmodel$isEndNode(stochNonDataIDs))
if(anyPPnodes) {
    ## all potential (candidate) posterior predictive branch nodes:
    candidateBranchNodeIDs <- stochNonDataIDs[!Rmodel$isEndNode(stochNonDataIDs)]
    dataNodeIDs <- Rmodel$getNodeNames(dataOnly = TRUE, returnType = 'ids')
    dataNodeParentIDs <- Rmodel$expandNodeNames(Rmodel$getParents(dataNodeIDs, stochOnly = TRUE), returnType = 'ids')
    ## remove from candidate branch nodes all direct parents of data nodes:
    candidateBranchNodeIDs <- setdiff(candidateBranchNodeIDs, dataNodeParentIDs)
    nCandidate <- length(candidateBranchNodeIDs)
    nextCandInd <- 1
    while(nextCandInd <= nCandidate) {
        thisCandNodeID <- as.numeric(candidateBranchNodeIDs[nextCandInd])
        stochDownstreamNoSelfIDs <- Rmodel$getDependencies(thisCandNodeID, self = FALSE, stochOnly = TRUE, downstream = TRUE, returnType = 'ids')
        ## skip candidate nodes that have any downstream data nodes:
        if(length(intersect(stochDownstreamNoSelfIDs, dataNodeIDs)) > 0)   { nextCandInd <- nextCandInd + 1;   next }
        ## found posterior predictive branch node:
        posteriorPredictiveBranchNodeIDs <- c(posteriorPredictiveBranchNodeIDs, thisCandNodeID)
        ## everything downstream from (and including) branch node is a posterior predictive node:
        posteriorPredictiveNodeIDs <- c(posteriorPredictiveNodeIDs, thisCandNodeID, stochDownstreamNoSelfIDs)
        ## update candidateBranchNodeIDs, removing downstream stochastic dependencies of this branch node from the candidate set:
        candidateBranchNodeIDs <- candidateBranchNodeIDs[-(1:nextCandInd)]
        candidateBranchNodeIDs <- setdiff(candidateBranchNodeIDs, stochDownstreamNoSelfIDs)
        nCandidate <- length(candidateBranchNodeIDs)
        nextCandInd <- 1
    }
}
## set into the model object's fields:
postPredNodeIDs <<- sort(unique(posteriorPredictiveNodeIDs))       ## can contain duplicates
postPredBranchNodeIDs <<- posteriorPredictiveBranchNodeIDs





constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)



1

library(nimble)


## We conduct CV on the classic "dyes" BUGS model.

code <- dyesCode <- nimbleCode({
    for (i in 1:BATCHES) {
        for (j in 1:SAMPLES) {
            y[i,j] ~ dnorm(mu[i], tau.within);
        }
        mu[i] ~ dnorm(theta, tau.between);
    }
    ##
    theta ~ dnorm(0.0, 1.0E-10);
    tau.within ~ dgamma(0.001, 0.001);  sigma2.within <- 1/tau.within;
    tau.between ~ dgamma(0.001, 0.001);  sigma2.between <- 1/tau.between;
})

data <- dyesData <- list(y = matrix(c(1545, 1540, 1595, 1445, 1595,
                                      1520, 1440, 1555, 1550, 1440,
                                      1630, 1455, 1440, 1490, 1605,
                                      1595, 1515, 1450, 1520, 1560, 
                                      1510, 1465, 1635, 1480, 1580,
                                      1495, 1560, 1545, 1625, 1445), 
                                    nrow = 6, ncol = 5))

constants <- dyesConsts <- list(BATCHES = 6,
                                SAMPLES = 5)

inits <- dyesInits <- list(theta = 1500, tau.within = 1, tau.between =  1)

Rmodel <- dyesModel <- nimbleModel(code = dyesCode,
                                   constants = dyesConsts,
                                   data = dyesData,
                                   inits = dyesInits)

# Define the fold function.
# This function defines the data to leave out for the i'th fold
# as the i'th row of the data matrix y.  This implies we will have
# 6 folds.

ff <- dyesFoldFunction <- function(i){
    foldNodes_i <- paste0('y[', i, ', ]')  # will return 'y[1,]' for i = 1 e.g.
    return(foldNodes_i)
}




# We define our own loss function as well.
# The function below will compute the root mean squared error.

loss <- RMSElossFunction <- function(simulatedDataValues, actualDataValues){
    dataLength <- length(simulatedDataValues) # simulatedDataValues is a vector
    SSE <- 0
    for(i in 1:dataLength){
        SSE <- SSE + (simulatedDataValues[i] - actualDataValues[i])^2
    }
    MSE <- SSE / dataLength
    RMSE <- sqrt(MSE)
    return(RMSE)
}

conf <- dyesMCMCconfiguration <- configureMCMC(dyesModel)

pppp

debug(runCrossValidate)
debug(calcCrossVal)
debug(calcCrossValSD)



cv <- runCrossValidate(MCMCconfiguration = dyesMCMCconfiguration,
                       k = 6,
                       foldFunction = dyesFoldFunction,
                       lossFunction = RMSElossFunction,
                       MCMCcontrol = list(niter = 5000,
                                          nburnin = 500))


crossValOut1 <- calcCrossVal(1,
                             MCMCconfiguration,
                             foldFunction,
                             lossFunction,
                             niter,
                             nburnin,
                             returnSamples,
                             nBootReps,
                             FALSE,
                             silent)




1
setwd('~/github/nimble/nimbleHMC/joss/paper/')
f <- 'paper.md'
library(rmarkdown)
rmarkdown::render(f, output_format = 'pdf_document')
system('open paper.pdf')



1
df <- read.csv('~/Downloads/Lab Study Data.csv')

dim(df)
names(df)

grep('provider', names(df), value = TRUE)

head(df[,grep('provider', names(df), value = TRUE)], 20)

library(nimble)

code <- nimbleCode({
    logit(y) ~ dlogis(0, 1)
})

Rmodel <- nimbleModel(code)

Rmodel$getNodeNames()

ns <- ls(Rmodel$nodes)

i <- 2
ns[i]
Rmodel$nodes[[ns[i]]]$simulate
Rmodel$nodes[[ns[i]]]$calculate

x ~ T(dnorm(0, sd = 10), 0, a)

3106724
Lopetrone Goldflam, Luca Jaden
ljl3@williams.edu
Computer Science



3106794
Mortarelli, Milvano Graham (Mel)
mgm4@williams.edu
Computer Science



3107981
Leavy, Asher Morgan
aml19@williams.edu
Computer Science



x <- 1:50

case_when(
       x %% 35 == 0 ~ "fizz buzz",
       x %% 5 == 0 ~ "fizz",
       x %% 7 == 0 ~ "buzz",
       TRUE ~ as.character(x)
     )
     
     # Like an if statement, the arguments are evaluated in order, so you must
     # proceed from the most specific to the most general. This won't work:
     case_when(
       TRUE ~ as.character(x),
       x %%  5 == 0 ~ "fizz",
       x %%  7 == 0 ~ "buzz",
       x %% 35 == 0 ~ "fizz buzz"
     )
     
     # If none of the cases match, NA is used:
     case_when(
       x %%  5 == 0 ~ "fizz",
       x %%  7 == 0 ~ "buzz",
       x %% 35 == 0 ~ "fizz buzz"
     )
     
     # Note that NA values in the vector x do not get special treatment. If you want
     # to explicitly handle NA values you can use the `is.na` function:
     x[2:4] <- NA_real_
     case_when(
       x %% 35 == 0 ~ "fizz buzz",
       x %% 5 == 0 ~ "fizz",
       x %% 7 == 0 ~ "buzz",
       is.na(x) ~ "nope",
       TRUE ~ as.character(x)
     )
     
     # All RHS values need to be of the same type. Inconsistent types will throw an error.
     # This applies also to NA values used in RHS: NA is logical, use
     # typed values like NA_real_, NA_complex, NA_character_, NA_integer_ as appropriate.
     case_when(
       x %% 35 == 0 ~ NA_character_,
       x %% 5 == 0 ~ "fizz",
       x %% 7 == 0 ~ "buzz",
       TRUE ~ as.character(x)
     )
     case_when(
       x %% 35 == 0 ~ 35,
       x %% 5 == 0 ~ 5,
       x %% 7 == 0 ~ 7,
       TRUE ~ NA_real_
     )
     
     # case_when() evaluates all RHS expressions, and then constructs its
     # result by extracting the selected (via the LHS expressions) parts.
     # In particular NaN are produced in this case:
     y <- seq(-2, 2, by = .5)
     case_when(
       y >= 0 ~ sqrt(y),
       TRUE   ~ y
     )
     
     # This throws an error as NA is logical not numeric
     ## Not run:
     
     case_when(
       x %% 35 == 0 ~ 35,
       x %% 5 == 0 ~ 5,
       x %% 7 == 0 ~ 7,
       TRUE ~ NA
     )
     ## End(Not run)
     
     
     # case_when is particularly useful inside mutate when you want to
     # create a new variable that relies on a complex combination of existing
     # variables
     starwars %>%
       select(name:mass, gender, species) %>%
       mutate(
         type = case_when(
           height > 200 | mass > 200 ~ "large",
           species == "Droid"        ~ "robot",
           TRUE                      ~ "other"
         )
       )





library(nimble)
library(dplyr)
library(posterior)
library(bayesplot)

dlatentnorm <- nimbleFunction(
    run = function(
                   x = double(0), mean = double(0, default = 0), y = integer(0), log = integer(0, default = 0)
                   ) {
        returnType(double(0))
        if ( x < 0 & y == 1 ) {
            logProb <- -Inf
        } else if ( x > 0 & y == 0) {
            logProb <- -Inf
        } else {
            logProb <- dnorm(x, mean = mean, sd = 1, log = 1) - pnorm(0, mean = mean, sd = 1, lower.tail = (1-y), log.p = 1)
        }
        if ( log )
            return(logProb)
        return(exp(logProb))
    }
)
platentnorm <- nimbleFunction(
    run = function(
                   q = double(0), mean = double(0, default = 0), y = integer(0)
                 , lower.tail = integer(0, default = 1)
                 , log.p = integer(0, default = 0)
                   ) {
        returnType(double(0))
        if ( y == 0 ) {
            if ( q >= 0 ) {
                logProb <- Inf
            } else {
                logProb <- pnorm(q, mean, 1, log.p = 1) - pnorm(0, mean, 1, log.p = 1)
            }
        } else {
            if ( q < 0 ) {
                logProb <- -Inf
            } else {
                lowerProb <- pnorm(0, mean = mean, sd = 1)
                logProb <- log( pnorm(q, mean) - lowerProb ) - log(1 - lowerProb)
            }
        }
        if ( lower.tail ) {
            if ( log.p )
                return(logProb)
            else
                return(exp(logProb))
        } else {
            prob <- 1 - exp(logProb)
            if ( log.p ) {
                return(log(prob))
            } else {
                return(prob)

            }
        }
        return(0) ## never reached
    }
)

qlatentnorm = nimbleFunction(
    run = function(
                   p = double(0), mean = double(0, default = 0), y = integer(0)
                 , lower.tail = integer(0, default = 1)
                 , log.p = integer(0, default = 0)
                   ) {
        returnType(double(0))
        if (log.p)
            p <- exp(p)
        if (!lower.tail)
            p <- 1 - p
        if ( y == 0 ) {
            logp <- log(p) + pnorm(0, mean = mean, log.p = 1)
            return( qnorm(logp, mean = mean, log.p = 1) )
        } else {
            lowProb <- pnorm(0, mean = mean)
            return( qnorm( p + lowProb * (1 - p), mean = mean ) )
        }
        return(0)  ## never reached
    }
)

rlatentnorm = nimbleFunction(
    run = function(
                   n = integer(0), mean = double(0, default = 0), y = integer(0)
                   ) {
        returnType(double(0))
        return( qlatentnorm(runif(1), mean = mean, y = y) )
    }
)

cdlatentnorm <- compileNimble(dlatentnorm)
cplatentnorm <- compileNimble(platentnorm)
cqlatentnorm <- compileNimble(qlatentnorm)
crlatentnorm <- compileNimble(rlatentnorm)


latentNormalSampler <- nimbleFunction(
    name = 'latentNormalSampler',
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        ## Get mean parameters
        means <- model$getParents(target)
        ## Get corresponding binary variables
        ybin <- model$getDependencies(target, stochOnly = TRUE, self = FALSE)
        ## Get number of latent variables to update
        nupdates <- length(ybin)
    },
    run = function() {
        ## Empty vector for draws
        draw <- rep(0, nupdates)
        for ( i in 1:nupdates ) {
            mean_i  <- values(model, means[i])[1]
            ybin_i  <- values(model, ybin[i])[1]
            draw[i] <- rlatentnorm(1, mean_i, ybin_i)
        }
        model[[target]] <<- draw
        model$calculate(target)
        nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
        
    },
    methods = list(
        reset = function() { }
    )
)

code <- nimbleCode({
    for ( i in 1:n ) {
        mean[i]  <- beta0 + beta1 * x[i]
        ystar[i] ~ dnorm(mean[i], sd = 1)
        y[i]     ~ dinterval(ystar[i], 0)
    }
    beta0 ~ dnorm(0, sd = 10)
    beta1 ~ dnorm(0, sd = 10)
})

## Simulate data
n     = 100
x     = rnorm(n, sd = 2)
ystar = rnorm(n, -1 + x, sd = 1)
y     = ifelse(ystar < 0, 0, 1)

## Run nimble code
const  <- list('n' = n)
data   <- list('y' = y, 'x' = x)
init   <- list('beta0' = 0, 'beta1' = 0, 'ystar' = rnorm(const$n, ifelse(data$y == 0, -1, 1), sd = 0.2) )
model  <- nimbleModel(code, constants = const, data = data, inits = init)
cmodel <- compileNimble(model)
conf   <- configureMCMC(model, monitors = c('beta0', 'beta1'))
conf$removeSampler('ystar')
conf$addSampler('ystar', 'latentNormalSampler')
mcmc   <- buildMCMC(conf)
cmcmc  <- compileNimble(mcmc, project = model)
smpl   <- runMCMC(cmcmc)
summ   <- smpl %>% summarize_draws()
summ
summary(glm(y ~ x, family = binomial('probit')))






s <- c(4,3,1,2)
identical(sort(s), as.numeric(1:length(s)))

longestLoop <- function(s) {
    s <- as.numeric(s)
    if(!identical(sort(s), as.numeric(1:length(s)))) stop()
    visited <- rep(FALSE, length(s))
    longest <- 0
    while(any(!visited)) {
        start <- which(!visited)[1]
        current <- start
        thisLength <- 1
        visited[current] <- TRUE
        while(s[current] != start) {
            current <- s[current]
            thisLength <- thisLength + 1
            visited[current] <- TRUE
        }
        if(thisLength > longest)   longest <- thisLength
    }
    return(longest)
}



nreps <- 100000
n <- 100

1 - mean(replicate(nreps, longestLoop(sample(1:n))) > (n/2))







#simulate data
require(mgcv)
set.seed(2) 
n <- 400
dat <- gamSim(1,n=n,dist="normal",scale=2)

#get spline code
jd <- jagam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,
            file="jagam.txt",
            sp.prior="gamma")

#first fit using jags as standard
library(rjags)
library(jagsUI)

out <- jags(data = jd$jags.data,
            parameters.to.save = c("mu"),
            model.file = "jagam.txt",
            n.thin=2, n.chains=3, n.burnin=1000,n.iter=2000,
            parallel=T)
#works ok

#also using NIMBLE
library(nimble)
library(igraph)
library(coda)

#sort data
nimbleConstants <- jd$jags.data[c("n","zero")]
nimbleData <- jd$jags.data[c("y","X","S1","S2","S3","S4")]

#model code - using jagam.txt above - except adding in indices
mynimbleCode <- nimbleCode({
    
    for (i in 1:n) {
        y[i] ~ dnorm(mu[i],tau)
        ##mu[i] <- X[i,1:37] * b[1:37]
        mu[i] <- sum(X[i,1:37] * b[1:37])
    }
    tau ~ dgamma(.05,.005)
    
    ## Parametric effect
    for (i in 1:1) {
        b[i] ~ dnorm(0,0.01)
    }
    
    ## prior for s(x0)...
    K1[1:9,1:9] <- S1[1:9,1:9] * lambda[1]  + S1[1:9,10:18] * lambda[2]
    b[2:10] ~ dmnorm(zero[2:10],K1[1:9,1:9])
    ## prior for s(x1)...
    K2[1:9,1:9] <- S2[1:9,1:9] * lambda[3]  + S2[1:9,10:18] * lambda[4]
    b[11:19] ~ dmnorm(zero[11:19],K2[1:9,1:9])
    ## prior for s(x2)...
    K3[1:9,1:9] <- S3[1:9,1:9] * lambda[5]  + S3[1:9,10:18] * lambda[6]
    b[20:28] ~ dmnorm(zero[20:28],K3[1:9,1:9])
    ## prior for s(x3)...
    K4[1:9,1:9] <- S4[1:9,1:9] * lambda[7]  + S4[1:9,10:18] * lambda[8]
    b[29:37] ~ dmnorm(zero[29:37],K4[1:9,1:9])
    
    ## smoothing parameter priors
    for (i in 1:8) {
        lambda[i] ~ dgamma(.05,.005)
        rho[i] <- log(lambda[i])
    }
    
})

nimbleInits <- list(lambda = rep(1, 8))

#fit model
Rmodel <- nimbleModel(mynimbleCode, constants = nimbleConstants)
Rmodel <- nimbleModel(mynimbleCode, constants = nimbleConstants, data = nimbleData, inits = nimbleInits)
#error here:
#Error in chol.default(model$K1[1:9, 1:9]) :
#  the leading minor of order 1 is not positive definite

Rmodel$calculate()
Rmodel$initializeInfo()


nimbleData$S1[1:9,1:9]



            
## I am trying to define a model that uses all available covariates at each time period. For example in time period t=1, there are two available and complete variables, x1 and x2. In time period t=2, x1, x2, and x3 are available.
## I am trying to use the following model to accomplish this (only the highlighted bits are relevant to the issue):


library(nimble)



log(lambda[((t-1)*S+s)]) <-
    equals(t,1) * (off[((t-1)*S+s)] + StInt[states[s]] + mu[t] + inprod(X1[((t-1)*S+s),],beta1[]) + u[s]) + 
    (1 - equals(t,1)) * (log(lambda[((t-1)*S+s)]) <- off[((t-1)*S+s)] + StInt[states[s]] + mu[t] + inprod(X2[((t-1)*S+s),],beta2[]) + u[s])

code <- model_code <- nimbleCode({
    for(t in 1:T){ # Loop through time periods
        for(s in 1:S){ # Loop through different areas
            y[((t-1)*S+s)] ~ dZIP(lambda[((t-1)*S+s)],zeroProb=p[s])
            ##
            ##
            if (t==1) {
                log(lambda[((t-1)*S+s)]) <- off[((t-1)*S+s)] + StInt[states[s]] + mu[t] + inprod(X1[((t-1)*S+s),],beta1[]) + u[s]
            } else {
                log(lambda[((t-1)*S+s)]) <- off[((t-1)*S+s)] + StInt[states[s]] + mu[t] + inprod(X2[((t-1)*S+s),],beta2[]) + u[s]
            }
            ##
        }
        ##
        mu[t] ~ dflat()
        ##
    }
    #### define spatial random effect with exponential cov function
    cov[1:S, 1:S] <- expcov(dists[1:S, 1:S], rho, sigma)
    u[1:S] ~ dmnorm(zeros[1:S], cov = cov[1:S, 1:S])
    ##
    #### define prior distributions
    for(i in 1:bp1){
        beta1[i] ~ dflat()
    }
    for(i in 1:bp2){
        beta2[i] ~ dflat()
    }
    for(i in 1:numStates) {
        StInt[i] ~ dflat()
    }
    rho ~ dunif(0,5)
    sigma ~ dunif(0, 100)
    for (s in 1:S) {
        p[s] ~ dunif(0,1)
    }
})


constants <- list(S = n, T=T, dists = dist.mat, zeros = rep(0, n),
                  X1 = X1, X2 = X2, off=log(Pop),bp1=dim(X1)[2], bp2=dim(X2)[2],
                  states = States, numStates = max(States))

data <- list(y = VFdata$cases)

inits <- list(beta1 = rep(0,ncol(X1)), beta2 = rep(0,ncol(X2)), StInt = rep(0,max(States)), sigma = 1, rho = 0.2,mu=rep(-10,T),p=rep(.5, n))

set.seed(1)

## setup initial spatially-correlated latent process values
inits$cov <- cExpcov(dist.mat, inits$rho, inits$sigma)
inits$u <-  as.vector(t(chol(inits$cov)) %*% rnorm(n))
#inits$u <- inits$s[ , 1]  # so can give nimble a vector rather than one-column matrix
inits$y <- y
inits$y[which(is.na(y)==TRUE)]=0

model <- nimbleModel(model_code, constants = constants, data = data, inits = inits)

## I think this approach makes sense, but when I try to define the nimble model, an error is thrown:
## Defining model
## Error in t == 1 :
##    comparison (1) is possible only for atomic and list types
## Error in codeProcessIfThenElse(code[[i]], constants, envir) :
##    Cannot evaluate condition of 'if' statement: t == 1.
## Condition must be able to be evaluated based on values in 'constants'.
##  
## I understand that if-else statements are only used to create model variants at the time of definition, but is there any way to do what I am trying to accomplish in nimble?


v <- c(T, T, T, F, F, F, T)
v[c(3:4)] <- FALSE
v[-c(3:4)] <- FALSE

v[numeric()] <- FALSE
v[-numeric()] <- FALSE


v

library(nimble)

nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
nimbleOptions(MCMCsaveHistory = TRUE)

code <- nimbleCode({
    for(i in 1:5) {
        a[i] ~ dnorm(0, 1)
    }
    x[1:5] ~ dmnorm(mu[1:5], cov = Sigma[1:5,1:5])
    y ~ dnorm(a[1] + a[2] + x[1], 1)
})
inits <- list(a=rep(0,5), x=rep(0,5), mu=rep(0,5), Sigma = diag(5))
data <- list(y = 0)

Rmodel <- nimbleModel(code, inits = inits, data = data)

Rmodel$calculate()

conf <- configureMCMC(Rmodel)

conf$addSampler(c('x[1:3]', 'a[1]'), 'RW_block')
##conf$addSampler(c('a[1:5]'), 'RW_block')
##conf$addSampler(paste0('a[', 1:5, ']'), 'RW_block')

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Cmcmc$run(1000)

pppp

Cmcmc$samplerFunctions[[6]]$getScaleHistory()
Cmcmc$samplerFunctions[[6]]$getAcceptanceHistory()

pppp
ppp

debug(conf$printSamplersByType)
ppp




              

## Defining model
## Error in getSymbolicParentNodesRecurse(code, constNames, indexNames, nimbleFunctionNames,  : 
##   getSymbolicParentNodesRecurse: only scalar indices are allowed; vector indexing found in x[index[1:10]]




library(distill)
library(rmarkdown)

getwd()
setwd('~/github/nimble/workshops/ISEC-2022-olivier/hmm-cr-nimble-isec2022-workshop')

rmarkdown::render_site()

render_site('slides/2_transition.Rmd')

setwd('~/github/nimble/workshops/ISEC-2022-olivier/hmm-cr-nimble-isec2022-workshop')
getwd()

render('slides/2_transition.Rmd')

remove.packages('magrittr')
install.packages('magrittr')
library('magrittr')

install.packages('kableExtra')
install.packages('tidyverse')


render_site('.')

data.frame(colnames(y), 1:151)

y1 <- y[,1:145]
y2 <- y[,146:151]

identical(y, cbind(y1,y2))

seen1 <- apply(y1, 1, sum) > 0
seen2 <- apply(y2, 1, sum) > 0


table(seen1 + seen2)

ind <- which(seen1 + seen2 > 1)

y[ind,]

yBreak <- cbind(y1,NA,y2)
yBreak[ind,]



tot <- '98'
cur <- 98


x <- 0
nines <- 0

while(x != 79 && nines < 100) {
    x <- x*10
    nines <- nines + 1
    while(x < 979) x <- x+19
    x <- x-900
    cat(paste0('nines: ', nines, ', x: ', x, '\n'))
}

nines


n <- 1:16
x <- sapply(n, function(nn) as.numeric(paste0(c(rep('9',nn), 8), collapse='')))
remainder <- x %% 19
remainder
verify <- round(((x / 19) %% 1) * 19)

newway <- numeric(length(n))
for(i in seq_along(n)) {
    cur <- n[i]
    leftover <- 0
    while(cur > 0) {
        leftover <- 10*leftover + 9
        leftover <- leftover %% 19
        cur <- cur - 1
    }
    ##print(leftover)
    leftover <- leftover * 10 + 8
    leftover <- leftover %% 19
    newway[i] <- leftover
}

ar <- cbind(n, x, remainder, verify, newway)
rownames(ar) <- n
ar
ar[16,'x']

n <- 16
as.numeric(paste0(c(rep('9',n), '8'), collapse = ''))




(as.numeric(paste0(c(rep('9',n), 8), collapse='')) -> x)

x %% 19

979 - 950

880 %% 19

950/19

x


9999998 / 19


9999999999998 / 19



a <- 1
b <- 0

while(b < 5) {
    b <- b + 1
    a <- a*2
}






dDHMMo <- nimbleFunction(
  run = function(x = double(1),    ## Observed capture (state) history
                 init = double(1),##
                 probObs = double(3),
                 probTrans = double(3),
                 len = double(),## length of x (needed as a s
                 ))


for (i in 1:N){
    ##z[i,first[i]] ~ dcat(delta[1:2])
    ##for (j in (first[i]+1):(last[i])){
        ##z[i,j] ~ dcat(gamma[z[i,j-1], 1:2, j-1, i])
    y[i,first[i]:last[i]] ~ dDHMMo(
        init = delta[1:2],
        probObs = omega[1:2, 1:2, 1:T, flow[i]],
        probTrans = gamma[1:2, 1:2, 1:T, flow[i]],
        len = last[i] - first[i] + 1
    )
    }
}





f <- function(...) {
    browser()
    1
    2
    3
}


f(4, 8)


match.call

dsf
e <- try(dsf, silent = TRUE)



stop(e)
ls(e)
e$condition


dsf
e
errorMessage <- sub('^Error.+: ', '', e[1])
errorMessage

stop(errorMessage)




library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(0, 1)
    for(i in 1:10) c[i] ~ dnorm(0, 1)
})

Rmodel <- nimbleModel(code)

conf <- configureMCMC(Rmodel)
conf$printSamplers()

conf$removeSamplers('sdfsdf')

conf$addSampler(target = 'c', type = 'xxx')

conf$addSampler(target = 'c', type = 'RW')

pppp

conf$replaceSampler(target = 'c', type = 'sliceXXX')

pppp

conf$removeSamplers('b', print = TRUE)
conf$removeSamplers('b', print = TRUE)

conf$removeSamplers('c[4:6]', print = TRUE)
conf$removeSamplers('c[4:8]', print = TRUE)

conf$removeSamplers('er', print = TRUE)




conf$addSampler('a')
conf$addSampler('b')

conf$printSamplers()


ind <- 'c'
ind <- 'd'

ind

conf$findSamplersOnNodes('c')
conf$findSamplersOnNodes('d')
findSamplersOnNodes(ind)


model <- Rmodel

model$expandNodeNames('df', returnScalarComponents = TRUE)

nodes <- model$expandNodeNames(nodes, returnScalarComponents = TRUE, sort = TRUE)

samplerConfNodesList <- lapply(samplerConfs, function(sc) sc$targetAsScalar)
return(unique(unlist(lapply(nodes, function(n) which(unlist(lapply(samplerConfNodesList, function(scn) n %in% scn)))))))

##modelDef <- model$getModelDef()

modelDef$nodeName2GraphIDs('dd', TRUE, unique = TRUE)





    

x <- c(3,8,9,10,11,20,29,30,31,31,31,34,41)  ## GDP
y <- c(1,2,4,7,8,18,12,13,11,12,16,26,26)    ## oil consumption

## scatter plot in R:
## plot(x, y)

plot(x, y)


## correlation coefficient (r)
zx <- (x - mean(x)) / sd(x)

zy <- (y - mean(y)) / sd(y)

length(x)

zx
zy

sum(zx * zy) / 12

## cor(x, y)

r <- cor(x, y)

## bhat = sy / sx * r

r

bhat <- sd(y) / sd(x) * r

bhat

mean(y) - bhat * mean(x)

## fit a linear model in R:
## lm(y ~ x)

m <- lm(y ~ x)

m

ls(m)

coef <- m$coefficients
coef[1]
coef[2]

ls(m)

m
y - (-.1055 + .5464*x)

unname(m$residuals)

## Find residual for Canada
## x = GDP = 34, y = oil = 26






## Permutation Distribution

survey <- read.csv('~/github/courses/stat202/data/STAT202.csv')
head(survey)

## assocuation between gender, and # of Facebook friends ..?
gender <- survey$Gender
friends <- survey$Friends
gender
friends

## histogram
hist(friends)


## boxplots
boxplot(friends)
## boxplot(y ~ x)
boxplot(friends ~ gender)

gender <- gender[-27]
friends <- friends[-27]

table(gender)

## permutation distribution
N <- 10000
dif <- numeric(N)
for(i in 1:N) {
    friendsP <- sample(friends)
    femalesP <- friendsP[1:16]
    malesP <- friendsP[17:36]
    dif[i] <- mean(femalesP) - mean(malesP)
}

## histogram of permutation distribution
hist(dif, breaks = 50)
obsF <- friends[gender == 'female']
obsM <- friends[gender == 'male']
obsDif <- mean(obsF) - mean(obsM)
obsDif
hist(dif, breaks = 50)
abline(v = obsDif, lwd = 2, col = 'blue')
obsDif
dif
sum((1:10 > 8) | (1:10 < 2))
mean((1:10 > 8) | (1:10 < 2))
mean(abs(dif) > obsDif)
mean((dif > obsDif) | (dif < -obsDif))

## confidence interval
## assuming *no association*

## the observed difference

## permutation p-value

## two-samples t-test
## t.test(group1, group2)


y <- c(5,7,9,10,12,12,12,13,13,15,15,20,
       5,7,7,8,10,10,11,12,12,14,14,14,16,18,20,20,20,22,23,25,40)

y
length(y)

y[1:12]
y[13:33]

N <- 100000
difs <- numeric(N)

for(i in 1:N) {
    newy <- sample(y)
    difs[i] <- mean(newy[1:12]) - mean(newy[13:33])
}

hist(difs, breaks = 50)

difs







ci <- quantile(difs, c(0.025, 0.975))
ci







abline(v = ci, col = 'blue', lwd = 2)
obsdif <- mean(y[1:12]) - mean(y[13:33])
obsdif

abline(v = obsdif, col = 'red', lwd = 2)
mean(difs < obsdif | difs > -obsdif)


t.test(y[1:12], y[13:33])









library(nimble)

# Create dataset
set.seed(1234)
nsite <- 150
nrep <- 8
nyears <- 2
pa <- rbinom(nsite*nrep*nyears, 1, 0.8)
y <- array(pa, dim = c(150, 8, 2))
# Randomly assign NAs
for(i in 1:nsite){
    for(k in 1:nyears){
        s <- sample(1:(nrep+1),1)
        if(s < 9){ y[i,s:nrep,k] <- NA}
    }
}
# Check that sites contain data at least in one of the two years
na_y1 <- which(is.na(y[,1,1]))
na_y2 <- which(is.na(y[,1,2]))
na12 <- na_y1[na_y1%in%na_y2]
y[na12,1,1] <- 1 # Assign 1 to first year for sites with NA both years

# Model
code <- occupancy_static <- nimbleCode({
    ## PRIORS ##
    # DETECTION
    p_int~dnorm(0,0.01)
    # OCCUPANCY
    for(i in 1:nyears){
        psi_year[i] ~ dnorm(0,0.1)  
    }
    ## Ecological model
    for (i in 1:nsite){
        for(k in 1:nyears){
            z[i,k] ~ dbern(psi[i,k])
            logit(psi[i,k]) <- psi_year[year[k]]
        } #k
    } #i
    # Observation model
    for (i in 1:nsite){
        for(k in 1:nyears){
            for(j in 1:J[i,k]){
                muy[i,j,k] <- z[i,k]*p[i,j,k]
                logit(p[i,j,k]) <- p_int
                y[i,j,k] ~ dbern(muy[i,j,k])
                # Goodness of Fit
                f[i,j,k] <- abs(y[i,j,k] - p[i,j,k]) # Discrepancy for real data
                y.new[i,j,k] ~ dbern(muy[i,j,k]) # Simulate data
                f.new[i,j,k] <- abs(y.new[i,j,k] - p[i,j,k]) # Discrepancy for simulated data
            } #j
        } #k
    } #i
    ##f.sum <- sum(f[,,])
    f.sum <- sum(f[1:150,1:8,1:2])
})

# Create an object used to exclune NA samples
J <- matrix(NA, nrow = nrow(y), ncol=2)
for (i in 1:nrow(y)){
    for(k in 1:2){
        if (sum(is.na(y[i,,k])) != ncol(y)) J[i,k] <- ncol(y) - sum(is.na(y[i,,k]))
        else J[i,k] <- NA  
    }
}
J[is.na(J)]<-1

# Bundle data
data <- win.data <- list(y = y)

constants <- list(nsite = dim(y)[1], nrep = dim(y)[2], J = J, nyears = dim(y)[3],year = 1:dim(y)[3])

# List of parameters to monitor
params<-c("psi_year","p_int")

# List of inits
set.seed(0)
inits <-list(psi_year = rnorm(2,0,1),p_int = rnorm(1,0,1))

# Nimble model
Rmodel <- nimbleModel(code=occupancy_static, constants=constants, data=win.data, inits = inits)


## > Rmodel <- nimbleModel(code=occupancy_static, constants=constants, data=win.data, inits = inits)
## Defining model
## Building model
## Setting data and initial values
## Running calculate on model
##   [Note] Any error reports that follow may simply reflect missing values in model variables.
## Checking model sizes and dimensions
##   [Note] This model is not fully initialized. This is not an error.
##          To see which variables are not initialized, use model$initializeInfo().
##          For more information on model initialization, see help(modelInitialization).



Rmodel$initializeInfo()


# MCMC settings
ni <- 4*10^3
nb <- ni*3/4
nt <- (ni-nb)/1000 # Extract 1000 posterior for each chain
nc <- 3


# mcmc
conf <- configureMCMC(Rmodel, print = F)
conf$addMonitors(c("f","f.new"))
Rmcmc <- buildMCMC(conf)
# Compile the model and MCMC algorithm
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

# RUN THE MODEL
start.time <- Sys.time()
out <- runMCMC(mcmc = Cmcmc, niter = ni, nburnin = nb, nchains = nc, thin = nt,
               samplesAsCodaMCMC = T, inits = inits)
Sys.time()-start.time

## Matthijs, I tried f.sum <- sum(f[1:150,1:8,1:2]) but it still does not work. 







txt <- header[26]


grep('AAA', txt, useBytes = TRUE)





library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0, b = 1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)

conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)

samples <- runMCMC(Cmcmc, 1000, nchains = 3)

names(samples)
lapply(samples, dim)

all_samples <- do.call(rbind, samples)

dim(all_samples)


colnames(samples)
samplesSummary(samples)
samplesPlot(samples)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()

stochVars <- unique(nimble:::removeIndexing(Rmodel$getNodeNames(stochOnly = TRUE)))
for(v in stochVars) {
    lp <- Rmodel$calculate(v)
    cat(v, ': ', lp, '\n')
}






















## install.packages('RSQLite')

library(RSQLite)


driver <- dbDriver("SQLite")

class(driver)


con <- dbConnect(driver,  dbname = "~/Downloads/lahman2016.sqlite")

con



## dbListTables

dbListTables(con)


## dbListFields

dbListFields(con, "Teams")
dbListFields(con, "Salaries")


## dbGetQuery
## specify and fetch the results of
##..... a SQL query!


## this print the resulting df to the
## screen
dbGetQuery(con, '
QUERY HERE
MULTILE LINES
')

## stores the result of the query
## into a variable, which will be an R df.
df <- dbGetQuery(con, '
QUERY HERE
MULTILE LINES
')


df <- dbGetQuery(con, '
SELECT .....
FROM [table name here]
')


head(
    dbGetQuery(con, '
SELECT *
FROM Teams
'),
10)


dbListFields(con, "Parks")


parks <- dbGetQuery(con, '
SELECT city, state, country
FROM Parks
LIMIT 10
')


dbGetQuery(con, '
SELECT city, state, country
FROM Parks
WHERE country != "US"
')


dbGetQuery(con, '
SELECT "park.name", city, state, country
FROM Parks
WHERE state = "CA"
')



dbGetQuery(con, '
SELECT "park.name" AS ballpark, city, state, country AS c
FROM Parks
WHERE country != "US"
')


dbListFields(con, "Homegames")


dbGetQuery(con, '
SELECT "park.key" AS ballpark, attendance
FROM HomeGames
LIMIT 20
')

dbGetQuery(con, '
SELECT *
FROM HomeGames
WHERE "year.key" = 1900 AND games < 70
')


dbGetQuery(con, '
SELECT *
FROM HomeGames
WHERE "year.key" > 1910 AND "year.key" < 1920
')

dbGetQuery(con, '
SELECT games,
       attendance,
       attendance/games AS avg_attendance
FROM HomeGames
LIMIT 5
')



dbGetQuery(con, '
SELECT *
FROM HomeGames
WHERE "year.key" = 2000
ORDER BY attendance
LIMIT 10
')


dbGetQuery(con, '
SELECT *
FROM HomeGames
WHERE "year.key" = 2000
ORDER BY attendance DESC
LIMIT 5
')


dbGetQuery(con, '
SELECT COUNT(*) AS N
FROM HomeGames
')


dbListFields(con, "HomeGames")

dbGetQuery(con, '
SELECT MIN("year.key") AS first_year,
       MAX("year.key") AS final_year,
       openings
FROM HomeGames
')


dbGetQuery(con, '
SELECT MIN("year.key") AS first_year,
       MAX("year.key") AS final_year,
       first_year + final_year AS sum_years
FROM HomeGames
')


dbGetQuery(con, '
SELECT DISTINCT state, country
FROM Parks
')


dbGetQuery(con, '
SELECT COUNT(DISTINCT state),
       COUNT(DISTINCT country)
FROM Parks
')


5 %in% c(1, 3, 6, 10, 5)

parks

dbListFields(con, "Parks")


1































m1 <- 13.2
s1 <- 8
n1 <- 16

m2 <- 7.3
s2 <- 6
n2 <- 68

sp <- sqrt((s1^2 * (n1-1) + s2^2 * (n2-1)) / (n1+n2-2))
sp

se <- sp * sqrt((1/n1 + 1/n2))
se

df <- n1+n2 - 2
df
t <- qt(0.975, df)
t

m1-m2

m1 - m2 + c(-1,1) * se * t





1
remove.packages('nimbleHMC')
remove.packages('nimbleHMC')

library(devtools)

## option 1:
install_github('nimble-dev/nimbleHMC', ref = 'master', subdir = 'nimbleHMC')

## option 2:
setwd('~/github/nimble/nimbleHMC')

document('nimbleHMC')

system('R CMD BUILD nimbleHMC')

check('nimbleHMC')

(tarFiles <- grep('\\.tar\\.gz', list.files(), value = TRUE))
(lastTarFile <- tarFiles[length(tarFiles)])
message('installing package version ', gsub('\\.tar\\.gz$', '', lastTarFile))
system(paste0('R CMD install ', lastTarFile))


environment(sampler_BASE)$methodControl
sampler_HMC

library(nimble)

environment(sampler_BASE)$methodControl
sampler_HMC

library(nimbleHMC)

environment(sampler_BASE)$methodControl
sampler_HMC

nimbleOptions(enableDerivs = TRUE)
nimbleOptions(buildDerivs = TRUE)

code <- nimbleCode({
    b0 ~ dnorm(0, 0.001)
    b1 ~ dnorm(0, 0.001)
    sigma ~ dunif(0, 10000)
    for(i in 1:N) {
        mu[i] <- b0 + b1 * x[i]
        y[i] ~ dnorm(mu[i], sd = sigma)
    }
})
N <- 10
constants <- list(N = N, x = 1:N)
data <- list(y = 1:N)
inits <- list(b0=0, b1=10, sigma=100)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$setSamplers()
conf$addSampler('b0')
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)








build(addDefaultSampler)
library(testthat)



print(paste0('g', 5))


debug(factorization)

undebug(factorization)

factorization <- function(x) {
    ## check for non-integers, negatives
    if(x==0) return(1)
    if(x==1) return(1)
    factors <- numeric()
    d <- 2
    while(x > 1) {
        print(paste0('x = ', x))
        print(paste0('d = ', d))
        if(x %% d == 0) {
            factors <- c(factors, d)
            x <- x/d
        } else d <- d+1
    }
    return(factors)
}


factorization(20)


sodas <- downloadStockPairDF("KO", "PEP", 2015)

plotRatio(sodas)   ## default value of k = 1

ratio <- sodas$ratio
m <- mean(ratio)
s <- sd(ratio)

positions <- findPositions(ratio, m, s)   ## default value of k = 1
positions

addPositions(ratio, positions)

positions[[1]]

head(sodas)

shares1 <- 1 / sodas$stock1[1]
shares2 <- 1 / sodas$stock2[1]

shares1
shares2

## short position on stock 1:
## spend to recover stock 1
- shares1 * sodas$stock1[28]

## earned from selling the long position on stock2:


- shares1 * sodas$stock1[28] + shares2 * sodas$stock2[28]


## spent $1 twice....

(1 + 1 + shares1 * sodas$stock1[28] +  shares2 * sodas$stock2[28]) * 0.003



args(positionProfit)

length(positions)

positionProfit(sodas, positions)
positionProfit(sodas, positions, net = FALSE)

sum(positionProfit(sodas, positions, net = FALSE))


positionProfit <- function(...) {
    nPositions <- length(positions)
    profits <- numeric(nPositions)
    for(i in 1:nPositions) {
        ....
        profits[i] <- .....

    }
    if(!net) return(profits)
    return(sum(profits))
}

list()



positions

positionProfit(sodas, positions, net = FALSE)

positionProfit(sodas, positions)    ## net profit 11.6%

## using k = 2
plotRatio(sodas, k = 2)
ratio <- sodas$ratio; m <- mean(ratio); s <- sd(ratio)
positions <- findPositions(ratio, m, s, k = 2)
positions
addPositions(ratio, positions)
positionProfit(sodas, positions, net = FALSE)
positionProfit(sodas, positions)    ## net profit 7.3%

## using k = 0.5
plotRatio(sodas, k = 0.5)
ratio <- sodas$ratio; m <- mean(ratio); s <- sd(ratio)
positions <- findPositions(ratio, m, s, k = 0.5)
positions
addPositions(ratio, positions)
p <- positionProfit(sodas, positions, net = FALSE)
which(p < 0)
positionProfit(sodas, positions)    ## net profit 8.1%





## just hit enter... it will execute the next line of code
## 




1


















library(nimble)

n = 10

code <- nimbleCode({
    gamma ~ T(dbeta(2, 2), 0, 0.75)
    for ( i in 1:n ) {
        y[i] ~ dbinom(size = 1, prob = gamma)
    }
})

set.seed(0)

## Generate binomial data
y <- rbinom(n, size = 1, prob = 0.25)

## Model / configure
mod <- nimbleModel(code, constants = list('n' = n), data = list('y' = y), init = list('gamma' = 0.1))


mySampler <- nimbleFunction(
    name = 'mySampler',
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        ## query the model, to get all the stochastic dependents
        ## of the 'target' node:
        dependentNodes <- model$getDependencies(target, stochOnly = TRUE, self = FALSE)
        ## just for the for-loop in the run code:
        nDependentNodes <- length(dependentNodes)
        ## do a quick check here, that all
        ## the stochastic dependents are 'dbin'-distributed.
        ## note NIMBLE has changed the distribution name to 'dbin'.
        ## but you could put whatever checks you want here, or none.
        if(!all(model$getDistribution(dependentNodes) == 'dbin')) stop('dependent nodes are not binomial')
    },
    run = function() {
        for(i in 1:nDependentNodes) {
            size <- model$getParam(dependentNodes[i], 'size')
            prob <- model$getParam(dependentNodes[i], 'prob')
            value <- values(model, dependentNodes[i])
            print(dependentNodes[i], ':')
            print('size=', size, ', prob=', prob, ', value=', value)
        }
        ## NOTE: you can also use:
        ## values(model, dependentNodes)
        ## to get a vector of *all* the dependent node values
        valuesVector <- values(model, dependentNodes)
        print('all dependent node values:')
        print(valuesVector)
    },
    methods = list(
        reset = function() { }
    )
)

modConf <- configureMCMC(mod)

modConf$removeSamplers('gamma')

modConf$addSampler(target = 'gamma', type = 'mySampler')

## print sampler configuration:
modConf

## build MCMC:
Rmcmc <- buildMCMC(modConf)

## compile model and MCMC:
Cmod <- compileNimble(mod)
Cmcmc <- compileNimble(Rmcmc, project = mod)

## run:
Rmcmc$run(1)   ## uncompiled
Cmcmc$run(1)   ## compiled



## why is my BatchGetSymbols not having adjusted prices listed (as it should)...
## and you said that I would need?

##... then, restrict yourself (and your analyese) to years prior to 2019 ....

library(BatchGetSymbols)

?BatchGetSymbols

n <- length(stock1)

plot(1:n, stock1, type = 'l')


a <- BatchGetSymbols('AAPL', first.date = '2000-01-01', last.date = '2000-12-31', be.quiet = TRUE)


stock1 <- rnorm(10, 4, 1)
stock2 <- rnorm(10, 10, 1)

stock1
stock2

## plotStocks
## line plot with stock1 and stock2

n <- length(stock1)
if(length(stock2) != n) stop('helpful error message here')

range(stock1)

## an issue with the ylimits for plot()
plot(1:n, stock1, type = 'l', ylim = c(0,max(stock2)*1.05))
lines(1:n, stock2, col = 'green')
legend('topright', legend = c('stock1', 'stock2'), lty = 1, col = c('black', 'green'))




sodas <- downloadStockPairDF("KO", "PEP", 2015)

head(sodas)

plotStocks(sodas)

plotRatio(sodas)   ## default value of k = 1

head(sodas$stock1 / sodas$stock2)


N <- 1000000
x <- rnorm(N)
res <- logical(N)

## non-vectorized manner: that means using a for loop!
system.time(for(i in 1:N) { res[i] <- (x[i] > 0) })

## vectorized: NO for loop:
system.time(res <- (x > 0))


res

1

head(sodas)

ratio <- sodas$ratio
m <- mean(ratio)
s <- sd(ratio)


positions <- findPositions(ratio, m, s)

## guess: open 1, close 25
## open 40, close 50

class(positions)
length(positions)

positions
positions[[1]]

length(ratio)

ratio
positions

addPositions(ratio, positions)




k <- 1

plotRatio(sodas, k = k)

positions <- findPositions(ratio, m, s, k = k)
length(positions)
positions

addPositions(ratio, positions)


positions <- findPositions(ratio, m, s, k = 5)  ## does not fail

length(positions)







findPositions <- function(ratio, m, s, k = 1) {
    positions <- list()
    upper <- m + k*s
    lower <- m - k*s
    current <- 1
    while(.... the current day is still less than n) {
        ## see if there's ever a day (greater than current day)
        ## when we would open a new pair of positions
        ## ?????
        ######### NO: for(day in current:n) {
        ######### NO:     ## check whether ratio[day] is > upper, or < lower
        ######### NO: }
        ## need a boolean vector here...
        ## we will ONLY open positions on days >= current
        possibleOpenDays <- ((ratio > upper) | (ratio < lower)) & (1:n >= current)

        ## check if there are any TRUEs in possibleOpenDays
        any(possibleOpenDays)
        openDay <-which(possibleOpenDays)[1]

        ## also need to determind highLow
        ## by checking if on openDay, ratio > upper, or ratio < lower

        

        n

        ## record when we would open posisition (openDay)

        ## record when we would close those positions (closeDay)
        ## consider when it crosses the mean (when we close the position)
        closeDay <- 28

        ## store (openDay, closeDay, highLow) into the positions list...
        positions[[length(positions) + 1]] <- c(1, 10, 1)
        positions[[length(positions) + 1]] <- c(21, 50, -1)

        ## finally... update "current"... since now, we're only
        ## looking *forward* from closeDay
        current <- closeDay + 1

        
    }
    return(positions)
}

## C++ style
&&   ||
1:10 && 0

## vectorized "R" style "and", and "or" are  &, and |
1:10 & (-4):5



## hint... you need to return a *list* at the end of this function.......

ratio
m
upper
lower
n <- length(ratio)..... the maximal time horizon.
maintain the "current" day that were looking at (and moving forward from)

## hint.... we need to think about *when* we might open a pair of positions....
##












































## STAT202

## The Bootstrap

## daily Williamstown MA temperatures in September:

temps <- c(87, 81, 79, 87, 75, 73, 67, 73, 75, 77,
           81, 81, 84, 88, 91, 88, 91, 86, 81, 91,
           68, 68, 76, 80, 65, 67, 72)

## let's explore the data:
##(description statistics, plots, etc)

n <- length(temps)
n

mean(temps)

## boxplot

boxplot(temps, horizontal = TRUE)

## histogram

hist(temps, breaks = 15)
hist(temps)

## time series

ts.plot(temps)

## the "sample" function in R:


sample(1:10)  ## just permutes the data
## taking a smaple of the same size, without replacement

## replacement or no replacement?
## default is *no* replacement... NOT what we want

## default take a sample of the same size as the original data.... that IS what we want..

sample(1:10, replace = TRUE)

sample(1:10, replace = TRUE, size = 100)
sample(1:10, size = 100)

sample(1:10, replace = TRUE)


sample(1:10, size = 100)

## a single bootstrap resample would be:
## using the data: temps
## USE THE DATA!!!
## not 1:10

sample(temps, replace = TRUE)


## Now: Bootstrapping
## parameter of interest:
## average daily temperature in September

## for() loops .....

nreps <- 10000

means <- numeric(nreps)
means

for(i in 1:nreps) {
    ## create our bootstrap resamples:
    ## do we need to save all of the full (size n) resamples?
    ## or save something else?
    resample <- sample(temps, replace = TRUE)
    ## let's save it in that vector we created!
    ## that seems like a good idea
    means[i] <- mean(resample)
}

hist(means)

## MISTAKE
## this is NOT BOOTSTRAPPING CI
mean(means) +c(-1,1) * 1.96 * sd(means)



hist(means, xlim = range(temps))
##points(x, y)
points(temps, rep(0,length(temps)), pch=19, col="blue")

## find the 95% bootstrap CI for the mean.... how?

## find the 2.5% and 97.5% quantiles of the bootstrap resampled means

quantile(1:100)
quantile(1:100, probs = c(0.025, 0.975))

## means, recall, is the vector of bootstrap resampled means
bci <- quantile(means, probs = c(0.025, 0.975))

bci

##  horizontal lines, vertical lines, or diagonal lines
abline(v = bci, col = 'green', lwd = 2)

## say we want a 99% Bootstrap CI?
## we need the 0.5% and 99.5% quantiles

bci99 <- quantile(means, probs = c(0.005, 0.995))

abline(v = bci99, col = 'purple', lwd = 2)

## 90% 95% 99%, 99.9%

mean(temps)

abline(v = mean(temps), col = 'orange', lwd = 5)

## change gears..
## bootstrap CI for the population.... median
## great choice

## it's almost embarassing how easy it is to change
## this:

## sample size n
## that's just a function of the data we have.

## the number of bootstrap resamples we choose to use
## *you* get to choose this

nreps <- 10000

medians <- numeric(nreps)
medians

for(i in 1:nreps) {
    ## create our bootstrap resamples:
    ## do we need to save all of the full (size n) resamples?
    ## or save something else?
    resample <- sample(temps, replace = TRUE)  ## SAME
    ## let's save it in that vector we created!
    ## that seems like a good idea
    ##means[i] <- mean(resample) ## no longer the mean
    medians[i] <- median(resample) ## now: median
    ## of each bootstrap resample
}

hist(medians)


mean = x1 + x2 + x3 + ... / n
did math to calcualte se(mean)


hist(medians, xlim = range(temps))
##points(x, y)
points(temps, rep(0,length(temps)), pch=19, col="blue")

## find the 95% bootstrap CI for the mean.... how?

## find the 2.5% and 97.5% quantiles of the bootstrap resampled medians

quantile(medians, probs = c(0.025, 0.975))

bci

abline(v = bci, lwd = 2, col = 'yellow')


## for now: categorical data....
## bootstrap CI for the population proportion
## (or any other statistic of the categorical data)



## is this coin fair
## statistic of interest: the population proportion of heads ....?

nreps <- 10

props <- numeric(nreps)
## for each bootstrap resample, we'll caluclate the
## sample proportion of heads.
## because, our parameter of interest is the proportion of heads


flips <- c(rep("H", 5), rep("T", 8))

5 / 13

flips
sample(flips, replace = TRUE)

n <- length(flips)
n

table(flips)[['H']] / n
table(flips)[['T']]

nreps <- 10000
props <- numeric(nreps)

for(i in 1:nreps) {
    resample <- sample(flips, replace = TRUE)  ## SAME
    ##props[i] <- table(resample)[['H']] / 13
    props[i] <- mean(resample == 'H')
}

props

hist(props, xlim = c(0,1))

## find the 95% bootstrap CI for the mean.... how?

## find the 2.5% and 97.5% quantiles of the bootstrap resampled medians

## is this coin fair?
## does the 95% BCI contain 0.5 = 1/2 ???

bci <- quantile(props, probs = c(0.025, 0.975))

bci

abline(v = bci, lwd = 2, col = 'green')

abline(v = 0.5, lwd = 2)




## bootstrap CI for the 10% percentile of the distribution
## bootstrap CI for the standard deviation
## or for the .... IQR ...
## any statistic!
## pretty easy.

## 


sum
1

## let's find a CI for the population mean
## this is called a Bootstrap CI (!)


## how about a traditional (frequentist) CI for the mean temperature?



## Categorical data:
## Bootstrap CI for the sample proportion









1


























@perrydv I updated this PR to remove the `sampler_BASE2`, and instead to use the new:

`methodControl = list( METHODNAME = list(abstract = FALSE) )`

way of specifying `nimbleFunctionVirtual` methods with defaults.

Noting also that the package doesn't build, at present, on this branch `ADoak_without_HMC`, until we merge this update to `nimbleFunctionVirtual` in.
'





## helping Alireza Beheshty
## on spatial econometric model

## Y = inverse(rho*W)*X*beta
## Y is Vector N*1
## rho ~ dunif( )
## W is matrix N*N
## X ix matrix N*K
## beta(i) is Vector K*1
##  
## Can you help me please?
## my idea is this code but it has an error.

library(spdep)
library(spatialreg)
##
data("baltimore")
##
N <- S <- 211
XY = cbind(baltimore$X, baltimore$Y)
##
# Adjcency matrix
distMat <- as.matrix(dist(XY))
distMat <- distMat / max(distMat) * 10
distMat <- distMat + 0.01 * diag(rep(1, 211))
##
# Compute eigenvalues for SLM model (as in Havard's code)
e <- eigen(distMat)
e.values <- e$values
rho.max <- 1 / max(e.values)
rho.min <- 1 / min(e.values)
##
X.mat = matrix(c(baltimore$DWELL,
                 baltimore$NBATH,baltimore$PATIO,
                 baltimore$FIREPL,baltimore$AC,
                 baltimore$BMENT, baltimore$GAR,
                 baltimore$CITCOU,baltimore$LOTSZ) , nrow = N)
##
library(nimble)
##
code <- SLMMCode <- nimbleCode({
    # rho.max = 0.0014  ,  rho.min = -0.00392  ,  mean = -0.00126
    rho ~ dunif(-0.00392 , 0.0014)
    # inverse(I-rho*W)
    I.rhoW[1:S, 1:S] <- diag(S) - (rho * Dist[1:S, 1:S])
    inv[1:S, 1:S] <- inverse(I.rhoW[1:S, 1:S])
    # [inverse(I-rho*W)]*X
    inv.X[1:S, 1:9] <- inv[1:S, 1:S] %*% X.mat[1:S, 1:9]
    for (i in 1:S) {
        y[i] ~ dnorm(mu_y[i], tau = tau_y_SLM[i, i])
        mu_y[i] <- b[i, 1] * inv.X[i,1] + b[i, 2] * inv.X[i,2] +
            b[i, 3] * inv.X[i,3] + b[i, 4] * inv.X[i,4] + b[i, 5] * inv.X[i,5] +
            b[i, 6] * inv.X[i,6] + b[i, 7] * inv.X[i,7] + b[i, 8] * inv.X[i,8] +
            b[i, 9] * inv.X[i,9]
        b[i, 1:9] <- bm[latent[i], 1:9]
        latent[i] ~ dcat(zlatent[1:M])
    }
    tau_y_SLM[1:S, 1:S] <- tau_y * inv[1:S, 1:S]
    for (k in 1:M) {
        ##bm[k, 1:9] ~ dmnorm(mu_bm[1:9], cov = var_bm[1:9, 1:9])   ## CHANGE
        for(j in 1:9) {
            bm[k, j] ~ dnorm(mu_bm[j], tau_bm)
        }
    }
    ##var_bm[1:9, 1:9] <- 1/tau_bm * diag(rep(1, 9))    ## CHANGE
    tau_bm ~ dgamma(1, 1)
    for (j in 1:9) {
        mu_bm[j] ~ dnorm(0, 1)
    }
    zlatent[1:M] <- stick_breaking(vlatent[1:(M - 1)])
    for (j in 1:(M - 1)) {
        vlatent[j] ~ dbeta(1, alpha)
    }
    alpha ~ dgamma(1, 1)
    tau_y ~ dgamma(1, 1)
})
##
##Rmodel <- nimbleModel(code, constants, data, inits)
##Rmodel$calculate()
##
data <- SLMMdata <- list(y = baltimore$PRICE, Dist = distMat, X.mat = X.mat)
##
constants <- SLMMConsts <- list(S = 211, M = 50)
##
set.seed(12345)
##
inits <- SLMMInits <- list(tau_y  = 1,
                           latent = rep(1, SLMMConsts$S),
                           alpha  = 2,
                           tau_bm = 1,
                           mu_bm  = rnorm(9),
                           rho    = -0.00126,
                           vlatent = rbeta(SLMMConsts$M - 1, 1, 1),
                           bm = array(0, c(SLMMConsts$M, 9))  ## NEW: initial value for bm
                           )
##
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()


conf <- configureMCMC(Rmodel, nodes = NULL)
conf <- configureMCMC(Rmodel, nodes = character())
conf <- configureMCMC(Rmodel)
conf

conf$replaceSampler('vlatent', 'RW_block', print = TRUE)
conf$replaceSampler('vlatent', 'RW_block', print = TRUE, expandTarget = TRUE)
conf$replaceSampler(target = 'bm[1]', type = 'slice', print = TRUE)
ppp
pppp
conf$printSamplers('bm')



conf2 <- configureMCMC(Rmodel, nodes = character())
conf2$addSampler(c('rho','alpha','tau_y','tau_bm'), default = TRUE)
conf2$addSampler(c('mu_bm','vlatent','latent','bm'), default = TRUE)
conf2$addSampler(c('mu_bm','vlatent','latent','bm'), type = 'slice', expandTarget = TRUE)
conf2
conf2$printSamplers()

conf$addSampler(nodes = c('rho', 'alpha'), type = 'RW', print = TRUE)
conf$addSampler(nodes = c('rho', 'alpha'), type = 'RW', print = TRUE)
conf$addSampler(c('rho', 'alpha'), 'slice', print = TRUE)
conf$addSampler(nodes = c('rho', 'alpha'), type = 'slice', print = TRUE)
conf$addSampler(nodes = c('rho', 'alpha'), type = 'slice', print = TRUE, default = TRUE)
conf$addSampler('latent', default = TRUE)
conf$addSampler('vlatent')
conf$addSampler('vlatent', default = TRUE)
conf$addSampler(nodes = 'latent', default = TRUE)
conf$addSampler('vlatent', print = TRUE)
conf$addSampler(nodes = 'vlatent', default = TRUE)
conf$addSampler('vlatent', expandTarget = TRUE, print = TRUE, default = TRUE)
conf$addSampler('vlatent', print = TRUE, default = TRUE)
conf$addSampler(nodes = 'vlatent', print = TRUE, default = TRUE)
conf$addSampler('latent', expandTarget = TRUE, print = TRUE, default = TRUE)
conf$addSampler('latent', print = TRUE, default = TRUE)
conf$addSampler(nodes = 'latent', print = TRUE, default = TRUE)
conf$addSampler('latent', scalarComponents = TRUE, print = TRUE)
conf$addSampler('latent', expandTarget = TRUE, print = TRUE)
conf$addSampler('latent', print = TRUE)
conf$addSampler('tau_y')
conf$addSampler('tau_y', 'slice')
conf$addSampler('tau_y', default = TRUE)
conf
conf$printSamplers()
debug(conf$printSamplersByType)
conf$setSamplers(c(2,7))
conf$addSampler(c('rho', 'alpha'), 'RW', print = TRUE)
conf$addSampler(c('rho', 'alpha'), 'XXX', print = TRUE, default = TRUE)

conf$removeSamplers('latent')

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('vlatent', print = TRUE)
conf$addSampler('vlatent', expandTarget = TRUE, print = TRUE)
conf

## THE ERROR:
## slice sampler (1)
##   - rho, alpha 
## RW sampler (3)
##   - rho
##   - alpha
##   - vlatent
## conjugate sampler (98)
##   - vlatent[]  (98 elements)
## categorical sampler (422)
##   - latent[]  (422 elements)


mcmc.out <- nimbleMCMC(code = SLMMCode,
                       data = SLMMdata,
                       constants = SLMMConsts,
                       inits = SLMMInits,
                       monitors = c("bm","b","rho","alpha", "latent", "tau_y"),
                       niter = 50000,
                       thin = 10,
                       nchains = 1,
                       setSeed = TRUE)

## Error in genVarInfo3() :
##   Dimension of inv.X is 0, which does not match its usage in 'mu_y[i]
## <- b[i, 1] * inv.X[i, 1] + b[i, 2] * inv.X[i, 2] + b[i, 3] * inv.X[i,
## 3] + b[i, 4] * inv.X[i, 4] + b[i, 5] * inv.X[i, 5] + b[i, 6] *
## inv.X[i, 6] + b[i, 7] * inv.X[i, 7] + b[i, 8] * inv.X[i, 8] + b[i, 9]
## * inv.X[i, 9]'.








gapminder <- read.csv('~/Downloads/gapminder.csv')

names(gapminder)

countries <- unique(gapminder$country)


grep("([[:alpha:]]).\\1.\\1", countries, value = TRUE)


grep("([[:alpha:]]).*\\1.*\\1", countries, value = TRUE)
grep("([[:alpha:]]).*\\1.*\\1", countries, value = TRUE)

grep(".([[:alpha:]]).*\\1.*\\1", countries, value = TRUE)
grep(".+([[:alpha:]]).*\\1.*\\1", countries, value = TRUE)
grep(".*([[:alpha:]]).*\\1.*\\1", countries, value = TRUE)

grep(".*([[:alpha:]]).*\\1.*\\1", countries, value = TRUE)


grep("([[:alpha:]]).\\1.\\1", 'xxx', value = TRUE)

grep("([[:alpha:]]).*\\1.*\\1", 'AAA', value = TRUE) 


sumDice <- function(n = 1, nDice = 5, nSides = 6) {
    replicate(n,
              sum(sample(1:nSides,
                         size = nDice,
                         replace = TRUE)))
}

replicate(3, sample(1:6, size = 2))
    
sumDice(10)
x <- sumDice(100000, nDice = 5, nSides = 10)

mean(x)
sd(x)



ceiling(runif(2) * 6)
sample(1:6, size = 2, replace = TRUE)

library(ggplot2)

as.data.frame(diamonds)


library(BatchGetSymbols)



stockData  <- BatchGetSymbols(tickers = "AAPL",
                              first.date = "2000-01-01",
                              last.date = "2001-01-01")

stockData <- stocks

class(stockData)
names(stockData)

stockData$df.control  ## meta information about the download


stockData$df.control$download.status

stockPrices <- stockData$df.tickers

head(stockPrices)
class(stockPrices)

dim(stockPrices)   ## 252 trading days in the year 2000

names(stockPrices)



prices <- stockPrices$price.adjusted

class(prices)
length(prices)
mean(prices)


dates <- stockPrices$ref.date

class(stockPrices$ref.date)

print.Date

class(dates[1])
class(as.character(dates[1]))

as.character(dates[1])

Sys.time()





stock <- "AAPL"    ## Apple

downloadPriceDF(stock, start = 2014)
apple <- downloadPriceDF(stock, start = 2014)

class(apple)


head(apple, 10)
dim(apple)    ## 252 trading days in 2014

apple2 <- downloadPriceDF(stock, start = 2014, nyears = 2)

head(apple2, 10)
tail(apple2)
dim(apple2)    ## 504 trading days in 2014-2015


downloadPriceDF <- function(stock, start = 2010, nyears = 1) {
    ## use BatchGetSymolrs
    ## use string manipulations to create start and end dates
    ## download data
    ## cast dates as character strings
    ## create (and return) a data from with prices and dates
    return(stockDF)
}


downloadStockPairDF <- function(stock1, stock2, start = 2010, nyears = 1) {
    ## should internally make use of your function "downloadPriceDF"
    ## after 2 calls to downloadPriceDF....
    ## make sure the date ranges for all prices are the same
    return(stockPairDF)
}



identical(TRUE, TRUE)
identical(TRUE, 1)

class(2L)
class(2)

identical(2L, 2)
identical('a', 'a')


sodas <- downloadStockPairDF("KO", "PEP", 2015)

class(sodas)
names(sodas)
head(sodas)
sodas$ratio

sodas$stock1 / sodas$stock2


plotStocks <- function(stocksDF) {
    [your code here]
}

sodas <- downloadStockPairDF("KO", "PEP", 2015, nyears = 5)
plotStocks(sodas)

cor(sodas$stock1, sodas$stock2)  ## want pairs of stocks which are highly positively correlated...
## that will make pairs trading work well.


ts.plot(sodas$ratio)

mean(sodas$ratio)
sd(sodas$ratio)



sodas <- downloadStockPairDF("KO", "PEP", 2015, nyears = 1)

mean(sodas$ratio)
sd(sodas$ratio)

plotRatio(sodas)

plotRatio(sodas, k = 0.5)
plotRatio(sodas, k = 2)


































## weights:
133 120 97 106 124



x <- c(133, 120, 97, 106, 124)

x

## I need:
## xbar +/- (t-multipler) * s/sqrt(n)

xbar <- mean(x)
xbar

s <- sd(x)

n <- length(x)

## n = 5 ===> df = n-1 = 4

qnorm(0.975)   ## is the 97.5th percentile of Z

qt(0.975)  ## what else does it need ..???
## is there only one t-distribution...?
## no way, prof!
## it depends on the degrees of freedom!
## c'mon already!

t <- qt(0.975, df = 4)

t <- qt(0.995, df = 4)
t


xbar
s
n
t

c(-1, 1) * 5 + 10

xbar + c(-1,1) * t * s/sqrt(n)

## yay















-0.5 Ideally (as requested) line plots for the population-by-country plots.  But the values plotted are all correct

-1 Please don't include the full output from:
gapminder %>% select(country, year, pop) %>% arrange(pop)

'

-1 Missing output of df_summary(diamonds)


-1 Missing output of factorization function for larger inputs

-2 Missing derivation for the standard deviation (last problem)

-3 Missing derivations for the mean and standard deviation (last problem)

-2 That's not a derivation for standard deviation (last problem)

'

-2 Incorrect derivation for the standard deviation (last problem)





You'll need "full indexing" on all vectors and matrices in the model code.

So, for example, instead of distMat, you need to say distMat[1:nDim1, 1:nDim2]

This will apply for distMat, I.rhoW, inv, W, X, and any other vectors or matrices in the model code.

Also, I see in the data list,


  # inverse(I-rho*W)
  I.rhoW <- diag(N)- (rho*distMat)

  inv <- inverse(I.rhoW)

  # [inverse(I-rho*W)]*X

  inv.X <- inv %*% X.mat



library(spdep)
library(spatialreg)

data("baltimore")

N <- S <- 211
XY = cbind(baltimore$X,baltimore$Y)
#Adjcency matrix
distMat <- as.matrix(dist(XY))
distMat <- distMat / max(distMat) * 10
distMat <- distMat + 0.01 * diag(rep(1, 211))
#Compute eigenvalues for SLM model (as in Havard's code)
e <- eigen(distMat)
e.values<- e$values
rho.max <- 1 / max(e.values)
rho.min <- 1 / min(e.values)
X.mat = matrix(c(baltimore$DWELL,
                 baltimore$NBATH,baltimore$PATIO,
                 baltimore$FIREPL,baltimore$AC,
                 baltimore$BMENT, baltimore$GAR,
                 baltimore$CITCOU,baltimore$LOTSZ) , nrow = N)


library(nimble)

code <- SLMMCode <- nimbleCode({

  # rho.max = 0.0014  ,  rho.min = -0.00392  ,  mean = -0.00126
  rho ~ dunif(-0.00392 , 0.0014)



  # inverse(I-rho*W)
  I.rhoW <- diag(N)- (rho*distMat)

  inv <- inverse(I.rhoW)

  # [inverse(I-rho*W)]*X

  inv.X <- inv %*% X.mat




  for (i in 1:S) {
    y[i] ~ dnorm(mu_y[i], tau = tau_y_SLM)


    mu_y[i] <- b[i, 1] * inv.X[i,1] + b[i, 2] * inv.X[i,2] +
      b[i, 3] * inv.X[i,3] + b[i, 4] * inv.X[i,4] + b[i, 5] * inv.X[i,5] +
      b[i, 6] * inv.X[i,6] + b[i, 7] * inv.X[i,7] + b[i, 8] * inv.X[i,8] +
      b[i, 9] * inv.X[i,9]



    b[i, 1:9] <- bm[latent[i], 1:9]
    latent[i] ~ dcat(zlatent[1:M])


  }

  tau_y_SLM <- tau_y*inv


  for (k in 1:M) {
    bm[k, 1:9] ~ dmnorm(mu_bm[1:9], cov = var_bm[1:9, 1:9])
  }

  var_bm[1:9, 1:9] <- 1/tau_bm * diag(rep(1, 9))
  tau_bm ~ dgamma(1, 1)
  for (j in 1:9) {
    mu_bm[j] ~ dnorm(0, 1)
  }

  zlatent[1:M] <- stick_breaking(vlatent[1:(M - 1)])
  for (j in 1:(M - 1)) {
    vlatent[j] ~ dbeta(1, alpha)
  }
  alpha ~ dgamma(1, 1)
  tau_y ~ dgamma(1, 1)
})


data <- SLMMdata <- list(y = baltimore$PRICE,
                 Dist = distMat)

constants <- SLMMConsts <- list(S = 211, M = 50)

set.seed(12345)

inits <- SLMMInits <- list(tau_y  = 1,
                  latent = rep(1, SLMMConsts$S),
                  alpha  = 2,
                  tau_bm = 1,
                  mu_bm  = rnorm(9),
                  rho    = -0.00126,
                  vlatent = rbeta(SLMMConsts$M - 1, 1, 1))


Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)
samplesPlot(samples)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()

stochVars <- unique(nimble:::removeIndexing(Rmodel$getNodeNames(stochOnly = TRUE)))
for(v in stochVars) {
    lp <- Rmodel$calculate(v)
    cat(v, ': ', lp, '\n')
}



mcmc.out <- nimbleMCMC(code = SLMMCode, data = SLMMdata,
                       constants = SLMMConsts,
                       inits = SLMMInits,
                       monitors = c("bm","b","rho",
                                    "alpha", "latent", "tau_y"),
                       niter = 50000,
                       thin = 10, nchains = 1, setSeed = TRUE)


Defining model
Error in genVarInfo3() :
  Dimension of inv.X is 0, which does not match its usage in 'mu_y[i]
<- b[i, 1] * inv.X[i, 1] + b[i, 2] * inv.X[i, 2] + b[i, 3] * inv.X[i,
3] + b[i, 4] * inv.X[i, 4] + b[i, 5] * inv.X[i, 5] + b[i, 6] *
inv.X[i, 6] + b[i, 7] * inv.X[i, 7] + b[i, 8] * inv.X[i, 8] + b[i, 9]
* inv.X[i, 9]'.



library(nimble)

## If Y = X 1{a < X < b}. Let f = PDF of Y and g/G = PDF/CDF of X.
## f(y) = 1 / [G(b) - G(a)] g(y) 1{a < y < b}
dtruncbeta <- nimbleFunction(
    run = function(
                   x = double(0), shape1 = double(0), shape2 = double(0),
                   a = double(0,default = 0), b = double(0, default = 1),
                   log = integer(0, default = 0)
                   ) {
        returnType(double(0))
        if ( x <= a ) {
            lp <- -Inf
        } else if ( x >= b ) {
            lp <- -Inf
        } else {
            lp      <- dbeta(x, shape1, shape2, log = TRUE)   ## initialize to beta PDF
            logCDFa <- -Inf
            logCDFb <- 0
            if ( a > 0 ) {
                logCDFa <- pbeta(a, shape1, shape2, log.p = TRUE)
            }
            if ( b < 1 ) {
                logCDFb <- pbeta(b, shape1, shape2, log.p = TRUE)
            }
            lognc <- logCDFb + log( 1 - exp(logCDFa - logCDFb) )
            lp    <- -lognc + lp
        }
        if(log){
            return(lp)
        }
        return(exp(lp))
    }
)


## If Y = X 1{a < X < b}. Let F = CDF of Y and G = CDF of X.
## F(y) = [G(y) - G(a)] / [G(b) - G(a)]
ptruncbeta <- nimbleFunction(
    run = function(
                   q = double(0), shape1 = double(0), shape2 = double(0),
                   a = double(0,default = 0), b = double(0, default = 1),
                   lower.tail = double(0, default = 0),
                   log.p = integer(0, default = 0)
                   ) {
        returnType(double(0))
        if ( x <= a ) {
            lp <- -Inf
        } else if (x >= b) {
            lp <- 0
        } else {
            logCDFx <- pbeta(x, shape1 = shape1, shape2 = shape2, log.p = TRUE)
            logCDFa <- -Inf
            logCDFb <- 1
            if ( a > 0 ){
                logCDFa <- pbeta(a, shape1, shape2, log.p = TRUE)
            }
            if ( b < 1 ) {
                logCDFb <- pbeta(b, shape1, shape2, log.p = TRUE)
            }
            logNum <- logCDFx + log(1 - exp(logCDFa - logCDFx))
            logDen <- logCDFb + log(1 - exp(logCDFa - logCDFb))
            lp     <- logNum - logDen
        }
        if(log.p) {
            return(lp)
        }
        return(exp(lp))
    }
)


qtruncbeta <- nimbleFunction(
    run = function(
                   p = double(0), shape1 = double(0), shape2 = double(0),
                   a = double(0,default = 0), b = double(0, default = 1),
                   lower.tail = double(0, default = 0),
                   log.p = integer(0, default = 0)
                   ) {
        returnType(double(0))
        if(log.p) {
            x <- exp(x)
        }
        if ( x <= 0 ) {
            return(NaN)
        } else if ( x >= 1) {
            return(NaN)
        } else {
            CDFa  <- pbeta(a, shape1, shape2, log.p = FALSE)
            CDFb  <- pbeta(b, shape1, shape2, log.p = FALSE)
            return( qbeta( CDFa + (CDFb - CDFa) * x , shape1 = shape1, shape2 = shape2) )
        }
        return(NaN)
    }
)

## Use inverse-CDF method for random number generation
rtruncbeta <- nimbleFunction(
    run = function(
                   n = integer(0), shape1 = double(0), shape2 = double(0),
                   a = double(0,default = 0), b = double(0, default = 1)
                   )
        return(qtruncbeta(runif(n), shape1 = shape1, shape2 = shape2, a = a, b = b, log.p = FALSE))
)


registerDistributions(
    list(dtruncbeta = list(
             BUGSdist = 'dtruncbeta(shape1, shape2, a, b)',
             types = c('shape1 = double()', 'shape2 = double()', 'a = double()', 'b = double()'),
             pqAvail = TRUE)
         )
)

nimble:::conjugacyRelationshipsInputList[['XXX']] <- 22


nimble:::conjugacyRelationshipsInputList$dtruncbeta <- 
    ## dtruncbeta
    list(prior = 'dtruncbeta',
         link = 'identity',
         dependents = list(
             dbinom   = list(param = 'prob', contribution_shape1 = 'value', contribution_shape2 = '1 - value', contribution_a = '0', contribute_b = '0')),
         posterior = 'dtruncbeta(shape1 = prior_shape1 + contribution_shape1,
                             shape2 = prior_shape2 + contribution_shape2,
                             a      = prior_a,
                             b      = prior_b)')



n = 100

code <- nimbleCode({
    gamma ~ dtruncbeta(2, 2, 0, 0.75)
    # gamma ~ T(dbeta(2, 2), 0, 0.75)
    for ( i in 1:n ) {
        y[i] ~ dbinom(size = 1, prob = gamma)
    }
})

## Generate binomial data
set.seed(0)
y <- rbinom(100, size = 1, prob = 0.25)

## Model / configure
mod <- nimbleModel(code, constants = list('n' = n), data = list('y' = y), init = list('gamma' = 0.1))
mod$calculate()

modConf <- configureMCMC(mod)









library(spdep)
library(spatialreg)

data("baltimore")
N <- S <- 211
XY = cbind(baltimore$X,baltimore$Y)
#Adjcency matrix

distMat <- as.matrix(dist(XY))
distMat <- distMat / max(distMat) * 10
distMat <- distMat + 0.01 * diag(rep(1, 211))

#Compute eigenvalues for SLM model (as in Havard's code)
e <- eigen(distMat)
e.values<- e$values
rho.max <- 1 / max(e.values)
rho.min <- 1 / min(e.values)
X.mat = matrix(c(baltimore$DWELL,
  baltimore$NBATH,baltimore$PATIO,
  baltimore$FIREPL,baltimore$AC,
  baltimore$BMENT, baltimore$GAR,
  baltimore$CITCOU,baltimore$LOTSZ) , nrow = N)


dim(X.mat)

library(nimble)

SLMMCode <- nimbleCode({
  # rho.max = 0.0014  ,  rho.min = -0.00392  ,  mean = -0.00126
  rho ~ dunif(-0.00392 , 0.0014)
  # inverse(I-rho*W)
  I.rhoW <- diag(N)- (rho*distMat)
  inv <- inverse(I.rhoW)
  # [inverse(I-rho*W)]*X
  inv.X <- inv %*% X.mat

    for (i in 1:S) {
      y[i] ~ dnorm(mu_y[i], tau = tau_y_SLM)


    #  mu_y[i] <- b[i, 1] * T1[i] + b[i, 2] * T2[i] +
     #   b[i, 3] * T3[i] + b[i, 4] * T4[i] + b[i, 5] * T5[i] +
      #  b[i, 6] * T6[i] + b[i, 7] * T7[i] + b[i, 8] * T8[i] +
       # b[i, 9] * T9[i]



      b[i, 1:9] <- bm[latent[i], 1:9]
      latent[i] ~ dcat(zlatent[1:M])


    }

 tau_y_SLM <- tau_y*inv


  for (k in 1:M) {
    bm[k, 1:9] ~ dmnorm(mu_bm[1:9], cov = var_bm[1:9, 1:9])
  }

  var_bm[1:9, 1:9] <- 1/tau_bm * diag(rep(1, 9))
  tau_bm ~ dgamma(1, 1)
  for (j in 1:9) {
    mu_bm[j] ~ dnorm(0, 1)
  }

  zlatent[1:M] <- stick_breaking(vlatent[1:(M - 1)])
  for (j in 1:(M - 1)) {
    vlatent[j] ~ dbeta(1, alpha)
  }
  alpha ~ dgamma(1, 1)
  tau_y ~ dgamma(1, 1)
})


SLMMdata <- list(y = baltimore$PRICE, T1 = inv.X[,1],
                 T2 = inv.X[,2], T3 = inv.X[,3],
                 T4 = inv.X[,4], T5 = inv.X[,5],
                 T6 = inv.X[,6], T7 = inv.X[,7],
                 T8 = inv.X[,8], T9 = inv.X[,9],
                 Dist = distMat)

inv.X <- inverse(X.mat)

Error: object 'inv.X' not found






'hello'
"hello"

var <- 'a'
var
class(var)

letters[1:5]
LETTERS

f <- c('a', 'b', 'c', 'd', 'e', 'f')

"he's ok"

## paste

paste('a', 'b', 'dd', sep = '')
paste('a', 'b', 'dd', sep = ',')

paste0('a', 'b', 'd')


paste0(letters, collapse = '')
paste0(letters, 'X', collapse = ',')

paste0(c('a', 'b', 'c', 'd'), c('d', 'e'))

1:10 + 1:10

paste(letters, 'X', collapse = ',')
paste(letters, 'X')

paste0(paste0(letters, collapse=''), 'X')

paste0('ab', 'X')

message('hello')

a <- strsplit("several words together", " ")
a[[1]]

a <- strsplit(c('abcd', 'abacadaf'), 'a')

class(a)

a[[1]]
a[[2]]

letters[5]


strsplit(c("first string", "second string", "third string"), "i")

strsplit(c("first string", "second string", "third string"), "ii")

nchar('aaaa')

nchar(letters)

substr("abcdefg", 2, 4)
substr("abcdefg", 2, 4)

aa <- "abcdefg"
substr(aa, 4, nchar(aa))
substr(aa, 1, 4)

aa
substr(aa, 1, 4) <- 'X'
aa


paste0('A', substr(aa, 5, nchar(aa)))

dim(x) <- ddd
names(x) <- newnames

`names<-`

toupper("Williams College")
tolower("Williams College")



## replace all 'ab's with 'cd's ????

aa <- 'abXXabYYababZZ'

paste0(strsplit(aa, 'ab')[[1]], collapse = 'cd')

names <- c("Alice", "Bob", "Charlie", "David")

args(grep)

grep("a", names, value = TRUE)
grep("a", names, value = TRUE, ignore.case = TRUE)

grep("A", names, value = TRUE, ignore.case = TRUE, invert = TRUE)


grep(".", c("", "abc", "a.c"), value = TRUE)

grep("\\.", c("", "abc", "a.c"), value = TRUE)

grep("\\.?", c("", "abc", "a.c"), value = TRUE)

grep("^[[:alpha:]]{2,3}[[:digit:]]+@williams\\.edu$", strings, value = TRUE)

grep("([abc]).*\\1{2}", c("abcde", "abcaa", "abcabcX"), value = TRUE)

grep("([abc]).*\\1.+\\1", c("abcde", "abcaa", "abcabcX"), value = TRUE)



    




















1











library(dplyr)
library(nimble)

Estuary <- read.csv("~/Downloads/data set.csv", sep = ",")



site <- Estuary$River
sex <- Estuary$Sex
species <- Estuary$Species
size <- Estuary$ZSize
WL <- Estuary$ZresWL
year <- Estuary$Year

y <- select(Estuary, -c(1:6))
y <- as.matrix(y)

### MODEL HMM VF ----
#---

code <- phitpt <- nimbleCode({
 
  # coefficients
  alpha ~ dnorm(mean = 0, sd = 1.5)
  beta_site[1] <- 0
  beta_site[2] ~ dnorm(0, 1.5)
  beta_site[3] ~ dnorm(0, 1.5)
  beta_site[4] ~ dnorm(0, 1.5)
  beta_species[1] <- 0
  beta_species[2] ~ dnorm(0, 1.5)
  beta_sex[1] <- 0
  beta_sex[2] ~ dnorm(0, 1.5)
  beta_size ~ dnorm(0, 1.5)
  beta_WL ~ dnorm(0, 1.5)
  beta_dist ~ dnorm(0, 1.5)
  delta[1] <- 1
  delta[2] <- 0
 
   
  # phi: coded to vary in between receivers (r)
  for (i in 1:N) {
    for (r in first[i]:last[i]-1) {
      logit(phi[i, r]) <- alpha + beta_site[site[i]] + beta_species[species[i]] + beta_sex[sex[i]] +
                          (beta_size * size[i]) + (beta_WL * WL[i]) 
    }
  }
 
  # p: vary between receivers (r) and year of study (t)
  for (r in 1:T-1){
    for (t in 1:2){
      p[t, r] ~ dunif(0,1)
    }
  }
 

  # transition matrices
  for (i in 1:N){
    for (r in first[i]:last[i]-1){
     
      gamma[1, 1, i, r] <- phi[i, r]            # alive at t-1 & alive at t
      gamma[1, 2, i, r] <- 1 - phi[i, r]        # alive at t-1 & dead at t
      gamma[2, 1, i, r] <- 0                    # dead at t-1 & alive at t
      gamma[2, 2, i, r] <- 1                    # dead at t-1 & dead at t
     
      omega[1, 1, i, r] <- 1 - p[year[i], r]       # alive & detected at rec a
      omega[1, 2, i, r] <- p[year[i], r]   # alive & not detected at rec a
      omega[2, 1, i, r] <- 1                   # dead & detected at rec a
      omega[2, 2, i, r] <- 0                    # dead & not detected at rec r
     
    }
  }
 
  # state-space model likelihood
  for (i in 1:N) {
    z[i, first[i]] ~ dcat(delta[1:2])
    for (r in (first[i]+1):last[i]) {
      z[i, r] ~ dcat(gamma[z[i, r - 1], 1:2, i, r - 1])
      y[i, r] ~ dcat(omega[z[i, r], 1:2, i, r - 1])
    }
  }
 })



constants$N
Rmodel$calculate('z')
Rmodel$calculate('z[,]')
Rmodel$calculate('z[1,2]')
constants$first[1]
constants$last[1]
Rmodel$logProb_z

zinits[1,2]
zinits[1,1]

Rmodel$gamma[1,,1,1]
Rmodel$calculate('gamma')
Rmodel$phi[1,1]

dim(Rmodel$gamma)
dim(Rmodel$omega)

inits$z
## data

my.data <- list(y = y + 1)

## constants
first <- apply(y, 1, function(x) min(which(x !=0)))
last <- apply(y, 1, function(x) max(which(! is.na(x))))

my.constants <- list(N = nrow(y),
                     T = ncol(y),
                     site = site,
                     year = year,
                     first = first,
                     size = size,
                     species = species,
                     sex = sex,
                     WL = WL,
                     last = last)

## Set initial values
zinits <- y + 1 # non-detection -> alive

dim(zinits)
dim(y)
y
zinits


zinits[zinits == 2] <- 1 # dead -> alive
initial.values <- function() list(alpha = rnorm(1,0,1),
                                  beta_site = rep(0,4),   ## length 4
                                  beta_species = rep(0,2),   ## length 2
                                  beta_sex = rep(0,2),       ## length 2
                                  beta_size = rnorm(1,0,1),
                                  beta_WL = rnorm(1,0,1),
                                  beta_dist = rnorm(1,0,1),
                                  z = zinits)

# Define parameters to save
#---
parameters.to.save <- c("phi","p", "z", "beta_site", "beta_species", "beta_sex", "beta_size",
                        "beta_WL", "beta_dist")

## mcmc details
n.iter <- 5000
n.burnin <- 1000
n.chains <- 2
n.thin <- 1


code <- phitpt
constants <- my.constants
data <- my.data
set.seed(0)
inits <- initial.values()

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

i <- 1

dim(Rmodel$z)

for(i in 1:47) {
    print(i)
    print(Rmodel$calculate(paste0('z[', i, ',]')))
}

i <- 40
Rmodel$z[i,]
constants$first[i]
constants$last[i]
y[40,]
which(y[40,] == 1)
zinits[40,]

y

Rmodel$calculate('z[40,25]')

conf <- configureMCMC(Rmodel, monitors = parameters.to.save)
pppp
conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc


mcmc.phitpt$samples[[1]][2,]
Cmodel <- compileNimble(Rmodel)
Cmodel$calculate()

Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 100)

dim(samples)
samples

Cmodel$calculate()




## fit
mcmc.phitpt <- nimbleMCMC(code = phitpt,
                      constants = my.constants,
                      data = my.data,
                      monitors = parameters.to.save,
                      inits = initial.values,
                      thin = n.thin,
                      niter = n.iter,
                      nburnin = n.burnin,
                      nchains = n.chains,
                      WAIC = TRUE)


names(mcmc.phitpt)

mcmc.phitpt$samples



substr("abcdefg", 2, 4)     ## extract from position 2 to 4

x <- "abcdefg"

substr(x, 2, 4) <- "123"    ## replace positions 2-4 with "123"

x

substr(x, 2, 4) <- "+"    ## now remove replace positions 2-4

x

n <- 1000
mean(rpois(n, 50))
var(rpois(n, 50))


mean(rexp(n, 50))
var(rexp(n, 50))

install.packages('gutenbergr')
library(gutenbergr)

gutenberg_works()

library(gutenbergr)
library(dplyr)

gutenberg <- gutenberg_works()     ## works available for download
gutenberg %>% filter(author == "Einstein, Albert") %>% pull(title)
gutenberg %>% filter(author == "Twain, Mark")

relativity <- gutenberg_download(7333)   ## download texts by ID number
relativity


unique(gutenberg$author)    ## list authors available.... 12,988 of them!
gutenberg$title[1000:1010]  ## peruse some titles available

gutenberg_metadata$language

gutenberg_metadata %>% filter(language == "fr", has_text == TRUE)

(25000-23800)/4380

1 - pnorm(25000, 23800, 4380)
qnorm(0.1,  23800, 4380)
23800 - 1.28 * 4380

pnorm(-2.33)* 100

.3^5

0.7^5

pbinom(2, 5, .3)


(c(18000, 22000)-23800)/4380

list()


pnorm(-.411)
pnorm(-1.324)
pnorm(-.411) - pnorm(-1.324)

## unnamed lists
a <- list(3, 'a', 1:10, FALSE, rep(c(TRUE,FALSE), 4))

a
a[[1]]
a[[2]]
a[[3]]

class(a)
class(a[[2]])
class(a[[4]])

## a[] will return a list
a[1]
a[1:2]
a[c()]

c(1,4,5)
b <- a[c(1,4,5)]

b
class(b)

## inspect a list
str(a)
class(a)
length(a)
names(a)






## named lists

1:4 > 2

mean

lst <- list(a = 3,         ## numeric scalar
            b = 1:10,      ## integer vector
            c = 1:4 > 2,   ## boolean (logical) vector
            d = array(1:6, c(2,3)),    ## 2x3 numeric array
            e = "here's a string",     ## character string
            f = mean,                  ## an R function
            g = function(x) x+3)       ## our own inline function

lst
length(lst)
names(lst)

str(lst)

a


lst2 <- list(5, a, lst)

lst2

lst2[[1]]
lst2[[2]][[3]]
lst2[[2]][3]

lst

a
b
c
d


## LIST$NAME does *not* evaluate "name"
lst$a

## LIST[[VARIABLE WHICH CONTAINS A NAME]]

lst$d

lst[[d]]

somename <- 'd'
somename

lst[[somename]]


accessListElementByName <- function(l, name) {
    l[[name]]
}

accessListElementByName(lst, 'd')
accessListElementByName(lst, 'e')

lst$zzz

lst

names(lst) <- c('aa', 'bb', 'cc', .....)
names(lst)[7] <- 'newname'

lst

names(lst)

lst[[6]] <- NULL

lst[4] <- NULL

lst[[3]] <- list(NULL)
lste

lst[3] <- NULL


lst

lst[[3]] <- 100
lst
lst$e <- 200
lst


## lapply "list apply" applies *any function*
## to each element of a list,
## and returns a **list** of the results

f([[1]])
f([[2]])
f([[3]])

lst2

lst <- list(a = 3,         ## numeric scalar
            b = 1:10,      ## integer vector
            c = 1:4 > 2,   ## boolean (logical) vector
            d = array(1:6, c(2,3)),    ## 2x3 numeric array
            e = "here's a string",     ## character string
            f = mean,                  ## an R function
            g = function(x) x+3) 




lst

args(lapply)

lapply(lst, class)

lapply(lst, is.numeric)


mean('d')
lapply(lst, mean)

## most useful... apply *your own*
## often in-line function

lapply(lst, class)
lapply(lst, function(x) class(x))

lapply(lst, function(x) list(class(x), x))

## sapply

a <- list(1, 1:10, array(1:6, c(2,3)))
a

lapply(a, mean)   ## return a list

sapply(a, mean)

sapply(a, is.function)

lapply(a, function(x) array(0, c(2,1)))

sapply(a, function(x) array(0, c(2,1)))

lst

lapply(lst, function(x)
    if(is.numeric(x)) x+1 else x) 


sapply(lst, function(x)
    if(is.numeric(x)) x+1 else x) 


args(apply)


mat <- matrix(1:6, nrow = 2)

mat
mat+1

apply(mat, c(1,2), function(x) x+1)

1*.99 / (1*.99 + (100000-1)/100) * 100










1





library(nimble)
library(coda)
##library(R2WinBUGS)
library(MCMCpack)
#library(MCMCglmm)



trial<-read.table("~/Downloads/proba.txt",h=TRUE)
trial
dim(trial)
head(trial)
#length(trial$yi)

### definition of factor in the data set
trial$sys<-as.factor(trial$sys)
#trial$system<-as.factor(trial$system)
#trial$year<-as.factor(trial$year)
#trial$site<-as.factor(trial$site)
trial$var<-as.factor(trial$var)
trial$rep<-as.factor(trial$rep)
trial$env<-as.factor(trial$env)
#trial$s<-as.factor(trial$s)


N<-length(trial$yield)
n<-N/2
ne<-6
nv<-11
nr<-3
me1<-aggregate(yield/10~var,data=trial[1:n,],na.rm=TRUE,FUN="mean")
va1<-var(me1[,2]);va1
me2<-aggregate(yield/10~var,data=trial[(n+1):N,],na.rm=TRUE,FUN="mean")
va2<-var(me2[,2]);va2
s1<-2.5*sqrt(va1);
s2<-2.5*sqrt(va2)


S<-matrix(nrow=2,ncol=2)
S[1,1]<-va1/2
S[1,2]<-0
S[2,1]<-0
S[2,2]<-va2/2

#Rinv<-solve(R);Rinv


R2<-diag(1,2)

y<-trial$yield/10#;y
sys<-trial$sys#;sys
env<-trial$env
rep<-trial$rep
v<-trial$var#;v


uppertri_mult_diag <- nimbleFunction(
run = function(mat = double(2), vec = double(1)) {
returnType(double(2))
p <- length(vec)
out <- matrix(nrow = p, ncol = p, init = FALSE)
for(i in 1:p)
out[ , i] <- mat[ , i] * vec[i]
return(out)
})

#### Model definition
code <- model<-nimbleCode({
    for(i in 1:N){
        y[i]~dnorm(mu[i],tau=tau[sys[i]])#dt(mu[i],tau=tau[sys[i]],df=7)
        mu[i]<-b[rep[i],env[i],sys[i]]+va[v[i],sys[i]]+ge[env[i],v[i],sys[i]]
    }
    for(l in 1:2){tau[l]~dgamma(2,10)
        sig2[l]<-1/tau[l]}
    cc[1]<-1
    cc[2]<-1
    S2[1:2,1:2]<-diag(cc[1:2])
    covV[1:2,1:2]~dinvwish(S=S[1:2,1:2],df=3)
    covEV[1:2,1:2]~dinvwish(S=S2[1:2,1:2],df=4)
    mu1[1]<-0
    mu1[2]<-0
    for(r in 1:nr){
        for(j in 1:ne){
            for(l in 1:2){
                b[r,j,l]~dnorm(0,tau=0.000001)
            }
        }
    }
    for(k in 1:nv){
        va[k,1:2]~dmnorm(mu1[1:2],cov=covV[1:2,1:2])
    }
    for(j in 1:ne){
        for(k in 1:nv){
            ge[j,k,1:2]~dmnorm(mu1[1:2],cov=covEV[1:2,1:2])
        }
    }
})

X<-riwish(4,diag(2))
Z1<-riwish(3,diag(2))
Z2<-riwish(3, diag(c(0.001,1000),2));Z2
Z3<-riwish(3,diag(c(500,0.1),2));Z3# diag(2)

#### Data and constants

constants <- modelConsts<-list(N=N,ne=ne,nv=nv,nr=nr,rep=c(rep),env=c(env),sys=c(sys),v=c(v))


data <- modelData<-list(y=c(y),S=S)

### initial valuestauR = c(0.01,1000), tauR= c(100,0.1),tauR=c(1000,0.001),

inits <- inits1<-list(covV = diag(2),covEV = diag(2) ,tau=c(0.01,0.01))#
inits2<-list(covV=diag(c(100,0.1),2),covEV=Z2,tau=c(10,100))#
inits3<-list(covV=X,covEV=Z3,tau=c(0.001,0.001))#

initsList <- Inits<- list(inits1,inits2,inits3)


#### MCMC setup
Rmodel <- nimbleModel(code = model, constants = modelConsts,data = modelData,inits=inits1)

Rmodel$calculate()

##myModel <- compileNimble(Rmodel)
## 
##myModel$calculate()
## 
##myMCMC <- buildMCMC(Rmodel)

myMCMCconf<-configureMCMC(Rmodel,enableWAIC=TRUE,print=TRUE)

##myMCMCconf$addMonitors(c("sig2","covV","covEV","tau","b","va","ge"))
##CmyModel <- buildMCMC(myMCMCconf)
##CmyMCMC <- compileNimble(CmyModel,project=myModel)
## 
###### MCMC
##mcmc.out <- runMCMC(CmyMCMC, niter = 20000, nburnin=10000,thin=40,
##nchains = 3,inits = Inits,
##summary = TRUE, progressBar=TRUE,
##samples=TRUE,samplesAsCodaMCMC=TRUE)
## 
###### MCMC summary statistics and WAIC
## 
##print(mcmc.out$summary$all.chains,digit=4)
##calculateWAIC(CmyMCMC)


##options(error = recover)
##undebug(runCrossValidate)
##debug(nimble:::calcCrossVal)

runCrossValidate(MCMCconfiguration=myMCMCconf,
                        k=4,
                        foldFunction="random",
                        lossFunction="predictive",
                        MCMCcontrol = list(niter = 5000,
                  nburnin = 500))

#calculateWAIC(mcmc.out,CmyMCMC)

###### Samples
mcmc1<-as.mcmc(mcmc.out$samples$chain1)#
mcmc2<-as.mcmc(mcmc.out$samples$chain2)
mcmc3<-as.mcmc(mcmc.out$samples$chain3)
mcmcb<-mcmc.list(mcmc1,mcmc2,mcmc3)

###### Diagnostic plots
gelman.diag(mcmcb,multivariate=FALSE)
gelman.plot(mcmcb,ask=TRUE)
plot(mcmcb,ask=TRUE)

When I've run the code I've obtained the following error
 
> runCrossValidate(MCMCconfiguration=myMCMCconf,
+ k=4,
+ foldFunction="random",
+ lossFunction="predictive",
+ MCMCcontrol = list(niter = 5000,
+                   nburnin = 500))
dropping data fold 1
Checking model sizes and dimensions
  [Note] This model is not fully initialized. This is not an error.
         To see which variables are not initialized, use model$initializeInfo().
         For more information on model initialization, see help(modelInitialization).
Compiling
  [Note] This may take a minute.
  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.
Compiling
  [Note] This may take a minute.
  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.
|-------------|-------------|-------------|-------------|
|-------------------------------------------------------|
Error in as.matrix(C.modelMCMC$mvSamples)[, leaveOutNames, drop = FALSE] :
  subscript out of bounds



library(nimble)
library(basicMCMCplots)
library(coda)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b <- Rnf2(a)
})
constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Rmodel$getNodeNames()

Rmodel$nodes
ref <- Rmodel$nodes[[ls(Rmodel$nodes)[2]]]
ref
ls(ref)
ref$calculate
ref$simulate

Cmodel <- compileNimble(Rmodel)

Rmodel$calculate('b')
Rmodel$b

Cmodel$calculate('b')
Cmodel$b



Rnf2 <- nimbleFunction(
    run = function(arg = double()) {
        returnType(double())
        x <- rnorm(1, 0, 1)
        return(x)
    }
)

Cnf2 <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

set.seed(0)
Rnf2(1)
set.seed(0)
Cnf2(1)

stochVars <- unique(nimble:::removeIndexing(Rmodel$getNodeNames(stochOnly = TRUE)))
for(v in stochVars) {
    lp <- Rmodel$calculate(v)
    cat(v, ': ', lp, '\n')
}



df <- read.csv('~/Downloads/STAT202_2.csv')
str(df)
summary(df)

summary(df$Friends)

https://williams.zoom.us/j/99485772817

gapminder <- read.csv('~/Downloads/gapminder.csv')

barplot(
    table(
        subset(gapminder, select = c(unique(country), continent))
    )
)



countries <- numeric()
for(cont in unique(gapminder$continent)) {
    countries[cont] <- length(unique(subset(gapminder, subset = continent == cont)$country))
}
barplot(countries)




## assignment; always safe
a <- 4

## sometimes... will get you into trouble ....
a = 4

## non-standard evlaution... can sometimes get you into trouble.



install.packages('tidyverse')


str(gapminder)
dim(gapminder)

## the "pipe" operator %>%     >

FUN(arg1, arg2)
arg1 %FUN% arg2

+(1, 4)
1 + 4




## select (for selecting certain variables)

class(gapminder$lifeExp)

library(dplyr)

head(select(gapminder, lifeExp))

hist(select(gapminder, lifeExp))

hist(gapminder$lifeExp)

pull(gapminder, lifeExp)


## select returns a data frame
## pull get the contents of one variable into an R vector

mean(pull(gapminder, lifeExp))


## %>% takes the argument on the left, and makes is at the
## *first* argument to the function on the right.

####  ARG1 %>% FUN(b1, b2, b3)
####  ## is EXACTLY the same as:
####  FUN(ARG1, b1, b2, b3)

ARG1 is generally a data frame .
And, all functions in the tidyverse take a *dataframe* as the first arg

a <- gapminder %>% select(lifeExp, gdpPercap)
class(a)
head(a)

str(gapminder)

gapminder %>% pull(lifeExp)

a <- gapminder %>% pull(gdpPercap, lifeExp)
class(a)
length(a)

## select let us choose *columns*

## filter
## lets you select certain *rows* from a dataframe

gapminder %>% pull(lifeExp) %>% range()

gapminder %>% filter(lifeExp > 80)

gapminder %>% filter(lifeExp > 80, continent == 'Europe') %>% nrow()

## base R:

gapminder[FILTER , SELECT]

gapminder[ROWS, COLUMS]

gapminder[1:10, ]
gapminder[c(1,4,6,8) , ]


gapminder[c(1,4,6,8) , 1:2 ]
gapminder[c(1,4,6,8) , c('country', 'year') ]
gapminder[c(1,4,6,8) , c('country', 'continent') ]

gapminder$lifeExp > 80


gapminder[ gapminder$lifeExp > 80 , c('country', 'continent') ]

gapminder[ gapminder$lifeExp > 80 ,]
gapminder[ gapminder$lifeExp > 80 , c(-1,-2)]


gapminder[ gapminder$lifeExp > 80 & gapminder$continent == 'Europe',]

dim(gapminder)

gapminder[1:60, ]


## R also procies a subset function


subset(gapminder, subset ... filter, select.. which colums)

subset(gapminder, subset = lifeExp > 80, select = c('lifeExp', 'continent'))

lifeExp > 80
lifeExp

## won't work in programs (oftentimes, anyway):
gapminder %>% filter(lifeExp > 80)

gapminder[ gapminder$continent %in% c('Europe', 'Asia'), ]

continent
continent %in% c('Europe', 'Asia')
gapminder$continent

gapminder$continent %in% c('Europe', 'Asia')



dfdsfdsfdsf

## functions in R
## are fun(ction) !!!
## and really powerful


a <- function(ARGUMENTS) {
    BODY
}

a()

SOMETHING


addTen <- function(arg) return(arg+10)
addTen <- function(arg) arg+10

addTen <- function(arg) {
    return(invisible(arg+10))
}


b <- addTen(13)

b

addTen(13)


addTen <- function(arg = 4) {
    arg + 10
}

addTen(100)
addTen()


## ways to print helpful stuff
## print, stop, warning, message, cat

addTen <- function(a) {
    if(missing(a)) stop('argument was not provided')
    return(a + 10)
}

addTen(4)

addTen()


addTen <- function(a) {
    if(missing(a)) warning('argument was not provided')
    return(a + 10)
}



addTen(5)

addTen()

b <- 4
c <- 'str'

## really useful way of printing helpful things to the screen
message('heres something', b, 'more text ', c)
## less useful ways of doing so: print, cat


## factorial function
factorial <- function(n) {
    if(n != floor(n)) stop('argument n is not an integer')
    if(n < 0) stop('argument n is negative')
    prod <- 1
    for(i in 1:n) {
        prod <- prod * i
    }
    return(prod)
}

factorial(3)
factorial(3.5)
factorial(-3)

## recursive function

fac2 <- function(n) {
    ## base case
    if(n == 1) return(1)
    ## recursive case (when n >= 2)
    return( n * fac2(n-1) )
}


fac2(2)

fac2 with n = 2
return( 2 * fac2(1) )
fac2(1): returns 1
return( 2 * 1 )

fac(3)
3 * fac(2)
3 * 2 *  fac(1)
3 * 2 *  1

factorial <- function(n) {
    ## put some error checking here,
    ## and in case n is missing
    prod(1:n)
}


factorial(-5)

a
df
b <- 5

a()
df
b

class(a)
class(df)
class(b) == 'function'

is.numeric(b)
is.function(a)


a
rm(a)

mean <- 1:5

mean
mean(mean)

mean
rm(mean)
mean

df <- data.frame(a = 1:5, b = 11:15)
df

modifyCol <- function(df, colNum, newValue) {
    colNum <- colNum + 1
    df[ , colNum ] <- newValue
    return(df)
}


## R functions pass arguments... BY VALUE
## what this means to you... is they do NOT modify their arguments

df

modifyCol(df, 2, 21:25)

## what is in the 2nd column of the df variable ...?
## what this means to you... is they do NOT modify their arguments
df

df <- modifyCol(df, 1, 100)
df

colNum <- 1
df <- modifyCol(df, colNum, 100)
df
colNum


## what happens here...?

df <- modifyCol(df, 1, c(1,2) )

v <- 1:10
v[c(TRUE, FALSE, FALSE)]



## pass a function as an argument


applyGeneralFunction <- function(x, f) {
    f(x)
}

g <- function(x) x > 4
g

applyGeneralFunction(1:10, g)


## creating and returning a function

createDivisionFunction <- function(divisorqq) {
    f <- function(x) return(x / divisor)
    return(f)
}


divide6 <- createDivisionFunction(6)

class(divide6)
divide6(12)

divide6(600)






conf$getSamplers()
conf$getSamplers('thetaTilde')



findSamplersOnNodes
nimble:::findSamplersOnNodes
conf$findSamplersOnNodes


library(nimble)
library(testthat)

code <- nimbleCode({
    for(i in 1:2)
        for(j in 1:2)
            x[i,j] <- 1
})

Rmodel <- nimbleModel(code)

Rmodel$getNodeNames()
Rmodel$expandNodeNames('x', sort = TRUE)


gapminder <- read.csv('~/Downloads/gapminder.csv')
library(dplyr)


x = subset(gapminder, subset = country == "Mexico" | country == "Zimbabwe" | country == "Denmark" | country == "China" | country == "United States", select = c(country, year, pop))

YearVector2 = c(unique(x$year))

Population= c()

for(ctry in unique(x$country)) {
  for(yr in YearVector2) {
      Population <- append(Population, subset(x, subset = year == yr & country == ctry)$pop)
  }
    ##plot(YearVector2, Population, xlab="Year", type="l", main= "Population over Time")
}



gapminder %>% select(pop)
gapminder %>% pull(pop)

gapminder %>% select(pop, year, country) %>% arrange(desc(pop)) %>% head(8) %>% nrow()

library(ggplot2)

countryList <- c("Mexico", "Zimbabwe", "Denmark", "China", "United States")

gapminder %>%
    filter(country %in% countryList) %>%
        ggplot(aes(x = year, y = pop, color = country)) +     ## notice plus (+) here
    geom_line() + scale_y_log10()



N <- 4000000
suc <- 0
x <- c(1,1,2,2,3,3)
for(i in 1:N) {
    xp <- sample(x)
    if(xp[1] != 1 && xp[2] != 1 && xp[3] != 2 && <xp[4] != 2 && xp[5] != 3 && xp[6] != 3) suc <- suc + 1
}
suc/N
1/9    



library(spdep)
library(spatialreg)
data("baltimore")

N <- S <- 211
XY = cbind(baltimore$X,baltimore$Y)
#Adjcency matrix
distMat <- as.matrix(dist(XY))
distMat <- distMat / max(distMat) * 10
distMat <- distMat + 0.01 * diag(rep(1, 211))

e <- eigen(distMat)
e.values<- e$values
rho.max <- 1 / max(e.values)
rho.min <- 1 / min(e.values)

library(nimble)

code <- SLMMCode <- nimbleCode({
    for (i in 1:S) {
        y[i] ~ dnorm(mu_y[i], tau = tau_y)
        mu_y[i] <- (inverse(I[1:N,1:N]-rho*distMat))*(b[i, 1] * x1[i] +
                                                      b[i, 2] * x2[i] +
                                                      b[i, 3] * x3[i] + b[i, 4] * x4[i] + b[i, 5] * x5[i] +
                                                      b[i, 6] * x6[i] + b[i, 7] * x7[i] + b[i, 8] * x8[i] +
                                                      b[i, 9] * x9[i])
        b[i, 1:9] <- bm[latent[i], 1:9]
        latent[i] ~ dcat(zlatent[1:M])
    }
    phi ~ dunif(0, D)
    rho ~ dunif(rho.min, rho.max)
    for (k in 1:M) {
        bm[k, 1:9] ~ dmnorm(mu_bm[1:9], cov = var_bm[1:9, 1:9])
    }
    var_bm[1:9, 1:9] <- 1/tau_bm * diag(rep(1, 9))
    tau_bm ~ dgamma(1, 1)
    for (j in 1:9) {
        mu_bm[j] ~ dnorm(0, 1)
    }
    zlatent[1:M] <- stick_breaking(vlatent[1:(M - 1)])
    for (j in 1:(M - 1)) {
        vlatent[j] ~ dbeta(1, alpha)
    }
    alpha ~ dgamma(1, 1)
    tau_y ~ dgamma(1, 1)
})


data <- SLMMdata <- list(y = baltimore$PRICE, x1 = baltimore$DWELL,
                 x2 = baltimore$NBATH,x3 = baltimore$PATIO,
                 x4 = baltimore$FIREPL,x5 = baltimore$AC,
                 x6 =baltimore$BMENT, x7 = baltimore$GAR,
                 x8 =baltimore$CITCOU, x9 = baltimore$LOTSZ,
                 Dist = distMat)

constants <- SLMMConsts <- list(S = 211, M = 50, D = 100)

set.seed(12345)

inits <- SLMMInits <- list(tau_y = 1,
                           latent = rep(1, SLMMConsts$S),
                           alpha = 2,
                           tau_bm = 1,
                           mu_bm = rnorm(9),
                           phi = 1,
                           rho =-0.0013,
                           vlatent = rbeta(SLMMConsts$M - 1, 1, 1))


Rmodel <-  nimbleModel(code, constants, data, inits)

mcmc.out <- nimbleMCMC(code = SLMMCode, data = SLMMdata,
                       constants = SLMMConsts,
                       inits = SLMMInits,
                       monitors = c("bm","b","phi",
                                    "alpha", "latent", "tau_y"),
                       niter = 50000,
                       thin = 10, nchains = 1, setSeed = TRUE)



which.min(c(1,1,0,0)

modelCode_2MIX <- nimbleCode({
    
    p0[1] ~ dunif(0, 1)
    p0[2] ~ dunif(0, 1)
    one ~ dconstraint(p0[1] <= p0[2])
    pi ~ dunif(0, 1)
    for (j in 1:n.detectors){
        u[j] ~ dbern(pi)
    }
    p0Vec[1:n.detectors] <- (1-u[1:n.detectors])*p0[1] +  u[1:n.detectors]*p0[2]
    
    sigma ~ dunif(0,50)
    psi ~ dunif(0,1)

    for (i in 1:n.individuals){
        sxy[i, 1:2] ~ dbernppAC(
            lowerCoords = lowerHabCoords[1:numHabWindows, 1:2],
            upperCoords = upperHabCoords[1:numHabWindows, 1:2],
            logIntensities = logHabIntensity[1:numHabWindows],
            logSumIntensity = logSumHabIntensity,
            habitatGrid = habitatGrid[1:y.max,1:x.max],
            numGridRows =  y.max,
            numGridCols = x.max
        )
        
        z[i] ~ dbern(psi)

        yBinded[i, 1:dimYbinded] ~ dbinomLocal_normal(
            size = trials[1:n.detectors],
            p0Traps = p0Vec[1:n.detectors],
            s = sxy[i,1:2],
            sigma = sigma,
            trapCoords =  detector.xy[1:n.detectors,1:2],
            localTrapsIndices = localTrapsIndices[1:n.cells,1:maxNBDets],
            localTrapsNum = numLocalTraps[1:n.cells],
            resizeFactor = ResizeFactor,
            habitatGrid = habitatGridDet[1:y.maxDet,1:x.maxDet],
            indicator = z[i],# 1,
            lengthYCombined = dimYbinded)
    }#i
    N <- sum(z[1:n.individuals])
    
})



9S 7H 3H AD 9C QS 5D TS
9D 5C 7S 5H AH 8C 9H 3C
2H 7D AC 8H 6C QD 4C KH
KD 4S KS JH JS 8S 6H TC
3S TH 7C 2S 5S 2C 6D 4H
TD JD KC QH JC 2D AS 4D
3D 6S QC 8D


sampler_MYNEWSAMPLER <- nimbleFunction(
    name = 'sampler_MYNEWSAMPLER',
    contains = sampler_BASE,     ## must be exactly like this
    setup = function(model, mvSaved, target, control) {  ## must be exactly like this
        ## any R setup code here
        ## anything that executes in R
        ## generally has any of the following components:
        ## control list extraction
        ## node list generation
        ## numeric value generation
        ## checks
    },
    run = function() {
        ## this function is called on every iteration of the MCMC
        ## performs any stochastic update to the "target" node
        ## the important thing, is at the end of this function,
        ## the values in the "model" and values in the "mvSaved" object are identical
        ## for accept/reject algorithms (which may or may not transisiton,
        ## this generally looks like:
        if(transitioned) {   ## copy from the updated model object, into the mvSaved object
            nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
        } else {             ## copy original values from mvSaved, over-writing the rejected changes in model
            nimCopy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE)
        }  ## where calcNodes is all nodes which might have been changed
    },
    methods = list(
        reset = function() { }    ## this method must exist
        ## you can optionally add other methods here, also
    )
)




gapminder <- read.csv("~/Downloads/gapminder.csv")

names(gapminder)
dim(gapminder)
ncol(gapminder)
nrow(gapminder)
head(gapminder)
str(gapminder)
head(gapminder)
tail(gapminder, 10)
summary(gapminder)


hist(gapminder$lifeExp)

str(gapminder)

levels(gapminder$continent)
nlevels(gapminder$continent)
table(gapminder$continent)
summary(gapminder$continent)   ## same as table


# Use the distribution in R
(lowerCoords <- matrix(c(0, 0, 1, 0, 0, 1, 1, 1), nrow = 4, byrow = TRUE))
(upperCoords <- matrix(c(1, 1, 2, 1, 1, 2, 2, 2), nrow = 4, byrow = TRUE)) 
(logIntensities <- log(c(1:4)))
(logSumIntensity <- log(sum(c(1:4))))
(habitatGrid <- matrix(c(1:4), nrow = 2, byrow = TRUE))
(numGridRows <- nrow(habitatGrid))
(numGridCols <- ncol(habitatGrid))

dbernppAC(c(0.5, 1.5), lowerCoords, upperCoords, logIntensities, logSumIntensity, 
          habitatGrid, numGridRows, numGridCols, log = TRUE)




##################################
##-- 2-Group Mixture SCR Model
##################################

library(nimble)
library(testthat)
library(nimbleSCR)

configureRJ(conf, targetNodes = c('beta1', 'sigma'), priorProb = 0.5)

code <- nimbleCode({
    p0[1] ~ dunif(0, 1)
    p0[2] ~ dunif(0, 1)
    one ~ dconstraint(p0[1] <= p0[2])
    pi ~ dunif(0, 1)
    for (j in 1:n.detectors){
        u[j] ~ dbern(pi)
    }
    p0Vec[1:n.detectors] <- (1-u[1:n.detectors])*p0[1] +  u[1:n.detectors]*p0[2]
    ##
    sigma ~ dunif(0,50)
    psi ~ dunif(0,1)
    ##
    for (i in 1:n.individuals){
        sxy[i, 1:2] ~ dbernppAC(
            lowerCoords = lowerHabCoords[1:numHabWindows, 1:2],
            upperCoords = upperHabCoords[1:numHabWindows, 1:2],
            logIntensities = logHabIntensity[1:numHabWindows],
            logSumIntensity = logSumHabIntensity,
            habitatGrid = habitatGrid[1:y.max,1:x.max],
            numGridRows =  y.max,
            numGridCols = x.max
        )
        ##
        z[i] ~ dbern(psi)
        ##
        yBinded[i, 1:dimYbinded] ~ dbinomLocal_normal(
            size = trials[1:n.detectors],
            p0Traps = p0Vec[1:n.detectors],
            s = sxy[i,1:2],
            sigma = sigma,
            trapCoords =  detector.xy[1:n.detectors,1:2],
            localTrapsIndices = localTrapsIndices[1:n.cells,1:maxNBDets],
            localTrapsNum = numLocalTraps[1:n.cells],
            resizeFactor = ResizeFactor,
            habitatGrid = habitatGridDet[1:y.maxDet,1:x.maxDet],
            indicator = z[i],# 1,
            lengthYCombined = dimYbinded)
    }#i
    N <- sum(z[1:n.individuals])
})

constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)







library(nimble)


code <- nimbleCode({
  beta0 ~ dnorm(0, sd = 100)
  beta1 ~ dnorm(0, sd = sigma)
  beta2 ~ dnorm(0, sd = 100)
  sigma ~ dunif(0, 100) 
  z1 ~ dbern(psi)   ## indicator variable associated with beta1
  z2 ~ dbern(psi)   ## indicator variable associated with beta2
  psi ~ dunif(0, 1) ## hyperprior on inclusion probability
  for(i in 1:N) {
    Ypred[i] <- beta0 + beta1 * z1 * x1[i] + beta2 * z2 * x2[i]
    Y[i] ~ dnorm(Ypred[i], sd = sigma)
  }
})

## simulate some data
set.seed(1)
N <- 100
x1 <- runif(N, -1, 1)
x2 <- runif(N, -1, 1) ## this covariate is not included
Y <- rnorm(N, 1 + 2.5 * x1, sd = 1)

## build the model
rIndicatorModel <- nimbleModel(code, constants = list(N = N),
                               data = list(Y = Y, x1 = x1, x2 = x2), 
                               inits = list(beta0 = 0, beta1 = 0, beta2 = 0, sigma = sd(Y),
                               z1 = 1, z2 = 1, psi = 0.5))

indicatorModelConf <- configureMCMC(rIndicatorModel)

## Add reversible jump  
configureRJ(conf = indicatorModelConf,        ## model configuration
            targetNodes = c( "beta2"), ## coefficients for selection
            indicatorNodes = c("z2"),    ## indicators paired with coefficients
            control = list(mean = 0, scale = 2))




code <- nimbleCode({
    beta0 ~ dnorm(0, sd = 100)
    sigmaX <- sigma + 1
  beta1 ~ dnorm(0, sd = sigmaX)
  beta2 ~ dnorm(0, sd = 100)
  sigma ~ dunif(0, 100)
  for(i in 1:N) {
    Ypred[i] <- beta0 + beta1 * x1[i] + beta2 * x2[i]
    Y[i] ~ dnorm(Ypred[i], sd = sigma)
  }
})

rNoIndicatorModel <- nimbleModel(code, constants = list(N = N),
                                 data = list(Y = Y, x1 = x1, x2 = x2), 
                                 inits=  list(beta0 = 0, beta1 = 0, beta2 = 0, sigma = sd(Y)))

noIndicatorModelConf <- configureMCMC(rNoIndicatorModel)

## Add reversible jump  
configureRJ(conf = noIndicatorModelConf,      ## model configuration
            targetNodes = c("beta1", "beta2"), ## coefficients for selection   
            priorProb = 0.5,                   ## prior probability of inclusion
            control = list(mean = 0, scale = 2))






metricsEffBM %>% filter(model == 'co2', HMC == TRUE)


for(thisModel in unique(metricsEffBP$model)) {
    p <- metricsEffBP %>%
        filter(model == thisModel) %>%
        ggplot(aes(x = Parameter, y = efficiency)) +
        facet_wrap(vars(regime)) +
        geom_line(aes(group = MCMC, color = MCMC)) +
        labs(title = paste0('Model: ', thisModel), y = 'Efficiency') +
        theme(plot.title = element_text(hjust = 0.5))
    if(print) print(p)
    if(save) ggsave(filename = paste0(thisModel, '_EffbyParam.pdf'),
                    plot = p, path = plotsDirectory,
                    width = 10, height = 7, units = 'in')
}


df <- metricsEffBP %>%
        filter(model == thisModel) %>%
        group_by(MCMC, Parameter, regime) %>% summarize(efficiency = mean(efficiency))

df <- metricsEffBP %>% filter(model == thisModel)

df %>% filter(regime == 'Sampling', Parameter == 'alpha[1]')
df2 <- df %>% filter(regime == 'Sampling', Parameter == 'alpha[1]', MCMC == 'nimble')

mean(df2$efficiency)
[1] 301.2038


df2 <- df %>% filter(regime == 'Post-Burnin', Parameter == 'sigma', MCMC == 'jags')

mean(df2$efficiency)
[1] 301.2038
[1] 1374.135




code <- nimbleCode({
    for(a in 1:10) {
        for(b in 1:10) {
            x[a,b] <- 1
        }
    }
    n <- sum(x[1:10,1:10])
    m <- mean(x[1:10,1:10])
})

Rmodel <- nimbleModel(code)
Rmodel$calculate()
Rmodel$x
Rmodel$n
Rmodel$m

code <- nimbleCode({
    for(i in 1:3) {
        for(j in 1:3) {
            for(k in 1:3) {
                x[i,j,k] <- 0
            }
        }
    }
})

Rmodel <- nimbleModel(code)

Rmodel$getNodeNames()

Rmodel$expandNodeNames('x')


Rmodel$calculate()
Rmodel$x




## testing compilation of character strings

library(nimble)

nfDef <- nimbleFunction(
    setup = function(nodes) {},
    run = function(i = double()) {
        print(nodes)
        returnType(double())
        return(1)
    }
)

nodes <- letters[1]
nodes <- letters[1:2]
nodes <- letters

Rnf <- nfDef(nodes)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

i <- 1
i <- 2
i <- 26

Rnf$run(i)
Cnf$run(i)




library(nimble)

build(devel)

dmy_fun <- nimbleFunction(
    run = function(x = double(0), prob = double(1), min = double(0), log = logical(0, default = FALSE)) {
        returnType(double(0))
        p <- prob[x - min + 1]
        if(log) return(log(p)) else return(p)
    }
)

registerDistributions(
    list(dmy_fun = list(
             BUGSdist = 'dmy_fun(prob, min)',
             types = c('prob = double(1)', 'min = double(0)'),
             discrete = TRUE)
         )
)

code <- nimbleCode({
    ## other model code here
    x ~ dmy_fun(prob = probs[1:n_probs], min = minimum)
})

## CODE HERE TO DEFINE VECTOR_OF_PROBS and MINIMUM variables
VECTOR_OF_PROBS <- c(0.1, 0.2, 0.3, 0.4)
MINIMUM <- 2

constants <- list(minimum = MINIMUM, n_probs = length(VECTOR_OF_PROBS), probs = VECTOR_OF_PROBS)

data <- list()

inits <- list(x = 2)

Rmodel <- nimbleModel(code, constants, data, inits)

exp(Rmodel$calculate())   ## 0.1

Cmodel <- compileNimble(Rmodel)

Cmodel$x <- 2
exp(Cmodel$calculate())   ## 0.1
Cmodel$x <- 3
exp(Cmodel$calculate())   ## 0.2
Cmodel$x <- 4
exp(Cmodel$calculate())   ## 0.3
Cmodel$x <- 5
exp(Cmodel$calculate())   ## 0.4



my_dfun <- function(mean, sd, min, max, prob) {
    f <- nimbleFunction(
        run = function(x = double(0),
                       log = logical(0, default = FALSE)) {
            returnType(double(0))
            p <- prob[x - min + 1]
            if (log) return(log(p)) else return(p)
        }
    )
    return(f)
}


dFun <- my_dfun(mean=4, sd=1, min=0, max=5, prob = 1:4)





gapminder <- read.csv("~/Downloads/gapminder.csv")

install.packages('gapminder')
library(gapminder)
gapminder

names(gapminder)
summary(gapminder)
str(gapminder)

gapminder$lifeExp

library(dplyr)



select(gapminder, lifeExp)

pull(gapminder, lifeExp)





library(nimble)   ## branch ADoak

code <- nimbleCode({
    for(t in 1:(k-1)) {
        s[t] ~ dunif(0, 1)
    }
    for(i in 1:3) {
        a[i] ~ dgamma(1, 1) 
        psiD[i] <- a[i]/sum(a[1:3]) 
        b[i] ~ dgamma(1, 1) 
        psiV[i] <- b[i]/sum(b[1:3]) 
        c[i] ~ dgamma(1, 1) 
        psiF[i] <- c[i]/sum(c[1:3]) 
    }
    for(t in 1:(k-1)) {
        probTrans[1,1,t] <- s[t] * psiV[1]
        probTrans[1,2,t] <- s[t] * psiV[2]
        probTrans[1,3,t] <- s[t] * psiV[3]
        probTrans[1,4,t] <- 1 - s[t]
        probTrans[2,1,t] <- s[t] * psiF[1]
        probTrans[2,2,t] <- s[t] * psiF[2]
        probTrans[2,3,t] <- s[t] * psiF[3]
        probTrans[2,4,t] <- 1 - s[t]
        probTrans[3,1,t] <- s[t] * psiD[1]
        probTrans[3,2,t] <- s[t] * psiD[2]
        probTrans[3,3,t] <- s[t] * psiD[3]
        probTrans[3,4,t] <- 1 - s[t]
        probTrans[4,1,t] <- 0
        probTrans[4,2,t] <- 0
        probTrans[4,3,t] <- 0
        probTrans[4,4,t] <- 1
    }
    for(i in 1:nind) {
        ## z[i, f[i]] ~ dcat(init[1:4])
        ## y[i, f[i]] ~ dcat(probObs[z[i,f[i]], 1:3])
        ## for(t in (f[i]+1):k) {
        ##     z[i, t] ~ dcat(probTrans[z[i,t-1], 1:4, t-1])
        ##     y[i, t] ~ dcat(probObs[z[i,t], 1:3])
        ## }
        y[i, f[i]:k] ~ dDHMM(init = init[1:4],
                             probObs = probObs[1:4,1:3],
                             probTrans = probTrans[1:4,1:4,f[i]:(k-1)],
                             ##len = k-f[i]+1,
                             len = lengthForDHMM[i])
    }
})

load('~/github/HMCpaper/analyses/models/multiCR.RData')
## artificially add a final 3 (not obsreved) to all capture histories
y <- cbind(y, 3)
k <- dim(y)[2]

lengthForDHMM <- k - f + 1

constants <- list(
    f = f,
    k = k,
    lengthForDHMM = lengthForDHMM,
    nind = nind,
    init = c(0.5, 0.5, 0, 0)
)

data <- list(
    probObs = array(c(1,0,0,0,0,1,0,0,0,0,1,1), c(4,3)),
    y = y
)

inits <- list(
    s = rep(0.5, k-1),
    a = rep(1, 3),
    b = rep(1, 3),
    c = rep(1, 3)
)


dDHMM <- nimbleFunction(
    run = function(x = double(1),    ## Observed capture (state) history
                   init = double(1),
                   probObs = double(2),
                   probTrans = double(3),
                   len = double(),## length of x (needed as a separate param for rDHMM)
                   log = integer(0, default = 0)) {
        pi <- init # State probabilities at time t=1
        logL <- 0
        nObsClasses <- dim(probObs)[2]
        lengthX <- length(x)
        for (t in 1:lengthX) {
            ##xt <- x[t]
            Zpi <- probObs[, x[t]] * pi # Vector of P(state) * P(observation class x[t] | state)
            sumZpi <- sum(Zpi)    # Total P(observed as class x[t])
            logL <- logL + log(sumZpi)  # Accumulate log probabilities through time
            if (t != lengthX) pi <- ((Zpi %*% probTrans[,,t])/sumZpi)[1, ] # State probabilities at t+1
        }
        returnType(double())
        if (log) return(logL)
        return(exp(logL))
    },
    enableDerivs = list(
        run = list(noDeriv_vars = c('t', 'x', 'len'))
    )
)

rDHMM <- nimbleFunction(
    run = function(n = integer(),    ## Observed capture (state) history
                   init = double(1),
                   probObs = double(2),
                   probTrans = double(3),
                   len = double()) {
        nStates <- length(init)
        if (nStates != dim(probObs)[1]) stop("In rDHMM: Length of init does not match nrow of probObs in dDHMM.")
        if (nStates != dim(probTrans)[1]) stop("In rDHMM: Length of init does not match dim(probTrans)[1] in dDHMM.")
        if (nStates != dim(probTrans)[2]) stop("In rDHMM: Length of init does not match dim(probTrans)[2] in dDHMM.")
        if (len - 1 > dim(probTrans)[3]) stop("In rDHMM: len - 1 does not match dim(probTrans)[3] in dDHMM.")
        if (abs(sum(init) - 1) > 1e-6) stop("In rDHMM: Initial probabilities must sum to 1.")
        returnType(double(1))
        ans <- numeric(len)
        trueState <- rcat(1, init)
        for (i in 1:len) {
            ans[i] <- rcat(1, probObs[trueState,])
            if (i != len) {
                trueState <- rcat(1, probTrans[trueState, , i])
            }
        }
        return(ans)
    }
)

registerDistributions(
    list(
        dDHMM = list(
            BUGSdist = "dDHMM(init, probObs, probTrans, len)",
            Rdist = "dDHMM(init, probObs, probTrans, len)",
            discrete = TRUE,
            types = c('value = double(1)',
                      'init = double(1)',
                      'probObs = double(2)',
                      'probTrans = double(3)',
                      'len = double()'),
            mixedSizes = TRUE,
            pqAvail = FALSE)
    ),
    verbose = FALSE
)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Cmodel <- compileNimble(Rmodel)

printErrors()







orderCoordinatesMMD2 <- function(coords, exact = FALSE) {
    ## input coords: an Nx2 array of spatial coordinates
    N <- dim(coords)[1]
    if(N < 3) return(coords)
    if(!exact) {       ## approximate MMD ordering
        initialOrdering <- sample(1:N)
        orderedIndices <- c(initialOrdering, rep(NA, 3*N))
        indexLookupVector <- order(initialOrdering)
        maxNeighbors <- floor(sqrt(N))
        NN <- FNN::get.knn(coords, k = maxNeighbors)$nn.index
        nextSpot <- N+1
        cycleCheckIndex <- -1
        for(i in 2:(3*N)) {
            (targetIndex <- orderedIndices[i])
            if(cycleCheckIndex == targetIndex) break
            if(cycleCheckIndex == -1) cycleCheckIndex <- targetIndex
            targetNeighbors <- NN[targetIndex, 1:min(maxNeighbors, round(N/(i+N-nextSpot)))]
            targetNeighborLocs <- indexLookupVector[targetNeighbors]
            if(min(targetNeighborLocs) < i) {   ## relocate this index to the back
                orderedIndices[nextSpot] <- targetIndex
                orderedIndices[i] <- NA
                indexLookupVector[targetIndex] <- nextSpot
                nextSpot <- nextSpot + 1
            } else cycleCheckIndex <- -1
        }
        orderedIndicesNoNA <- orderedIndices[!is.na(orderedIndices)]
        orderedCoords <- coords[orderedIndicesNoNA,]
    } else {           ## exact MMD ordering
        availableIndices <- 1:N
        orderedCoords <- array(NA, c(N,2))
        sbar <- apply(coords, 2, mean)   ## group centroid
        iNext <- which.min(sapply(1:N, function(i) sum((coords[i,] - sbar)^2)))
        orderedCoords[1,] <- coords[iNext,]
        availableIndices <- setdiff(availableIndices, iNext)
        for(i in 2:N) {
            aIndNext <- which.max(    ## this indexes the availableIndices vector
                sapply(1:(N-i+1), function(j) {
                    min(sapply(1:(i-1), function(k) sum((coords[availableIndices[j],] - orderedCoords[k,])^2)))
                }))
            iNext <- availableIndices[aIndNext]   ## this indexes rows of the original s[] array
            orderedCoords[i,] <- coords[iNext,]
            availableIndices <- setdiff(availableIndices, iNext)
        }
        orderedIndicesNoNA <- NULL
    }
    return(list(orderedCoords = orderedCoords, orderedIndicesNoNA = orderedIndicesNoNA))
}





library(BayesNSGP)

set.seed(0)

N <- 10
coords <- cbind(runif(N), runif(N))

set.seed(0)
orderCoordinatesMMD(coords)
set.seed(0)
orderCoordinatesMMD2(coords)

set.seed(0)
orderCoordinatesMMD(coords, exact = FALSE)
set.seed(0)
orderCoordinatesMMD2(coords, exact = FALSE)


orderCoordinatesMMD(coords, exact = TRUE)
orderCoordinatesMMD2(coords, exact = TRUE)



library(nimble)


code <- nimbleCode({
    a[1] ~ dnorm(0, 1)
    a[2] ~ dnorm(0, 1)
    mat[1:2,1:2] <- diag(a[1:2])
})

constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code)

Rmodel$a
Rmodel$mat
Rmodel$a <- c(1,2)


Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)
samplesPlot(samples)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()



https://umass-amherst.zoom.us/j/7996919907



##library(nimble)

1
build(ADoak_without_HMC)
build(ADoak)


Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

##compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
##Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

Cmodel <- compileNimble(Rmodel)

printErrors()

Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)
samplesPlot(samples)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()



library(nimble)

RdensityFunction <- function(x, prob, log = integer(0, default = 0)) {
    require(poibin)
    val <- dpoibin(x, pp = prob)
    returnType = double(0)
    if(log) return(log(val)) else return(val)
}

dPoisBin <- nimbleRcall(
    prototype = function(x = double(0), prob = double(1), log = integer(0)) {},
    returnType = double(0),
    Rfun = 'RdensityFunction'
)

registerDistributions(list(
    dPoisBin = list(
        BUGSdist = 'dPoisBin(prob)',
        Rdist = 'dPoisBin(prob)',
        types = c('prob = double(1)')
    )
))


code <- nimbleCode({
    a ~ dPoisBin(prob[1:3])
})

constants <- list()
data <- list()
inits <- list(a = 2, prob = c(0.2, 0.2, 0.6))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Cmodel <- compileNimble(Rmodel)
Cmodel$calculate()



## phi[1:n.cells,t] <-  1-    (exp(- exp(phi0 +  log(dens[1:n.cells, t]+1) * betaPhi)))
## phi = 1 -  exp( -exp(b0 + b1*log(dens+1))  )
## simulation parameters:
## b0: 1.6
## b1: -1

mx <- 5
x <- seq(-mx, mx, by = 0.01)
y1 <- 1 / (1 + exp(-x))
y2 <- 1 - exp(-exp(x))
plot(x, y1, type = 'l', ylim = range(c(y1,y2)), xlab='', ylab='')
lines(x, y2, col = 'red')
b0 <- 1.6
b1 <- -1
ds <- 0:3
##abline(v = 1.6 - 1:3, lty = 2)
abline(v = b0 + b1*log(ds+1), lwd = ds)




build(ADoak)
##library(nimble)

code <- nimbleCode({
    for(t in 1:(k-1)) {
        s[t] ~ dunif(0, 1)
    }
    for(i in 1:3) {
        a[i] ~ dgamma(1, 1) 
        psiD[i] <- a[i]/sum(a[1:3]) 
        b[i] ~ dgamma(1, 1) 
        psiV[i] <- b[i]/sum(b[1:3]) 
        c[i] ~ dgamma(1, 1) 
        psiF[i] <- c[i]/sum(c[1:3]) 
    }
    for(t in 1:(k-1)) {
        probTrans[1,1,t] <- s[t] * psiV[1]
        probTrans[1,2,t] <- s[t] * psiV[2]
        probTrans[1,3,t] <- s[t] * psiV[3]
        probTrans[1,4,t] <- 1 - s[t]
        probTrans[2,1,t] <- s[t] * psiF[1]
        probTrans[2,2,t] <- s[t] * psiF[2]
        probTrans[2,3,t] <- s[t] * psiF[3]
        probTrans[2,4,t] <- 1 - s[t]
        probTrans[3,1,t] <- s[t] * psiD[1]
        probTrans[3,2,t] <- s[t] * psiD[2]
        probTrans[3,3,t] <- s[t] * psiD[3]
        probTrans[3,4,t] <- 1 - s[t]
        probTrans[4,1,t] <- 0
        probTrans[4,2,t] <- 0
        probTrans[4,3,t] <- 0
        probTrans[4,4,t] <- 1
    }
    for(i in 1:nind) {
        y[i, f[i]:k] ~ dDHMM(init = init[1:4],
                             probObs = probObs[1:4,1:3],
                             probTrans = probTrans[1:4,1:4,f[i]:(k-1)],
                             len = k-f[i]+1)
    }
})

load('~/github/HMCpaper/analyses/models/multiCR.RData')
##load('~/Downloads/multiCR.RData')
## artificially add a final 3 (not obsreved) to all capture histories
y <- cbind(y, 3)
k <- dim(y)[2]

constants <- list(f = f, k = k, nind = nind, init = c(0.5, 0.5, 0, 0))
data <- list(probObs = array(c(1,0,0,0,0,1,0,0,0,0,1,1), c(4,3)), y = y)
inits <- list(s = rep(0.5, k-1), a = rep(1, 3), b = rep(1, 3), c = rep(1, 3))

dDHMM <- nimbleFunction(
    run = function(x = double(1),    ## Observed capture (state) history
                   init = double(1),
                   probObs = double(2),
                   probTrans = double(3),
                   len = double(),## length of x (needed as a separate param for rDHMM)
                   log = integer(0, default = 0)) {
        if (length(init) != dim(probObs)[1]) stop("In dDHMM: Length of init does not match nrow of probObs in dDHMM.")
        if (length(init) != dim(probTrans)[1]) stop("In dDHMM: Length of init does not match dim(probTrans)[1] in dDHMM.")
        if (length(init) != dim(probTrans)[2]) stop("In dDHMM: Length of init does not match dim(probTrans)[2] in dDHMM.")
        if (length(x) != len) stop("In dDHMM: Length of x does not match len in dDHMM.")
        if (len - 1 != dim(probTrans)[3]) stop("In dDHMM: len - 1 does not match dim(probTrans)[3] in dDHMM.")
        if (abs(sum(init) - 1) > 1e-6) stop("In dDHMM: Initial probabilities must sum to 1.")
        pi <- init # State probabilities at time t=1
        logL <- 0
        nObsClasses <- dim(probObs)[2]
        lengthX <- length(x)
        for (t in 1:lengthX) {
            xt <- x[t]
            Zpi <- probObs[, xt] * pi # Vector of P(state) * P(observation class x[t] | state)
            sumZpi <- sum(Zpi)    # Total P(observed as class x[t])
            logL <- logL + log(sumZpi)  # Accumulate log probabilities through time
            if (t != lengthX) pi <- ((Zpi %*% probTrans[,,t])/sumZpi)[1, ] # State probabilities at t+1
        }
        returnType(double())
        if (log) return(logL)
        return(exp(logL))
    },
    enableDerivs = list(
        run = list(noDeriv_vars = c('t', 'xt', 'x', 'len'))
    )
)

rDHMM <- nimbleFunction(
    run = function(n = integer(),    ## Observed capture (state) history
                   init = double(1),
                   probObs = double(2),
                   probTrans = double(3),
                   len = double()) {
        nStates <- length(init)
        if (nStates != dim(probObs)[1]) stop("In rDHMM: Length of init does not match nrow of probObs in dDHMM.")
        if (nStates != dim(probTrans)[1]) stop("In rDHMM: Length of init does not match dim(probTrans)[1] in dDHMM.")
        if (nStates != dim(probTrans)[2]) stop("In rDHMM: Length of init does not match dim(probTrans)[2] in dDHMM.")
        if (len - 1 > dim(probTrans)[3]) stop("In rDHMM: len - 1 does not match dim(probTrans)[3] in dDHMM.")
        if (abs(sum(init) - 1) > 1e-6) stop("In rDHMM: Initial probabilities must sum to 1.")
        returnType(double(1))
        ans <- numeric(len)
        trueState <- rcat(1, init)
        for (i in 1:len) {
            ans[i] <- rcat(1, probObs[trueState,])
            if (i != len) {
                trueState <- rcat(1, probTrans[trueState, , i])
            }
        }
        return(ans)
    }
)

registerDistributions(
    list(
        dDHMM = list(
            BUGSdist = "dDHMM(init, probObs, probTrans, len)",
            Rdist = "dDHMM(init, probObs, probTrans, len)",
            discrete = TRUE,
            types = c('value = double(1)',
                      'init = double(1)',
                      'probObs = double(2)',
                      'probTrans = double(3)',
                      'len = double()'),
            mixedSizes = TRUE,
            pqAvail = FALSE)
    ),
    verbose = FALSE
)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()   ## -2940.723

Cmodel <- compileNimble(Rmodel)

printErrors()

Cmodel$calculate()



code
constants
data
inits

library(nimble)
library(basicMCMCplots)
library(coda)

nimbleOptions(experimentalEnableDerivs = TRUE)
nimbleOptions(experimentalEnableDerivs = FALSE)


Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Cmodel <- compileNimble(Rmodel)

Cmodel$calculate()


Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)
samplesPlot(samples)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()



yNode <- 'y[1, 1:11]'
yNode <- 'y[3, 6:11]'

yNode <- 'y[1, 1:11]'
yNode <- 'y[2, 1:11]'
yNode <- 'y[3, 6:11]'
yNode <- 'y[4, 1:11]'
yNode <- 'y[5, 1:11]'
yNode <- 'y[6, 1:11]'
yNode <- 'y[7, 8:11]'
yNode <- 'y[8, 10:11]'

i <- 8

Rmodel$y[i,]
constants$f[i]

ls(Rmodel$nodes)
Rmodel$nodes$y_L28_UID_28
debug(Rmodel$nodes$y_L28_UID_28$calculate)

Rmodel$calculate(yNode)

lp <- Rmodel$calculate()
lp

Rmodel$getNodeNames()
Rmodel$nodes


Rmodel$getLogProb('y')
Rmodel$logProb_y



i <- 1
i <- 2
t <- 1

dim(Rmodel$y)
Rmodel$y[i,t]
Rmodel$z[i,t]
Rmodel$Z

(yNode <- paste0('y[', i, ',', t, ']'))
Rmodel$
Rmodel$calculate(yNode)
Rmodel$z[i,t]
Rmodel$y[i,t]

ls(Rmodel$nodes)

Rmodel$nodes$y_L45_UID_46$calculate
debug(Rmodel$nodes$y_L45_UID_46$calculate)
Rmodel$calculate(yNode)

Rmodel$Z
Rmodel$Z[1:3, Rmodel$z[i, t]]

Rmodel$t


setwd('~/github/HMCpaper/analyses/results/')

files <- list.files()

load(files[1])

ls()

str(metrics$byParameter)




library(nimble)
nimbleOptions(MCMCmultivariateNodesAsScalars = TRUE)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf <- configureMCMC(Rmodel, onlySlice = TRUE)

conf <- configureMCMC(Rmodel, nodes = NULL)
nonDataStoch <- model$getNodeNames(stochOnly = TRUE, includeData = FALSE)
conf$addSampler(type = 'HMC', target = nonDataStoch)
conf


conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 500)

samplesSummary(samples)




library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b <- (a < 0) * 3 + (a > 0) * 4
})

constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$a <- -02
Rmodel$calculate()
c(Rmodel$a, Rmodel$b)


Cmodel <- compileNimble(Rmodel)

Cmodel$a <- 0
Cmodel$calculate()
c(Cmodel$a, Cmodel$b)


Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)
samplesPlot(samples)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()





library(rstan)

suppressMessages(rm('ppp', 'pppp', 'qqq', 'rrr'))

constants <- list(
    N = 10,
    t = c(94.3, 15.7, 62.9, 126, 5.24, 31.4, 1.05, 1.05, 2.1, 10.5)
)

data <- list(
    y = c(5, 1, 5, 14, 3, 19, 1, 1, 4, 22)
)

inits <- list(
    alpha = 1,
    beta = 1,
    theta = rep(0.1, 10)
)

monitors <- c('alpha', 'beta')

stan_code <- '
data {
  int<lower=0> N;
  int y[N];
  vector<lower=0>[N] t;
}

parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
  vector<lower=0>[N] theta;
}

model {
  vector[N] lambda;
  alpha ~ exponential(1);
  beta ~ gamma(0.1, 1);
  for(i in 1:N) {
    theta[i] ~ gamma(alpha, beta);
    lambda[i] = theta[i] * t[i];
  }
  y ~ poisson(lambda);
}

'

stan_mod <- rstan::stan_model(model_code = stan_code)

stan_data <- c(constants, data)

## niter and warmup notes:
## - if you omit 'warmup' argument, then half of niter is used as warmup
## - if you specify 'warmup' and 'niter', then the value for 'niter'
##   also includes the warmup iterations

stan_out <- rstan::sampling(
                       stan_mod,
                       data = stan_data,
                       chains = 1,
                       warmup = 5000,
                       iter = 10000,
                       pars = monitors,
                       init = list(inits)  ## or a list of length 'chains'
                   )

rstan::get_elapsed_time(stan_out)    ## timings

samples_stan <- as.matrix(stan_out)  ## samples

head(samples_stan)

## if using multiple chains, then use:
## rstan::extract(stan_out, permute = FALSE)
## which returns a 3D (niter x nchains x nparams) array



## demo of using nimbleRcall

build(devel)

library(nimble)

RdensityFunction <- function(x, arg1, arg2) {
    ## x must be a scalar
    ## arg1 must be a numeric scalar
    ## arg2 must be a numeric vector
    ## here, require(PACKAGENAME) if the density evaluation
    ## comes from PACKAGENAME.
    ## also, make sure this package is installed where you're running R.
    lp <- arg1
    ## the returnType = double(0)  means the return of the function is a scalar,
    ## which is what you'd have for a density evaluation
    return(lp)
}


useThisFunctionInYourModelOrDistribution <- nimbleRcall(
    prototype = function(x = double(0), arg1 = double(0), arg2 = double(1)) {},
    returnType = double(0),
    Rfun = 'RdensityFunction'
)


otherNimbleFunction <- nimbleFunction(
    run = function(arg1 = double(0)) {
        arg2 <- c(0, 0)   ## just to have a vector for arg2
        value <- useThisFunctionInYourModelOrDistribution(0, arg1, arg2)
        returnType(double(0))
        return(value)
    }
)

otherNimbleFunction(2)
## 2

CotherNimbleFunction <- compileNimble(otherNimbleFunction)

CotherNimbleFunction(3)
## 3





## normal-normal conjugacy for Anna

tau_0
tau
taup <- tau_0 + n*tau
taup

mu_0
y_bar
y_bar*n*tau / taup
pmu

pvar
1/taup




mu_0 = 0
tau_0 = 1/4^2
n = 50
tau = 1/(3^2)
y_bar = 2.5

post_mean <- function(mu_0, tau_0, n, tau, y_bar){
  new_mu = mu_0*(tau_0/(tau_0+n*tau)) + y_bar*(n*tau/(tau_0 + n*tau))
  return(new_mu)
}

pmu = post_mean(mu_0, tau_0, n, tau, y_bar)

post_var <- function(tau_0, n, tau){
  new_prec = tau_0 + n*tau
  new_var = 1/(new_prec)
  return(new_var)
}

pvar = post_var(tau_0, n, tau)


x <- seq(-10, 10, length=1000)

hx <- dnorm(x,0,4)

##hy <-dnorm(x, 2.5, 3)
hy <-dnorm(x, 2.5, 1/sqrt(n*tau))

hpost <- dnorm(x, pmu, sqrt(pvar))



plot(x, hpost,  col = 'red', type = 'l', lwd= 4, xlab = '', ylab = '',
     xaxt = "n", yaxt = "n", axes = FALSE, ylim=c(0,1))
text(-6.5, 0.05, 'prior', col = 'blue', cex=2)
text(6.5, 0.15, 'observations', col = 'black', cex=2)
text(-3, 0.18, 'posterior', col = 'red', cex = 2)
lines(x, hy, col = 'black', type = 'l', lwd= 4)
lines(x, hx, col = "blue", type = 'l', lwd = 3)

    


## STAT202
## ANOVA

cars <- read.csv('~/github/courses/stat202/data/UsedCars.csv')
cars

## say there was a categorical predictor with 3 levels:
## 1 = unleaded gasoline
## 2 = diesel
## 3 = hybrid

Power <- c(rep("gas",10), rep("diesel",6), rep("hybrid",3))

Power
class(Power)

cars$Power <- as.factor(Power)

cars


## look at the data:
library(ggplot2) 

ggplot(cars, aes(y = Price, x = Power, color = Power)) + geom_point(size = 3) + ylim(0, 15000)

## multiple regression with (categorical) Power variable

m <- lm(Price ~ Power, data = cars)
summary(m)

## ANOVA

anova(m)



## try new assignments of Power:
Power <- c('gas', 'hybrid', 'diesel', 'hybrid', 'hybrid', 'gas', 'hybrid', 'gas', 'diesel', 'diesel', 'diesel', 'hybrid', 'diesel', 'gas', 'diesel', 'hybrid', 'hybrid', 'hybrid', 'gas')

cars$Power <- as.factor(Power)


## look at the data:
ggplot(cars, aes(y = Price, x = Power, color = Power)) + geom_point(size = 3) + ylim(0, 15000)




    

## STAT365
## Continuous-Time AR(1) Process Model

## data generation
n <- 100

a <- 6
b <- 0.8

sigmaPN <- 2
sigmaOE <- 4

set.seed(0)








waic2




library(nimble)

df <- read.csv('~/Downloads/UsedCars.csv')

code <- nimbleCode({
    b0    ~ dnorm(0, sd = 1000)
    bage  ~ dnorm(0, sd = 100)
    bhp   ~ dnorm(0, sd = 100)
    btype ~ dnorm(0, sd = 100)
    sigma ~ dunif(0, 10000)
    for(i in 1:N) {
        mu[i] <- b0 + bage*age[i] + bhp*hp[i] + btype*type[i]
        y[i] ~ dnorm(mu[i], sd = sigma)
    }
})

constants <- list(N = dim(df)[1], age=df$Age, hp=df$HP, type=df$Type)

data <- list(y = df$Price)

inits <- list(b0=0, bage=0, bhp=0, btype=0, sigma=1)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()  ## -851490867

conf <- configureMCMC(Rmodel)
conf <- configureMCMC(Rmodel, enableWAIC = TRUE)

conf$enableWAIC
conf$enableWAIC <- TRUE

Rmcmc <- buildMCMC(conf)

## one compilation call:
compiledList <- compileNimble(list(Rmodel, Rmcmc))
Cmodel <- compiledList[[1]]
Cmcmc <- compiledList[[2]]

set.seed(0)

samples <- runMCMC(Cmcmc, 10000, nburnin=5000)

library(coda)
cor(samples)
crosscorr(samples)
crosscorr.plot(as.mcmc(samples))


runMCMC(Cmcmc, 100000, samples=FALSE, WAIC=TRUE, nchains=3, nburnin=20000)
runMCMC(Cmcmc, 100000, samples=FALSE, summary=TRUE, nchains=3, nburnin=20000)

Cmcmc$getWAIC()












library(nimbleHMC)

code <- nimbleCode({
    b0 ~ dnorm(0, 0.001)
    b1 ~ dnorm(0, 0.001)
    sigma ~ dunif(0, 10000)
    for(i in 1:N) {
        mu[i] <- b0 + b1 * x[i]
        y[i] ~ dnorm(mu[i], sd = sigma)
    }
})
N <- 10
constants <- list(N = N, x = 1:N)
data <- list(y = 1:N)
inits <- list(b0 = 1, b1 = 0.1, sigma = 1)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate() ## -138.5709

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler(target = c('b0', 'b1', 'sigma'), type = 'HMC')

##debug(buildMCMC)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 1000)

samples[900:903,]
##                  b0        b1         sigma
## [1,] 0.000006061093 0.9999974 0.00001559453
## [2,] 0.000006350245 1.0000009 0.00001513972
## [3,] 0.000005969301 1.0000006 0.00001372741
## [4,] 0.000006456972 0.9999998 0.00001372790



class(samplerFunction)
samplerFunction
##debug(samplerFunction)

environment(samplerFunction)$nfRefClass
environment(samplerFunction)$nfRefClass$initialize
environment(samplerFunction)$nfRefClass$initializeXXX
environment(samplerFunction)$nfRefClass
environment(samplerFunction)$nfRefClass$methods('callSuper')
environment(samplerFunction)$nfRefClass$methods('initialize')
environment(samplerFunction)$nfRefClass$methods('initializeXXX')
environment(samplerFunction)$nfRefClass$methods('new')
environment(samplerFunction)$nfRefClass$methods('initialize')()
environment(samplerFunction)$nfRefClass$methods('initializeXXX')()
environment(samplerFunction)$nfRefClass()
environment(samplerFunction)$nfRefClass$new()



samplerFunction(model=model, mvSaved=mvSaved, target=target, control=control)

nfRefClass
nfRefClass$logH
nfRefClass$finalize
nfRefClass$initialize





library(nimbleHMC)

code <- nimbleCode({
    b0 ~ dnorm(0, 0.001)
    b1 ~ dnorm(0, 0.001)
    sigma ~ dunif(0, 10000)
    for(i in 1:N) {
        mu[i] <- b0 + b1 * x[i]
        y[i] ~ dnorm(mu[i], sd = sigma)
    }
})
N <- 10
constants <- list(N = N, x = 1:N)
data <- list(y = 1:N)
inits <- list(b0 = 1, b1 = 0.1, sigma = 1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$addSampler(target = c('b0', 'b1', 'sigma'), type = 'RW_block')
conf$addSampler(target = c('b0', 'b1', 'sigma'), type = 'HMC')

samps <- conf$getSamplers()
##samps[[1]]$samplerFunction
##s1 <- samps[[1]]$samplerFunction
##cont <- environment(s1)$contains
##environment(cont)$className   ## same
##environment(cont)$name        ## same

lapply(samps, function(s) s$baseClassName)







library(nimble)
code <- nimbleCode({
    mu ~ dnorm(0, sd = 10000)
    sigma ~ dunif(0, 10000)
    for(i in 1:4) {
        y[i] ~ dnorm(mu, sd = sigma)
    }
})
constants <- list()
data <- list(y = c(100, 110, 112, 118))
inits <- list(mu = 0, sigma = 1)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()   ## -24307.02
## need to "enable" WAIC calculations!
## this can be done either in configureMCMC(), or buildMCMC():
conf <- configureMCMC(Rmodel)      ## enable WAIC here,
Rmcmc <- buildMCMC(conf)           ## or here
compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
## then, specify WAIC = TRUE, to return the WAIC value:
set.seed(0)
out <- runMCMC(Cmcmc, 10000)
## say we forgot to specify WAIC = TRUE, for a long run.
## don't worry, there's still a way to calculate it:
## finally, let's see this using nimbleMCMC() again:
args(nimbleMCMC)


## DEMO OF WAIC VALUES:

N <- 50   
set.seed(0)   
y <- rnorm(N, 5, 10)   
code <- nimbleCode({
    mu ~ dnorm(0, sd = 100)
    sigma ~ dunif(0, 1000)
    for(i in 1:N) {
        y[i] ~ dnorm(mu, sd = sigma)
    }
})
constants <- list(N = N)
data <- list(y = y)
inits <- list(mu = 0, sigma = 1)
Rmodel <- nimbleModel(code, constants, data, inits)
waic1 <- nimbleMCMC(model = Rmodel,
                    niter = 10000, nburnin = 1000, nchains = 3,
                    samples = FALSE, WAIC = TRUE)
waic1
code <- nimbleCode({
    for(i in 1:N) {
        mu[i] ~ dnorm(0, sd = 1)
        sigma[i] ~ dunif(0, 1000)
        y[i] ~ dnorm(mu[i], sd = sigma[i])
    }
})
constants <- list(N = N)
data <- list(y = y)
inits <- list(mu = rep(0,N), sigma = rep(1,N))
Rmodel <- nimbleModel(code, constants, data, inits)
waic2 <- nimbleMCMC(model = Rmodel,
                    niter = 10000, nburnin = 1000, nchains = 3,
                    samples = FALSE, WAIC = TRUE)
waic2



## STAT202
## Multiple Regression using Cars dataset

cars <- read.csv('~/Downloads/UsedCars.csv')
head(cars)
dim(cars)
cars
cars <- cars[1:19, c('Price', 'Age', 'HP')]
cars


## correlation matrix using cor():
cor(cars)

## pairwise scatterplots using pairs():
pairs(cars)

pairs(cars, diag.panel = function(x) boxplot(x, add = TRUE))


## let's do multiple regression,
## trying to predict y = Price,
## using covariates x1 = Age, and x2 = HP

m <- lm(Price ~ Age + HP, data = cars)
m

## regression coefficients
b <- m$coef

## model summary
summary(m)

## calculating multiple-R
## use m$fitted.values to get y-hat values

## these are the ..... y-hat values!!!!

yhat <- m$fitted.values
b[1] + b[2] * cars$Age + b[3]*cars$HP

## plot yhat vs. y to "see" the multiple-R
plot(yhat, cars$Price)

mR <- cor(yhat, cars$Price)

## calculating multiple-R^2
mR^2

## residuals
## calculate as e = yhat - y
## and also using m$residuals

yhat - cars$Price

SSE <- sum(m$residuals^2)


## calculating s = residual standard error
## sqrt(SSE / (n-p))
## where SSE = sum(e^2)
## what is n ?
## what is p ?
dim(cars)
n <- 19


sqrt(SSE / (n-3) )
summary(m)
sigma(m)

## R can also calculate s by using sigma(m):




## Daily Problem 20:
df <- read.csv('~/Downloads/house_selling_prices_OR.csv')

head(df)
dim(df)
names(df)

df <- df[    , c('House.Price..USD.', 'House.Size', 'Lot.Size')]

df
head(df)

names(df)
names(df) <- c('Price','HouseSize','LotSize')
names(df)
boxplot(df$Price)
boxplot(df$HouseSize)
boxplot(df$LotSize)

price <- df$Price

pairs(df)


cor(df)

m <- lm(Price ~ HouseSize + LotSize, data = df)

summary(m)


    


library(devtools)
library(roxygen2)
library(usethis)

package <- 'temp1'
path <- '~/temp/temp_repos/temp1'

setwd(path)
getwd()

create(package, rstudio = FALSE)


use_github_action_check_standard()
use_github_action_check_standard


document(package)
system(paste0('R CMD BUILD ', package))
check(package)

suppressMessages(try(remove.packages(package), silent = TRUE))
tarFiles <- grep('\\.tar\\.gz', list.files(), value = TRUE)
lastTarFile <- tarFiles[length(tarFiles)]
message('installing package version ', gsub('\\.tar\\.gz$', '', lastTarFile))
system(paste0('R CMD install ', lastTarFile))

library(temp2)

tempFunction
?tempFunction
tempFunction(1)
tempFunction(10)




library(devtools)
library(nimble)
setwd('~/github/nimble/nimbleHMC')
setwd('~/github/nimble/nimbleHMC/nimbleHMC')

document('nimbleHMC')

system('R CMD BUILD nimbleHMC')

check('nimbleHMC')

library(rprojroot)
find_root(is_testthat)

read.csv('~/Downloads/Dyes.csv')


## STAT365
## Fixed vs. Random effects


dyes <- read.csv('~/Downloads/Dyes.csv')

dyes
batch <- dyes$batch

batch
y <- dyes$concentration
y

N <- length(y)

library(nimble)

code <- nimbleCode({
    ## priors:
    sigma_batch ~ dhalfflat()
    sigma_obs ~ dhalfflat()
    mu ~ dflat()
    ## latent states (random effects):
    for(j in 1:Nbatches) {
        delta_batch[j] ~ dnorm(0, sd = sigma_batch)
    }
    ## likelihood:
    for(i in 1:N) {
        mu_batch[i] <- mu + delta_batch[batch[i]]
        y[i] ~ dnorm(mu_batch[i], sd = sigma_obs)
        ## alternatively:
        ##y[i] ~ dnorm(mu + batch_delta[batch[i]], sd = sigma_obs)
    }
})

constants <- list(N=N, Nbatches=max(batch), batch=batch)
data <- list(y = y)
inits <- list(sigma_batch=1, sigma_obs=1, mu=0)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$initializeInfo()







dim(samples)
colnames(samples)
head(samples)

samplesPlot(samples)
samplesPlot(samples, 'bp')
samplesPlot(samples, 'bp[5]')



# This file contains code adapted from
# https://github.com/daniele-falco/software_comparison/tree/main/linear_models
#
# In this file we fix the way the model is written to allow nimble to
# detect conjugacy.

absConjTest3b <- nimbleCode({
    for(i in 1:n) {
        mu[i] <- inprod(X[i,n], beta[1:k])
        Y[i] ~ dnorm(mu[i], var = sigmasq)
    }
    for(i in 1:k){    
        b0[i] <- 0
    }
    nu0 <- 0.001
    sigma0 <- 1
    sigmasq ~ dinvgamma(nu0*0.5,nu0*sigma0*sigma0*0.5)
    tau <- pow(sigmasq,-1)
    ##sigma <- pow(sigmasq,0.5)
    for(i in 1:k) {
        beta[i] ~ dnorm(b0[i], var = sigmasq)
    }
})

n <- 5
k <- 2
constants <- list(n = n, k = k)
data <- list()
inits <- list(sigmasq = 1)

nimbleOptions(MCMCjointlySamplePredictiveBranches = TRUE)
nimbleOptions(MCMCjointlySamplePredictiveBranches = FALSE)
nimbleOptions('MCMCjointlySamplePredictiveBranches')
nimbleOptions('MCMCjointlySamplePredictiveBranches')

Rmodel <- nimbleModel(code, constants, data, inits)

debug(configureMCMC)
debug(MCMCconf$new)


conf <- configureMCMC(Rmodel)

conf$printSamplers()


run_nimble <- function(code, case) {
  t1<- system.time(
    {
      linear <- nimbleModel(code = code,
                            dimensions = case$dimensions,
                            name = "linear",
                            constants = case$constants,
                            data = case$data,
                            inits = case$inits)
      Clinear <- compileNimble(linear)
 writeLines("about to configure")
 browser()
      linearConf <- configureMCMC(linear, print = TRUE)
      linearConf$addMonitors(c("tau", "sigmasq", "beta"))
      linearMCMC <- buildMCMC(linearConf)
      ClinearMCMC <- compileNimble(linearMCMC, project = linear)
      t2<- system.time(
        {
          Psamples <- runMCMC(ClinearMCMC, niter=11000, nburnin=1000, thin=2,
                              nchains=1,samplesAsCodaMCMC = TRUE,summary=TRUE)
        })
    })
  list(t1 = t1, t2 = t2, Psamples = Psamples)
}


test <- run_nimble(linearCodeZconj, cases[[1]])





library(nimble)

N <- 100
set.seed(0)
x <- runif(N)
b0 <- 1
bx <- .4
bc <- -.6
c <- sample(c(0, 1), N, replace = TRUE)
y <- rnorm(N, b0+bx*x+bc*c, 3)

## nimbleModel
code <- nimbleCode({
    ## priors for parameters
    beta0 ~ dunif(-1, 10)
    betax ~ dnorm(0, sd = 1000)
    betac ~ dunif(-100, 100)
    sigma ~ dunif(0, 1000)
    for(i in 1:N) {
        ## b0 + bx*x + bc*c
        mu[i] <- beta0 + betax*xx[i] + betac*cc[i]
        y[i] ~ dnorm(mu[i], sd = sigma)
    }
})

constants <- list(N=N, xx=x, cc=c)
data <- list(y = y)
inits <- list(beta0=0, betax=50, betac=0, sigma=1)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()
Rmodel$initializeInfo()

Rmodel$beta0
Rmodel$mu

conf <- configureMCMC(Rmodel)

conf$printSamplers(byType = TRUE)
conf$printSamplers

conf$removeSampler('sigma')
conf$printSamplers()

conf$printMonitors()
conf$resetMonitors()
conf$addMonitors()
conf$setMonitors()

conf$addSampler(target = 'sigma', type = 'RW', logScale=TRUE)

conf$samplerConfs

Rmcmc <- buildMCMC(conf)

## compile model and MCMC
Cmodel <- compileNimble(Rmodel)
Cmodel$mu
Cmodel$sigma
Cmodel$beta0

Cmcmc <- compileNimble(Rmcmc, project = Cmodel)

## uncompiled execution
Rmodel$sigma
Rmodel$beta0

system.time(samples <- runMCMC(Rmcmc, 20))

samples
Rmodel$sigma
Rmodel$beta0

system.time(samples <- runMCMC(Cmcmc, 2000))

Cmodel$sigma
tail(samples)

## samplesSummary
samplesSummary(samples, round = 2)

ts.plot(samples[,'sigma'])


Cmodel$betac
Cmodel$betac <- 100

system.time(samples <- runMCMC(Cmcmc, 500))

ts.plot(samples[,'betac'])

Cmodel$beta0
Cmodel$beta0 <- 100

samplesSummary(samples, 2)

Cmodel$beta0
Cmodel$calculate()

















library(XML) # To import the data
library(nimble) # for the MCMC


temp <- tempfile()
download.file("https://figshare.com/ndownloader/files/5591666",temp)

EnvData.tmp <- readHTMLTable(unzip(temp, "appendix-A.htm"))[[1]]
SpData.tmp <- readHTMLTable(unzip(temp, "appendix-B.htm"))[[1]]
TraitData.tmp <- readHTMLTable(unzip(temp, "appendix-C.htm"))[[1]]
unlink(temp)

# FormatEnvData
names(EnvData.tmp) <- gsub(" +\\r\\n +", " ", names(EnvData.tmp))
EnvData.tmp$`Land use` <- gsub(" +\\r\\n +", " ", EnvData.tmp$`Land use`)

ConvData <- function(dat, whNum) {
  dat2 <- sapply(names(dat), function(nm, df, nums) {
    if(nm %in%nums) df[,nm] <- as.numeric(df[,nm])
    df[,nm]
  }, df = dat, nums = whNum, simplify =FALSE)
  as.data.frame(dat2)
}

VarToNum <- c("No.", "Sampling year", "Texture", "Org", "pH", "Avail P", 
              "Avail K", "Moist", "Bare", "Litter", "Bryophyte", "Plants/m2", 
              "Canopy height", "Stem density", "Biom 0-5", "Biom 5+", "Repro biom", 
              "Elevation", "Management")
EnvData <- ConvData(EnvData.tmp, whNum = VarToNum) 
SiteTab <- table(EnvData$Code)
SitesToUse <- names(SiteTab)[SiteTab==1]
EnvData <- EnvData[EnvData$Code%in%SitesToUse,]
rownames(EnvData) <- EnvData$Code

# FormatSpeciesData}
names(SpData.tmp) <- SpData.tmp[1,]
SpData.tmp <- SpData.tmp[-1,]
SpData <- ConvData(SpData.tmp, whNum = names(SpData.tmp)[names(SpData.tmp)!="Code"])
rownames(SpData) <- SpData$Code

# FormatTraitData

TraitData.tmp$CODE <- gsub(" +\\r\\n +", " ", TraitData.tmp$CODE)
# TraitData.tmp$CODE%in%SpData$Code # check that all the species are there
ToNum <- names(TraitData.tmp)[!names(TraitData.tmp)%in%c("CODE", "SPECIES")]
TraitData <- ConvData(TraitData.tmp, whNum = ToNum) 
rownames(TraitData) <- TraitData$CODE

rm(SpData.tmp, TraitData.tmp, EnvData.tmp) # clean up

# Abund}
SpToUse <- TraitData$CODE
EnvNames <- c("Texture", "Org", "pH", "Avail.P", "Avail.K", "Moist", "Bare", 
            "Litter", "Bryophyte", "Plants.m2", "Canopy.height", "Stem.density", 
            "Biom.0.5", "Biom.5.", "Repro.biom", "Elevation")

Sites <- EnvData[SitesToUse, EnvNames]
Abund <- t(SpData[SpToUse,SitesToUse])
Traits <- TraitData[SpToUse, grep("^L", names(TraitData))]

Site <- EnvData[,]
rownames(Site) <- NULL
rownames(Site) <- EnvData$Code

OrdConsts <- list(
  NEnv = ncol(Sites),
  NTraits = ncol(Traits),
  NSites = nrow(Abund),
  NSpecies = ncol(Abund),
  NLatent = 10)

OrdData <- list(
  Sites = data.frame(apply(Sites, 2, scale)), 
  Abund = Abund,
  Traits = data.frame(apply(Traits, 2, scale))
  
)



HeirModel2D <- nimbleCode({
  for (sites in 1:NSites) {
    for (species in 1:NSpecies) {
      Mu[sites, species] <- Mu0 + alSiteSTAR[sites] + alSpeciesSTAR[species] + 
        inprod(L_v_STAR[sites,1:NLatent], lamSpSTAR[species,1:NLatent])
      log(lambda[sites, species]) <- Mu[sites, species]
      Abund[sites, species] ~ dpois(lambda[sites, species])
    }      
    for (l in 1:NLatent) {
      muL[sites, l] <- inprod(Sites[sites, 1:NEnv], betaEnvSTAR[1:NEnv, l])
      L_v_STAR[sites,l] ~ dnorm(muL[sites, l], sd=sdEnv[l])
      Latent[sites, l] <- (L_v_STAR[sites, l] - mean(L_v_STAR[1:NSites, l]))/Sitesd[l]
      ResLat[sites,l] <- (L_v_STAR[sites,l] - muL[sites, l])/Sitesd[l]
    }
    
    alSiteSTAR[sites] ~ dnorm(muSite, sd=sdSites)
    alSite[sites] <- alSiteSTAR[sites] - mean(alSiteSTAR[1:NSites])
  }
  
  for(species in 1:NSpecies) {
    for (l in 1:NLatent) {
      muLam[species, l] <- inprod(Traits[species, 1:NTraits], betaTraitsSTAR[1:NTraits, l])
      lamSpSTAR[species, l] ~ dnorm(muLam[species, l], sd=sdFS[l])
      lamSp[species, l] <- (lamSpSTAR[species, l]-mean(lamSpSTAR[1:NSpecies, l])) * Sitesd[l]
      ResLamSp[species, l] <- (lamSpSTAR[species, l] - muLam[species, l]) * Sitesd[l]
    }
    
    alSpeciesSTAR[species] ~ dnorm(muSpecies, sd=sdSpecies)
    alSpecies[species] <- alSpeciesSTAR[species] - mean(alSpeciesSTAR[1:NSpecies])
  }
  
  for (l in 1:NLatent) {
    for(e in 1:NEnv) {
      betaEnvSTAR[e, l] ~ dnorm(0, 1)
      betaEnv[e, l] <- betaEnvSTAR[e, l]/Sitesd[l]
    }
    for (t in 1:NTraits) {
      betaTraitsSTAR[t, l] ~ dnorm(0,1)
      betaTraits[t, l] <- betaTraitsSTAR[t, l]*Sitesd[l]
    }
    sdFS[l] ~ dexp(1)
    sdEnv[l] ~ dexp(1)
    sdTraits[l] ~ dexp(1)
    Sitesd[l] <- sd(L_v_STAR[1:NSites, l])
    
    MeanLV[l] <- mean(L_v_STAR[1:NSites, l])
    MeanLam[l] <- mean(lamSpSTAR[1:NSpecies, l])
  }

  sdSpecies ~ dexp(1)
  sdSites ~ dexp(1)
  muSpecies ~ dnorm(0, sd=1)
  muSite ~ dnorm(0, sd=1)
  Mu0 ~ dnorm(0, sd=1)
  Mean <- Mu0 + mean(alSiteSTAR[1:NSites]) + mean(alSpeciesSTAR[1:NSpecies]) +
    muSpecies + muSite + sum(MeanLV[1:NLatent]) + sum(MeanLam[1:NLatent])
})


OrdConsts$NLatent <- 2

OrdInits2D <- function(dat, NLat) {
  list(sdSpecies = rgamma(1,10,500), sdSites = rgamma(1,10,500), 
       sdEnv = rgamma(NLat,10,500), sdTraits = rgamma(NLat,10,500), 
       sdFS = rgamma(NLat,10,500),
       muSpecies = rnorm(1, 0, 0.05), muSite = rnorm(1,0,0.05), 
       Mu0 = rnorm(1,log(mean(dat$Abund)),0.05), 
       betaEnvSTAR = matrix(rnorm(nrow(dat$Sites), 0, 0.1), ncol=NLat), 
       betaTraitsSTAR = matrix(rnorm(nrow(dat$Traits), 0, 0.1), ncol=NLat), 
       
       L_v_STAR = matrix(rnorm(NLat*nrow(dat$Abund), 0, 0.05), ncol=NLat),
       alSiteSTAR = rnorm(nrow(dat$Abund), 0, 0.05), 
       lamSpSTAR = matrix(rnorm(NLat*ncol(dat$Abund), 0, 0.05), ncol=NLat), 
       alSpeciesSTAR = rnorm(nrow(dat$Abund), 0, 0.05)
  )
}
ToMonitor <- c('betaEnv','betaTraits', 'Latent', 'lamSp', 'Mean', 'alSite', 
               'alSpecies', 'sdEnv', 'sdSpecies', 'ResLat', 'ResLamSp')


HeirOrd2D <- nimbleModel(HeirModel2D,
                      constants = OrdConsts,
                      data      = OrdData,
                      inits     = OrdInits2D(OrdData, 2))
HeirOrd2D.Comp <- compileNimble(HeirOrd2D)
HeirOrd2D.Conf <- configureMCMC(HeirOrd2D.Comp, monitors=ToMonitor)

HeirOrd2D.Conf$removeSamplers(c('alSpeciesSTAR', 'alSiteSTAR', 'L_v_STAR', 'lamSpSTAR'))

HeirOrd2D.Conf$addSampler(target = 
                            c('alSpeciesSTAR', 'alSiteSTAR', 'L_v_STAR', 'lamSpSTAR'), 
                          type = "RW_block")
# If I block these separately, runMCMC() crashes the R session 
# HierOrd2D.Conf$addSampler(target = 'alSpeciesSTAR', type = "RW_block")
# HierOrd2D.Conf$addSampler(target = 'alSiteSTAR', type = "RW_block")
# HierOrd2D.Conf$addSampler(target = 'L_v_STAR', type = "RW_block")
# HierOrd2D.Conf$addSampler(target = 'lamSpSTAR', type = "RW_block")
HeirOrd2D.MCMC <- buildMCMC(HeirOrd2D.Conf)

HeirOrd2D.CompMCMC <- compileNimble(HeirOrd2D.MCMC)
suppressWarnings(
HeirOrd2D.samp <- runMCMC(HeirOrd2D.CompMCMC, 
                          nchains = 2, niter = 2e2, nburnin = 1e2, thin =1,
                          summary = TRUE, WAIC = FALSE)
  
)



iMax <- 2
dev.new(height=5, width=8)
par(mfrow = c(iMax,3))
titles <- c('f(x)', 'log f(x)', '- log f(x)')
xs <- seq(0, 2, by = 0.001)
y <- array(0, c(length(xs), 3))
for(i in 1:iMax) {
    if(i == 1) f <- function(x) dnorm(x, 1, .2)
    if(i == 2) f <- function(x) .5*dgamma(x,3,10) + .5*dnorm(x,1.2,.1)
    ##
    y[,1] <- f(xs)
    y[,2] <- log(y[,1])
    y[,3] <- -y[,2]
    ##
    for(j in 1:3) {
        plot(xs, y[,j], type = 'l', xlab='', ylab='',
             main = ifelse(i==1, titles[j], ''), lwd=2, cex.main=2)
    }
}
dev.copy2pdf(file = '~/Desktop/talk.pdf')





library(devtools)
has_devel()

library(pkgbuild)
##pkgbuild::has_compiler(TRUE)  ## Mac
##pkgbuild::has_rtools()        ## Windows

pkgbuild::has_compiler(TRUE)

pkgbuild::has_rtools()        ## Windows


library(nimble)

code <- nimbleCode({
    ## priors
    mu ~ dnorm(0, tau = tau0)
    tau ~ dgamma(0.001, v0)
    ## likelihood
    for(i in 1:N) {
        y[i] ~ dnorm(mu, tau)
    }
})


## nimbleModel

N <- 4
constants <- list(N = N, tau0 = 1)

set.seed(0)
somethingelse <- rnorm(N, 5, 7)
    
data <- list(y = somethingelse)

inits <- list(mu = 3, tau = 2, v0 = 1)

## nimbleModel(code, constants, data, inits)
code
constants
data
inits

Rmodel <- nimbleModel(code, constants, data, inits)

## model variables

Rmodel$tau0
Rmodel$v0
Rmodel$v0 <- 3
Rmodel$y

Rmodel$tau0 <- 1
Rmodel$tau0

## querying a model

Rmodel$getNodeNames()
Rmodel$getNodeNames(stochOnly = TRUE)
Rmodel$getNodeNames(determOnly = TRUE)
Rmodel$getNodeNames(dataOnly = TRUE)
Rmodel$getNodeNames(topOnly = TRUE)

## plotGraph

Rmodel$plotGraph()

## calculate, simulate

## simulate for random draws from distribution
Rmodel$mu
Rmodel$simulate('mu')
Rmodel$mu

Rmodel$y
Rmodel$simulate('y', includeData = TRUE)


Rmodel$mu <- 2
Rmodel$mu
Rmodel$calculate('mu')
dnorm(2, 0, 1, log = TRUE)

Rmodel$calculate('y')

sum(log(dnorm(Rmodel$y, Rmodel$mu, 1/sqrt(Rmodel$tau))))

Rmodel$logProb_mu
Rmodel$logProb_y

## logProbs



code <- nimbleCode({
    b0 ~ dnorm(0, 0.001)
    b1 ~ dnorm(0, 0.001)
    sigma ~ dunif(0, 10000)
    for(i in 1:N) {
        mu[i] <- b0 + b1 * x[i]
        y[i] ~ dnorm(mu[i], sd = sigma)
    }
})

constants <- list(N = 10)
Rmodel <- nimbleModel(code, constants)
conf <- configureMCMC(Rmodel)
conf$setSamplers()
conf$addSampler(target = c('b0', 'b1', 'sigma'), type = 'RW_block')
Rmcmc <- buildMCMC(conf)









library(nimble)
library(testthat)

code <- nimbleCode({
    for(i in 1:2) {
        x[i] ~ dnorm(0, 1)
        y[i] ~ dnorm(0, 1)
    }
})
Rmodel <- nimbleModel(code)
conf <- configureMCMC(Rmodel)

expect_true(all(conf$findSamplersOnNodes(c('x')) == c(1,2)))
expect_true(all(conf$findSamplersOnNodes(c('y')) == c(3,4)))
expect_true(all(conf$findSamplersOnNodes(c('x', 'y')) == c(1,2,3,4)))
expect_true(all(conf$findSamplersOnNodes(c('y', 'x')) == c(3,4,1,2)))
expect_true(all(conf$findSamplersOnNodes(c('x[1]', 'y[1]')) == c(1,3)))
expect_true(all(conf$findSamplersOnNodes(c('x[1]', 'y[2]')) == c(1,4)))
expect_true(all(conf$findSamplersOnNodes(c('y[1]', 'x[2]')) == c(3,2)))

conf$addSampler(c('x[2]', 'y[2]'), 'AF_slice')

expect_true(all(conf$findSamplersOnNodes(c('x')) == c(1,2,5)))
expect_true(all(conf$findSamplersOnNodes(c('y')) == c(3,4,5)))
expect_true(all(conf$findSamplersOnNodes(c('x', 'y')) == c(1,2,5,3,4)))
expect_true(all(conf$findSamplersOnNodes(c('y', 'x')) == c(3,4,5,1,2)))
expect_true(all(conf$findSamplersOnNodes(c('y[1]', 'x[2]')) == c(3,2,5)))

conf$setSamplers(c('y', 'x'))
pppp

library(nimbleSCR)

# Creat habitat grid
habitatGrid <- matrix(c(1:(4^4)), nrow = 4, ncol=4, byrow = TRUE)
coordsHabitatGridCenter <- matrix(c(0.5, 3.5,
                                    1.5, 3.5,
                                    2.5, 3.5,
                                    3.5, 3.5,
                                    0.5, 2.5,
                                    1.5, 2.5,
                                    2.5, 2.5,
                                    3.5, 2.5,
                                    0.5, 1.5,
                                    1.5, 1.5,
                                    2.5, 1.5,
                                    3.5, 1.5,
                                    0.5, 0.5,
                                    1.5, 0.5,
                                    2.5, 0.5,
                                    3.5, 0.5), ncol = 2,byrow = TRUE)
colnames(coordsHabitatGridCenter) <- c("x","y")
# Create habitat windows
lowerCoords <- coordsHabitatGridCenter-0.5
upperCoords <- coordsHabitatGridCenter+0.5
colnames(lowerCoords) <- colnames(upperCoords) <- c("x","y")
# Plot check
plot(lowerCoords[,"y"]~lowerCoords[,"x"],pch=16, xlim=c(0,4), ylim=c(0,4),col="red") 
points(upperCoords[,"y"]~upperCoords[,"x"],col="red",pch=16) 
points(coordsHabitatGridCenter[,"y"]~coordsHabitatGridCenter[,"x"],pch=16) 

# Rescale coordinates 
ScaledLowerCoords <- scaleCoordsToHabitatGrid(coordsData =  lowerCoords,
                                              coordsHabitatGridCenter = coordsHabitatGridCenter)
ScaledUpperCoords <- scaleCoordsToHabitatGrid(coordsData =  upperCoords,
                                              coordsHabitatGridCenter = coordsHabitatGridCenter)
ScaledUpperCoords$coordsDataScaled[,2] <- ScaledUpperCoords$coordsDataScaled[,2] + 1
ScaledLowerCoords$coordsDataScaled[,2] <- ScaledLowerCoords$coordsDataScaled[,2] - 1
habitatMask <- matrix(1, nrow = 4, ncol=4, byrow = TRUE)
# Create local objects 
HabWindowsLocal <- getLocalObjects(habitatMask = habitatMask,
                                   coords = coordsHabitatGridCenter,
                                   dmax=4,
                                   resizeFactor = 1,
                                   plot.check = TRUE
                                   )

s <- c(1, 1) # Currrent activity center location
lambda <- 0.1
numWindows <- nrow(coordsHabitatGridCenter)
baseIntensities <- rep(1,numWindows)
numRows <- nrow(habitatGrid)
numCols <- ncol(habitatGrid)

# The log probability density of moving from (1,1) to (1.2, 0.8) 
dbernppLocalACmovement_exp(x = c(1.2, 0.8), lowerCoords, upperCoords, s,
                           lambda, baseIntensities, habitatGrid, 
                           HabWindowsLocal$habitatGrid, HabWindowsLocal$resizeFactor,
                           HabWindowsLocal$localIndices, HabWindowsLocal$numLocalIndices,
                           numRows, numCols, numWindows, log = TRUE)






# create a new raster and set all its values to unity
library(raster)
set.seed(2021)
rcov <- raster(nrows = 4, ncols = 4) 
rcov <- setValues(rcov, runif(ncell(rcov),0,1))
plot(rcov)

# get coordinates
x <- coordinates(rcov)

# fake value for resistance param
alpha <- 0.1

noneuc_distance <- function(alpha, rcov, x) {
    ## parameters
    # alpha is a scalar
    # rcov is a RasterLayer
    # x is a matrix of dims nsites x 2
    ##
    ## obtain resistance surface
    cost <- exp(alpha * rcov)
    ##
    ## calculate conductances among neighbours 
    # probability of transition from one cell to another
    tr <- gdistance::transition(x = cost, transitionFunction = function(x) 1/mean(x), directions = 16) # class TransitionLayer
    ##
    ## adjust diag.conductances
    trCorrC <- gdistance::geoCorrection(tr, type = "c", multpl = FALSE, scl = FALSE) # class TransitionLayer
    ##
    ## compute ecological distance
    D <- gdistance::costDistance(trCorrC, x, x) / 1000 # class Matrix
    D
}

# use function
noneuc_distance(alpha, rcov, x)






library(nimble)

## Code
modfile <- nimbleCode( {
    mu.n ~ dunif(0.0001,8)
    sd~dinvgamma(0.001, 0.001)
    for (i in 1:T){
        data[i] ~ dnorm(mu.n,sd=sd)
        ##data[i] ~ T(dnorm(mu.n,sd=sd),0,1)
    }
})

mod.inits <- list(mu.n = c(1), sd = c(0.5))

data <- c(0.013,0.077,0.069,0.055,0.176,0.03,0.097,0.097,0.108,0.006,0.016,0.016,0.125,0.015,0.004,0.013,0.022,0.042,0.032,0.035,0.017,0.031,0.032,0.564,0.01,0.023,0.017,0.123,0.387,0.097,0.147,0.11,0.265,0.114,0.003,0.053,0.062,0.012,0.087,0.034,0.066,0.075,0.162,0.017,0.121,0.276,0.01,0.575,0.205,0.008)

mod.data <- list(data = data)
mod.consts <- list(T = length(data))

set.seed(0)
results <- nimbleMCMC(code=modfile,constants=mod.consts,data=mod.data,inits=mod.inits,niter=50000,nburnin =25000)

modfileTrunc <- nimbleCode( {
    mu.n ~ dunif(0.0001,8)
    sd~dinvgamma(0.001, 0.001)
    for (i in 1:T){
        ##data[i] ~ dnorm(mu.n,sd=sd)
        data[i] ~ T(dnorm(mu.n,sd=sd),0,1)
    }
})

set.seed(0)
resultsTrunc <- nimbleMCMC(code=modfileTrunc,constants=mod.consts,data=mod.data,inits=mod.inits,niter=50000,nburnin =25000)

samplesSummary(results)
##            Mean     Median    St.Dev.  95%CI_low 95%CI_upp
## mu.n 0.09498151 0.09497028 0.01825259 0.05967935 0.1316005
## sd   0.12761111 0.12641033 0.01317482 0.10505308 0.1570089

samplesSummary(results, round = 3)

mean(data)
## [1] 0.09502

samplesSummary(resultsTrunc)
##            Mean      Median    St.Dev.    95%CI_low  95%CI_upp
## mu.n 0.01261093 0.009554858 0.01114038 0.0004931849 0.04167774
## sd   0.15632670 0.154910845 0.01628674 0.1283629081 0.19204721


## plot histogram of the data, with truncated Normal likelihood
## curves, one using the non-truncated parameter posterior means, and the
## other using the truncated parameter posterior means
xlim <- c(0, 1)
ylim <- c(0, 10)
plot(-10, -10, xlim=xlim, ylim=ylim, xlab='', ylab='')
hist(data, freq = FALSE, breaks = 20, add = TRUE)
dnormTrun01 <- function(x, mu, s) dnorm(x, mu, s) / (pnorm(1, mu, s) - pnorm(0, mu, s))
plot(function(x) dnormTrun01(x, 0.01261093, 0.15632670), add = TRUE, col = 'blue')
plot(function(x) dnormTrun01(x, 0.09498151, 0.12761111), add = TRUE, col = 'red')
legend('topright', legend = c('Truncated', 'Non-truncated'), col = c('blue', 'red'), lwd=1)

modelTrunc <- nimbleModel(code=modfileTrunc,constants=mod.consts,data=mod.data,inits=mod.inits)

## use the non-truncated parameter posterior means:
modelTrunc$mu.n <- 0.09498151
modelTrunc$sd <- 0.12761111
modelTrunc$calculate()
## likelihood of the data:
modelTrunc$getLogProb('data')
## [1] 46.18509

## now, using the truncated parameter posterior means:
modelTrunc$mu.n <- 0.01261093
modelTrunc$sd <- 0.15632670
modelTrunc$calculate()
## likelihood of the data:
modelTrunc$getLogProb('data')
## [1] 55.60003    ## larger






## a more simple example of a truncated model

library(nimble)

code <- nimbleCode({
    mu  ~ dunif(0, 10)
    muT ~ dunif(0, 10)
    s   ~ dunif(0, 10)
    sT  ~ dunif(0, 10)
    for(i in 1:N) {
        y[i]  ~   dnorm(mu,  sd = s)
        yT[i] ~ T(dnorm(muT, sd = sT), 0, 1)
    }
})

N <- 6
constants <- list(N = N)

inits <- list(mu=1, muT=1, s=1, sT=1)

ys <- seq(from=0.1, by=0.1, length=N)
data <- list(y=ys, yT=ys)

res  <- nimbleMCMC(code,  constants, data, inits, niter = 100000, nburnin = 50000)

library(basicMCMCplots)
samplesPlot(res)

head(res[, 'muT'], 100)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

set.seed(0)
niter <- 500
samples <- runMCMC(Rmcmc, 500)
head(samples)
samples
samplesPlot(samples)




## different results from truncated normal model

library(nimble)

code <- nimbleCode({
    mu.n ~ dunif(0.0001, 8)
    sd ~ dinvgamma(0.001, 0.001)
    ##sd ~ dunif(0, 10)
    for(i in 1:T) {
        dat[i] ~ dnorm(mu.n, sd=sd)
        ##dat[i] ~ T(dnorm(mu.n, sd=sd), 0, 1)
    }
})

codeT <- nimbleCode({
    mu.n ~ dunif(0.0001, 8)
    sd ~ dinvgamma(0.001, 0.001)
    ##sd ~ dunif(0, 10)
    for(i in 1:T) {
        ##dat[i] ~ dnorm(mu.n, sd=sd)
        dat[i] ~ T(dnorm(mu.n, sd=sd), 0, 1)
    }
})

inits <- list(mu.n = 0.5, sd = 1)

dat <- c(0.013,0.077,0.069,0.055,0.176,0.03,0.097,0.097,0.108,0.006,0.016,0.016,0.125,0.015,0.004,0.013,0.022,0.042,0.032,0.035,0.017,0.031,0.032,0.564,0.01,0.023,0.017,0.123,0.387,0.097,0.147,0.11,0.265,0.114,0.003,0.053,0.062,0.012,0.087,0.034,0.066,0.075,0.162,0.017,0.121,0.276,0.01,0.575,0.205,0.008)

data <- list(dat = dat)
constants <- list(T = length(dat))


res  <- nimbleMCMC(code,  constants, data, inits, niter = 100000, nburnin = 50000)
resT <- nimbleMCMC(codeT, constants, data, inits, niter = 100000, nburnin = 50000)

library(basicMCMCplots)
samplesPlot(res)
samplesPlot(resT)

resALL <- cbind(res, resT)
colnames(resALL) <- c('mu', 'sd', 'muT', 'sdT')
samplesPlot(resALL)

results  <- nimbleMCMC(code, constants, data, inits, niter = 50000, nburnin = 25000)
res <- results
samplesSummary(res)
##            Mean     Median    St.Dev.  95%CI_low 95%CI_upp
## mu.n 0.09504669 0.09498054 0.01811170 0.05911074 0.1304167
## sd   0.12757325 0.12645599 0.01319015 0.10478337 0.1561961


resultsT <- nimbleMCMC(codeT, constants, data, inits, niter = 50000, nburnin = 25000)
resT <- resultsT
samplesSummary(resT)
##            Mean      Median    St.Dev.    95%CI_low  95%CI_upp
## mu.n 0.01337706 0.009834399 0.01230486 0.0004758795 0.04582061
## sd   0.15636405 0.155223858 0.01636312 0.1278468493 0.19200974



plot(resultsT[, 'mu.n'])


Rmodel  <- nimbleModel(code,  constants, data, inits)
RmodelT <- nimbleModel(codeT, constants, data, inits)
Rmodel$calculate()
RmodelT$calculate()

Rmodel$calculate('dat[1]')
RmodelT$calculate('dat[1]')

Rmodel$sd
Rmodel$mu.n
Rmodel$dat[1]

RmodelT$mu.n <- 0.013
RmodelT$sd <- 0.158

RmodelT$mu.n <- 0.0951
RmodelT$sd <- 0.1275

RmodelT$mu.n
RmodelT$sd
RmodelT$calculate()

## with resultsT mode parameters
RmodelT$calculate('dat')
## 55.55176
RmodelT$logProb_dat
 [1]  1.5558546  1.4738164  1.4930441  1.5205237  1.0237083  1.5500662
 [7]  1.4145311  1.4145311  1.3750943  1.5548732  1.5556743  1.5556743
[13]  1.3046128  1.5557745  1.5542322  1.5558546  1.5542322  1.5390103
[19]  1.5486242  1.5461606  1.5555341  1.5493652  1.5486242 -4.5249217
[25]  1.5556743  1.5538517  1.5555341  1.3135056 -1.2456997  1.4145311
[31]  1.1962167  1.3674032  0.2839430  1.3515404  1.5538517  1.5238084
[37]  1.5077653  1.5558346  1.4461766  1.5470219  1.4995936  1.4788637
[43]  1.1111943  1.5555341  1.3222382  0.1704796  1.5556743 -4.7701348
[49]  0.8175114  1.5553539

## with results mode params
RmodelT$calculate('dat')
## 46.1563   
RmodelT$logProb_dat
 [1]  1.1919843  1.3892253  1.3783495  1.3498435  1.1980004  1.2689516
 [7]  1.3991907  1.3991907  1.3941834  1.1551246  1.2068585  1.2068585
[13]  1.3718043  1.2019620  1.1440396  1.1919843  1.2349462  1.3125779
[19]  1.2768378  1.2882057  1.2116936  1.2729254  1.2768378 -5.3632422
[25]  1.1765563  1.2394122  1.2116936  1.3753599 -1.2214014  1.3991907
[31]  1.3164533  1.3924733  0.5114583  1.3883149  1.1384049  1.3447870
[37]  1.3656036  1.1869031  1.3972838  1.2844779  1.3732561  1.3868755
[43]  1.2616436  1.2116936  1.3786694  0.3927716  1.1765563 -5.6842510
[49]  1.0278136  1.1659635

dnorm(Rmodel$dat[1], Rmodel$mu.n, Rmodel$sd, log = TRUE) - log(C)


dnorm(RmodelT$dat[1], RmodelT$mu.n, RmodelT$sd, log = TRUE) - log(C)

RmodelT$calculate('dat[1]')

C <- pnorm(1, RmodelT$mu.n, RmodelT$sd) - pnorm(0, RmodelT$mu.n, RmodelT$sd)


xlim <- c(0, 1)
ylim <- c(0, 10)
plot(-10, -10, xlim=xlim, ylim=ylim, xlab='', ylab='')
hist(dat, freq = FALSE, breaks = 20, add = TRUE)
dnormTrun01 <- function(x, mu, s) dnorm(x, mu, s) / (pnorm(1, mu, s) - pnorm(0, mu, s))
plot(function(x) dnormTrun01(x, 0.013, 0.156), add = TRUE, col = 'blue')
plot(function(x) dnormTrun01(x, 0.095, 0.127), add = TRUE, col = 'red')
legend('topright', legend = c('Truncated', 'Non-truncated'), col = c('blue', 'red'), lwd=1)



f <- function(x) {
    ##Rmodel$dat[1] <- x[1]
    ##lp <- Rmodel$calculate('dat[1]')[1]
    lp <- Rmodel$dat[1]
    ##lp <- x
    y <- x
    return(exp(lp))
}

f <- function(x) {
    return(exp(x))
}

integrate(f, -10, 10)

f(-100)

Rmodel$dat




##library(nimble)
### Code
##modfile<-nimbleCode( {
##    mu.n ~ dunif(0.0001,8)
##    sd~dinvgamma(0.001, 0.001)
##    for (i in 1:T){
##        data[i] ~ T(dnorm(mu.n,sd=sd),0,1)
##        #  data[i] ~ dnorm(mu.n,sd=sd)
##    }
##})
####
##mod.inits <- list(mu.n = c(1), sd = c(0.5))
##data<-c(0.013,0.077,0.069,0.055,0.176,0.03,0.097,0.097,0.108,0.006,0.016,0.016,0.125,0.015,0.004,0.013,0.022,0.042,0.032,0.035,0.017,0.031,0.032,0.564,0.01,0.023,0.017,0.123,0.387,0.097,0.147,0.11,0.265,0.114,0.003,0.053,0.062,0.012,0.087,0.034,0.066,0.075,0.162,0.017,0.121,0.276,0.01,0.575,0.205,0.008)
####
##mod.data <- list(data = data)
##mod.consts <- list(T = length(data))
## 
##results<-nimbleMCMC(code=modfile,constants=mod.consts,data=mod.data,inits=mod.inits,niter=50000,nburnin =25000)
## 
##plot(results[,1])

samplesSummary(results)







-2 Problem 2(b), Gamma(3, 5) is tantamount to having observed 5 data points thus far, which sum to 3.
-2 Problem 2(d) missing any plots to really investigate this distribution.
                                                      
-2 Problem 3, should express as a Gamma distribution.

-2 Problem 5(d) need to include the actual Gamma pdf functions inside the integral.

-3 Problem 10.3, posterior distribution is wrong, should be Gamma(123, 200)

-1 Problem 10.4, prior should be Gamma(9. 1.5)





mu <- 7
tau <- 0.1

## sd = 1/sqrt(tau)
1/sqrt(.1)

## y ~ Normal(7, 3)

set.seed(1)

n <- 4
y <- rnorm(n, mu, sd = 1/sqrt(tau))
y

## priors:
## prior 1: Jeffreys (flat)
## prior 2: Normal(5, sd = 3)
## prior 3: Normal(0, sd = 1000)


## graph priors:
par(mfrow = c(2,1), mar = c(4,2,2,1))
xmax <- 20
xmin <- -10
ymax <- 2
plot(-1, -1, xlim = c(xmin,xmax), ylim = c(0,ymax), xlab='', ylab='', main = 'Priors & Likelihood')
xs <- seq(xmin, xmax, length = 10000)

legend(x = 'topright', legend = c('Jeffreys (flat)', 'Normal(5, 3)', 'Normal(0, 1000)'), col = c('blue', 'green', 'red'), lwd=2)

## prior 1: Jeffreys (blue)
lines(xs, rep(1, length(xs)), lwd=2, col = 'blue')

## prior 2: Normal(5, 3)
lines(xs, dnorm(xs, 5, 3)*10, lwd=2, col = 'green')

## prior 3: Normal(0, 1000)
lines(xs, dnorm(xs, 0, 1000), lwd=2, col = 'red')

## Likelihood
likelihood <- sapply(xs, function(x) prod(dnorm(y,x,1/sqrt(tau))))
likelihood_scaled <- likelihood/max(likelihood) * ymax
lines(xs, likelihood_scaled, lwd = 2)

## Posteriors:
plot(-1, -1, xlim = c(xmin,xmax), ylim = c(0,ymax), xlab='', ylab='', main = 'Posteriors')

legend(x = 'topright', legend = c('Jeffreys (flat)', 'Normal(5, 3)', 'Normal(0, 1000)'), col = c('blue', 'green', 'red'), lwd=2)

## prior 1: Jeffreys (blue)
mu0 <- 999
tau0 <- 0
mu_post <- tau0/(tau0+n*tau)*mu0 + n*tau/(tau0+n*tau)*mean(y)
tau_post <- tau0 + n*tau
ys <- dnorm(xs, mu_post, 1/sqrt(tau_post))
thismax <- max(ys)
lines(xs, ys/thismax*ymax, lwd=2, col = 'blue')

## prior 2: Normal(5, 3)
mu0 <- 5
tau0 <- 1/9
mu_post <- tau0/(tau0+n*tau)*mu0 + n*tau/(tau0+n*tau)*mean(y)
tau_post <- tau0 + n*tau
ys <- dnorm(xs, mu_post, 1/sqrt(tau_post))
thismax <- max(ys)
lines(xs, ys/thismax*ymax, lwd=2, col = 'green')

## prior 3: Normal(0, 1000)
mu0 <- 0
tau0 <- 1/(1000^2)
mu_post <- tau0/(tau0+n*tau)*mu0 + n*tau/(tau0+n*tau)*mean(y)
tau_post <- tau0 + n*tau
ys <- dnorm(xs, mu_post, 1/sqrt(tau_post))
thismax <- max(ys)
lines(xs, ys/thismax*ymax, lwd=2, col = 'red')

points(y, rep(0,n), col = 'blue', pch=19)

abline(v = mu, lwd = 2)

## 95% BCI
qnorm(...)







#### looking at Beta(1/2, 1/2) prior effects:
##par(mfrow = c(1,1))
##plot(function(x) dbeta(x,1,1), xlim = c(0,1), ylim = c(0,3))
##text(.05, 0.9, 'Beta(1, 1)', cex=.8)
##plot(function(x) dbeta(x, 1/2, 1/2), add = TRUE, col = 'red')
##text(.9, 2.5, 'Beta(1/2, 1/2)', cex=.8, col = 'red')
##plot(function(x) dbeta(x, 1/2, 3/2), add = TRUE, col = 'blue')
##text(.15, 2.4, 'Beta(1/2, 3/2)', cex=.8, col = 'blue')
##plot(function(x) dbeta(x, 1/2, 5/2), add = TRUE, col = 'orange') 
##text(.22, 1.9, 'Beta(1/2, 5/2)', cex=.8, col = 'orange') 
##plot(function(x) dbeta(x, 3/2, 3/2), add = TRUE, col = 'purple')
##text(.6, 1.35, 'Beta(3/2, 3/2)', cex=.8, col = 'purple')
##plot(function(x) dbeta(x, 3/2, 5/2), add = TRUE, col = 'green')
##text(.25, 1.7, 'Beta(3/2, 5/2)', cex=.8, col = 'green')
##plot(function(x) dbeta(x, 5/2, 5/2), add = TRUE, col = 'grey')
##text(.6, 1.75, 'Beta(5/2, 5/2)', cex=.8, col = 'grey')
##points(0.25, 1.1, lwd=3)
##text(0.35, 1.1, 'what\'s going on?', cex=0.7)
##abline(v = 0.25, col = 'darkgreen', lty = 2)
##text(0.31, .2, 'x = 1/4', cex=.8, col = 'darkgreen')
##text(0.494, 1.1, '.. y ~= 1.103', cex=.7)



lambda_true <- 5

lambda_true

set.seed(0)

n <- 4
y <- rpois(n, lambda_true)
y

## priors:
## prior 1: Jeffreys'
## prior 2: Half-Flat
## prior 3: Gamma(0.001, 0.001), a common choice


## graph priors:
par(mfrow = c(2,1), mar = c(4,2,2,1))
xmax <- lambda_true * 2.5
ymax <- 2
plot(-1, -1, xlim = c(0,xmax), ylim = c(0,ymax), xlab='', ylab='', main = 'Priors & Likelihood')
xs <- seq(0, xmax, length = 100000)

legend(x = 'topright', legend = c('Jeffreys', 'halfflat', 'Gamma(0.001,0.001)'), col = c('blue', 'green', 'red'), lwd=2)

## prior 1: Jeffreys (blue)
lines(xs, 1/sqrt(xs), lwd=2, col = 'blue')

## prior 2: Half-Flat (green)
lines(xs, xs/xs, lwd=2, col = 'green')

## prior 3: common Gamma(0.001, 0.001) (red)
lines(xs, dgamma(xs, 0.001, 0.001), lwd=2, col = 'red')

## Likelihood
likelihood <- sapply(xs, function(x) prod(dpois(y,x)))
likelihood_scaled <- likelihood/max(likelihood) * ymax/1.5
lines(xs, likelihood_scaled, lwd = 2)

## Posteriors:
plot(-1, -1, xlim = c(0,xmax), ylim = c(0,ymax), xlab='', ylab='', main = 'Posteriors')

legend(x = 'topright', legend = c('Jeffreys', 'halfflat', 'Gamma(0.001,0.001)'), col = c('blue', 'green', 'red'), lwd=2)

## prior 1: Jeffreys (blue)
ys <- dgamma(xs, 1/2+sum(y), 0+n)
thismax <- max(ys) * 1.3
lines(xs, ys/thismax*ymax, lwd=2, col = 'blue')

## prior 2: Flat (green)
ys <- dgamma(xs, 1+sum(y), 0+n)
lines(xs, ys/thismax*ymax, lwd=2, col = 'green')

## prior 3: common Gamma(0.001, 0.001) (red)
ys <- dgamma(xs, 0.001+sum(y), 0.001+n)
lines(xs, ys/thismax*ymax, lwd=2, col = 'red')

abline(v = lambda_true, lwd = 2)




1

library(devtools)
library(roxygen2)
library(nimble, warn.conflicts = FALSE)
if(Sys.info()['user'] == 'dturek') {
    baseDir <- '~/github/nimble/nimbleSCR/'                   ## Daniel
} else if(Sys.info()['user'] == 'pidu') {
    baseDir <- 'C:/Users/pidu/PACKAGES/nimbleSCR/'            ## Pierre
} else if(Sys.info()['user'] == 'cymi') {
    baseDir <- 'C:/Personal_Cloud/OneDrive/Work/nimbleSCR/'   ## Cyril
} else if(Sys.info()['user'] == 'arichbi') {
    baseDir <- 'C:/PROJECTS/nimbleSCR/'                       ## Richard
} else stop('unknown user')
if(!('makePackage.R' %in% list.files(baseDir))) stop('')

warnings()


##debug(document)
##debug(roxygenise)
##debug(pkgload::load_all)
##debug(pkgload::load_code)
##debug(pkgload:::source_many)


debug(pkgload:::source_one)

debug(nimble::nimbleFunction)
debug(nimble:::RCfunction)
debug(nimble:::nfMethodRC$new)
debug(methods::new)
debug(initialize)

document(paste0(baseDir, 'nimbleSCR'))

files
i <- 1

i
file <- files[i]
file
source_one(file, encoding, envir = envir)
##envir$integrateIntensity_normal
warnings()

i <- i+1




## STAT202 lecture on bootstrapping

## daily Williamstown MA temperatures in September:

temps <- c(87, 81, 79, 87, 75, 73, 67, 73, 75, 77,
           81, 81, 84, 88, 91, 88, 91, 86, 81, 91,
           68, 68, 76, 80, 65, 67, 72)

## let's explore the data:
##(description statistics, plots, etc)
n <- length(temps)
n

range(temps)
summary(temps)

boxplot(temps, horizontal = TRUE)

ts.plot(temps)
points(1:n, temps, col = 'blue')

## the "sample" function in R:

help(sample)
1:10

sample(1:10)

length(sample(1:10))

sample(1:10, size = 3)
sample(1:10, size = 3, replace = TRUE)

sample(1:10, replace = TRUE)

sample(1:10, size = n, replace = TRUE)

sample(1:10, size = n, replace = FALSE)

## BOOTSTRAPPING
## parameter of interest:
## average daily temperature in September

## for() .....

N <- 100000
thetaStars <- numeric(N)

for(i in 1:N) {
    ystar <- sample(temps, size = n, replace = TRUE)
    ##thetaStars[i] <- mean(ystar)
    ## now, theta is the population median temp in Sept.
    ##thetaStars[i] <- median(ystar)
    thetaStars[i] <- quantile(ystar, probs = 0.1)
}

thetaStars

mean(thetaStars)
mean(temps)

hist(thetaStars)
hist(thetaStars, xlim = range(temps))
points(x = temps, y=rep(0,n), pch =20, col = 'blue')

## find a CI for the population mean
## this is called a Bootstrap CI (!)
## say, a 95% Bootstrap CI .... how? anyone?
## yes!  use the quantiles of the bootstrap resamples !
bootstrapCI <- quantile(thetaStars, probs = c(0.025, 0.975))
bootstrapCI

abline(v = bootstrapCI, col = 'blue', lwd = 2)

xbar <- mean(temps)
s <- sd(temps)
se <- s/sqrt(n)
t <- qt(0.975, df = n-1)

fCI <- xbar + c(-1, 1) * t * se

abline(v = fCI, col = 'red', lwd = 2)

y <- c(1,1,1,1,0,0,0,1,1,0,0,1)
n <- length(y)
y
n
sum(y)

ystar <- sample(y, size = n, replace = TRUE)

sum(ystar) / n
mean(ystar)

## say you're only given "y" and n, the number
## of successes and the total sample size
y <- 7
n <- 12
y
n
## .....?,,,,,...&$^*@^$ ??? what???
##

## just create the data set itself:
## y 1's, and (n-y) 0's:
data <- c(rep(1, y), rep(0, n-y))
data







## flat prior on binomial p:
plot(function(x) dbeta(x,1,1), xlim = c(0,1), ylim = c(0,3))

n <- 100000

p <- runif(n)

hist(p, prob = TRUE, add = TRUE, breaks = 50)

## now transform prior, say p2 = p^2
p2 <- p^2

hist(p2, prob = TRUE, add = TRUE, breaks = 50, col = 'red')

## prior on normal variance?
var <- runif(n, 0, 10000)

hist(var, prob = TRUE)

## what's the implied prior on standard deviation?
sd <- sqrt(var)
hist(sd, col = 'red')





n <- 1000
p <- 0.55

se <- sqrt(p*(1-p)/n)
se



p + c(-1, 1) * 3 * se

pnorm(0.5, p, se)



## testing of dcar_proper with islands


library(nimble)

## Rdist    = c('dcar_proper(mu, C,                       adj, num, M,                  tau, gamma, evs = CAR_calcEVs3(C, adj, num))',
##              'dcar_proper(mu, C = CAR_calcC(adj, num), adj, num, M = CAR_calcM(num), tau, gamma, evs = CAR_calcEVs2(   adj, num))'),
## types    = c('value = double(1)', 'mu = double(1)', 'C = double(1)', 'adj = double(1)', 'num = double(1)', 'M = double(1)', 'tau = double(0)', 'gamma = double(0)', 'evs = double(1)'),


code <- nimbleCode({
    x[1:N] ~ dcar_proper(mu = mu[1:N],
                         C = C[1:L],
                         adj = adj[1:L],
                         num = num[1:N],
                         M = M[1:N],
                         tau = 1,
                         gamma = 0.5)
})

island <- FALSE
island <- TRUE

theta_true <- 0.8


if(!island) {
    adj <- c(2, 1, 3, 2, 4, 3)
    num <- c(1, 2, 2, 1)
}
if(island) {
    adj <- c(2, 1, 3, 2)
    num <- c(1, 2, 1, 0)
}
N <- length(num)
L <- length(adj)
weights <- rep(1, L)
CM <- as.carCM(adj, weights, num)
C <- CM$C
M <- CM$M
Cmatrix <- CAR_calcCmatrix(C, adj, num)
print(Cmatrix)
print(M)
mu <- rep(0, N)
constants <- list(N = N, L = L, C = C, M = M, adj = adj, num = num, mu = mu)
data <- list()
inits <- list(x = rep(0, N))

CAR_calcEVs3(C, adj, num)
CAR_calcEVs2(   adj, num)

solve(Cmatrix)
diag(N) - 

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Rmodel$getNodeNames()
Rmodel$calculate('x')

Cmodel <- compileNimble(Rmodel)
Cmodel$calculate()



library(nimble)
##build(browser)

library(rstan)

code <- nimbleCode({
    # priors
    sigma ~ dgamma(2, 2)   
    prec <- 1 / sigma^2
    rho ~ dunif(0, 1)
    # latent process
    s[1:N] ~ dcar_proper(mu[1:N],
                         C = C[1:L],
                         adj = adj[1:L],
                         num = num[1:N],
                         M = M[1:N],
                         tau = prec, 
                         gamma = rho)    
    # likelihood
    for(i in 1:N) {
        lambda[i] <- exp(s[i])
        y[i] ~ dpois(lambda[i])
    }
})

adj <- c(2, 1, 3, 2, 4, 3)
num <- c(1, 2, 2, 1, 0)
N <- length(num)
L <- length(adj)
weights <- rep(1, L)
CM <- as.carCM(adj, weights, num)
C <- CM$C
M <- CM$M
Cmatrix <- CAR_calcCmatrix(C, adj, num)
y <- c(4, 17, 18, 26, 5)
    
##mu <- rep(10, N)
mu <- c(.2, 1, 1.9, 4, 2)

constants <- list(N = N, L = L, C = C, M = M, adj = adj, num = num, mu = mu)
data <- list(y = y)
inits <- list(sigma = 1, rho = 0.5, s = rep(0, N))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$addMonitors('s')

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
##nimble_samples <- runMCMC(Cmcmc, 20000, nburnin = 10000)
nimble_samples <- runMCMC(Cmcmc, 100000, nburnin = 50000)


1


rm(ppp, pppp, qqq)

library(nimble)
##build(browser)

library(rstan)

code <- nimbleCode({
    # priors
    sigma ~ dgamma(2, 2)   
    prec <- 1 / sigma^2
    rho ~ dunif(0, 1)
    # latent process
    s[1:N] ~ dcar_proper(mu[1:N],
                         C = C[1:L],
                         adj = adj[1:L],
                         num = num[1:N],
                         M = M[1:N],
                         tau = prec, 
                         gamma = rho)    
    # likelihood
    for(i in 1:N) {
        lambda[i] <- exp(s[i])
        y[i] ~ dpois(lambda[i])
    }
})

adj <- c(2, 1, 3, 2, 4, 3)
num <- c(1, 2, 2, 1)
N <- length(num)
L <- length(adj)
weights <- rep(1, L)
CM <- as.carCM(adj, weights, num)
C <- CM$C
M <- CM$M
Cmatrix <- CAR_calcCmatrix(C, adj, num)
y <- c(4, 17, 18, 26)
    
##mu <- rep(10, N)
mu <- c(.2, 1, 1.9, 4)

constants <- list(N = N, L = L, C = C, M = M, adj = adj, num = num, mu = mu)
data <- list(y = y)
inits <- list(sigma = 1, rho = 0.5, s = rep(0, 4))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$addMonitors('s')

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
##nimble_samples <- runMCMC(Cmcmc, 20000, nburnin = 10000)
nimble_samples <- runMCMC(Cmcmc, 100000, nburnin = 50000)

file <- '~/temp/nimble_samples.RData'
##file <- '~/temp/nimble_samples_browser.RData'

save(nimble_samples, file = file)



stan_code <- "
data {
  // mortality data
  int n;
  int y[n];
  // CAR data
  vector[n] M_inv;
  matrix[n, n] Cmatrix;
  // NEW: mean data
  vector[n] mu;
}

transformed data {
  matrix[n, n] I = diag_matrix(rep_vector(1, n));
}

parameters {
  vector[n] phi;
  real<lower=0> sigma;
  real<lower=0, upper=1> rho;
}

model {
  matrix[n, n] Sigma_inv = diag_pre_multiply(1/sigma^2 * M_inv, (I - rho * Cmatrix));
  vector[n] mu_y = exp(phi);
  target += poisson_lpmf(y | mu_y);
  target += multi_normal_prec_lpdf(phi | mu, Sigma_inv);
  target += gamma_lpdf(sigma | 2, 2);
}

"


stan_car <- stan_model(model_code = stan_code)

stan_car

library(nimble)

adj <- c(2, 1, 3, 2, 4, 3)
num <- c(1, 2, 2, 1)
N <- length(num)
L <- length(adj)
weights <- rep(1, L)
CM <- as.carCM(adj, weights, num)
C <- CM$C
M <- CM$M
Cmatrix <- CAR_calcCmatrix(C, adj, num)
y <- c(4, 17, 18, 26)
    
##mu <- rep(10, N)
mu <- c(.2, 1, 1.9, 4)

stan_data <- list(n = N, M_inv = 1 / CM$M, Cmatrix = Cmatrix, y = y, mu = mu)


S <- rstan::sampling(stan_car, data = stan_data, chains = 1, iter = 1e3, pars = c('sigma', 'rho', 'phi'))

stan_samples <- as.matrix(S)[, c('sigma', 'rho', paste0('phi[', 1:4, ']'))]

file_stan <- '~/temp/stan_samples.RData'
save(stan_samples, file = file_stan)

load(file_stan)



file <- '~/temp/nimble_samples.RData'
load(file)

dimnames(nimble_samples)
dimnames(stan_samples)

nimble_samples <- nimble_samples[, c('sigma', 'rho', paste0('s[', 1:4, ']'))]

dimnames(nimble_samples)[[2]]
dimnames(stan_samples)[[2]]

nimble_mean <- apply(nimble_samples, 2, mean)
stan_mean <- apply(stan_samples, 2, mean)

cbind(nimble=nimble_mean, stan=stan_mean)

## using library(nimble) and the non-constant 'mu'
##         nimble      stan
##sigma 1.3293882 1.3661132
##rho   0.5456801 0.5432431
##s[1]  1.3209221 1.2228884
##s[2]  2.6663483 2.7099522
##s[3]  2.7739355 2.8173598
##s[4]  3.1932055 3.2779449





 <- (as.matrix(S, pars = "beta"))

# fit lm
fit.lm <- lm(log(y/E) ~ x)
lm.res <- coefficients(fit.lm)["x"]

# write results to disk
res <- matrix(c(nimble = nimble.res, stan = stan.res, lm = lm.res), nrow = 1)
write.table(as.data.frame(res),
            file = "sim-study-res.txt",
            append = TRUE,
            row.names = FALSE,
            col.names = FALSE
            )


##results = do.call("rbind", res)
results <- read.table("sim-study-res.txt", header = FALSE)
names(results) <- c("nimble", "stan", "lm")






## demo how to debug samplers for Paul VDB

library(nimble)
library(basicMCMCplots)
library(coda)

mysamp <- nimbleFunction(
    name = 'mysamp',
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        x <- 1
    },
    run = function() {
        x <<- x + 1
        print("variable x is: ", x)
        print("model node a[1]: ", model$a[1])
        print("model variable a: ", model$a)   ## prints all of a[]
        print("model logProb of a[1]: ", model$logProb_a[1])
    },
    methods = list(
        reset = function() {
            x <<- 1
        }
    )
)


N <- 10
code <- nimbleCode({
    for(i in 1:N) {
        a[i] ~ dnorm(0, 1)
    }
})
constants <- list(N = N)
data <- list()
inits <- list(a = rep(0, N))

Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
## identify the sampler number that you want to debug and step through:
conf$printSamplers()
conf$addSampler(type = 'mysamp', target = 'a[1]')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
## say you want to debug sampler #7:
##debug(Rmcmc$samplerFunctions[[7]]$run)
Rmcmc$run(10)


## testing STAT202 class data set


f <- '~/Downloads/STAT202.csv'
f <- '~/github/courses/stat202/data/STAT202.csv'
##f <- '~/Downloads/fl_student_survey.csv'

df <- read.csv(f)
##df <- read.csv(f, stringsAsFactors = TRUE)

str(df)

df$Gender

table(df$Gender)
table(df$Class)
table(df$Chocolate)
table(df$Phone)

summary(df$Minutes)
summary(df$Friends)

boxplot(df$Friends ~ df$Class)

dim(df)

n <- dim(df)[1]
y <- as.numeric(table(df$Phone)['iPhone'])

p <- y/n
se <- sqrt(p*(1-p)/n)
se

p + c(-1, 1) * 1.96 * se


## installing rstan package

1

install.packages('rstan')
remove.packages('rstan')

install.packages("StanHeaders",type="source")



## testing using Stan and rstan


1

library(rstan)

rm('ppp', 'pppp', 'qqq')
qqq
ppp
pppp

fn <- '~/Downloads/stan_demo.stan'
stan_code <- readLines('~/Downloads/stan_demo.stan')

debug(stan_model)

stan_mod <- stan_model(model_code = stan_code)

set.seed(0)
N <- 100
stan_data <- list(N = N, y = rnorm(N, 5, 2))

S <- rstan::sampling(stan_mod, data = stan_data)
S
stan.res <- mean(as.matrix(S, pars = "beta"))




example(stan_model, package = "rstan", run.dontrun = TRUE)




## NIMBLE & Stan (using a CAR model)
## example from Connor Donegan on Nimble users list



library(nimble)
library(CARBayesdata, quietly = TRUE)
library(sp, quietly = TRUE)
library(spdep, quietly = TRUE)
library(rstan)

Nsim <- 30

# draw from simultaneous autoregressive (SAR) model
sim_sar <- function (m = 1, mu = rep(0, nrow(w)), w, rho, sigma = 1, ...) {
    stopifnot(inherits(w, "matrix"))
    stopifnot(ncol(w) == nrow(w))
    N <- nrow(w)
    if (missing(mu)) {
        mu <- rep(0, N)
    }
    I <- diag(N)
    S <- sigma^2 * solve( (I - rho * t(w)) %*% (I - rho * w) )
    x <- MASS::mvrnorm(n = m, mu = mu, Sigma = S, ...)
    return(x)
}

nimble_code <- nimbleCode({
    # priors
    alpha ~ dnorm(0, sd = 10) #/#
    beta ~ dnorm(0, sd = 10)
    sigma ~ dgamma(2, 2)   
    prec <- 1 / sigma^2
    rho ~ dunif(0, 1)
    # latent process
    s[1:N] ~ dcar_proper(mu[1:N],
                         C = C[1:nC],
                         adj = adj[1:L],
                         num = num[1:N],
                         M = M[1:N],
                         tau = prec, 
                         gamma = rho)    
    # likelihood
    for(i in 1:N) {
        mu[i] <- alpha + beta * x[i]
        lambda[i] <- E[i] * exp(s[i])
        y[i] ~ dpois(lambda[i])
    }
})

stan_code <- "
data {
  // mortality data
  int n;
  int y[n];
  vector[n] x;  
  vector[n] E;
  // CAR data
  vector[n] M_inv;
  matrix[n, n] C;
}

transformed data {
  matrix[n, n] I = diag_matrix(rep_vector(1, n));
}

parameters {
  real alpha;
  real beta;  
  vector[n] phi;
  real<lower=0> sigma;
  real<lower=0, upper=1> rho;
}

model {
  matrix[n, n] Sigma_inv = diag_pre_multiply(1/sigma^2 * M_inv, (I - rho * C));
  vector[n] mu_y = E .* exp(phi);
  vector[n] mu_phi = alpha + x * beta;
  target += poisson_lpmf(y | mu_y);
  target += multi_normal_prec_lpdf(phi | mu_phi, Sigma_inv);
  target += gamma_lpdf(sigma | 2, 2);
  target += normal_lpdf(alpha | 0, 10);
  target += normal_lpdf(beta | 0, 10);  
  // implicit uniform(0,1) on rho
}

"


ppp
pppp

rm(ppp, pppp)

ppp
pppp


stan_car <- stan_model(model_code = stan_code)
stan_car <- rstan::stan_model(model_code = stan_code)

# the data
data(GGHB.IG)
data(respiratorydata)
respiratorydata_spatial <- merge(x = GGHB.IG, y = respiratorydata, by = "IG", all.x = FALSE)
E <- respiratorydata_spatial$expected
N <- length(E)

# Nimble data
W.nb <- poly2nb(respiratorydata_spatial, row.names =  rownames(respiratorydata_spatial@data))
nbInfo <- nb2WB(W.nb)
CM <- as.carCM(nbInfo$adj, nbInfo$weights, nbInfo$num)
constants <- list(N = N,
                  L = length(nbInfo$adj),
                  C = CM$C,
                  M = CM$M,
                  nC = length(CM$C),
                  adj = nbInfo$adj,
                  num = nbInfo$num, 
                  E = E
                  )
nimble_data <- list()
inits <- list(alpha = rnorm(1, 0, 1), beta = rnorm(1, 0, 1), prec = 4, s = rnorm(N))

# Stan data
C <- spdep::nb2mat(spdep::poly2nb(respiratorydata_spatial), style = "W")
stan_data <- list(
    n = nrow(C),
    E = respiratorydata_spatial$expected,
    M_inv = 1 / CM$M,
    C = C
    )

# Sim data
rho.x <- 0.75
rho.y <- 0.6
beta <- -1
alpha <-  0

res <- lapply(1:Nsim, function(i) {
    # draw data
    x <- sim_sar(w = C, rho = rho.x, sigma = 0.2)
    phi <- sim_sar(w = C, mu = alpha + x * beta, rho = rho.y, sigma = 0.2)
    y <- rpois(n = N, lambda = E * exp(phi))
    stan_data$x <- constants$x <- x 
    stan_data$y <- nimble_data$y <- y
    # par(mfrow=c(1,3)); hist(x); hist(y/E); plot(x, log(y/E))
    # sample Nimble
    model <- nimbleModel(nimble_code, constants = constants, data = nimble_data, inits = inits)
    cModel <- compileNimble(model)
    conf <- configureMCMC(model, monitors = c('alpha', 'beta', 'sigma', 's'))
    MCMC <- buildMCMC(conf)
    cMCMC <- compileNimble(MCMC, project = cModel)
    nimble.samples <- runMCMC(cMCMC, niter = 15000, nburnin = 7500)
    nimble.res <- mean(nimble.samples[,"beta"])
    
    # sample Stan
    S <- rstan::sampling(stan_car, data = stan_data, chains = 1, iter = 1e3, pars = "beta")
    stan.res <- mean(as.matrix(S, pars = "beta"))
    
    # fit lm
    fit.lm <- lm(log(y/E) ~ x)
    lm.res <- coefficients(fit.lm)["x"]

    # write results to disk
    res <- matrix(c(nimble = nimble.res, stan = stan.res, lm = lm.res), nrow = 1)
    write.table(as.data.frame(res),
                file = "sim-study-res.txt",
                append = TRUE,
                row.names = FALSE,
                col.names = FALSE
                )
    return (res)
}
)

##results = do.call("rbind", res)
results <- read.table("sim-study-res.txt", header = FALSE)
names(results) <- c("nimble", "stan", "lm")

message("Sampling from log-linear Poisson model with beta = -1 (nsim = ",nrow(results), ")")
message("Sample mean of Nimble estimates: ", mean(results[,"nimble"]))
message("Sample mean of Stan estimates: ", mean(results[,"stan"]))
message("Sample mean of lm estimates: ", mean(results[,"lm"]))

par(mfrow = c(1,3))
hist(results[,1])
hist(results[,2])
hist(results[,3])

Results:

> message("Sampling from log-linear Poisson model with beta = -1 (nsim = ",nrow(results), ")")
Sampling from log-linear Poisson model with beta = -1 (nsim = 34)
> message("Sample mean of Nimble estimates: ", mean(results[,"nimble"]))
Sample mean of Nimble estimates: -0.784268517370144
> message("Sample mean of Stan estimates: ", mean(results[,"stan"]))
Sample mean of Stan estimates: -0.999127910957048
> message("Sample mean of lm estimates: ", mean(results[,"lm"]))
Sample mean of lm estimates: -1.02010261324937
-0.869454220869161 -1.11441602278424 -1.10393272798944
-0.84119625465296 -1.11071508243597 -1.12079492982895
-0.800128392734161 -0.988316639931817 -1.06258154392354
-0.776819840463203 -0.938352608682106 -0.986763487133298
-0.792577668122557 -1.02208182896783 -0.979232853530182
-0.976453894222416 -1.19158695522661 -1.12225311022952
-0.740169374376679 -0.954207779781052 -1.07013744151004
-0.873376679996589 -1.0648919424246 -1.04558067401789
-0.724048339785209 -1.0080065846679 -1.00920131855944
-0.807474986904927 -0.996162062869756 -0.960728447973934
-0.73303293118834 -0.940479203284062 -0.994227107364423
-0.854968287498536 -0.985146857240806 -0.966415127323065
-0.776805521594512 -0.971800018628652 -1.01006883960142
-0.828408968073413 -1.0570795949291 -1.18683690272438
-0.79767688270586 -1.00572965600818 -1.00901297746585
-0.687822171652225 -0.902619838933142 -0.895493864956022
-0.772841193385266 -0.968033844390289 -0.936703309471773
-0.838221630553068 -1.05991583300346 -1.15261358649034
-0.816355511186022 -1.00201745889098 -0.948745638150729
-0.789788879631268 -1.10169418202533 -1.11972871406528
-0.767592460964726 -0.976893644419625 -0.900424789914287
-0.769887361464003 -0.982048088264885 -1.07134848049032
-0.837946145938048 -1.06072124415297 -1.11658243355564
-0.808884993812324 -1.00108613803788 -0.972081210171968
-0.710107378303388 -0.873966797825023 -0.796884219728618
-0.756694078375703 -1.01565550444563 -0.936535262319068
-0.812711014652591 -1.00041841936907 -1.13257559911405
-0.679599909649094 -0.883041246176913 -0.871967739230857
-0.757009048237822 -0.971855632286032 -1.05090172762642
-0.677675951799145 -0.876468795237584 -0.938243921290235
-0.664339061520318 -0.881208093005099 -0.95184423510215
-0.646853416369382 -0.821342599368114 -0.948134618193292
-0.813473539665214 -1.05308365560774 -1.01175369500405
-0.864733600236752 -1.18930511923717 -1.30315831642805




## questions of memory usage
## from Henry Scharf on Nimble users list

library(pryr)
mem_1 <- rep(NA, 10)

for(i in 1:length(mem_1)){
    a <- rnorm(1e7)
    mem_1[i] <- mem_used()
    var_list <- ls(all.names = T)
    rm(list = var_list[-which(var_list == "mem_1")])
    gc()
}

plot(mem_1)


mem_2 <- rep(NA, 4)

for(i in 1:length(mem_2)){
    library(nimble)
    code <- nimbleCode({
        mu ~ dnorm(mean = 0, sd = 1)
        for(i in 1:N){
            y[i] ~ dnorm(mean = mu, sd = 1)
        }
    })
    constants <- list(N = 1e3)
    data <- list(y = rnorm(constants$N))
    model <- nimbleModel(code = code, constants = constants, data = data)
    Cmodel <- compileNimble(model)
    conf <- configureMCMC(model = model, resetFunctions = TRUE)
    mcmc <- buildMCMC(conf = conf)
    Cmcmc <- compileNimble(mcmc, project = model, resetFunctions = TRUE)
    Cmcmc$run(niter = 1e3)
    samples <- as.matrix(Cmcmc$mvSamples)
    mem_2[i] <- mem_used()
    nimble:::clearCompiled(Cmodel)
    var_list <- ls(all.names = T)
    rm(list = var_list[-which(var_list == "mem_2")])
    gc()
}

plot(mem_2)

m <- model
m <- Cmodel

m$y[1:10]
m[['y[1]']]
m[['y[2]']]
m[['y[1:2]']]

## user Luca's question about posterior_predictive and
## posterior_predictive_branch samplers, and the need
## for the rX() function for the branch sampler

library(nimble)

dmymnorm <- nimbleFunction(
    run = function(x=double(1), mean=double(1), prec=double(2), log=integer(0, default=0)){
        lp <- -sum(asRow(x-mean) %*% prec %*% asCol(x-mean))/2
        if(log) return(lp)
        else return(exp(lp))
        returnType(double(0))
    })
assign('dmymnorm',dmymnorm,envir=.GlobalEnv)

code <- nimbleCode({
    mean ~ dgamma(shape=2, rate=2) ## just to make sure to avoid conjugacy
    means[1:2] <- c(mean, mean)
    prec[1:2,1:2] <- diag(c(1/1^2, 1/0.1^2))
    x[1:2] ~ dmymnorm(mean=means[1:2], prec=prec[1:2,1:2])
})

constants <- list()
inits <- list()
modeldata <- list()

model <- nimbleModel(code=code, name='model', constants=constants, inits=inits, data=modeldata)

Cmodel <- compileNimble(model, showCompilerOutput = TRUE, resetFunctions = TRUE)

conf <- configureMCMC(model)

## Does not work with the default sampler for 'mean'
confmodel <- configureMCMC(Cmodel, nodes=NULL)
confmodel$addSampler(target=c('mean'), type='posterior_predictive', control=list())
confmodel$addSampler(target='x', type='AF_slice', control=list(sliceAdaptFactorMaxIter=5000, sliceAdaptFactorInterval=500, sliceAdaptWidthMaxIter=500, sliceMaxSteps=100, maxContractions=100))
confmodel$setMonitors(c('mean','x','logProb_x'))
confmodel
## ===== Monitors =====
## thin = 1: mean, x, logProb_x
## ===== Samplers =====
## posterior_predictive sampler (1)
##   - mean
## AF_slice sampler (1)
##   - x
##

mcmc <- buildMCMC(confmodel)
Cmcmc <- compileNimble(mcmc)

samples <- runMCMC(mcmc=Cmcmc, niter=10000, nburnin=5000, thin=5, inits=list(x=1:2))





## User's question about calculating WAIC
## (using old calculateWAIC)
## from multiple chains of samples, but when
## the chains were run in parallel using 

library(nimble)

library(parallel)


# I have to run this first to get a CmyMCMC object in global environment
myCode <- nimbleCode({
    a ~ dunif(0, 100)
    b ~ dunif(0, 100)
    for (i in 1:length_y) {
        y[i] ~ dgamma(shape = a, rate = b)
    }
})

N <- 1000
set.seed(0)
y <- rgamma(N, shape = 2, rate = 5)


myModel <- nimbleModel(code = myCode,
                       data = list(y = y),
                       constants = list(length_y = N),
                       inits = list(a = 0.5, b = 0.5))

CmyModel <- compileNimble(myModel)

myMCMC <- buildMCMC(CmyModel)
CmyMCMC <- compileNimble(myMCMC)##, project = myModel)

## then I have to run it all again in parallel
# make cluster
this_cluster <- makeCluster(2)
seed <- set.seed(1234)

# Create a function with all the needed code
run_MCMC_allcode <- function(seed, data) {
    library(nimble)
    ##
    myCode <- nimbleCode({
        a ~ dunif(0, 100)
        b ~ dunif(0, 100)
        for (i in 1:length_y) {
            y[i] ~ dgamma(shape = a, rate = b)
        }
    })
    ##
    myModel <- nimbleModel(code = myCode,
                           data = list(y = data),
                           constants = list(length_y = length(data)),
                           inits = list(a = 0.5, b = 0.5))
    ##
    CmyModel <- compileNimble(myModel)
    ##
    myMCMC <- buildMCMC(CmyModel)
    CmyMCMC <- compileNimble(myMCMC)##, project = myModel)
    ##
    results <- runMCMC(CmyMCMC, niter = 10000, setSeed = seed)
    ##
    return(results)
}

my.data <- y

chain_output <- parLapply(cl = this_cluster, X = 1:2,
                          fun = run_MCMC_allcode,
                          data = my.data)

stopCluster(this_cluster)

posteriorSamplesMatrix <- rbind(chain_output[[1]], chain_output[[2]])

samplesSummary(chain_output[[1]])
samplesSummary(chain_output[[2]])
samplesSummary(posteriorSamplesMatrix)


colnames(posteriorSamplesMatrix) <- colnames(chain_output[[1]])
dim(posteriorSamplesMatrix)
colnames(posteriorSamplesMatrix)

set.seed(0)
posteriorSamplesMatrix <- array(rnorm(20)^2+1, c(10,2))
colnames(posteriorSamplesMatrix) <- c('a', 'b')
posteriorSamplesMatrix

nimble:::matrix2mv(posteriorSamplesMatrix, CmyMCMC$mvSamples)

as.matrix(CmyMCMC$mvSamples)

CmyMCMC$run(5)

CmyMCMC$calculateWAIC()

CmyMCMC$enableWAIC
CmyMCMC$enableWAIC <- TRUE
CmyMCMC$enableWAIC

CmyMCMC$calculateWAIC()




nimble:::matrix2mv(posteriorSamplesMatrix, myMCMC$mvSamples)

as.matrix(myMCMC$mvSamples)

myMCMC$calculateWAIC()

myMCMC$enableWAIC
myMCMC$enableWAIC <- TRUE
myMCMC$enableWAIC

myMCMC$calculateWAIC()

myMCMC$run(5)



## investingating some user's question
## about 0 * -Inf = NaN
library(nimble)

code <- nimbleCode({
    x <- step(N-0.1) * log(N)
    y <- N * log(N)
})

Rmodel <- nimbleModel(code)

Rmodel$N
Rmodel$N <- 0
Rmodel$N <- 3
Rmodel$N

Rmodel$calculate()

Rmodel$x
Rmodel$y


conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)




library(testthat)


code <- nimbleCode({
    a ~ dnorm(0, 1)
    y ~ dexp(a^2+1)
    b ~ dnorm(a, 1)
    c ~ dnorm(b, 1)
})
constants <- list()
data <- list(y = 3)
inits <- list(a = 0, b = 1, c = 1)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)
compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
set.seed(0)
Rmcmc$run(10)
set.seed(0)
Cmcmc$run(10)
for(node in Rmodel$getNodeNames(stochOnly = TRUE)) {
    rLP <- Rmodel$getLogProb(node)
    cLP <- Cmodel$getLogProb(node)
    expect_equal(rLP, Rmodel$calculate(node))
    expect_equal(cLP, Cmodel$calculate(node))
    expect_equal(rLP, cLP)
}

##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)
samplesPlot(samples)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()





## testing if we can do MH updates to only
## some (unobseved) dimensions of a MV node
## (not necessarily multivariate normal)
## when other dimensions are observed.
## do the "data" flags support this?

library(nimble)

code <- nimbleCode({
    a ~ dnorm(4, sd = 10)
    b ~ dgamma(1, 1)
    c ~ dunif(2, 10)
    d[1:3] ~ dmnorm(mu[1:3], cov = C[1:3,1:3])
    e[1:3,1:3] ~ dwish(R = C[1:3,1:3], df = 5)
})
##constants <- list(mu=rep(0,3), C=diag(3))
constants <- list(mu=1:3, C=diag(c(1,4,9)))
data <- list(a = 1, d = c(NA,2,5))
U <- matrix(c(.2,2,4,0,1,1,0,0,4), nrow=3, byrow=TRUE)
eInit <- t(U) %*% U
inits <- list(b=1, c=5, e=eInit)
o
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Rmodel$initializeInfo()

node <- 'a'
Rmodel[[node]]
Rmodel$isData(node)
Rmodel$isStoch(node)

node <- 'b'
Rmodel[[node]]
Rmodel$isData(node)
Rmodel$isStoch(node)

node <- 'e'
Rmodel[[node]]
Rmodel$isData(node)
Rmodel$isStoch(node)

node <- 'd'
Rmodel[[node]]
Rmodel$isData(node)
Rmodel$isStoch(node)
Rmodel$isMultivariate(node)
Rmodel$isDataEnv[[node]]

conf <- configureMCMC(Rmodel, nodes = NULL)
type <- 'ess'
conf$addSampler(target = 'a', type = type)   ## normal
conf$addSampler(target = 'd', type = type)   ## multivariate-normal
conf$printSamplers()

Rmcmc <- buildMCMC(conf)




# some testing for Paul VDB paul van der-bates

set.seed(0)
Rmodel <- nimbleModel(code, constants, data, inits = inits())

conf <- configureMCMC(Rmodel)
conf$setMonitors(c('sigma', 'lambda', 'psi', 'Nhat', 'z'))

conf$removeSamplers('X')
for(i in 1:M) conf$addSampler(target = paste0('X[', i, ', 1:2]'), type = 'myX', control = list(xlim = limits$xlim, ylim = limits$ylim, J = nrow(traps)))

conf$removeSamplers('z')
conf$addSampler('z', type = 'myDemo', scalarComponents = TRUE)

conf$removeSamplers('ID')
conf$addSampler('ID', type = 'myCategorical', scalarComponents = TRUE)

conf$printSamplers()
conf$printSamplers('z')

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

samples <- runMCMC(Cmcmc, 100)

samplesSummary(samples)




library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    for(i in 1:2) {
        b[i] ~ dnorm(0, 1)
        for(j in 1:2) {
            c[i,j] ~ dnorm(0, 1)
            ##for(k in 1:2) {
            ##    d[i,j,k] ~ dnorm(0, 1)
            ##}
        }
    }
    y ~ dnorm(a, 1)
    yx ~ dcat(p[1:5])
})
constants <- list()
data <- list(y = 0)
inits <- list(a = 0,
              b = rep(0, 2),
              c = array(0, c(2,2)),
              ##p = (1:5) / sum(1:5),
              p = (1:5),
              yx = 1)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()    ## -13.78408

Rmodel$p
Rmodel$yx
exp(Rmodel$calculate('yx'))
Rmodel$yx <- 2
exp(Rmodel$calculate('yx'))
Rmodel$yx <- 5
exp(Rmodel$calculate('yx'))

Rmodel$p
Rmodel$p[5] <- 10

set.seed(0); out <- nimbleMCMC(code, constants, data, inits,
                               niter = 10,
                               nchains = 1,
                               samplesAsList = FALSE,
                               summary = TRUE)

##lapply(out, round, digits = 3)
lapply(out$summary, round, 3)
$chain1
          Mean Median St.Dev. 95%CI_low 95%CI_upp
a        0.052 -0.121   0.552    -0.776     0.875
b[1]    -0.112 -0.089   0.539    -0.828     0.829
b[2]     0.060  0.003   1.290    -1.986     2.163
c[1, 1]  0.047 -0.300   1.314    -1.389     2.178
c[2, 1] -0.039 -0.033   1.120    -1.472     1.623
c[1, 2] -0.179  0.040   0.710    -1.392     0.488
c[2, 2] -0.064 -0.079   0.660    -0.938     0.808

$chain2
          Mean Median St.Dev. 95%CI_low 95%CI_upp
a        0.007  0.016   0.503    -0.673     0.913
b[1]    -0.049  0.018   0.687    -1.198     0.927
b[2]    -0.323 -0.765   1.143    -1.626     1.484
c[1, 1] -0.312 -0.388   0.725    -1.195     0.951
c[2, 1] -0.025 -0.118   0.731    -1.059     1.027
c[1, 2]  0.299  0.392   1.106    -1.083     1.641
c[2, 2]  0.254  0.319   1.036    -1.095     1.552

$all.chains
          Mean Median St.Dev. 95%CI_low 95%CI_upp
a        0.029 -0.034   0.515    -0.804     0.980
b[1]    -0.081  0.001   0.601    -1.106     0.994
b[2]    -0.132 -0.306   1.202    -1.966     2.035
c[1, 1] -0.133 -0.388   1.049    -1.348     1.886
c[2, 1] -0.032 -0.033   0.921    -1.396     1.472
c[1, 2]  0.060  0.239   0.937    -1.353     1.599
c[2, 2]  0.095  0.105   0.861    -1.055     1.521

lapply(out$samples, round, 3)
$chain1
           a   b[1]   b[2] c[1, 1] c[2, 1] c[1, 2] c[2, 2]
 [1,]  0.893 -0.326  1.330   1.272  -1.540   0.415  -0.929
 [2,] -0.208 -0.006  2.405   0.764  -1.148  -0.799  -0.289
 [3,] -0.212 -0.412  0.252  -0.892  -1.238   0.436  -0.224
 [4,]  0.267  0.133  0.804  -0.057   1.086   0.504  -0.691
 [5,] -0.908  0.047 -0.236  -0.543  -0.649  -0.433   0.727
 [6,]  0.815  0.992 -0.430   1.238   1.758  -0.279   0.561
 [7,] -0.320 -0.832 -1.167  -1.066   1.157  -1.564   0.832
 [8,] -0.161  0.266 -0.377   2.441  -0.055  -0.795   0.250
 [9,]  0.437 -0.173 -2.224  -1.264  -0.011   0.359  -0.941
[10,] -0.082 -0.815  0.242  -1.425   0.248   0.366   0.065

$chain2
           a   b[1]   b[2] c[1, 1] c[2, 1] c[1, 2] c[2, 2]
 [1,]  0.014  0.257 -0.649  -0.119   1.101   0.664   0.144
 [2,] -0.083 -0.912 -1.438  -0.797   0.772   1.254  -0.220
 [3,] -0.300 -0.419  0.997  -0.276   0.647   1.256   1.299
 [4,] -0.617  0.008 -0.881   0.596  -0.282   0.120   1.456
 [5,]  0.162  0.997  0.782  -0.777   0.047  -0.616  -1.130
 [6,]  0.408 -1.281  1.625  -0.501  -0.413   1.678  -0.972
 [7,]  0.018  0.027 -1.680   1.054   0.336  -1.120   0.495
 [8,]  0.098 -0.119  0.198  -1.069  -1.114  -0.803   1.580
 [9,]  1.059  0.263 -1.233  -0.004  -0.476   1.512   0.798
[10,] -0.689  0.689 -0.956  -1.232  -0.870  -0.957  -0.911


set.seed(0); out <- nimbleMCMC(code, constants, data, inits,
                               niter = 10,
                               nchains = 3,
                               samplesAsList = TRUE,
                               summary = TRUE)

round(out$summary, 3)

out$samples
length(out$samples)
names(out$samples)

out$samples[[1]]
out$samples[[2]]

round(out$samples$b, 3)
round(out$samples$c[1,,], 3)
round(out$samples$c[10,,], 3)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
Cmcmc$run(10)

mvSamp <- Cmcmc$mvSamples
x <- mvSamp
class(mvSamp)

nimble:::as.list.CmodelValues
nimble:::as.list.modelValuesBaseClass

mn <- as.matrix(mvSamp)
mn
mn[,1] <- 1
mn[,4] <- 4
mn
nimble:::matrix2mv(mn, mvSamp)
mvSamp

ln <- as.list(mvSamp)
ln

niter <- 10
monitorVars <- c('a', 'b', 'c', 'd')

constsAndData <- c(constants, data)
modelfile <- file.path(tempdir(), 'model.txt')
writeLines(paste0('model\n', paste0(deparse(code, width.cutoff=500L), collapse='\n')), con=modelfile)

## alternate, using jagsUI::jags
library(jagsUI)
set.seed(0)

out <- jags(
    model.file = modelfile,
    data = constsAndData,
    inits = list(inits),   ## a list, of length = n.chains
    parameters.to.save = monitorVars,
    n.iter = niter,
    n.chains = 1,
    n.burnin = 0,
    n.thin = 1)

## out$sims.list is a *named list* of samples for each parameter
class(out)
ls(out)
out$samples
class(out$samples)

l <- out$sims.list

class(l)
class(ln)

length(l)
length(ln)

names(l)
names(ln)

l$a
ln$a
length(l$a)
length(ln$a)

dim(l$b)
dim(ln$b)

dim(l$c)
dim(ln$c)

dim(l$d)
dim(ln$d)



## testing new ess elliptical slice sampler
## allowing it to work on univariate normal nodes, also

library(nimble)

code <- nimbleCode({
    a ~ dnorm(4, sd = 10)
    b ~ dgamma(1, 1)
    c ~ dunif(2, 10)
    d[1:3] ~ dmnorm(mu[1:3], cov = C[1:3,1:3])
    e[1:3,1:3] ~ dwish(R = C[1:3,1:3], df = 5)
})
##constants <- list(mu=rep(0,3), C=diag(3))
constants <- list(mu=1:3, C=diag(c(1,4,9)))
data <- list()
U <- matrix(c(.2,2,4,0,1,1,0,0,4), nrow=3, byrow=TRUE)
eInit <- t(U) %*% U
inits <- list(a=0, b=1, c=5, d=rep(0,3), e=eInit)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()


conf <- configureMCMC(Rmodel, nodes = NULL)
type <- 'ess'
conf$addSampler(target = 'a', type = type)   ## normal
conf$addSampler(target = 'd', type = type)   ## multivariate-normal
conf$printSamplers()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc)); Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)


##set.seed(0)
##samples <- runMCMC(Cmcmc, 3)

## a only
samples[c(500, 1000), 'a']

## d only
samples[c(500, 1000), 'd[1]']

## a and d both
samples[c(500, 1000), c('a','d[1]')]

aSamp <- samples[, 'a']
mean(aSamp)
sd(aSamp)

dSamp <- samples[, c('d[1]', 'd[2]', 'd[3]')]
apply(dSamp, 2, mean)
cov(dSamp)


## testing HMC sampler
## now using double-taping
## what results come?  different from exact results in test-mcmc

1
library(nimble)
library(testthat)


nimbleOptions(experimentalEnableDerivs = TRUE)
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
code <- nimbleCode({
    a[1] ~ dnorm(0, 1)
    a[2] ~ dnorm(a[1]+1, 1)
    a[3] ~ dnorm(a[2]+1, 1)
    d ~ dnorm(a[3], sd=2)
})
constants <- list()
data <- list(d = 5)
inits <- list(a = rep(0, 3))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('a', 'HMC', control = list(nwarmup = 1000))
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)
##
round(as.numeric(samples[1000,]), 5)
## -0.83500  1.14365  1.13153
round(as.numeric(apply(samples, 2, mean)), 7)
##  0.4163956 1.8431886 3.2691439
round(as.numeric(apply(samples, 2, sd)), 7)
##  0.9367285 1.2115729 1.3101386


> round(as.numeric(samples[1000,]), 5)
[1] -0.11556  0.88505  2.89503
> round(as.numeric(apply(samples, 2, mean)), 7)
[1] 0.4136721 1.8417534 3.2569954
> round(as.numeric(apply(samples, 2, sd)), 7)
[1] 0.9268822 1.2033593 1.3179108
> 

set.seed(0)
N <- 10
x <- rnorm(N)
y <- 2 - 0.2*x  + rnorm(N, 0, 0.1)
m <- lm(y ~ x)
coef(m)
##(Intercept)           x 
##  1.9687161  -0.2138309 


library(nimble)

# generating fake data
sim_probit = function(N,beta){
    set.seed(1)
    X = matrix(rnorm(N*length(beta),0,1),N,length(beta))
    Y = X%*%beta
    Y = Y+rnorm(N,0,0.75)		    
    for (i in 1:N) Y[i] = ifelse(pnorm(Y[i])>runif(1),1,0)
    return(list(Y=Y,X=X,N=N))
}

data = sim_probit(N = 100,beta = c(0.5,0.6))

##spp_code <- nimbleCode({
##    for (i in 1:K){
##        beta[i] ~ dnorm(0,0.01)
##    }
##    epsilon ~ dnorm(0,1)
##    y_star[1:N] <- x[1:N,1:K]%*%beta[1:K]+epsilon
##    for (j in 1:N){
##        y[j] ~ dbern(phi(y_star[j]))
##    }
##})

spp_code <- nimbleCode({
    for (i in 1:K){
        beta[i] ~ dnorm(0,0.01)
    }
    y_star[1:N] <- x[1:N,1:K]%*%beta[1:K]
    for (j in 1:N){
        epsilon[j] ~ dnorm(0,1)
        y[j] ~ dbern(phi(y_star[j] + epsilon[j]))
    }
})

spp_code <- nimbleCode({
    for (i in 1:K){
        beta[i] ~ dnorm(0,0.01)
    }
    y_star[1:N] <- x[1:N,1:K]%*%beta[1:K]
    for (j in 1:N){
        y_p[j] ~ dnorm(y_star[j], 1)
        y[j] ~ dbern(phi(y_p[j]))
    }
})


spp_Model <- nimbleModel(code=spp_code,constants=list(N = data$N,K=ncol(data$X)),
                         data=list(y = as.numeric(data$Y),x = apply(data$X,2,scale) ),#winbugs examples suggest scaling data_X 
                         inits=list(epsilon=rep(0,data$N),beta=rep(0,ncol(data$X)))
                         )


spp_Model <- nimbleModel(code=spp_code,constants=list(N = data$N,K=ncol(data$X)),
                         data=list(y = as.numeric(data$Y),x = apply(data$X,2,scale) ),#winbugs examples suggest scaling data_X 
                         inits=list(y_p=rep(0,data$N),beta=rep(0,ncol(data$X)))
                         )

spp_Model <- nimbleModel(code=spp_code,constants=list(N = data$N,K=ncol(data$X)),
                         data=list(y = as.numeric(data$Y),x = data$X ),
                         inits=list(y_p=rep(0,data$N),beta=rep(0,ncol(data$X)))
                         )


spp_md = nimbleMCMC(spp_Model,summary = TRUE,niter=5000, nburnin=100)

##printErrors()

round(spp_md$summary, 3)

#### results
####            mean     sd   t_val
#### beta[1] -0.6423 0.1652 -3.8875
#### beta[2]  0.8101 0.1796  4.5114
#### epsilon -0.1653 0.1447 -1.1427




https://williams.zoom.us/j/2979251765




(1/4) This email
(2/4) Deposition (day 1) transcripts, which is a 38 MB file
(3/4) Deposition (day 2) transcripts, which is a 15 MB file
(4/4) The records subpoenaedÂ from Williams College, which is a 22 MB file
'
 

set.seed(0)

n = 20                              # 'n' is assumed to be even
x1 = c(rep(0,n/2), rep(1,n/2))      # two groups: x1=0, and x1=1
x2 = rnorm(n, mean=10, sd=3)
y = rnorm(n, mean = 3*x1 + 0.1*x2)  # data generation

x1 = factor(x1)
m1 = glm(y ~ x1)                    # using 'glm' provides AIC values.
m2 = glm(y ~ x1 + x2)               # using 'lm' doesn't.
aic = c(m1$aic, m2$aic)
delta.aic = aic - min(aic)
model.weights = exp(-0.5*delta.aic) / sum(exp(-0.5*delta.aic))
residual.dfs = c(m1$df.residual, m2$df.residual)

p1 = predict(m1, se=TRUE, newdata=list(x1=factor(1), x2=15))
p2 = predict(m2, se=TRUE, newdata=list(x1=factor(1), x2=15))
theta.hats = c(p1$fit, p2$fit)
se.theta.hats = c(p1$se.fit, p2$se.fit)

#  AIC model weights
model.weights

#  95% Wald confidence interval for theta (under Model 1)
theta.hats[1] + c(-1,1)*qt(0.975, residual.dfs[1])*se.theta.hats[1]

#  95% Wald confidence interval for theta (under Model 2)
theta.hats[2] + c(-1,1)*qt(0.975, residual.dfs[2])*se.theta.hats[2]

#  95% MATA-Wald confidence interval for theta (model-averaging)
mata.wald(theta.hats, se.theta.hats, model.weights, mata.t = TRUE, residual.dfs = residual.dfs)
ci <- mata.wald(theta.hats, se.theta.hats, model.weights, mata.t = TRUE, residual.dfs = residual.dfs)
ci

dmata.wald(4, theta.hats, se.theta.hats, model.weights, mata.t = TRUE, residual.dfs = residual.dfs)

pmata.wald(5, theta.hats, se.theta.hats, model.weights, mata.t = TRUE, residual.dfs = residual.dfs)

pmata.wald(ci[1], theta.hats, se.theta.hats, model.weights, mata.t = TRUE, residual.dfs = residual.dfs)
pmata.wald(ci[2], theta.hats, se.theta.hats, model.weights, mata.t = TRUE, residual.dfs = residual.dfs)

ci <- mata.wald(theta.hats, se.theta.hats, model.weights, mata.t = TRUE, residual.dfs = residual.dfs, alpha = 0.25)
ci
pmata.wald(ci[1], theta.hats, se.theta.hats, model.weights, mata.t = TRUE, residual.dfs = residual.dfs)
pmata.wald(ci[2], theta.hats, se.theta.hats, model.weights, mata.t = TRUE, residual.dfs = residual.dfs)

pmata.wald(theta.hats[1]+1, theta.hats, c(0, 0), model.weights, mata.t = TRUE, residual.dfs = residual.dfs)


myf <- function(e) dmata.wald(e, theta.hats, se.theta.hats, model.weights, mata.t = TRUE, residual.dfs = residual.dfs)
myfvec <- Vectorize(myf)

##myf(0)
##myfvec(0)
##myfvec(0:5)
## 
##for(i in seq(-10, 10, by = 0.1)) print(myf(i))

integrate(f = myfvec, lower = -100, upper = 100)
integrate(f = myfvec, lower = ci[1], upper = ci[2])

myf <- function(x) dt(x, df = 17)
integrate(f = myf, lower = -100, upper = 100)


pmata.wald(ci[1], theta.hats, se.theta.hats, model.weights, mata.t = 1, residual.dfs = residual.dfs)


dmata.wald(4, theta.hats, se.theta.hats, model.weights, residual.dfs = residual.dfs)
dmata.wald(4, theta.hats, se.theta.hats, model.weights, mata.t = FALSE)
dmata.wald(4, theta.hats, se.theta.hats, model.weights, mata.t = TRUE)

mata.wald(theta.hats, se.theta.hats, model.weights, mata.t = FALSE)
mata.wald(theta.hats, se.theta.hats, c(-.5, 1.5), mata.t = FALSE)





max <- 1000
res <- 0.1
a <- seq(-max, max, by = res)
pos <- 1/2 * (a + sqrt(a^2+16))
neg <- 1/2 * (a - sqrt(a^2+16))
##ylim <- c(min(c(pos, neg)), max(c(pos, neg)))
ylim <- c(-1, 1) * 2
plot(a, pos, type = 'l', ylim=ylim)
lines(a, neg)
abline(h = c(-1, 1), lty = 2)
abline(v = c(-3, 3), col = 'red')
abline(h = 0, col = 'grey')




code <- nimbleCode({
    b0 ~ dnorm(0, 1)
    b1 ~ dnorm(0, 1)
    b2 ~ dnorm(0, 1)
    ### priors, etc....
    for(t in 1:T) {
        y[t] ~ dpois(exp(b0 + b1*x1[t]*S1[t] + b2*x2[t]*S2[t]))
    }
})

T <- 10
set.seed(0)
x1 <- rnorm(T, 0, 1)
x2 <- rnorm(T, 0, 1)
S1 <- rbinom(T, 1, prob = 0.5)
S2 <- rbinom(T, 1, prob = 0.5)
y <- rpois(T, 2)

constants <- list(T=T, x1=x1, x2=x2, S1=S1, S2=S2)
data <- list(y=y)
inits <- list(b0=0, b1=0, b2=0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()   ## -20.49922

#' \item llFunction. A specialized nimbleFunction that accepts no arguments and returns a scalar double number.  The return value must be the total log-likelihood of all stochastic dependents of the target nodes -- and, if includesTarget = TRUE, of the target node(s) themselves --  or whatever surrogate is being used for the total log-likelihood.  This is a required element with no default.
#' \item includesTarget. Logical variable indicating whether the return value of llFunction includes the log-likelihood associated with target.  This is a required element with no default.
#' }

llFunction_general <- nimbleFunction(
    setup = function(targetNode, indicatorNodes) {
    },
    run = function() {
        returnType(double())
        return(lp)
    }
)

conf <- configureMCMC(Rmodel)

conf$removeSamplers('b1')
conf$addSampler(target = 'b1', type = 'RW_llFunction', control = list(llFunction = xxx, includesTarget = FALSE))

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)





## implenting hamiltonian dynamics and HMC,
## to see what metric M is optimal, and make
## sure the calculations are correct.
library(mvtnorm)
library(numDeriv)

## rmvnorm = function(n, mean = rep(0, nrow(sigma)), sigma = diag(length(mean)), 
##     method = c("eigen", "svd", "chol"), pre0.9_9994 = FALSE, 
##     checkSymmetry = TRUE) 
## dmvnorm = function(x, mean = rep(0, p), sigma = diag(p), log = FALSE,
##     checkSymmetry = TRUE)

## target density: MVN(mean=0, cov=Sigma)
##Sigma <- diag(2)
##Sigma <- array(c(3, -.2, -.2, 1), c(2, 2))
##L <- array(c(2, 0.9, 0, 5), c(2,2))
##L
##Sigma <- L %*% t(L)
##Sigma
##U <- chol(Sigma)
##U
rho <- 0.7
sx <- 1
sy <- 1
Sigma <- array(c(sx^2, rho*sx*sy, rho*sx*sy, sy^2), c(2,2))

U1 <- function(q) -dmvnorm(q, sigma = Sigma, log = TRUE)
U2 <- function(q) log(2*pi) + 1/2*log(det(Sigma)) + 1/2*(t(q) %*% solve(Sigma) %*% q)[1,1]
U3 <- function(q) 1/2*(t(q) %*% solve(Sigma) %*% q)[1,1]
U <- U3
##q <- c(1, 1)
##q <- c(1, 3)
##q <- c(2, 1)
##U1(q)
##U2(q)

dUdq <- function(qArg) grad(func = U, x = qArg)
##q <- c(0, 0)
##q <- c(0, 1)
##q <- c(1, 0)
##q <- c(1, 1)
##q <- c(1, 3)
##q <- c(2, 1)
##dUdq(q)

## metric M
M <- diag(2)
##M <- diag(c(1, 2))
## M <- Sigma
## M <- solve(Sigma)

K <- function(p) 1/2*(t(p) %*% solve(M) %*% p)[1,1]

## Hamiltonian H
H <- function(q, p) U(q) + K(p)

epsilon <- 0.3

leapfrog <- function(q, p) {
    p1 <- p - epsilon/2 * dUdq(q)
    q2 <- q + epsilon * p1/diag(M)
    p2 <- p1 - epsilon/2 * dUdq(q2)
    return(list(q=q2, p=p2))
}


run <- function(n, q, p) {
    out <- array(NA, c(n+1,5))
    out[1,1:2] <- q
    out[1,3] <- H(q,p)
    out[1,4:5] <- p
    for(i in 1:n) {
        nextQP <- leapfrog(q, p)
        q <- nextQP$q
        p <- nextQP$p
        out[i+1,1:2] <- q
        out[i+1,3] <- H(q,p)
        out[i+1,4:5] <- p
    }
    return(out)
}

q <- c(0, 0)
p <- c(1, 3)


n <- 500
out <- run(n, q, p)




max <- 20
lims <- c(-max, max)
plot(x = out[,1], y = out[,2], type = 'p', pch = 'x', xlim=lims, ylim=lims)

##out[,3]
pplot(x = 1:(n+1), y = out[,3], type = 'l')

apply(out[,4:5], 2, mean)
cov(out[,4:5])
solve(M)

sample <- function(its, pM, n = 100) {
    samps <- array(NA, c(its, 4))
    q <- c(0, 0)
    p <- 1:2
    for(i in 1:its) {
        ##cat(i, '\n')
        ##p[1] <- rnorm(1, 0, sd = sqrt(pM[1,1]))
        ##p[2] <- rnorm(1, 0, sd = sqrt(pM[2,2]))
        p <- rmvnorm(1, sigma = pM)[,]
        out <- run(n, q, p)
        qProp <- out[n+1,1:2]
        pProp <- out[n+1,4:5]
        ##loga <- -(U(qProp) - U(q))
        loga <- H(qProp,pProp) - H(q,p)
        u <- runif(1, 0, 1)
        if(u < exp(loga)) q <- qProp
        samps[i,1:2] <- q
    }
    return(samps)
}

##samps <- sample(1000, pM, n = 10)
##samps


sampList <- list()
for(i in 1:3) {
    if(i == 1) pM <- diag(2)
    if(i == 2) pM <- Sigma
    if(i == 3) pM <- solve(Sigma)
    print(pM)
    sampList[[i]] <- sample(1000, pM, n = 10)
}


max <- 5
lims <- c(-max, max)
par(mfrow = c(1,3))
for(i in 1:3) {
    samps <- sampList[[i]]
    plot(x = samps[,1], y = samps[,2], type = 'p', pch = 'x', xlim=lims, ylim=lims)
}








## run testing for HMC sampler:

1

source('~/temp/test-mcmc.R')



library(nimble)

code <- nimbleCode({
    a[1] ~ dnorm(0, 1)
    a[2] ~ dnorm(a[1], 1)
    ##a[3] ~ dnorm(a[2]+1, 1)
    ##d ~ dnorm(a[3], sd=2)
    d ~ dnorm(a[2], sd=2)
})
constants <- list()
data <- list(d = 5)
inits <- list()##a = rep(0, 3))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Rmodel$getNodeNames()
Rmodel$getNodeNames(latentOnly = TRUE)

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('a', 'HMC', printM = TRUE, printTimesRan = TRUE)
Rmcmc <- buildMCMC(conf)

##set.seed(0)
##samples <- runMCMC(Rmcmc, 160)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

Cmodel$setInits(inits)
set.seed(0)
samples <- runMCMC(Cmcmc, 160, progressBar = FALSE)




## testing HMC sampler to correctly converge to
## a custom distribution (parabola, I hope)

library(nimble)

dmyDist <- nimbleFunction(
    run = function(x = double(), theta = double(1), a = double(), log = integer(0, default = 0)) {
        if(theta[2] > 10) return(-99)
        lp <- -abs(theta[2] - a*theta[1]^2)
        returnType(double())
        return(lp)
    },
    enableDerivs = TRUE
)
rmyDist <- nimbleFunction(
    run = function(n = integer(), theta = double(1), a = double()) {
        stop('called rmyDist')
        returnType(double())
        return(0)
    }
)
registerDistributions(list(
    dmyDist = list(BUGSdist = 'dmyDist(theta, a)',
                   types = c('value = double()', 'theta = double(1)', 'a = double()'))
))

code <- nimbleCode({
    for(i in 1:2) {
        theta[i] ~ dnorm(0, sd = 1000)
    }
    x ~ dmyDist(theta = theta[1:2], a = a)
})
constants <- list()
data <- list(x = 0)
inits <- list(theta = c(0,0), a = 1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('theta', 'HMC', printM = TRUE, printTimesRan = TRUE, printEpsilon = TRUE)
Rmcmc <- buildMCMC(conf)

##set.seed(0)
##samples <- runMCMC(Rmcmc, 160)

Cmodel <- compileNimble(Rmodel)
##Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)
##printErrors()
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

samples <- runMCMC(Cmcmc, 1000)

colnames(samples)
dim(samples)

indToPlot <- 1:1000
indToPlot <- 1:500
indToPlot <- 1:300

x <- samples[indToPlot, 'theta[1]']
y <- samples[indToPlot, 'theta[2]']
plot(x, y, type = 'p', pch = 'x')

library(coda)
effectiveSize(samples)
## theta[1] theta[2] 
## 34.03064 18.99514 








## trying to use nimble's which(), or nimWhich() in compiled code

library(nimble)

Rnf <- nimbleFunction(
    run = function(x = double(1)) {
        w <- which(x == 2)
        returnType(integer(1))
        return(w)
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

(x <- c(1:5, 1, 2, 2))
which(x == 2)

Rnf(x)
Cnf(x)





## Perry's tough case for posterior predictive branches

library(nimble)

code <- nimbleCode({
    mu ~ dnorm(0, 1) # prior
    ##y ~ dnorm(mu, 1) # data
    for(i in 1:N) {
        ypred[i] ~ dnorm(mu, 1) # predictive depth 1
        zpred[i] ~ dnorm(ypred[i], 1) # predictive depth 2
    }
})
codeY <- nimbleCode({
    mu ~ dnorm(0, 1) # prior
    y ~ dnorm(mu, 1) # data
    for(i in 1:N) {
        ypred[i] ~ dnorm(mu, 1) # predictive depth 1
        zpred[i] ~ dnorm(ypred[i], 1) # predictive depth 2
    }
})


N <- 10
constants <- list(N = N)
data <- list()
dataY <- list(y = 0)
inits <- list(mu = 0, ypred = rep(0,N), zpred = rep(0,N))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Rmodel$getDependenciesList()
Rmodel$getConditionallyIndependentSets()


RmodelY <- nimbleModel(codeY, constants, dataY, inits)
RmodelY$calculate()

RmodelY$getConditionallyIndependentSets(type = 'fromTop')
RmodelY$getConditionallyIndependentSets(type = 'fromBottom')
RmodelY$getConditionallyIndependentSets(type = 'both')

nodes <- 'mu'

RmodelY$getConditionallyIndependentSets(nodes = nodes, type = 'fromTop')
RmodelY$getConditionallyIndependentSets(nodes = nodes, type = 'fromBottom')
RmodelY$getConditionallyIndependentSets(nodes = nodes, type = 'both')



system.time(conf <- configureMCMC(Rmodel))
conf$printSamplers(byType = TRUE)

system.time(confY <- configureMCMC(RmodelY))   ## 20 seconds when N=10000
confY$printSamplers(byType = TRUE)

nimbleOptions(MCMCjointlySamplePredictiveBranches = FALSE)
nimbleOptions('MCMCjointlySamplePredictiveBranches')

system.time(confY <- configureMCMC(RmodelY))   ## 21 seconds when N=10000
confY$printSamplers(byType = TRUE)

system.time(confY <- configureMCMC(RmodelY, useConjugacy=FALSE))   ## 14 seconds when N=10000
confY$printSamplers(byType = TRUE)






## helping Wei with SCR + CAR, making agreement between proper and intrinsic
## CAR models


num <- c(1, 2,    3,       3,       2,    1)     ## for example
adj <- c(2, 1, 3, 2, 4, 5, 3, 5, 6, 3, 4, 4)     ## for example
N <- length(num)
L <- length(adj)
weights <- rep(1, L)     ## or otherwise, but this is simplest

library(nimble)
nimble:::CAR_normal_checkAdjWeightsNum(adj, weights, num)    ## safety check

Q <- array(0, c(N,N))
W <- array(0, c(N,N))
ind <- 1
for(i in 1:N) {
    if(num[i] > 0) {
        for(j in 1:num[i]) {
            Q[i,adj[ind]] <- -weights[ind]
            Q[i,i] <- Q[i,i] + weights[ind]
            W[i,adj[ind]] <- weights[ind]
            ind <- ind + 1
        }
    }
}

Q
W
(B <- W / apply(W, 2, sum))
(D <- diag(1/num))
(Dinv <- solve(D))
I <- diag(N)

## Q is equal to Dinv %*% (I - B)
all(Dinv %*% (I - B) == Q)

(eig <- eigen(Q))

v <- eig$vectors


t(t(v) / apply(v, 2, function(x) min(abs(x))))



## use to check the validity of adj, weights, num for use in dcar_normal:

##(carCM <- as.carCM(adj, weights, num))

##(C <- CAR_calcC(adj, num))
##(M <- CAR_calcM(num))
##(Cmatrix <- CAR_calcCmatrix(C, adj, num))
## 
##(Q <- diag(1/M) %*% (diag(N) - Cmatrix))
##(diag(N) - Cmatrix) %*% diag(1/M)

eig <- eigen(Q)
lambda <- eig$values
lambdaPlus <- ifelse(lambda < 1E-6, 0, 1/lambda)
Gamma <- eig$vectors

##round(Gamma %*% diag(lambda) %*% t(Gamma), 3)

## Gamma %*% sqrt(Lambda^+) %*% vec

##set.seed(0)
##n <- 2
##samp <- array(0, c(n, N))
##for(i in 1:n) {
##    z <- rnorm(N)
##    #samp[i,] <- t(Gamma %*% diag(sqrt(Lambda)) %*% z)
##    samp[i,] <- t(   Gamma %*% (sqrt(Lambda) * z)    )
##}
## 
##C <- cov(samp)
##solve(C)



muTrue <- 1:N   ## for example

set.seed(0)
z <- rnorm(N)
Gamma %*% (sqrt(lambdaPlus) * z) + muTrue  ## CAR realization



alpha <- 2  ## simulation value of overall mean
## X is a (Nxp) design matrix containing the p spatial covariates, which does *not* include a column of 1's.
## beta is a (px1) vector of fixed regression coefficient values, which we'll use for simulation
muTrue <- alphaÂ + X %*% beta


##n <- 1000
##out <- array(0, c(n,N))
##for(i in 1:n) {
##    z <- rnorm(N)
##    out[i,] <- Gamma %*% (sqrt(lambdaPlus) * z) + muTrue  ## CAR realization
##}
## 
##S <- cov(out)
##S
##solve(S)
##solve(Q)
##eigen(S)



## here's the code from the (second) email that I sent to Wei:

num <- c(1,   2,      3,         3,         2,      1)     ## or otherwise
adj <- c(2,   1, 3,   2, 4, 5,   3, 5, 6,   3, 4,   4)     ## or otherwise
N <- length(num)
L <- length(adj)
weights <- rep(1, L)     ## or otherwise, but this is simplest

tau <- 2       ## update: added true value of tau used for simulation

library(nimble)
nimble:::CAR_normal_checkAdjWeightsNum(adj, weights, num)    ## safety check

## create precision matrix
Q <- array(0, c(N,N))
ind <- 1
for(i in 1:N) {
    if(num[i] > 0) {
        for(j in 1:num[i]) {
            Q[i,adj[ind]] <- -weights[ind]
            Q[i,i] <- Q[i,i] + weights[ind]
            ind <- ind + 1
        }
    }
}

eig <- eigen(Q)
lambda <- eig$values
lambdaPlus <- ifelse(lambda < 1E-10, 0, 1/lambda)   ## update: lower threshold 1E-10
Gamma <- eig$vectors

muTrue <- 1:N   ## for example

set.seed(0)
z <- rnorm(N)
tau * Gamma %*% (sqrt(lambdaPlus) * z) + muTrue  ## ICAR realization, update: added tau


'dcar_normal(adj, weights,           num, tau, c,                                zero_mean = 0)'
'dcar_normal(adj, weights,           num, tau, c = CAR_calcNumIslands(adj, num), zero_mean    )'
'dcar_normal(adj, weights,           num, tau, c = CAR_calcNumIslands(adj, num), zero_mean = 0)'
'dcar_normal(adj, weights = adj/adj, num, tau, c,                                zero_mean    )'
'dcar_normal(adj, weights = adj/adj, num, tau, c,                                zero_mean = 0)'
'dcar_normal(adj, weights = adj/adj, num, tau, c = CAR_calcNumIslands(adj, num), zero_mean    )'
'dcar_normal(adj, weights = adj/adj, num, tau, c = CAR_calcNumIslands(adj, num), zero_mean = 0)'










identical(carCM$C, C)
identical(carCM$M, M)

## use to check the validity of adj, num, C, M for use in dcar_proper:
nimble:::CAR_proper_checkAdjNumCM(adj, num, C, M)
nimble:::CAR_proper_checkAdjNumCM(adj, num, carCM$C, carCM$M)




C <- CAR_calcC(adj, num)
M <- CAR_calcM(num)
carBounds(C, adj, num, M)

nimble::carBounds(C, adj, num, M)


## simulation using CAR proper:
## need:
## adj, num from the adjacency structure
## tau (scalar precision), gets a prior distribution
## mu, a vector of location-specific means
## (don't provide C and M; the defaut calucaltions will generate what's needed for the correct correspondance)

'dcar_proper(gamma)'
Rdist    = 'dcar_proper(mu, C,                       adj, num, M,                  tau, gamma)'
'dcar_proper(mu, C = CAR_calcC(adj, num), adj, num, M = CAR_calcM(num), tau, gamma)'
types    = c('value = double(1)', 'mu = double(1)', 'C = double(1)', 'adj = double(1)', 'num = double(1)', 'M = double(1)', 'tau = double(0)', 'gamma = double(0)', 'evs = double(1)')

rcar_proper(mu = mu, adj = adj, num = num, tau = tau, gamma = gamma)

## model fitting using CAR normal:
## need:
## adj, num from the adjacency structure
## weights
## zero_mean

##dcar_normal = list(BUGSdist = 'dcar_normal(weights, zero_mean)',
##                   Rdist    = c('dcar_normal(adj, weights,           num, tau, c,                                zero_mean = 0)',
##                                'dcar_normal(adj, weights,           num, tau, c = CAR_calcNumIslands(adj, num), zero_mean    )',
##                                'dcar_normal(adj, weights,           num, tau, c = CAR_calcNumIslands(adj, num), zero_mean = 0)',
##                                'dcar_normal(adj, weights = adj/adj, num, tau, c,                                zero_mean    )',
##                                'dcar_normal(adj, weights = adj/adj, num, tau, c,                                zero_mean = 0)',
##                                'dcar_normal(adj, weights = adj/adj, num, tau, c = CAR_calcNumIslands(adj, num), zero_mean    )',
##                                'dcar_normal(adj, weights = adj/adj, num, tau, c = CAR_calcNumIslands(adj, num), zero_mean = 0)'),
##                   types    = c('value = double(1)', 'adj = double(1)', 'weights = double(1)', 'num = double(1)', 'tau = double(0)', 'c = double(0)', 'zero_mean = double(0)'),

dcar_normal(adj = adj, weights = weights, num = num, tau = tau, zero-mean = 1) 


## working with Konstantinoudis Akis's issue,
## NA's in MCMC output, from nimble-users list

library(nimble)

findata.linear <- readRDS("~/Downloads/dat")

linear_Code <- nimbleCode({
    alpha ~ dflat()                                      
    exp.alpha <- exp(alpha)                              
    beta_1 ~ dnorm(0, 1)                                      
    exp.beta_1 <- exp(beta_1)
    for (i in 1:N){
        O[i] ~ dpois(mu[i])                              
        log(mu[i]) <- log(E[i]) + alpha + beta_1*lag0[i]
    }
})

tempData = list(O = as.numeric(findata.linear$`1`))

tempConsts <-list(
    N = nrow(findata.linear),  
    lag0 = findata.linear$meanTemp,
    E = findata.linear$expected)

params <- c("alpha", "mu", "beta_1")

##ni <- 50000  # nb iterations
##nt <- 10     # thinning interval
##nb <- 10000  # nb iterations as burn-in

inits <- list(alpha=0.01, beta_1=0)

model <- nimbleModel(linear_Code, constants = tempConsts, data = tempData, inits = inits)#####, calculate = FALSE)

model$getLogProb()

cModel <- compileNimble(model)
lp <- cModel$calculate()
lp   ## -26924.42
conf <- configureMCMC(model, monitors = params)
MCMC <- buildMCMC(conf)
cMCMC <- compileNimble(MCMC, project = cModel)

set.seed(0)
linear_Model <- runMCMC(cMCMC, niter = 100)

names <- c('alpha', 'beta_1', paste0('mu[', c(1, 100000, 200000, 300000, 400000), ']'))

round(linear_Model[, names], 3)




library(nimble)

findata.linear <- readRDS("~/Downloads/dat")

linear_Code <- nimbleCode({
    alpha ~ dflat()                                      
    exp.alpha <- exp(alpha)                              
    beta_1 ~ dnorm(0, 1)                                      
    exp.beta_1 <- exp(beta_1) 
    for (i in 1:N){
        O[i] ~ dpois(mu[i])                               
        log(mu[i]) <- log(E[i]) + alpha + beta_1*lag0[i]
    }
})

tempData = list(O = as.numeric(findata.linear$`1`))

tempConsts <-list(
    N = nrow(findata.linear),  
    lag0 = findata.linear$meanTemp, 
    E = findata.linear$expected)

params <- c("alpha", "mu", "beta_1")

##ni <- 50000  # nb iterations 
##nt <- 10     # thinning interval
##nb <- 10000  # nb iterations as burn-in 

inits <- list(alpha=0.01, beta_1=0)

model <- nimbleModel(linear_Code, constants = tempConsts, data = tempData, inits = inits, calculate = FALSE)

cModel <- compileNimble(model)
cModel$calculate()   ## -26924.42
conf <- configureMCMC(model, monitors = params)
MCMC <- buildMCMC(conf)
cMCMC <- compileNimble(MCMC, project = cModel)

set.seed(0)
linear_Model <- runMCMC(cMCMC, niter = 100)

names <- c('alpha', 'beta_1', paste0('mu[', c(1, 100000, 200000, 300000, 400000), ']'))

round(linear_Model[, names], 6)
##        alpha beta_1 mu[1] mu[100000] mu[200000] mu[300000] mu[400000]
##   [1,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##   [2,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##   [3,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##   [4,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##   [5,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##   [6,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##   [7,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##   [8,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##   [9,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [10,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [11,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [12,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [13,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [14,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [15,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [16,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [17,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [18,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [19,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [20,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [21,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [22,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [23,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [24,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [25,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [26,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [27,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [28,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [29,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [30,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [31,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [32,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [33,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [34,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [35,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [36,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [37,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [38,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [39,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [40,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [41,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [42,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [43,] 0.010      0 0.007      0.009       0.01       0.01      0.011
##  [44,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [45,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [46,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [47,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [48,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [49,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [50,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [51,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [52,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [53,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [54,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [55,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [56,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [57,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [58,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [59,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [60,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [61,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [62,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [63,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [64,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [65,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [66,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [67,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [68,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [69,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [70,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [71,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [72,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [73,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [74,] 0.006      0 0.007      0.009       0.01       0.01      0.011
##  [75,] 0.008      0 0.007      0.009       0.01       0.01      0.011
##  [76,] 0.008      0 0.007      0.009       0.01       0.01      0.011
##  [77,] 0.008      0 0.007      0.009       0.01       0.01      0.011
##  [78,] 0.008      0 0.007      0.009       0.01       0.01      0.011
##  [79,] 0.008      0 0.007      0.009       0.01       0.01      0.011
##  [80,] 0.008      0 0.007      0.009       0.01       0.01      0.011
##  [81,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [82,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [83,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [84,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [85,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [86,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [87,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [88,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [89,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [90,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [91,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [92,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [93,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [94,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [95,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [96,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [97,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [98,] 0.003      0 0.007      0.009       0.01       0.01      0.011
##  [99,] 0.003      0 0.007      0.009       0.01       0.01      0.011
## [100,] 0.003      0 0.007      0.009       0.01       0.01      0.011








names(modelList)
length(modelList)

##modelList <- modelList[8]
modelList <- modelList[15]
names(modelList)

##filename <- '~/github/npcr/results/df_cr_gand1.1.RData'
filename <- '~/github/npcr/results/df_cr_gand1.1Hs.RData'

runNum <- '11.1'   ## nSP = 2000, and therefore N = 4000,
runNum <- '11.1Hs3'  ## nSP = 200, and therefore N = 800,
filename <- paste0('~/github/npcr/results/df_oc_gand',runNum,'.RData')

## CR:

dfFinalNEW <- dfFinal
dfFinal
dfFinalNEW
dim(dfFinal)
dim(dfFinalNEW)

load(filename)

dfFinal
dim(dfFinal)

rbind(dfFinal, dfFinalNEW)
dim(rbind(dfFinal, dfFinalNEW))
dfFinal <- rbind(dfFinal, dfFinalNEW)

dim(dfFinal)

save(dfFinal, file=filename)


## occupancy:

dfFinalNEW_occ <- dfFinal_occ
dfFinal_occ
dfFinalNEW_occ
dim(dfFinal_occ)
dim(dfFinalNEW_occ)

load(filename)

dfFinal_occ
dim(dfFinal_occ)

rbind(dfFinal_occ, dfFinalNEW_occ)
dim(rbind(dfFinal_occ, dfFinalNEW_occ))
dfFinal_occ <- rbind(dfFinal_occ, dfFinalNEW_occ)

dim(dfFinal_occ)

save(dfFinal_occ, file=filename)






# Normal linear prediction:
# Generate single-model Wald and model-averaged MATA-Wald 95% confidence intervals
#
# Data 'y', covariates 'x1' and 'x2', all vectors of length 'n'.
# 'y' taken to have a normal distribution.
# 'x1' specifies treatment/group (factor).
# 'x2' a continuous covariate.
#
# Take the quantity of interest (theta) as the predicted response 
# (expectation of y) when x1=1 (second group/treatment), and x2=15.

n = 20                              # 'n' is assumed to be even
x1 = c(rep(0,n/2), rep(1,n/2))      # two groups: x1=0, and x1=1
x2 = rnorm(n, mean=10, sd=3)
y = rnorm(n, mean = 3*x1 + 0.1*x2)  # data generation

x1 = factor(x1)
m1 = glm(y ~ x1)                    # using 'glm' provides AIC values.
m2 = glm(y ~ x1 + x2)               # using 'lm' doesn't.
aic = c(m1$aic, m2$aic)
delta.aic = aic - min(aic)
model.weights = exp(-0.5*delta.aic) / sum(exp(-0.5*delta.aic))
residual.dfs = c(m1$df.residual, m2$df.residual)

p1 = predict(m1, se=TRUE, newdata=list(x1=factor(1), x2=15))
p2 = predict(m2, se=TRUE, newdata=list(x1=factor(1), x2=15))
theta.hats = c(p1$fit, p2$fit)
se.theta.hats = c(p1$se.fit, p2$se.fit)

se.theta.hats <- -3:4
se.theta.hats[2] = 0
se.theta.hats
se.theta.hats[se.theta.hats == 0] = 0.00000001
se.theta.hats




#  AIC model weights
model.weights

#  95% Wald confidence interval for theta (under Model 1)
theta.hats[1] + c(-1,1)*qt(0.975, residual.dfs[1])*se.theta.hats[1]

#  95% Wald confidence interval for theta (under Model 2)
theta.hats[2] + c(-1,1)*qt(0.975, residual.dfs[2])*se.theta.hats[2]

#  95% MATA-Wald confidence interval for theta (model-averaging)
mata.wald(theta.hats=theta.hats, se.theta.hats=se.theta.hats, 
          model.weights=model.weights, mata.t=TRUE, residual.dfs=residual.dfs)





cinit <- inits$cent
dim(cinit)

augmentedInd <- which(is.na(data$z))

cinitAug <- cinit[augmentedInd,]

cinitAug   ## all 0's


R <- data$trap.locs
dim(R)  ## 50  2 54

## extent of trapping grid:
range(R[,1,])   ## -120.6048  132.7274
range(R[,2,])   ## -166.1284  200.0766

## current limits for cent priors:
c(constants$Xl, constants$Xu)   ## -320.6048  332.7274
c(constants$Yl, constants$Yu)   ## -366.1284  400.0766




## calculating the mean and standard deviation
## of the Gamma prior distributions for alpha
## for the BNP npcr paper revisison

shape <- 1
rate <- 1

shape <- 1
rate <- 0.1

shape <- 2
rate <- 0.1

c(shape / rate, sqrt(shape) / rate)

n <- 100000
x <- rgamma(n, shape=shape, rate=rate)
c(mean(x), sd(x))




## working on drafting the posterior predictive branch
## sampler, and determining those branch nodes
## this is the new "bottom up" approach, using getParents:

getParentNodes <- nimble:::getParentNodes


devel (with conjugacy check): 17 seconds
devel (NO conjugacy check): 9 seconds

git checkout posterior_predictive_branches
PPbranches all data (using node names) (with conjugacy check): 17 seconds
PPbranches all data (using node names) (NO conjugacy check): 9 seconds

PPbranches 5000 PP branch (using node names) (with conjugacy check): 
PPbranches 5000 PP branch (using node names) (NO conjugacy check): 



library(nimble)
library(testthat)
##
expect_true(nimbleOptions('MCMCjointlySamplePredictiveBranches'))
##
code <- nimbleCode({
    tau ~ dgamma(0.1, 0.1)
    x[1] ~ dnorm(0, tau)
    for(t in 1:N) {
        y[t] ~ dnorm(2*x[t], 1)
    }
    for(t in 2:N) {
        x[t] ~ dnorm(x[t-1], tau)
    }
})
N <-  10
constants <- list(N = N)
inits <- list(tau = 1)
##
y <- 1:N
data <- list(y = y)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel, print = FALSE)
##conf$printSamplers()
expect_true(all(sapply(conf$getSamplers(), function(x) x$name) == c('conjugate_dgamma_dnorm', rep('conjugate_dnorm_dnorm', 10))))
##
y <- c(1, 2, 3, NA, 5, 6, 7, 8, NA, NA)
data <- list(y = y)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
##conf$printSamplers()
expect_true(all(sapply(conf$getSamplers(), function(x) x$name) ==
                c('conjugate_dgamma_dnorm',
                  rep('conjugate_dnorm_dnorm', 5),
                  'posterior_predictive',
                  rep('conjugate_dnorm_dnorm', 3),
                  'posterior_predictive_branch')))
##
y <- c(1:(N/2), rep(NA, N/2))
data <- list(y = y)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
##conf$printSamplers()
expect_true(all(sapply(conf$getSamplers(), function(x) x$name) ==
                c('conjugate_dgamma_dnorm',
                  rep('conjugate_dnorm_dnorm', 5),
                  'posterior_predictive_branch')))
##
y <- rep(NA, N)
data <- list(y = y)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
##conf$printSamplers()
expect_true(all(sapply(conf$getSamplers(), function(x) x$name) ==
                'posterior_predictive_branch'))
##
y <- c(NA, NA, NA, NA, 4, NA, NA, NA, NA, NA)
data <- list(y = y)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
##conf$printSamplers()
expect_true(all(sapply(conf$getSamplers(), function(x) x$name) ==
                c('conjugate_dgamma_dnorm',
                  rep('conjugate_dnorm_dnorm', 2),
                  'posterior_predictive',
                  'conjugate_dnorm_dnorm',
                  'posterior_predictive',
                  'conjugate_dnorm_dnorm',
                  'posterior_predictive',
                  'conjugate_dnorm_dnorm',
                  'posterior_predictive',
                  'posterior_predictive_branch')))
##
code <- nimbleCode({
    a ~ dnorm(0, 1)
    y ~ dnorm(a, 1)
    b[1] ~ dnorm(a, 1)
    b[2] ~ dnorm(a, 1)
    c ~ dnorm(b[1] + b[2], 1)
})
Rmodel <- nimbleModel(code)
conf <- configureMCMC(Rmodel)
##conf$printSamplers()
expect_true(sapply(conf$getSamplers(), function(x) x$name) == 'posterior_predictive_branch')
##
Rmodel <- nimbleModel(code, data = list(y = 1))
conf <- configureMCMC(Rmodel)
##conf$printSamplers()
expect_true(all(sapply(conf$getSamplers(), function(x) x$name) ==
                c('conjugate_dnorm_dnorm',
                  rep('posterior_predictive_branch', 2))))
##
Rmodel <- nimbleModel(code, data = list(y = 1))
conf <- configureMCMC(Rmodel, nodes = c('a', 'b[1]', 'c'))
##conf$printSamplers()
expect_true(all(sapply(conf$getSamplers(), function(x) x$name) ==
                c('conjugate_dnorm_dnorm',
                  rep('posterior_predictive_branch', 1))))
##
Rmodel <- nimbleModel(code, data = list(y = 1))
conf <- configureMCMC(Rmodel, nodes = c('a', 'b[1]', 'b[2]'))
##conf$printSamplers()
expect_true(all(sapply(conf$getSamplers(), function(x) x$name) ==
                c('conjugate_dnorm_dnorm',
                  rep('conjugate_dnorm_dnorm', 2))))






code <- nimbleCode({
    tau ~ dgamma(0.1, 0.1)
    x[1] ~ dnorm(0, tau)
    for(t in 1:N) {
        y[t] ~ dnorm(2*x[t], 1)
    }
    for(t in 2:N) {
        x[t] ~ dnorm(x[t-1], tau)
    }
    f ~ dnorm(x[50], 1)
    ##g ~ dnorm(f, 1)
    g ~ dnorm(f, L)
    i ~ dnorm(y[60], 1)
    j ~ dnorm(i, 1)
    k ~ dnorm(y[70], 1)
    L ~ dnorm(x[60], 1)
})
N <-  100
y <- c(1:(N-6), NA, NA, 7, NA, NA, NA)
constants <- list(N = N)
inits <- list(tau = 1)
data <- list(y = y)
Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()




stochNonData <- Rmodel$getNodeNames(stochOnly = TRUE, includeData = FALSE)
stochNonData
endNodeInd <- Rmodel$isEndNode(stochNonData)
stochNonDataEnd <- stochNonData[endNodeInd]
stochNonDataEnd

candidateNodes <- nimble:::getParentNodes(stochNonDataEnd, Rmodel, stochOnly = TRUE)
candidateNodes
dataInd <- Rmodel$isData(candidateNodes)
candidateNodes <- candidateNodes[!dataInd]
candidateNodes <- sort(candidateNodes)
candidateNodes

candidateNodes
candidateNodesNew
nCandNodes
i <- i + 1
i
candidateNodes[i]
newParents
newParentsKeepInd

while(TRUE) {
    candidateNodesNew <- candidateNodes
    for(i in seq_along(candidateNodes)) {
        newParents <- nimble:::getParentNodes(candidateNodes[i], Rmodel, stochOnly = TRUE)
        newParentsKeepInd <- sapply(newParents, function(np) length(Rmodel$getDependencies(np, stochOnly = TRUE, dataOnly = TRUE, downstream = TRUE)) == 0)
        newParents <- newParents[newParentsKeepInd]
        if(length(newParents) > 0) {
            candidateNodesNew <- setdiff(candidateNodesNew, candidateNodes[i])
            candidateNodesNew <- union(candidateNodesNew, newParents)
        }
    }
    candidateNodesNew <- sort(candidateNodesNew)
    if(identical(candidateNodes, candidateNodesNew)) break
    candidateNodes <- candidateNodesNew
}

candidateNodes



library(nimble)

nimbleOptions(MCMCjointlySamplePredictiveBranches = FALSE)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a, 1)
    c ~ dnorm(b, 1)
    d ~ dnorm(c, 1)
})
constants <- list()
data <- list(b = 0)
inits <- list()

Rmodel <- nimbleModel(code, constants, data, inits)

##n <- Rmodel$getNodeNames()
##n <- Rmodel$getNodeNames(returnType = 'ids')
## 
##Rmodel$isData(n)
##Rmodel$isEndNode(n)
## 
##arg <- 1
##arg <- 2
##arg <- 3
##arg <- 4
##Rmodel$getDependencies(arg)
##Rmodel$getDependencies(arg, returnType = 'ids')





## doing kriging example, from Wilke textbook
## simple Kriging spatio-temporal smoothing

s <- matrix(c(2, 0.2,
              2, 1,
              6, 0.2,
              6, 0.9),
            byrow = TRUE, nrow = 4)

s0 <- c(3, 0.5)

z <- c(15, 22, 17, 23)

mu <- 20

c00 <- 2

a <- 2
b <- 0.2
sigma2 <- c00
d <- 1
cfun <- function(magh, tau) sigma2 * exp(-b^2 * magh^2 / (a^2*tau^2+1)) / (a^2*tau^2+1)^(d/2)

dmatrix <- as.matrix(dist(s[,1]))
tmatrix <- as.matrix(dist(s[,2]))

Cz <- cfun(dmatrix, tmatrix)

d0matrix <- s[,1,drop=FALSE] - s0[1]
t0matrix <- s[,2,drop=FALSE] - s0[2]

c0 <- cfun(d0matrix, t0matrix)

w <- solve(Cz) %*% c0

(s0Pred <- mu + sum(w * (z - mu)))
(s0Var <- c00 - sum(w * c0))














## visual check:
## histograms of original dataset,
## and 19 simulated data sets

par(mfrow = c(5,4), mai = rep(.2,4))
xlim <- c(-3000, 20000)
breaks <- seq(-10000, 30000, by = 1000)

hist(data$y, main='', xlim=xlim, breaks=breaks, xaxt='n', yaxt='n', border='blue')

for(i in 31:49) hist(ystar[i,], main='', xlim=xlim, breaks=breaks, xaxt='n', yaxt='n')

## numerical check:
## define test statistic T(y)
## histogram of T(y) from simulated datasets,
## with red line showing T(y) from original data set:

par(mfrow = c(1,1), mar = c(2,5,5,3))
hist(apply(ystar, 1, mean), breaks = 20)
abline(v = mean(data$y), col = 'red')

T <- function(x) quantile(x, prob = 0.1)

par(mfrow = c(1,1), mar = c(2,5,5,3))

hist(apply(ystar, 1, T), breaks = 20)

T(data$y)
abline(v = T(data$y), col = 'red')





## Example showing *lack of model fit*
## Generate some log-normal data (skewed),
## then we'll fit a *normal* model to it:

N <- 200
y <- exp(rnorm(N, 2, .5))

par(mfrow = c(1,1))
hist(y, breaks = 40)

code <- nimbleCode({
    sigma ~ dunif(0, 10000)
    mu ~ dnorm(0, sd = 10000)
    for(i in 1:N) {
        y[i] ~ dnorm(mu, sd = sigma)
    }
})

constants <- list(N = N)
data <- list(y = y)
inits <- list(mu = 0, sigma = 1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Rmcmc <- buildMCMC(Rmodel)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)



## NOTE: our usual checks won't help us detect
## any "lack of model fit" ...!









## finding bug in state space model,
## introduced in nimble version 0.10.0
## due to a bug in compiled getParam and getBound

remove.packages("nimble")
library(devtools)
install_github("nimble-dev/nimble", ref = "v0.9.1", subdir = "packages/nimble")

---> Now, restart R

n <- 100
a <- 6
b <- 0.8
sigmaPN <- 2
sigmaOE <- 4
set.seed(0)
x <- numeric(n)
y <- numeric(n)
x[1] <- 1
y[1] <- rnorm(1, x[1], sigmaOE)
for(i in 2:n) {
    x[i] <- rnorm(1, a+b*x[i-1], sigmaPN)
    y[i] <- rnorm(1, x[i], sigmaOE)
}
head(y,5)  ## 6.051817 11.466730 15.121451  9.976140 16.339817

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, sd = 10000)
    b ~ dnorm(0, sd = 10000)
    sigmaPN ~ dunif(0, 10000)
    sigmaOE ~ dunif(0, 10000)
    x[1] ~ dnorm(0, sd = 10000)
    y[1] ~ dnorm(x[1], sd = sigmaOE)
    for(t in 2:N) {
        x[t] ~ dnorm(a + b*x[t-1], sd = sigmaPN)
        y[t] ~ dnorm(x[t], sd = sigmaOE)
    }
})
constants <- list(N = length(y))
data <- list(y = y)
inits <- list(a=0, b=0, sigmaOE=1, sigmaPN=1, x=y)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()    ## -41884.41


Rmodel$getConditionallyIndependentSets(type = 'fromTop')
Rmodel$getConditionallyIndependentSets(type = 'fromBottom')
Rmodel$getConditionallyIndependentSets(type = 'both')



conf <- configureMCMC(Rmodel)

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
set.seed(0)
samples <- runMCMC(Cmcmc, 50000, nburnin=2000)
round(samplesSummary(samples[,c('a', 'b', 'sigmaOE', 'sigmaPN')]), 2)



## forward and backward probability
## calculations for HMM hidden markov models
## working on this for cyril and project wildmap


nLatent <- 4
nObservable <- 3
asCol <- function(x) matrix(x, ncol = 1)

probObs <- array(0, c(nLatent, nObservable))
probObs[1,3] <- 1
probObs[2,1] <- 0.9
probObs[2,3] <- 0.1
probObs[3,2] <- 1
probObs[4,3] <- 1
if(!all(apply(probObs, 1, sum) == 1)) stop()
print(probObs)

probTrans <- array(0, c(nLatent, nLatent))
 probTrans[1,1] <- 0.25
probTrans[2,1] <- 0.75
probTrans[2,2] <- 0.5
probTrans[3,2] <- 0.1
probTrans[4,2] <- 0.4
probTrans[4,3] <- 1
probTrans[4,4] <- 1
if(!all(apply(probTrans, 2, sum) == 1)) stop()
print(probTrans)

T <- 4
y <- c(3, 1, 3, 2)
if(!(length(y) == T)) stop()
if(any(y < 1) || any(y > nObservable)) stop()
pi <- c(0.5, 0.5, 0, 0)
if(!(sum(pi) == 1)) stop()
L <- numeric(T)
Fmatrix <- array(NA, c(T, nLatent))  ## rows of Fmatrix correpond to time t

forward <- function(t) {
    if(missing(t)) stop()
    Zpi <- probObs[,y[t]] * pi # Vector of P(state) * P(observation class y[t] | state)
    sumZpi <- sum(Zpi)    # Total P(observed as class y[t])
    L[t] <<- sumZpi
    Fmatrix[t,] <<- Zpi / sum(Zpi)
    pi <<- (probTrans[,] %*% asCol(Zpi) / sumZpi)[ ,1] # State probabilities at t+1
}

forward(t=1)
forward(t=2)
forward(t=3)
forward(t=4)
Fmatrix
L

Bmatrix <- array(NA, c(T, nLatent))  ## rows of Bmatrix correspond to time t
##b <- array(1, c(nLatent, 1))
pi <- rep(1, nLatent)

backward <- function(t) {
    if(missing(t)) stop()
    Bmatrix[t,] <<- pi
    Zpi <- probObs[,y[t]] * pi
    backProb <- (t(probTrans) %*% asCol(Zpi))[,1]
    pi <<- backProb / sum(backProb)
}

backward(t=4)
backward(t=3)
backward(t=2)
backward(t=1)
Bmatrix


finalProbs <- Fmatrix * Bmatrix
finalProbs <- finalProbs / matrix(apply(finalProbs, 1, sum), nrow=T, ncol=nLatent)
finalProbs




library(nimble)

df <- read.csv('~/Downloads/UsedCars.csv')

code <- nimbleCode({
    b0    ~ dnorm(0, sd = 100000)
    bage  ~ dnorm(0, sd = 10000)
    bhp   ~ dnorm(0, sd = 10000)
    btype ~ dnorm(0, sd = 10000)
    sigma ~ dunif(0, 10000)
    for(i in 1:N) {
        mu[i] <- b0 + bage*age[i] + bhp*hp[i] + btype*type[i]
        y[i] ~ dnorm(mu[i], sd = sigma)
    }
})

constants <- list(N = dim(df)[1], age=df$Age, hp=df$HP, type=df$Type)

data <- list(y = df$Price)

inits <- list(b0=0, bage=0, bhp=0, btype=0, sigma=1)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()  ## -851490867

conf <- configureMCMC(Rmodel)
conf <- configureMCMC(Rmodel, autoBlock = TRUE)
Rmcmc <- buildMCMC(Rmodel)

## one compilation call:
compiledList <- compileNimble(list(Rmodel, Rmcmc))
Cmodel <- compiledList[[1]]
Cmcmc <- compiledList[[2]]

Cmodel$setInits(inits)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

library(basicMCMCplots)

samplesPlot(samples, 'sigma')

samplesSummary(samples)
samplesSummary(samples[5001:10000,])


names(df)
m <- lm(Price ~ Age + HP + Type, data = df)
summary(m)
samplesSummary(samples[5001:10000,])













dim(samples)

## alternative: nimbleMCMC()
## let's quickly look at NIMBLE User Manual...

samples <- nimbleMCMC(code, constants, data, inits)

dim(samples)
head(samples)

library(basicMCMCplots)

samplesPlot(samples)
samplesPlot(samples, densityplot = FALSE)
samplesPlot(samples, traceplot = FALSE)
samplesPlot(samples, traceplot = FALSE, densityplot = FALSE)

samplesSummary(samples)

chainsPlot(samplesList)
chainsPlot(samplesList, densityplot = FALSE)
chainsPlot(samplesList, traceplot = FALSE)
chainsPlot(samplesList, traceplot = FALSE, densityplot = FALSE)



## coda - a nice library of numerical MCMC utility functions

## library: coda

library(coda)

samples <- runMCMC(Cmcmc, 10000, samplesAsCodaMCMC = TRUE)


class(samples)


## from coda: acfplot()
## requires coda mcmc object!
## lag.max, thin

as.mcmc

samplesCoda <- as.mcmc(samples)
class(samplesCoda)

acfplot(samples)

acfplot(samplesCoda)

acfplot(samplesCoda, lag.max = 500)
acfplot(samplesCoda, lag.max = 500, thin = 10)

effectiveSize


dim(samples)
effectiveSize(samples)   ## can take an array

samples <- runMCMC(Cmcmc, 100000)

effectiveSize(samples)



## effective sample size (ESS)
## from coda: effectiveSize()




























library(nimble)

set.seed(0)
myData <- rnorm(8, 5, 10)

mean(myData)
sd(myData)

## code, constants, data, initial values

code <- nimbleCode({
    ## ~ stochastic declarations
    ## <- deterministic declarations
    ## prior for mu:
    mu ~ dnorm(0, sd = 100)
    sigma ~ dunif(0, 1000)
    ###  this line is meaningless: x <- mu + sigma
    for(i in 1:N) {
        y[i] ~ dnorm(mu, sd = sigma)
    }
})

## constants are for-loop maximum indices,
## and covariates
constants <- list(N = 8)

## data are the observed data values
data <- list(y = myData)

## initial values are initial values for any
## stochastic non-data nodes
inits <- list(mu = 0, sigma = 1)

## create a NIMBLE "model object"
Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$mu
Rmodel$sigma
Rmodel$y
Rmodel$getNodeNames()
Rmodel$plotGraph()

Rmodel$mu
Rmodel$simulate('mu')
Rmodel$mu
Rmodel$simulate('mu')
Rmodel$mu

Rmodel$calculate('mu')
dnorm(-58.4382, 0, 100)
dnorm(-58.4382, 0, 100, log = TRUE)
log(dnorm(-58.4382, 0, 100))

Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()

Rmcmc <- buildMCMC(conf)

set.seed(0)
Rmodel$mu <- 0
Rmodel$calculate()

samples <- runMCMC(Rmcmc, 100)
##             mu    sigma
##  [1,] 6.934380 1.000000
##  [2,] 6.192423 2.595281
##  [3,] 6.789729 2.595281
##  [4,] 6.216980 2.595281
##  [5,] 6.207192 4.107062
##  [6,] 7.052592 4.107062
##  [7,] 6.052140 4.107062
##  [8,] 6.463067 5.050898
##  [9,] 7.952145 5.644800
## [10,] 8.319032 6.426936
samples

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmodel$mu <- 0
Cmodel$sigma <- 1
Cmodel$calculate()

samples <- runMCMC(Cmcmc, 100000)

dim(samples)

head(samples)

samplesSummary(samples)

muSamp <- samples[, 'mu']
mean(muSamp)
median(muSamp)
quantile(muSamp, c(0.025, 0.975))






1























































library(nimble)
library(basicMCMCplots)

code <- nimbleCode({
    x ~ dcat(prob[1:6])
    y ~ dnorm(x, sd = 2)
})
constants <- list()
data <- list(y = 2)
inits <- list(prob = rep(1/6, 6), x = 0)
Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()

conf <- configureMCMC(Rmodel)

conf$removeSamplers('foo')
conf$printSamplers()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

set.seed(0)
niter <- 10
samples <- runMCMC(Cmcmc, niter)

samplesSummary(samples)
table(samples[, 'x']) / niter
##      1      2      3      4      5      6 
## 0.2286 0.2660 0.2225 0.1577 0.0884 0.0368 

Rmodel$x
debug(Rmcmc$run)
Rmcmc$run(4)
debug(my_initializeModel$run)
debug(initFunctionList[[i]]$run)
debug(initialize_stoch_non_data_node)









set.seed(0)
n <- 4
y <- rpois(n, lambda_true)
y

## priors:
## prior 1: Jeffreys': Gamma(1/2, 0)   (improper)
## prior 2: Flat:      1 ~ Gamma(1, 0) (improper)
## prior 3: common choice:  Gamma(0.001, 0.001)

par(mfrow = c(2,1), mar = c(4,2,2,1))
xmax <- lambda_true * 2.5
ymax <- 2

## graph priors:
plot(-1, -1, xlim = c(0,xmax), ylim = c(0,ymax), xlab='', ylab='', main = 'Priors & Likelihood')
xs <- seq(0, xmax, length = 100000)

## prior 1: Jeffreys (blue)
lines(xs, dgamma(xs, 1/2, 0), lwd=2, col = 'blue')

## prior 2: Flat (green)
lines(xs, xs/xs, lwd=2, col = 'green')

## prior 3: common Gamma(0.001, 0.001) (red)
lines(xs, dgamma(xs, 0.001, 0.001), lwd=2, col = 'red')
legend(x = 'topright', legend = c('Jeffreys', 'flat', 'Gamma(0.001,0.001)'), col = c('blue', 'green', 'red'), lwd=2)

## Likelihood
likelihood <- sapply(xs, function(x) prod(dpois(y,x)))
likelihood_scaled <- likelihood/max(likelihood) * ymax/2
lines(xs, likelihood_scaled, lwd = 2)

## Posteriors:
plot(-1, -1, xlim = c(0,xmax), ylim = c(0,ymax), xlab='', ylab='', main = 'Posteriors')

## prior 1: Jeffreys (blue)
ys <- dgamma(xs, 1/2+sum(y), 0+n)
thismax <- max(ys) * 1.3
lines(xs, ys/thismax*ymax, lwd=2, col = 'blue')

## prior 2: Flat (green)
ys <- dgamma(xs, 1+sum(y), 0+n)
lines(xs, ys/thismax*ymax, lwd=2, col = 'green')

## prior 3: common Gamma(0.001, 0.001) (red)
ys <- dgamma(xs, 0.001+sum(y), 0.001+n)
lines(xs, ys/thismax*ymax, lwd=2, col = 'red')
legend(x = 'topright', legend = c('Jeffreys', 'flat', 'Gamma(0.001,0.001)'), col = c('blue', 'green', 'red'), lwd=2)

## true value of lambda:
abline(v = lambda_true, lwd = 2)








## flat prior on binomial p:
plot(function(x) dbeta(x,1,1), xlim = c(0,1), ylim = c(0,3))

n <- 100000
p <- rbeta(n, 1, 1)

head(p, 100)

hist(p, prob = TRUE, add = TRUE, breaks = 50)

## now transform prior, say p2 = p^2
p2 <- p^2

hist(p2, prob = TRUE, add = TRUE, breaks = 50, col = 'red')

## prior on normal variance?
var <- runif(n, 0, 10000)

hist(var, prob = TRUE)

## what's the implied prior on standard deviation?
sd <- sqrt(var)

hist(sd, prob = TRUE, col = 'red')













## example for testing cross validation in nimble

library(nimble)
library(testthat)






## demo for STAT465/STAT365 for lecture 5 (or something like that...)
## prior distributions

a1 <- 1;   b1 <- 1     ## prior1 - uniform
a2 <- 0.5; b2 <- 0.5   ## prior2
a3 <- 5;   b3 <- 5     ## prior3
a4 <- 10;  b4 <- 20    ## prior4 
par(mfrow = c(2,1), mar = c(4,2,2,1)) 

## graph priors
ymax <- 5
plot(-1, -1, xlim = c(0,1), ylim = c(0,5), xlab='', ylab='', main = 'Priors')
plot(function(x) dbeta(x, a1, b1), add = TRUE, lwd=2, col = 'black')
plot(function(x) dbeta(x, a2, b2), add = TRUE, lwd=2, col = 'blue')
plot(function(x) dbeta(x, a3, b3), add = TRUE, lwd=2, col = 'red')
plot(function(x) dbeta(x, a4, b4), add = TRUE, lwd=2, col = 'green')
xs <- seq(.2,.8,.2); ys <- .25; points(xs, rep(ys,4)*ymax, pch=19, col = 'purple')

## graph posteriors

ymax <- 10
plot(-1, -1, xlim = c(0,1), ylim = c(0,ymax), xlab='', ylab='', main = 'Posteriors')
plot(function(x) dbeta(x, a1+y, b1+n-y), add = TRUE, lwd=2, col = 'black')
plot(function(x) dbeta(x, a2+y, b2+n-y), add = TRUE, lwd=2, col = 'blue')
plot(function(x) dbeta(x, a3+y, b3+n-y), add = TRUE, lwd=2, col = 'red')
plot(function(x) dbeta(x, a4+y, b4+n-y), add = TRUE, lwd=2, col = 'green')
ysp <- dbinom(y, n, xs); ysp <- ysp/sum(ysp); points(xs, ysp*ymax*0.95, pch=19, col = 'purple')
ysp

## posterior means:

round(
rbind((a1 + y) / (a1 + b1 + n),
      (a2 + y) / (a2 + b2 + n),
      (a3 + y) / (a3 + b3 + n),
      (a4 + y) / (a4 + b4 + n)), 2)

## posterior CIs (credible intervals):

round(
rbind(qbeta(c(0.025, 0.975), a1+y, b1+n-y),
      qbeta(c(0.025, 0.975), a2+y, b2+n-y),
      qbeta(c(0.025, 0.975), a3+y, b3+n-y),
      qbeta(c(0.025, 0.975), a4+y, b4+n-y)), 2)

## frequentist MLE for p

phat <- y/n
round(phat, 2)

## frequentist 95% CI for p
se <- sqrt(phat*(1-phat)/n)
round(se, 2)
round(phat + c(-1,1) * 1.96 * se, 2)


## true value of theta:
theta_true 












## helping some user

library(nimble)

# make some data
n <- 100 # number of data points
t <- seq(0,4*pi,,100)
a <- 3
b <- 2
amp <- 2
vi <- a*sin(b*t)+rnorm(n)*amp 
TAIR <- a*sin(b*t)+rnorm(n)*amp 

input.data = list(TAIR = TAIR)
input.data$lo <- rep(0,7)
input.data$up <- rep(1,7)
input.data$mu <- input.data$up - input.data$lo
input.data$steps <- n

inits.data = list(parset=runif(2,0,1))
inits.data$sdb <- 0.01
inits.data$sdp <- 0.01
inits.data$sdo <- 0.01
inits.data$m <- 1.0
inits.data$zstart <- 0.5

resp.data <- list(vi = vi)

ttrTimeNimble <- nimbleCode({
    for(i in 1: 2) {
        parset[i] ~ T( dnorm( mu[i], sdb), lo[i], up[i] )
    }
    sdp ~ dunif(lo[3], up[3])       # process error
    sdo ~ dunif(lo[4], up[4])       # observation error
    sdb ~ dunif(lo[6], up[6])       
    zstart ~ T( dnorm(mu[5],sd=sdp),lo[5], up[5])   
    m ~ T(dnorm(mu[7],sd=0.5),lo[7],)      
    z[1] <- zstart
    vi[1] ~ dnorm(z[1]*m,sd=sdo)   
    for (i in 2:steps) {
        zz[i] <- TAIR[i]*parset[1] + parset[2]
        z[i] ~  dnorm(zz[i],  sd = sdp)
        vi[i] ~ dnorm(z[i]*m, sd = sdo)
    }
})

ttrTimeModel <- nimbleModel(code = ttrTimeNimble, constants = input.data, 
                            data = resp.data, inits = inits.data)

##mcmc.out <- nimbleMCMC(code = ttrTimeNimble, constants = input.data,
##                       data = resp.data, inits = inits.data,
##                       nchains = 3, niter = 500,
##                       summary = TRUE, WAIC = FALSE,
##                       monitors = c('parset','m','sdo', 'sdp','sdb','zstart'))

#--now specify a particle filter                       
CttrTimeModel <- compileNimble(ttrTimeModel)

ttrTimeConf <- configureMCMC(ttrTimeModel, nodes = NULL,
                             monitors = c('parset','m','sdo', 'sdp','sdb','zstart'))                            

dd<-rep(1,7)
ttrTimeConf$addSampler(target = c('parset','m','sdo', 'sdp','sdb','zstart'),
                       type = 'RW_PF_block',
                       control = list(
                           propCov = diag(dd),
                           adaptScaleOnly = FALSE,
                           latents = "z",
                           pfOptimizeNparticles = TRUE,
                           pf = NULL))

##ttrTimeMCMC <- buildMCMC(ttrTimeConf)
## no error, but this is not using ttrTimeConf

ttrTimeMCMC <- buildMCMC(ttrTimeConf,resetFunctions=TRUE) 
#Error: $ operator is invalid for atomic vectors

ttrTimeMCMC <- buildMCMC(ttrTimeConf)
#Error: $ operator is invalid for atomic vectors

## something


library(nimble)

## I've been tinkering with my custom distribution 'db' and so I first call:

deregisterDistributions('db')

## Since my distribution is a function of data, I just stick those data into the density function as 'Si':

deregisterDistributions('db')
#BEGIN density function for beta, b
db<- nimbleFunction(
    run = function(x = double(0), log = integer(0, default = 0)) {
        Si =  c(1978,  2477,  2119, 13707, 13129,  3360,  1995,  8774,  2429,  4906, 16811, 15000, 19250,  1686,  4706, 20550, 1361,  7102)
        returnType(double(0))
        logProb <- log(sd(1/(x+Si))) 
        if(log) return(logProb)
        else return(exp(logProb)) 
    })

## I use rejection sampling for the random sampler.  This sure looks like 'b' is strictly positive:

rb<- nimbleFunction(
    run = function(n = integer(0)){
        Si =  c(1978,  2477,  2119, 13707, 13129,  3360,  1995,  8774,  2429,  4906, 16811, 15000, 19250,  1686,  4706, 20550, 1361,  7102)
        returnType(double(0))
        if(n != 1) print("rb only allows n = 1; using n = 1.")
        x<-runif(1,0,10000)
        b<-runif(1,0,0.0003)
        while (b > sd(1/(x+Si))){
            x<-runif(1,0,10000)
            b<-runif(1,0,0.0003)
        }
        return(b)
    })


BHCode <- nimbleCode({
    for (t in 1:Years) {
        logR[t] ~ dnorm(loga+log(S[t])-log(b+S[t]), sd = sigma) 
    }
    a<-exp(loga)
    loga ~ dunif(0, 20)
    b ~ db()
    sigma<-exp(logsigma)
    logsigma~dunif(-2,2)
})

summary(replicate(10^4, rb(1)))

x <- seq(-1, 1, length = 20)
n <- length(x)
lp <- numeric(n)
for(i in 1:n) lp[i] <- db(x[i])
cbind(x, lp)




BHConstants <- list(Years=length(S))
BHData<-list(logR=log(R),S=S)
BHInits <- list(loga = c(8), b = c(700) ,logsigma = c(0.3))    
mcmc.out <- nimbleMCMC(code = BHCode, constants = BHConstants,
                       data = BHData, inits = BHInits,
                       nchains = 3, niter = 1000000,
                       summary = TRUE, WAIC = FALSE,
                       monitors = c('a','b','sigma'))

##...I somehow get a posterior for 'b' with mean = -115. How is my posterior mean negative when my sampler doesn't (shouldn't) propose negative values?


R = c(2152, 2584, 2381, 1274,  812,  489, 1075, 4042, 4618, 2201, 1042,  896, 6371, 1002, 1016,  593,  450,  818)





## dDispersal_exp example
system.time({
## define model code
code <- nimbleCode({
    lambda ~ dgamma(0.001, 0.001)
    for(i in 1:N) {
        AC[i, 1, 1] ~ dunif(0, 100)
        AC[i, 2, 1] ~ dunif(0, 100)
        for(t in 2:T) {
            AC[i, 1:2, t+1] ~ dDispersal_exp(s = AC[i, 1:2, t], rate = lambda)
        }
    }
})

constants <- list(N = 10, T = 6)

## create NIMBLE model object
Rmodel <- nimbleModel(code, constants)

## use model object for MCMC, etc.
})


## dHabitatMask example

system.time({
## define model code
code <- nimbleCode({
    for(i in 1:N) {
        s[i, 1] ~ dunif(0, 100)
        s[i, 2] ~ dunif(0, 100)
        OK[i] ~ dHabitatMask( s = s[i,1:2],
                              xmax = 100,
                              xmin = 0,
                              ymax = 100,
                              ymin = 0,
                              habitatMask = habitatMask[1:100,1:100])
    }
})
 
N <- 20
 
habitatMask <- matrix(rbinom(10000,1,0.75), nrow = 100)
 
constants <- list(N = N, habitatMask = habitatMask)
 
data <- list(OK = rep(1, N))
 
inits <- list(s = array(runif(2*N, 0, 100), c(N,2)))
 
## create NIMBLE model object
Rmodel <- nimbleModel(code, constants, data, inits)
 
## use model object for MCMC, etc.
})


## dbinom_sparseLocalSCR example

system.time({
## define model code
code <- nimbleCode({
    psi ~ dunif(0,1)
    p0 ~ dunif(0,1)
    sigma ~ dunif(0,100)
    N <- sum(z[1:M])
    for(i in 1:M) {
        s[i, 1] ~ dunif(0, 100)
        s[i, 2] ~ dunif(0, 100)
        z[i] ~ dbern(psi)
        y[i,1:maxDetNum] ~ dbinom_sparseLocalSCR(detNums,
                                                 detIndices,
                                                 size,
                                                 p0,
                                                 sigma,
                                                 s[i,1:2],
                                                 trapCoords,
                                                 localTrapsIndices,
                                                 localTrapsNum,
                                                 resizeFactor,
                                                 habitatGrid,
                                                 z[i])
    }
})
 
## create NIMBLE model object
## Rmodel <- nimbleModel(code, ...)
 
## use model object for MCMC, etc.
})



## dbinom_vector example

system.time({
## define vectorized model code
code <- nimbleCode({
    p ~ dunif(0,1)
    p_vector[1:J] <- p
    y[1:J] ~ dbinom_vector(size = trials[1:J],
                           prob = p_vector[1:J])
})
 
## simulate binomial data
J <- 1000
trials <- sample(x = 10, size = J, replace = TRUE)
y <- rbinom_vector(J, size = trials, prob = 0.21)
 
constants <- list(J = J, trials = trials)
 
data <- list(y = y)
 
inits <- list(p = 0.5)
 
## create NIMBLE model object
Rmodel <- nimbleModel(code, constants, data, inits)
 
## use model object for MCMC, etc.
})


## getLocalTraps example

system.time({
colNum <- sample(20:100,1)
rowNum <- sample(20:100,1)
trapCoords <- expand.grid(list(x = seq(0.5, colNum, 1),
                               y = seq(0.5, rowNum, 1)))

habitatMask <- matrix(rbinom(colNum*rowNum, 1, 0.8), ncol = colNum, nrow = rowNum)

localTraps.list <- getLocalTraps(habitatMask, trapCoords, resizeFactor = 1, dmax = 7)
})




## getSparseY example

system.time({
y.full <- matrix(rbinom(5000, 5, 0.02), ncol = 100)

y <- getSparseY(y.full)
})


## localTrapCalculations example


system.time({

## generate random trap locations
nTraps <- 200
traps_xmin <- 0
traps_ymin <- 0
traps_xmax <- 100
traps_ymax <- 200
set.seed(0)
traps_xCoords <- round(runif(nTraps, traps_xmin, traps_xmax))
traps_yCoords <- round(runif(nTraps, traps_ymin, traps_ymax))
trap_coords <- cbind(traps_xCoords, traps_yCoords)
 
## buffer distance surrounding sides of rectangular discretization grid
## which overlays trap locations
buffer <- 10
 
## resolution of rectangular discretization grid
resolution <- 10
 
## creates grid and makeID function,
## for grid overlaying trap locations,
## and to lookup nearest grid cell to any AC
makeGridReturn <- makeGrid(xmin = traps_xmin, xmax = traps_xmax,
                           ymin = traps_ymin, ymax = traps_ymax,
                           buffer = buffer,
                           resolution = resolution)
 
grid <- makeGridReturn$grid
makeID <- makeGridReturn$makeID
 
## maximum radis within an individual AC to perform trap calculations,
dmax <- 30
 
## n = localTraps[i,1] gives the number of local traps
## localTraps[i, 2:(n+1)] gives the indices of the local traps
localTraps <- findLocalTraps(grid, trap_coords, dmax)
 
plotTraps <- function(i, grid, trap_coords, localTraps) {
    plot(grid[,1], grid[,2], pch = '.', cex=2)
    points(trap_coords[,1], trap_coords[,2], pch=20, col='forestgreen', cex=1)
    if(!missing(i)) {
        i <- max(i %% dim(grid)[1], 1)
        n <- localTraps[i,1]
        trapInd <- numeric(0)
        if(n > 0)  trapInd <- localTraps[i,2:(n+1)]
        theseTraps <- trap_coords[trapInd,, drop = FALSE]
        points(theseTraps[,1], theseTraps[,2], pch = 20, col = 'red', cex=1.5)
        points(grid[i,1], grid[i,2], pch = 'x', col = 'blue', cex=3)
    }
}
 
## visualise some local traps
plotTraps(10,  grid, trap_coords, localTraps)
plotTraps(200, grid, trap_coords, localTraps)
plotTraps(380, grid, trap_coords, localTraps)
 
## example model code
## using local trap calculations
code <- nimbleCode({
    sigma ~ dunif(0, 100)
    p0 ~ dunif(0, 1)
    for(i in 1:N) {
        S[i,1] ~ dunif(0, xmax)
        S[i,2] ~ dunif(0, ymax)
        Sdiscrete[i,1] <- round(S[i,1]/res) * res
        Sdiscrete[i,2] <- round(S[i,2]/res) * res
        id[i] <- makeID( Sdiscrete[i,1:2] )
        nLocalTraps[i] <- getNumLocalTraps(id[i], localTraps[1:LTD1,1], LTD1)
        localTrapIndices[i,1:maxTraps] <-
            getLocalTrapIndices(maxTraps, localTraps[1:LTD1,1:LTD2], nLocalTraps[i], id[i])
        d[i, 1:maxTraps] <- calcLocalTrapDists(
            maxTraps, nLocalTraps[i], localTrapIndices[i,1:maxTraps],
            S[i,1:2], trap_coords[1:nTraps,1:2])
        g[i, 1:nTraps] <- calcLocalTrapExposure(
            nTraps, nLocalTraps[i], d[i,1:maxTraps], localTrapIndices[i,1:maxTraps], sigma, p0)
        y[i, 1:nTraps] ~ dbinom_vector(prob = g[i,1:nTraps], size = trials[1:nTraps])
    }
})
 
## generate random detection data; completely random
N <- 100
set.seed(0)
y <- array(rbinom(N*nTraps, size=1, prob=0.8), c(N, nTraps))
 
## generate AC location initial values
Sinit <- cbind(runif(N, traps_xmin, traps_xmax),
               runif(N, traps_ymin, traps_ymax))
 
constants <- list(N = N,
                  nTraps = nTraps,
                  trap_coords = trap_coords,
                  xmax = traps_xmax,
                  ymax = traps_ymax,
                  res = resolution,
                  localTraps = localTraps,
                  LTD1 = dim(localTraps)[1],
                  LTD2 = dim(localTraps)[2],
                  maxTraps = dim(localTraps)[2] - 1)
 
data <- list(y = y, trials = rep(1,nTraps))
 
inits <- list(sigma = 1,
              p0 = 0.5,
              S = Sinit)
 
## create NIMBLE model object
Rmodel <- nimbleModel(code, constants, data, inits,
                      calculate = FALSE, check = FALSE)
 
## use model object for MCMC, etc.

})










## trying to help Ben Augustine on nimble users list,
## writing his custom sampler

myActivityCenterSampler_helperBASE <- nimbleFunctionVirtual(
    name = 'myActivityCenterSampler_helperBASE',
    methods = list(
        calc = function() { returnType(double()) },
        setValue = function(newValue = double(1)) { },
        copyToMVsaved = function() { },
        copyToModel = function() { },
        getLP = function() { returnType(double()) }
    )
)


myActivityCenterSampler_helper <- nimbleFunction(
    name = 'myActivityCenterSampler_helper',
    contains = myActivityCenterSampler_helperBASE,
    setup = function(model, node, mvSaved) {
        calcNodes <- model$getDependencies(node)
    },
    run = function() { print('should never run') },
    methods = list(
        calc = function() {
            returnType(double(0))
            lp <- model$calculate(calcNodes)
            return(lp)
        },
        setValue = function(newValue = double(1)) {
            values(model, node) <<- newValue
        },
        copyToMVsaved = function() {
            copy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
        },
        copyToModel = function() {
            copy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE)
        },
        getLP = function() {
            returnType(double())
            lp <- model$getLogProb(calcNodes)
            return(lp)
        }
    )
)


myActivityCenterSampler <- nimbleFunction(
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        # Defined stuff
        scale<-1  
        xlim<-control$xlim
        ylim<-control$ylim
        M<-control$M

        nfList <- nimbleFunctionList(myActivityCenterSampler_helperBASE)
        for(i in 1:M) {
            node <- paste("s[",i,",1:2]")
            nfList[[i]] <- myActivityCenterSampler_helper(model, node, mvSaved)
        }

    },
    
    run = function() {
        # grab current value of z
        z <- model$z
        for(i in 1:M){
            browser()
            ##calcNodes <- model$getDependencies(paste("s[",i,",1:2]"))
            ## If z==0 propose activity center and accept with probability 1.0
            if(z[i]==0){
                
                # store proposal into model
                ##model$s[i, 1:2] <<- c(runif(1, xlim[1], xlim[2]), runif(1, ylim[1], ylim[2]))
                nfList[[i]]$setValue(c(runif(1, xlim[1], xlim[2]), runif(1, ylim[1], ylim[2])))
                
                # update dependencies
                ##model$calculate(calcNodes)
                nfList[[i]]$calc()
                
                # Accept candidate trajectory with probability 1.0 and keep the model and mvSaved objects consistent
                ##copy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
                nfList[[i]]$copyToMVsaved()
                
            }else{#z.curr[i]==1
                
                # initial model logProb
                ##model_lp_initial <- model$getLogProb(calcNodes)
                model_lp_initial <- nfList[[i]]$getLP

                #truncated normal proposal
                sx <- qnorm(runif(1, pnorm(xlim[1], model$s[i, 1], sd=scale), pnorm(xlim[2], model$s[i, 1], sd=scale)), model$s[i, 1], sd=scale)
                sy <- qnorm(runif(1, pnorm(ylim[1], model$s[i, 2], sd=scale), pnorm(ylim[2], model$s[i, 2], sd=scale)), model$s[i, 2], sd=scale)
                ##model$s[i, 1:2] <<- c(sx, sy)
                nfList[[i]]$setValue(c(sx, sy))
                
                
                # proposal model logProb
                ##model_lp_proposed <- model$calculate(calcNodes)
                model_lp_proposed <- nfList[[i]]$calc()
                
                # log-Metropolis-Hastings ratio
                log_MH_ratio <- model_lp_proposed - model_lp_initial
                # Metropolis-Hastings step: determine whether or
                # not to accept the newly proposed value
                jump <- decide(log_MH_ratio)
                if(jump) {
                    ##copy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
                    nfList[[i]]$copyToMVsaved()
                } else {
                    ##copy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE)
                    nfList[[i]]$copyToModel()
                }
                
            }#z==1
        }
    },
    methods = list( reset = function () {} )
)



## testing new methods setMonitors and setMonitors2
## for MCMC configuration objects

library(nimble)
code <- nimbleCode({
    alpha ~ dexp(1)
    beta ~ dgamma(1, 1)
    for(i in 1:10) {
        lambda[i] ~ dgamma(alpha, beta)
        x[i] ~ dpois(lambda[i])
    }
}) 
constants <- list()
data <- list(x = c(5,1,5,14,3,19,1,1,4,22))
inits <- list(alpha = 1, beta = 1, lambda = rep(1, 10))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel)

conf$printMonitors()
conf$addMonitors('x')
conf$setMonitors('x')
conf$setMonitors()
conf$setMonitors('alpha')
conf$addMonitors('alpha', 'beta')
conf$addMonitors2('x')
conf$setMonitors2('lambda')
conf$setMonitors2()
conf$setMonitors2('lambda')
conf$addMonitors('alpha', 'beta')
conf$setMonitors('alpha')
conf$setMonitors('alphadffd')
conf$resetMonitors()




## demonstrating to a user question on email list
## how to record width history for the slice sampler

library(nimble)
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('a', 'slice')

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

niter <- 10000
set.seed(0)
widthHistory <- numeric(niter)

for(i in 1:niter) {
    Cmcmc$run(1, reset = (i==1))
    widthHistory[i] <- Cmcmc$samplerFunctions[[1]]$width
}

samples <- as.matrix(Cmcmc$mvSamples)

plot(1:niter, widthHistory, type = 'l')



## experinemting with the stick_breaking BNP distribution,
## and sampling methods applied to it

library(nimble)
set.seed(0)
x <- rbeta(5, 1, 1)

truth <- c(x[1], x[2:5]*cumprod(1-x[1:4]), prod(1-x[1:5]))

SB_code <- nimbleCode({
    for(i in 1:5) z[i] ~ dbeta(1, 1)
    w[1:6] <- stick_breaking(z[1:5])
    for(i in 1:10) {
        zeta[i] ~ dcat(w[1:6])
    }
})

set.seed(1)
Inits <- list(z = rbeta(5, 1, 1))
data <- list(zeta = c(1,2,3,4,5,6,6,6,6,6))
Rmodel <- nimbleModel(SB_code, inits=Inits, data = data)
conf <- configureMCMC(Rmodel)

conf$printSamplers()



## code blocks used for updating MCMC samplers,
## to no longer copy the *values* of dependent stochastic nodes
## (now only copies their logProbs)


## scalar sampler

## setup
targetAsScalar <- model$expandNodeNames(target, returnScalarComponents = TRUE)
calcNodes <- model$getDependencies(target)
calcNodesNoSelf <- model$getDependencies(target, self = FALSE)
isStochCalcNodesNoSelf <- model$isStoch(calcNodesNoSelf)   ## should be made faster
calcNodesNoSelfDeterm <- calcNodesNoSelf[!isStochCalcNodesNoSelf]
calcNodesNoSelfStoch <- calcNodesNoSelf[isStochCalcNodesNoSelf]

## run
jump <- decide(logMHR)
jump <- XXXXXXXXXXXXXX
if(jump) {
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodesNoSelfDeterm, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodesNoSelfStoch, logProbOnly = TRUE)
} else {
    nimCopy(from = mvSaved, to = model, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = mvSaved, to = model, row = 1, nodes = calcNodesNoSelfDeterm, logProb = FALSE)
    nimCopy(from = mvSaved, to = model, row = 1, nodes = calcNodesNoSelfStoch, logProbOnly = TRUE)
}

## block sampler

## setup
## node list generation
targetAsScalar <- model$expandNodeNames(target, returnScalarComponents = TRUE)
calcNodes <- model$getDependencies(target)
finalTargetIndex <- max(match(model$expandNodeNames(target), calcNodes))
if(!is.integer(finalTargetIndex) |
   length(finalTargetIndex) != 1 |
   is.na(finalTargetIndex[1]))
    stop('Problem with target node in sampler_RW_block')
calcNodesProposalStage <- calcNodes[1:finalTargetIndex]
calcNodesDepStage <- calcNodes[-(1:finalTargetIndex)]
##calcNodesNoSelf <- model$getDependencies(target, self = FALSE)
isStochCalcNodesDepStage <- model$isStoch(calcNodesDepStage)   ## should be made faster
calcNodesDepStageDeterm <- calcNodesNoSelf[!isStochCalcNodesDepStage]
calcNodesDepStageStoch <- calcNodesNoSelf[isStochCalcNodesDepStage]
## numeric value generation

## run
jump <- FALSE
##jump <- my_decideAndJump$run(lpMHR, 0, 0, 0) ## will use lpMHR - 0
lpD <- lpD + calculateDiff(model, calcNodesDepStage)
jump <- decide(lpD)
if(jump) {
    nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodesProposalStage, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodesDepStageDeterm, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodesDepStageStoch, logProbOnly = TRUE)
} else {
    nimCopy(from = mvSaved, to = model, row = 1, nodes = calcNodesProposalStage, logProb = TRUE)
    nimCopy(from = mvSaved, to = model, row = 1, nodes = calcNodesDepStageDeterm, logProb = FALSE)
    nimCopy(from = mvSaved, to = model, row = 1, nodes = calcNodesDepStageStoch, logProbOnly = TRUE)
}



## Testing, seeing if parameter estimate bias remains
## in NPCR non-parametric capture-recapture paper simulation study

library(nimble)
library(nimbleEcology)

y <- as.matrix(read.table('~/github/npcr/data/loups.txt'))
dimnames(y) <- NULL
T <- ncol(y)
first <- apply(y, 1, function(x) min(which(x==1)))
y <- y[which(first < T),]
first <- apply(y, 1, function(x) min(which(x==1)))
len <- T - first + 1
nInd <- nrow(y)
M <- 100

code <- nimbleCode({
    phi ~ dunif(0, 1)
    ##alpha ~ dgamma(1, alphaR)
    alpha ~ dunif(0, 100)
    xi[1:nInd] ~ dCRP(conc = alpha, size = nInd)
    for(i in 1:M)   p[i] ~ dunif(0, 1)
    for(i in 1:nInd) {
        y[i,first[i]:T] ~ dCJS_ss(phi, p[xi[i]], len=len[i])
    }
})

constants <- list(nInd=nInd, T=T, M=M, first=first, len=len, alphaR=1)
data <- list(y=y)
inits <- list(phi=0.5, alpha=1, xi=rep(1,nInd), p=rep(0.5,M))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()


conf <- configureMCMC(Rmodel)
conf$printSamplers('alpha')   ## CRP_concentration sampler: alpha

conf$printSamplers(byType = TRUE)

conf$printSamplers('xi')
conf$printSamplers()



## working with issues in CAR proper model
## for Wei, for his SCR examples, using dcar_proper,
## and when the Cmatrix is non-invertible, and
## gives imaginary eigenvalues

## inst/include/nimble/EigenTypedefs.h
## line 153
## "Run-time warning: matrix used in call to nimEigen() has a complex eigenvalue."

library(nimble)

## Divide the spatial domain into 5 by 5 windows
## choose only one:
nnn <- 2
nnn <- 3
nnn <- 5

if(nnn == 2) {
    nHabRows <- nHabCols <- 2
    num <- rep(2, 4)
    adj <- c(2,3,1,4,1,4,2,3)
    numHabWindows <- nHabRows * nHabCols 
} else {
    if(nnn == 3) {
        nHabRows <- nHabCols <- 3
    } else if(nnn == 5) {
        nHabRows <- nHabCols <- 5
    } else stop()
    adj <- NULL
    numadj <- NULL
    numHabWindows <- nHabRows * nHabCols 
    for(i in 1:numHabWindows){
        {
            lenadj <- length(adj)
            if(i == 1) adj <- c(adj, c(2, nHabCols + 1))
            else if((2 <= i) & (i <= nHabCols - 1)) adj <- c(adj, c(i-1, i+1, i+nHabCols))
            else if(i == nHabCols) adj <- c(adj, c(i-1, i+nHabCols))
            else if(i %in% c(1:(nHabRows-2)*nHabCols+1)) adj <- c(adj, c(i-nHabCols, i+1, i+nHabCols))
            else if(i %in% c(2:(nHabRows-1)*nHabCols)) adj <- c(adj, c(i-nHabCols, i-1, i+nHabCols))
            else if(i == (nHabRows-1)*nHabCols+1) adj <- c(adj, c(i-nHabCols, i+1))
            else if(((nHabRows-1)*nHabCols+2 <= i) & (i <= nHabCols*nHabRows-1)) adj <- c(adj, c(i-nHabCols, i-1, i+1))
            else if(i == nHabCols*nHabRows) adj <- c(adj, c(i-nHabCols, i-1))
            else adj <- c(adj, c(i-nHabCols, i-1, i+1, i+nHabCols))
            numadj[i] <- length(adj) - lenadj
        }
    }
    num <- numadj
}

## Parameter values for the proper CAR model
mu0 <- 0
tau <- 1
gamma <- 0.5
mu <- rep(mu0, numHabWindows)
##
mu     ## vector of 25 0's
adj    ## length 80
num    ## length 25
tau    ## = 1
gamma  ## = 0.5
##
C <- CAR_calcC(adj, num)
M <- CAR_calcM(num)
##nimble:::CAR_proper_checkAdjNumCM
nimble:::CAR_proper_checkAdjNumCM(adj, num, C, M)
Cmatrix <- CAR_calcCmatrix(C, adj, num)

round(Cmatrix, 2)
round(nimEigen(Cmatrix)$values, 2)
round(solve(Cmatrix), 2)

##round(1/gamma*(diag(numHabWindows) - Cmatrix), 2)
##round(diag(numHabWindows) - gamma*Cmatrix, 2)
##solve(round(diag(numHabWindows) - gamma*Cmatrix, 2))
##nimEigen(round(diag(numHabWindows) - gamma*Cmatrix, 2))

##CAR_calcEVs3

CAR_calcEVs31 <- nimbleFunction(
    name = 'CAR_calcEVs3',
    run = function(C = double(1), adj = double(1), num = double(1)) {
        Cmatrix <- CAR_calcCmatrix(C, adj, num)
        evs <- nimEigen(Cmatrix, only.values = TRUE)$values
        ## new code below, to replace NaN eigenvalues of Cmatrix with 0,
        ## which occurs when Cmatrix is singular
        ## -DT June 2020
        if(any_nan(evs)) {
            for(i in 1:length(evs)) {
                if(is.nan(evs[i])) evs[i] <- 0
            }
        }
        returnType(double(1))
        return(evs)
    }
)

CAR_calcEVs31_c <- compileNimble(CAR_calcEVs31)

xev <- CAR_calcEVs31(C, adj, num)
xev <- CAR_calcEVs31_c(C, adj, num)
class(xev)
length(xev)
round(xev, 2)



evs <- nimEigen(Cmatrix, only.values = TRUE)$values
round(evs, 2)

gamma
C
M
round(Cmatrix, 2)
round(evs, 2)

## Simulate values from a propoer CAR model: this does not work
rcar_proper(n = 1, mu = mu, adj = adj, num = numadj, tau = tau, gamma = gamma)

## Calculate the probability density: does not work either
##debug(dcar_proper)
##dcar_proper
dcar_proper(rep(0, numHabWindows), mu = mu, adj = adj, num = numadj, tau = tau, gamma = gamma)

## double dcar_proper(double* x, double* mu, double* C, double* adj, double* num, double* M, double tau, double gamma, double* evs, int N, int L, int give_log) {
##   // This method implements the following density calculation:
##   // x ~ MVN( mean = mu,   cov  = (I-gamma*Cmatrix)^-1* %*% Mmatrix / tau )
##   // note that the scalar 'tau' is a precision term.

library(mvtnorm)
##dmvnorm
## function (x, mean = rep(0, p), sigma = diag(p), log = FALSE)
x <- rep(0, numHabWindows)
## cov = (I-gamma*Cmatrix)^-1* %*% Mmatrix / tau )
round(nimEigen(diag(numHabWindows) - gamma*Cmatrix)$values, 2)
##solve(diag(numHabWindows) - gamma*Cmatrix)
CovMatrix <- solve(diag(numHabWindows) - gamma*Cmatrix) %*% diag(M) / tau
round(solve(CovMatrix), 2)
dmvnorm(x, mean = mu, sigma = CovMatrix)   ## 0.0146307
## (2pi)^(-N/2) * |Cov|^(-1/2) * exp(-1/2 * (x-mu)' %*% Cov^-1 %*% (x-mu))
(2*pi)^(-numHabWindows/2) * det(CovMatrix)^(-1/2)   ## 0.0146307


det(solve(diag(numHabWindows) - gamma*Cmatrix))


set.seed(0)
N <- 3
a <- array(rnorm(N*N), c(N,N))
A <- t(a) %*% a
gamma <- 0.2

det(solve(diag(N) - gamma*A))   ## 65.28867
aev <- nimEigen(solve(diag(N) - 1*A))$values
aaev <- nimEigen(A)$values
1/prod(1 - gamma*aaev)   ## 65.28867

det(diag(N) - gamma*A)   ## 0.01531659
aev <- nimEigen(diag(N) - 1*A)$values
aaev <- nimEigen(A)$values
prod(1 - gamma*aaev)   ## 0.01531659

##eigout <- nimEigen(A)
##evals <- eigout$values
##evecs <- eigout$vectors
##evecs %*% diag(evals) %*% t(evecs) - A
##evecs %*% diag(evals) %*% solve(evecs) - A


evs <- nimEigen(Cmatrix, only.values = TRUE)$values
round(evs, 2)
evs[which(is.nan(evs))] <- 0
round(evs, 2)



##double dcar_proper(double* x, double* mu, double* C, double* adj, double* num, double* M, double tau, double gamma, double* evs, int N, int L, int give_log) {
## This method implements the following density calculation:
## x ~ MVN( mean = mu,   cov  = (I-gamma*Cmatrix)^-1* %*% Mmatrix / tau )
## note that the scalar 'tau' is a precision term.
lp <- 0
count <- 1
N <- length(num)
L <- length(adj)
x <- rep(0, numHabWindows)
for(i in 1:N) {
    xi <- x[i] - mu[i]
    lp <- lp + xi^2 / M[i]      ## (x-mu)' M^-1 (x-mu)
    for(j in 1:num[i]) {
        index <- adj[count]
        xj <- x[index] - mu[index]
        lp <- lp - gamma*xi*xj*C[count]/M[i]  ## -gamma (x-mu)' M^-1 C (x-mu)
        count <- count + 1
    }
}
if(count != (L+1)) stop()
lp <- lp * (-1/2) * tau
## now add -1/2*log(|2*pi*Sigma|) to lp:
## det(Sigma) = det((I-gamma*Cmatrix)^-1 %*% M / tau) = prod(M) / prod(1 - gamma*evs) / tau^N
## where evs = eigen(Cmatrix)$values, and tau is precision
for(i in 1:N) {
    lp <- lp + (log(1 - gamma*evs[i]) - log(M[i])) / 2
}
lp <- lp + (N/2) * (log(tau) - log(2*pi));
exp(lp)








## testing *which* nimDerivs(model$calculate()) is correct !!
## between old inside-HMC-sampler transform,
## new ADoak system,
## new parameterTransformation NF,
## and the actual underlying calculations that take place.

1
system('rm -rf /Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble')
library(nimble)

## (a) old nimble commit: 6f7471729dc6e1a689ccac26420597bcc402417d
system('rm -rf /Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble; cp -r ~/builds/built_oldPassingHMC.commit /Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble')

## (b) current ADoak branch, using parameterTransform system
##system('rm -rf ~/builds/ADoak.built'); system('cp -r /Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble ~/builds/ADoak.built')
system('rm -rf /Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble; cp -r ~/builds/ADoak.built /Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble')

opt <- 1   ## tests log of gamma(p1, p2) dist
opt <- 2   ## tests logit of uniform(L, U) dist

library(nimble)
library(testthat)
library(numDeriv)
nimbleOptions(experimentalEnableDerivs = TRUE)
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
tol <- 1E-7
##
if(opt == 1) {
    param1 <- 2
    param2 <- 3
    lvalue <- log(0.1)
    (lp_true <- dgamma(exp(lvalue), param1, param2, log = TRUE))   ## -0.4053605
    code <- nimbleCode({ x ~ dgamma(p1, p2) })
    constants <- list(p1 = param1, p2 = param2)
    data <- list()
    inits <- list(x = exp(lvalue))
    Rmodel <- nimbleModel(code, constants, data, inits)
    f <- function(newlogx) { Rmodel$x <- exp(newlogx); return(Rmodel$calculate('x')) }
}
if(opt == 2) {
    L <- 2
    U <- 13
    lvalue <- logit((10-L)/(U-L))
    (lp_true <- dunif(ilogit(lvalue)*(U-L) + L, min=L, max=U, log=TRUE))  ## -2.397895
    code <- nimbleCode({ x ~ dunif(L, U) })
    constants <- list(L=L, U=U)
    data <- list()
    inits <- list(x = ilogit(lvalue)*(U-L) + L)
    Rmodel <- nimbleModel(code, constants, data, inits)
    f <- function(newlogitx) { Rmodel$x <- ilogit(newlogitx)*(U-L) + L; return(Rmodel$calculate('x')) }
}
##
(lp1 <- Rmodel$calculate())
(lp2 <- Rmodel$calculate('x'))
expect_equal(lp1, lp_true)
expect_equal(lp2, lp_true)
##
(flp <- f(lvalue))
expect_equal(flp, lp_true)
##
## taking derivative myself:
delta_x <- 1E-4; (f(lvalue+delta_x) - f(lvalue)) / (delta_x)
delta_x <- 1E-6; (f(lvalue+delta_x) - f(lvalue)) / (delta_x)
delta_x <- 1E-7; (f(lvalue+delta_x) - f(lvalue)) / (delta_x)
delta_x <- 1E-8; (f(lvalue+delta_x) - f(lvalue)) / (delta_x)
deriv_true <- (f(lvalue+delta_x) - f(lvalue)) / (delta_x)
##
## using numDeriv grad() function:
(deriv_grad <- grad(f, lvalue))
expect_equal(deriv_grad, deriv_true, tolerance = 1E-7)

## using (a) or(b) above
conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('x', 'HMC')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

##Rmcmc$samplerFunctions$contentsList[[1]]$run
##Rmcmc$samplerFunctions$contentsList[[1]]$gradient
Rmcmc$samplerFunctions$contentsList[[1]]$tapedModelCalculateCalcNodes
##debug(Rmcmc$samplerFunctions$contentsList[[1]]$run)

##set.seed(0); runMCMC(Rmcmc, 1)
##set.seed(0); runMCMC(Cmcmc, 1)

## testing parameterTransform NF
Cmcmc$samplerFunctions$contentsList[[1]]$grad
Cmcmc$samplerFunctions$contentsList[[1]]$gradient
Cmcmc$samplerFunctions$contentsList[[1]]$gradient(lvalue)
Cmcmc$samplerFunctions$contentsList[[1]]$grad








## example of bug in nimbleEcology
## dHMM and dHMMo distributions

library(nimble)
library(nimbleEcology)

code <- nimbleCode({
    y[1:3] ~ dHMM(init = init[1:3],
                  probObs = probObs[1:3, 1:2],       ## dimension Nl x No
                  probTrans = probTrans[1:3, 1:3],   ## dimension Nl x Nl
                  len = len)
})

## number of latent states: Nl = 3
## number of observable states: No = 2
y <- c(1,1,2)
len <- length(y)
init <- c(1, 0, 0)  ## initially in state x=1 with certainty
probObs <- matrix(
    c(1, 0,           ## x=1 => guaranteed observation of y=1
      1, 0,           ## x=2 => guaranteed observation of y=1
      0, 1),          ## x=3 => guaranteed observation of y=2
    nrow = 3,         ## dimension = Nl x No = 3 x 2
    ncol = 2,
    byrow = TRUE
)
probTrans <- matrix(
    c(1/3, 1/3, 1/3,  ## x=1 => could transition to x=1,2,3
      1/3, 1/3, 1/3,  ## x=2 => could transition to x=1,2,3
      0,   0,   1),   ## x=3 is an absorbing state (e.g., dead)
    nrow = 3,         ## dimension = Nl x Nl = 3 x 3
    ncol = 3,
    byrow = TRUE
)

constants <- list(init = init,
                  len = len,
                  probObs = probObs,
                  probTrans = probTrans)
data <- list(y = y)
inits <- list()

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()  ## -Inf

constants <- list(init = init,
                  len = len,
                  probObs = probObs,
                  probTrans = t(probTrans))   ## transposed
data <- list(y = y)
inits <- list()

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()       ## -1.504077
exp(Rmodel$calculate())  ## 0.2222222 = 2/9

## can verify via a brief thought-exercise: 1 * 2/3 * 1/3 = 2/9

debug(dHMM)



## brief code snippet for Claudia, on making a 2-factor
## barplot using ggplot2

df <- data.frame(
    X1 = as.factor(rep(1:3, 4)),
    X2 = as.factor(rep(1:4, each=3)),
    data = 1:12
)

library(ggplot2)

ggplot(df, aes(y = data, x = X2, fill = X1)) +
    geom_bar(stat = 'identity', position = 'dodge', color = 'black', width=0.5) +
        xlab('x axis label') +
            ylab('y axis label')

ggsave('~/Desktop/my_plot.pdf', width = 4, height = 3)



## looking at model for Emily Grace Simmonds
## from NTNU workshop in Trondheim, Norway

library(nimble)

test_sims <- read.csv('~/Downloads/test_sims.csv')

code <- nimbleCode({
    ## priors for hyperparameters
    prec[1:M, 1:M] ~ dwish(Rho[1:M,1:M], M)  ## DT: full indexing for rho
    ## priors for parameters
    ri ~ dnorm(1.5,4)
    rj ~ dnorm(1.5,4)
    alphaij ~ dnorm(0,300)
    alphaji ~ dnorm(0,300)
    ci ~ dnorm(0.6, 25)
    cj ~ dnorm(0.6, 25)
    ##
    N[1, 1:M] ~ dmnorm(mu[1,1:M], prec[1:M,1:M])
    ##
    for(i in 1:(n-1)) {
        ## likelihoods
        ## state process
        ## sample both populations from one distribution
        mu[i+1,1:M] <- c(ri + (ci*N[i,1])+(alphaij*N[i,2]),
                         rj + (cj*N[i,2])+(alphaji*N[i,1]))
        ##
        N[i+1, 1:M] ~ dmnorm(mu[i+1,1:M], prec[1:M,1:M]) # likelihood for process SPi
        ## observation process
        Yi[i+1] ~ dpois(exp(N[i+1, 1]))
        Yj[i+1] ~ dpois(exp(N[i+1, 2]))
    }
})

n <- length(test_sims[,1])

constants <- list(n=length(test_sims[,1]), M=2)

data <- list(
    Yi = (round(exp(test_sims$Yi))),
    Yj = (round(exp(test_sims$Yj)))
)

inits <- list(
    ri = 1.5, rj = 1.5,
    ci = 0.6, cj = 0.6,
    alphaij = 0.1, alphaji = 0.1,
    prec = matrix(c(3,0.5,0.5,3),2,2),
    N = matrix(c(6,6),n,2),
    mu = matrix(rep(6,n*2),n,2),
    Rho = matrix(c(3,0.5,0.5,3),2,2)
)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)
samplesPlot(samples)
apply(samples, 2, effectiveSize)







## testing correctness of calcLogDegJacobian
## for dirichlet distribution, as used in parameterTransform NF

ff <- function(x) -log(exp(x) + exp(-x) + 2)

(nodes <- Rmodel$getNodeNames(stochOnly = TRUE)[3])
(nodes <- Rmodel$getNodeNames(stochOnly = TRUE))

pt <- parameterTransform(Rmodel, nodes)
(vals <- values(Rmodel, nodes))
theNF <- pt
(tVals <- theNF$transform(vals))
(vals2 <- theNF$inverseTransform(tVals))

theNF$calcLogDetJacobian(tVals)

tVals
ff(tVals)
sum(ff(tVals))

log(1-sum(vals[1:1]))

sum(ff(tVals))  ## 2
sum(ff(tVals)) + log(1-sum(vals[1:1]))  ## 3
sum(ff(tVals)) + log(1-sum(vals[1:1])) + log(1-sum(vals[1:2])) + log(1-sum(vals[1:3])) ## 5


## minimal reproducible example of
## compilation problem for parameterTransform

## build package on branch 'ADoak'
library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)

code <- nimbleCode({x2[1:2] ~ ddirich(a2[1:2])})
constants <- list(a2 = rep(1, 2))
data <- list()
inits <- list(x2 = c(.5, .5))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()   ## 0
nodes <- Rmodel$getNodeNames(stochOnly = TRUE)
pt <- parameterTransform(Rmodel, nodes)
Cmodel <- compileNimble(Rmodel)
Cpt <- compileNimble(pt, project = Rmodel)




## testing parameterTransform for ddirch distributions
## specifically, the correctness of transform() and inverseTransform() methods

K <- 2
theseValues <- c(0.5, 0.5)
theseValues <- c(0.2, 0.8)

K <- 3
theseValues <- c(0.5, 0.25, 0.25)
theseValues <- c(0.2, 0.8/5, 1-0.2-0.8/5)

K <- 4
theseValues <- c(0.2, 0.3, 0.1, 0.4)

if(sum(theseValues) != 1) stop()
theseValues_orig <- theseValues

## testing transform() method
(dd <- K-1)
theseTransformed <- nimNumeric(dd)
theseTransformed[1] <- logit( theseValues[1] )
print(theseTransformed)

if(dd > 1) {
    runningSum <- 0
    for(j in 2:dd) {
        runningSum <- runningSum + theseValues[j-1]
        (theseTransformed[j] <- logit( theseValues[j] / (1-runningSum) ))
    }
}

print(theseTransformed)
(theseValues <- theseTransformed)

## testing inverseTransform() method
dd <- K
(theseInvTransformed <- nimNumeric(dd))
theseInvTransformed[1] <- ilogit( theseValues[1] )
print(theseInvTransformed)

if(dd > 2) {
    runningSum <- 0
    for(i in 2:(dd-1)) {
        runningSum <- runningSum + theseInvTransformed[i-1]
        theseInvTransformed[i] <- (1-runningSum) * ilogit( theseValues[i] )
    }
}
theseInvTransformed[dd] <- 1 - sum(theseInvTransformed[1:(dd-1)])
print(theseInvTransformed)

print(theseInvTransformed - theseValues_orig)
if(!all(theseInvTransformed - theseValues_orig == 0)) stop()

## testing calcLogDetJacobian method
dd <- K
theseInvTransformed <- nimNumeric(dd)
theseInvTransformed[1] <- ilogit( theseValues[1] )
if(dd > 2) {
    runningSum <- 0
    for(i in 2:(dd-1)) {
        runningSum <- runningSum + theseInvTransformed[i-1]
        theseInvTransformed[i] <- (1-runningSum) * ilogit( theseValues[i] )
    }
}
print(theseInvTransformed)

x <- theseValues[1]
lpAdd <- -log(exp(x)+exp(-x)+2)   ## alternate: -2*log(1+exp(-x))-x)
print(lpAdd)

if(dd > 2) {
    runningSum <- 0
    for(i in 2:(dd-1)) {
        runningSum <- runningSum + theseInvTransformed[i-1]
        x <- theseValues[i]
        lpAdd <- lpAdd + log(1-runningSum)
        lpAdd <- lpAdd - log(exp(x)+exp(-x)+2)   ## alternate: -2*log(1+exp(-x))-x)
    }
}




## experimenting with nimSwitch

library(nimble)

Rnf <- nimbleFunction(
    run = function(a = integer()) {
        nimSwitch(a, ## MUST BE INTEGER
                  ## either below c(...), or 1:X both work:
                  ## GIVES POSSIBLE VALUES FOR a -- SPECIAL HANDLING, EVALUATED IN R AT *COMPILE TIME*. Hence must be static:
                  ##c(1, 2, 3, 4, 5),
                  1:5,
                  print(1),   ## do this for first match (can be in {})
                  print(2),   ## do this for second match, etc.
                  print(3),
                  {
                      print(4)
                  },
                  {
                      a <- 2+3
                      print(a)
                  }
                  )
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

for(a in 1:5) {
    cat(paste0('----- a=', a, ' -----\n'))
    Rnf(a)
    Cnf(a)
    cat('---------------\n')
}




## helping user Xin Wang provide pre-calculated eigenvalues
## evs argument to dcar_proper

library(nimble)
library(Matrix)
library(mvtnorm)

nr <- nc <- 5

cmat.fun <- function(nr,nc) {
    # input is the dimension of the grid 
    N <- nr*nc
    index.mat <- matrix(0, N, 4)
    index.mat[,1] <- (1:N) - nr
    index.mat[,2] <- (1:N) - 1
    index.mat[,3] <- (1:N) + 1
    index.mat[,4] <- (1:N) + nr
    index.mat[(index.mat <0) | (index.mat > N)] <- 0
    index.mat[(nr*(1:(nr-1))+1),2] <- 0
    index.mat[nr*(1:nr),3] <- 0
    ##
    Cmat <- Matrix(0, N, N)
    for (i in 1:N) {
        Cmat[i,c(index.mat[i,],i)] <- 1
    }
    return(Cmat)
}

W <- cmat.fun(nr,nc)  
diag(W) <- 0
W <- as.matrix(W)

tau <- 1
rho <- 0.95
ns <- ncol(W)
Dmat <- diag(colSums(W))
set.seed(0)   ## new
sp <- rmvnorm(1,rep(0,ns),tau*solve(Dmat-rho*W))

adj <- unlist(lapply(1:ncol(W), function(x){which(W[x,]==1)}))
num <- colSums(W)

##CAR_calcEVs4 <- nimbleFunction(
##    name = 'CAR_calcEVs4',
##    run = function(C = double(1), adj = double(1), num = double(1)) {
##        returnType(double(1))
##        return(evs)
##    }
##)

weights <- rep(1, length(adj))
CM <- as.carCM(adj, weights, num)
C <- CM$C
M <- CM$M

Cmatrix <- CAR_calcCmatrix(C, adj, num)
evs <- eigen(Cmatrix, only.values = TRUE, symmetric = TRUE)$values

dcar_proper(sp, mu = rep(1,length(sp)), adj = adj, num = num, tau = 1, 
            gamma = 0.95, evs = evs, log = TRUE)
## -24.8071

code <- nimbleCode({
    sp[1:N] ~ dcar_proper(mu=mu0[1:N], C=C[1:L], adj=adj[1:L], num=num[1:N], M=M[1:N], tau=1, gamma=0.95, evs=evs[1:N])
})

N <- length(num)
L <- length(adj)

constants <- list(mu0 = rep(1,N), C = C, adj = adj, num = num, M = M, N = N, L = L, evs = evs)
data <- list(sp = sp[1,])
inits <- list()

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()
## -24.8071

Cmodel <- compileNimble(Rmodel)

Cmodel$calculate()
## -24.8071

Rmodel$sp
Rmodel$logProb_sp
Rmodel$nodes[[names(Rmodel$nodes)[1]]]
ls(Rmodel$nodes[[names(Rmodel$nodes)[1]]])

Rmodel$nodes[[names(Rmodel$nodes)[1]]]$calculate
debug(Rmodel$nodes[[names(Rmodel$nodes)[1]]]$calculate)
Rmodel$nodes[[names(Rmodel$nodes)[1]]]$calculate()

.Call(nimble:::C_dcar_proper, as.double(data$sp), as.double(rep(1,N)), as.double(C), 
      as.double(adj), as.double(num), as.double(M), as.double(1), 
      as.double(0.95), as.double(evs), as.logical(1))


BUGSdist = 'dcar_proper(mu, C, adj, num, M, tau, gamma, evs)',
Rdist    = c(
    'dcar_proper(mu, C,                       adj, num, M,                  tau, gamma, evs = CAR_calcEVs3(C, adj, num))',
    'dcar_proper(mu, C = CAR_calcC(adj, num), adj, num, M = CAR_calcM(num), tau, gamma, evs = CAR_calcEVs2(   adj, num))'),





##dcar_proper(sp, mu = rep(1,length(sp)),adj = adj, num = num, tau = 1, 
##            gamma = 0.95, log = TRUE) ### does not work

##dcar_proper(sp, mu = rep(1,length(sp)),adj = adj, num = num, tau = 1, 
##            gamma = 0.95, evs = CAR_calcEVs4(C, adj, num), log = TRUE)





dcar_proper
CAR_calcC
C <- CAR_calcC(adj, num)
C
CAR_calcM
M <- CAR_calcM(num)
M
CAR_calcEVs3(C, adj, num)
CAR_calcEVs3
CAR_calcCmatrix
Cmatrix <- CAR_calcCmatrix(C, adj, num)
Cmatrix
nn <- 10; round(Cmatrix[1:nn, 1:nn], 3)
nimEigen
nimEigen(Cmatrix, only.values = TRUE)$values
eigen(Cmatrix, symmetric = TRUE)








## package build from branch 'ADoak'
library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)

code <- nimbleCode({
    sigma ~ dunif(0, 10)
    y ~ dnorm(0, sd = sigma)
})
constants <- list()
data <- list(y = 0)
inits <- list(sigma = 1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Rpt <- parameterTransform(Rmodel, c('sigma', 'y'))

Cmodel <- compileNimble(Rmodel)
Cpt <- compileNimble(Rpt, project = Rmodel, showCompilerOutput = TRUE)




library(nimble)

nf <- nimbleFunction(
    setup = function() {},
    run = function(x = double(1)) {
        ans <- sum(x^2)
        return(ans)
        returnType(double())
    },
    methods = list(
        runDerivs = function(x = double(1),
            order = double(1)) { ## order is conceptually integer but we use double
            ans <- nimDerivs(run(x), order = order) ## derivatives of run(x)
            return(ans)
            returnType(ADNimbleList())
        }
    ),
    enableDerivs = 'run' ## Must say here that run will have derivatives.  This might become unnecessary later.
)

nf1 <- nf()
nf2 <- nf()

x <- 1:3
sum(x^2)
nf1$run(x)
nf1$runDerivs(x, order = 0:0)
nf1$runDerivs(x, order = 0:1)
nf1$runDerivs(x, order = 0:2)

Cnf1 <- compileNimble(nf1)
Cnf1$run(x)
Cnf1$runDerivs(x, order = 0:0)
Cnf1$runDerivs(x, order = 0:1)
Cnf1$runDerivs(x, order = 0:2)

## some very preliminary testing of merging usage of
## the new parameterTransform nimbleFunction into
## the HMC sampler,
## all taking place on branch ADoak

library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)

code <- nimbleCode({
    mu ~ dnorm(0, sd = 10)
    tau ~ dgamma(0.01, 0.01)
    sigma ~ dunif(0, 10)
    p ~ dunif(0, 1)
    mu4 <- mu * p
    y1 ~ dnorm(mu, tau)
    y2 ~ dnorm(mu, tau)
    y3 ~ dnorm(mu, sd = sigma)
    y4 ~ dnorm(mu4, tau)
})

constants <- list()
data <- list(y1 = 10, y2 = 9, y3 = 9, y4 = 7)
inits <- list(mu = 0, tau = 1, sigma = 1, p = 0.5)

##Rmodel <- nimbleModel(code, constants, data, inits)
##Rmodel$calculate()
##conf <- configureMCMC(Rmodel)
##Rmcmc <- buildMCMC(conf)
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##set.seed(0)
##samples <- runMCMC(Cmcmc, 100000, nburnin = 10000)
##means <- apply(samples, 2, mean)
##sds <- apply(samples, 2, sd)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel, nodes = NULL)

conf$addSampler(c('mu','tau','sigma','p'), 'HMC', control = list(nwarmup = 1000))

Rmcmc <- buildMCMC(conf)
Rmcmc$samplerFunctions$contentsList[[1]]$my_parameterTransform$getOriginalLength()
Rmcmc$samplerFunctions$contentsList[[1]]$my_parameterTransform$getTransformedLength()

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

set.seed(0)
samplesHMC <- runMCMC(Cmcmc, 100)   ## temp
samplesHMC <- runMCMC(Cmcmc, 100000, nburnin=20000)   ## original

meansHMC <- apply(samplesHMC, 2, mean)
sdsHMC <- apply(samplesHMC, 2, sd)

expect_true(all(abs(means - meansHMC) < 0.03))   ## new AD system
expect_true(all(abs(sds   - sdsHMC)   < 0.09))   ## new AD system




library(nimble)

code <- nimbleCode({
    prod[1:p, 1:p] <- sigma[1:p] * U[1:p, 1:p]
})

p <- 5
constants <- list(p = p)
data <- list()
inits <- list(sigma = 1:p, U = matrix(1:(p^2), nrow=p, byrow=TRUE))

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()
Rmodel$sigma
Rmodel$U
Rmodel$prod

Cmodel <- compileNimble(Rmodel)

Cmodel$calculate()   ## ERROR
Cmodel$sigma
Cmodel$U
Cmodel$prod          ## INCORRECT

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)
samplesPlot(samples)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()




library(nimble)

code <- nimbleCode({
    beta0 ~ dnorm(0, sd = 100)
    beta1 ~ dnorm(0, sd = 100)
    beta2 ~ dnorm(0, sd = 100)
    sigma ~ dunif(0, 100)        # prior for variance components based on Gelman (2006)
    for(i in 1:n) {
        ##y[i] ~ dnorm(beta0 + beta1*x1[i] + beta2*x2[i], sd = sigma) # manual entry of linear predictors
        y[i] ~ dt(beta0 + beta1*x1[i] + beta2*x2[i], sigma, df = 3) # manual entry of linear predictors
    }
})

constants <- list(n = 5)
data <- list()
inits <- list()


Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()



## helping Michel Raymond with his aggregated data question

library(nimble)

df<-read.csv("~/Downloads/OB_meta_analyse.csv")

## prÃ©paration du fichier
df<-df[!is.na(df$sibs),]
df$sex<-as.character(df$sex)
df<-df[-which(df$pop=="Samoa"),]  #<====== only European+north america
sexPref<-ifelse(df$sex=="hetero",0,1)
N <- nrow(df)
Nsib <- df$sibs+df$focal  # <--- To get the sibship (including focals)
Nfocal<-df$focal
ob <-df$OB
yob <- df$mean_yob
## standardized year of birth:
yob<-scale(yob, scale=FALSE)
yob<-as.vector(yob)

df<-read.csv("~/Downloads/OB_meta_analyse.csv")
df<-df[!is.na(df$sibs),]
df$sex<-as.character(df$sex)
df<-df[-which(df$pop=="Samoa"),]  #<====== only European+north america
N <- nrow(df)
Nsib <- df$sibs+df$focal  # <--- To get the full sibship (including focals)
sexPref<-ifelse(df$sex=="hetero",0,1)
Nfocal<-df$focal
ob <-df$OB
yob <- df$mean_yob
## standardized year of birth:
yob<-scale(yob, scale=FALSE)
yob<-as.vector(yob)

modelInfoList <- list(
    `a*B^gamma` = list(
        code = nimbleCode({
            p0 ~ dunif(0, 1)
            a ~  dunif(-0.1, 100)
            gamma ~ dunif(0, 100)
            for(i in 1:3) { c[i] ~ dnorm(0, sd = 100) }
            for(i in 1:N) {
                lambda[i]     <- exp(c[1] + c[2]*yob[i])
                lambda_obs[i] <- exp(c[3] + c[2]*yob[i])
                f[i] <- a * (lambda_obs[i]^gamma)
                p[i] <- min(p0 + f[i], 0.999999)
                ob[i] ~ dnorm(Nfocal[i]*lambda_obs[i],sd=sqrt(Nfocal[i]*lambda_obs[i]) )
                Nsib[i] ~ dnorm(Nfocal[i]*lambda[i],sd=sqrt(Nfocal[i]*lambda[i]))
                sexPref[i] ~ dbern(p[i]) }
        }),
        constants = list(N=N, yob=yob, Nfocal=Nfocal),
        data = list(Nsib=Nsib, sexPref=sexPref,ob=ob),
        inits = list(p0=0.5, c=c(1,0.1,1), a=0.1,
            gamma=0.1)
    ),
    `a*B` = list(
        code = nimbleCode({
            p0 ~ dunif(0, 1)
            a ~ dunif(-0.1, 100)
            for(i in 1:3) { c[i] ~ dnorm(0, sd = 100) }
            for(i in 1:N) {
                lambda[i]     <- exp(c[1] + c[2]*yob[i])
                lambda_obs[i] <- exp(c[3] + c[2]*yob[i])
                f[i] <- a * lambda_obs[i]
                p[i] <- min(p0 + f[i], 0.999999)
                ob[i]      ~ dnorm(Nfocal[i]*lambda_obs[i],sd=sqrt(Nfocal[i]*lambda_obs[i]) )
                Nsib[i]    ~ dnorm(Nfocal[i]*lambda[i]    ,sd=sqrt(Nfocal[i]*lambda[i])     )
                sexPref[i] ~ dbern(p[i]) }
        }),
        constants = list(N=N, yob=yob, Nfocal=Nfocal),
        data = list(Nsib=Nsib, sexPref=sexPref,ob=ob),
        inits = list(p0=0.5, c=c(1,0.1,1),a=0.1)
    ),
    ## Null model
    `null model` = list(
        code = nimbleCode({
            p0 ~ dunif(0, 1)
            for(i in 1:3) { c[i] ~ dnorm(0, sd = 100) }
            for(i in 1:N) {
                lambda[i]     <- exp(c[1] + c[2]*yob[i])
                lambda_obs[i] <- exp(c[3] + c[2]*yob[i])
                ob[i]      ~ dnorm(Nfocal[i]*lambda_obs[i],sd=sqrt(Nfocal[i]*lambda_obs[i]) )
                Nsib[i]    ~ dnorm(Nfocal[i]*lambda[i]    ,sd=sqrt(Nfocal[i]*lambda[i]))
                sexPref[i] ~ dbern(p0) }
        }),
        constants = list(N=N, yob=yob, Nfocal=Nfocal),
        data = list(Nsib=Nsib, sexPref=sexPref,ob=ob),
        inits = list(p0=0.5, c=c(1,0.1,1))
    )
)

names(modelInfoList)
##i <- 1
##modelInfoList <- modelInfoList[i]
##names(modelInfoList)

nrep <- 1
nchains <- 1   #parameters set for a fast evaluation,
niter <- 10000
nburnin <- 500

set.seed(0)

## alternate, to reset back to initial values
## between each run:

runModel2 <- function(mi, niter, nburnin, nchains, nrep) {
    Rmodel <- nimbleModel(mi$code, mi$constants, mi$data, mi$inits)
    lp <- Rmodel$calculate()
    Rmodel$initializeInfo() ####
    if(is.na(lp) || is.nan(lp)) stop('model not initialized')
    conf <- configureMCMC(Rmodel, enableWAIC = TRUE)
    Rmcmc <- buildMCMC(conf)
    compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
    Cmodel <- compiledList$model
    Cmcmc <- compiledList$mcmc
    ##waics <- replicate(nrep,
    ##                   runMCMC(Cmcmc,
    ##                           niter=niter, nburnin=nburnin, nchains=nchains,
    ##                           progressBar = FALSE, samples = FALSE,
    ##                           summary = FALSE, WAIC = TRUE))
    waics <- numeric(nrep)
    for(i in 1:nrep) {
        Cmodel$setInits(mi$inits)
        waics[i] <- runMCMC(Cmcmc,
                            niter=niter, nburnin=nburnin, nchains=nchains,
                            progressBar = FALSE, samples = FALSE,
                            summary = FALSE, WAIC = TRUE)
    }
    return(waics)
}

waicList2 <- lapply(modelInfoList, runModel2, niter, nburnin, nchains, nrep)


names(modelInfoList)
i <- 3
mi <- modelInfoList[[i]]
code <- mi$code
constants <- mi$constants
data <- mi$data
inits <- mi$inits


Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
Rmodel$initializeInfo()

code
constants
data
inits
str(constants)
constants$Nfocal
constants$yob

Rmodel$lambda_obs


Rmodel$calculate()
Rmodel$calculate('p0')
Rmodel$calculate('a')
Rmodel$calculate('gamma')
Rmodel$calculate('c')
Rmodel$lambda
Rmodel$lambda_obs
Rmodel$a
Rmodel$f
Rmodel$calculate('sexPref')
Rmodel$calculate('ob')
Rmodel$calculate('Nsib')

lp <- Rmodel$calculate()
lp
Rmodel$initializeInfo() ####
if(is.na(lp) || is.nan(lp)) stop('model not initialized')
conf <- configureMCMC(Rmodel, enableWAIC = TRUE)
Rmcmc <- buildMCMC(conf)
compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model
Cmcmc <- compiledList$mcmc
##waics <- replicate(nrep,
##                   runMCMC(Cmcmc,
##                           niter=niter, nburnin=nburnin, nchains=nchains,
##                           progressBar = FALSE, samples = FALSE,
##                           summary = FALSE, WAIC = TRUE))
waics <- numeric(nrep)



## showing a user question how to use model$setData()

library(nimble)

N <- 10
y.first <- 1:N

code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:N) {
        y[i] ~ dnorm(mu, sd = sigma)
    }
})
constants <- list(N = N)
data <- list(y = y.first)
inits <- list(mu = 0, sigma = 1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

samples.first <- runMCMC(Cmcmc, 10000)
samplesSummary(samples.first)

y.second <- 2*(1:N)
Cmodel$setData(y = y.second)

samples.second <- runMCMC(Cmcmc, 10000)
samplesSummary(samples.second)





## fixing autoBlock, so it works with dirichlet, wishart nodes

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dgamma(1, 1)
    c ~ dunif(2, 10)
    d[1:3] ~ dmnorm(mu[1:3], cov = C[1:3,1:3])
    e[1:3,1:3] ~ dwish(R = C[1:3,1:3], df = 5)
    f[1:3,1:3] ~ dinvwish(R = C[1:3,1:3], df = 5)
    g[1:3] ~ ddirch(alpha[1:3])
    
})

constants <- list(mu=rep(0,3), C=diag(3)
                , alpha = rep(1,3)
                  )
data <- list()
U <- matrix(c(.2,2,4,0,1,1,0,0,4), nrow=3, byrow=TRUE)
eInit <- t(U) %*% U
inits <- list(a=0, b=1, c=5, d=rep(0,3)
            , e = eInit
            , f = eInit
            , g = rep(1/3, 3)
              )

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()

conf <- configureMCMC(Rmodel, autoBlock = TRUE, autoIt = 10000)




## Magnus NygÃ¥rd Osnes question on nimble users list.
## this model addresses covid-19 coronavirus cases.
## looks like it might be an infinite recursion in conjugacy checking.

library(nimble)

code <- nimbleCode({
    m ~ dgamma(0.5, 1)
    Rt <- mu         # growth rate
    Rm <- mu*exp(-m) # reduced growth rate
    ym ~ dexp(1.0/tau)      # seed the beginning of the epidemic
    tau <- 5.72
    prediction[1:N0] <- ym  # learn infected 6 first days
    for(i in (N0+1):Nend) {
        store_sum[i-N0,1] <- 0
        for(j in 2:i) {
            store_sum[i-N0,j] <- store_sum[i-N0,j-1]+prediction[j-1]
        }
        ##contribution[i] <- store_sum[i-N0,i-1]  ## THIS WORKS
        contribution[i] <- store_sum[i-N0,i]    ## THIS DOESN'T WORK
        prediction[i] <- Rt*(chooseR[i])*contribution[i] + Rm*(1-chooseR[i])*contribution[i]
    }
})

chooseR <- c(rep(1,18), rep(0,22))
constants <- list(N0 = 6, Nend = 41, chooseR = chooseR)
data <- list()
inits <- list(mu = 2.4, tau = 5.72, ym = 1/5.72, m = 1)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()   ## -3.346898

conf <- configureMCMC(Rmodel)


##

library(nimble)

code <- nimbleCode({
    m ~ dgamma(0.5, 1)
    Rt <- mu         # growth rate
    Rm <- mu*exp(-m) # reduced growth rate
    ##ym ~ dexp(1.0/tau)      # seed the beginning of the epidemic
    for(i in 1:N0) {
        prediction[i] <- ym  # learn infected 6 first days
    }
    for(i in (N0+1):Nend) {
        store_sum[i-N0,1] <- 0
        for(j in 2:i) {
            store_sum[i-N0,j] <- store_sum[i-N0,j-1]+prediction[j-1]
        }
        ##contribution[i] <- store_sum[i-N0,i-1]  ## THIS WORKS
        contribution[i] <- store_sum[i-N0,i]    ## THIS DOESN'T WORK
        prediction[i] <- Rt*(chooseR[i])*contribution[i] + Rm*(1-chooseR[i])*contribution[i]
    }
})

##chooseR <- c(rep(1,18), rep(0,22))
##constants <- list(N0 = 6, Nend = 41, chooseR = chooseR)
num1 <- 18
num2 <- 18
chooseR <- c(rep(1,num1), rep(0,num2))
constants <- list(N0 = 6, Nend = num1+num2, chooseR = chooseR)
data <- list()
inits <- list(mu = 2.4, ym = 1/5.72, m = 1)##, tau = 5.72)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()   ## -3.346898

debug(nimble:::conjugacyRelationshipsObject$checkConjugacy_new)
## takes forever: depPathsByNode <- lapply(nodeIDsFromOneDecl, model$getDependencyPaths)
## depPathsByNode <- lapply(nodeIDsFromOneDecl, model$getDependencyPaths)
## modelDef$maps$nimbleGraph$getDependencyPaths(node = node)
debug(model$getDependencyPaths)
debug(modelDef$maps$nimbleGraph$getDependencyPaths)

conf <- configureMCMC(Rmodel)





## Matthew Grainger's issue using nimbleEcology package
## GitHub issue #10 on nimbleEcology

library(nimble)
library(nimbleEcology)

len <- 5 # length of dataset
dat <- c(1,2,1,1,2) # A vector of observations
init <- c(0.4, 0.2, 0.4) # A vector of initial state probabilities

probObs <- t(array( # A matrix of observation probabilities
    c(1, 0,
      0, 1,
      0.2, 0.8), c(2, 3)))

probTrans <- t(array( # A matrix of transition probabilities
    c(0.6, 0.3, 0.1,
      0, 0.7, 0.3,
      0, 0, 1), c(3,3)))

# Define code for a nimbleModel
nc <- nimbleCode({
    x[1:5] ~ dHMM(init[1:3], probObs = probObs[1:3,1:2],   ## DT: modified
                  probTrans = probTrans[1:3, 1:3], len = 5)
    for (i in 1:3) {
        ##init[i] ~ dunif(0,1)   ## DT: removed line
        for (j in 1:3) {
            probTrans[i,j] ~ dunif(0,1)
        }
        probObs[i, 1] ~ dunif(0,1)
        probObs[i, 2] <- 1 - probObs[i, 1]   ## DT: modified
    }
})

# Build the model
HMM_model <- nimbleModel(nc,
                         data = list(x = dat),
                         inits = list(init = init,
                             probObs = probObs,
                             probTrans = probTrans))

# Calculate log probability of data from the model
HMM_model$calculate()   ## -5.872438




## 'lpRecorder' sampler, to record the "model global loglikelhood"
## for Floriane Plard.

library(nimble)

lpRecorder <- nimbleFunction(
    name = 'lpRecorder',
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        stochNodes <- model$getNodeNames(stochOnly = TRUE)
        stochNodes <- setdiff(stochNodes, 'lpSum')
    },
    run = function() {
        model[[target]] <<- model$getLogProb(stochNodes)
    },
    methods = list(
        reset = function() { }
    ), where = getLoadingNamespace()
)

code <- nimbleCode({
    a[1] ~ dnorm(0, 1)
    a[2] ~ dnorm(0, 1)
    for(i in 1:10) {
        y[i] ~ dnorm(a[1], sd = a[2]^2+1)
    }
    lpSum ~ dnorm(0, 1)
})
set.seed(0)
y <- rnorm(10, 3, 5)
constants <- list()
data <- list(y = y)
inits <- list(a = c(1,1), lpSum = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()   ## -78.7376

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$removeSamplers('lpSum')
conf$addSampler(target = 'lpSum', type = 'lpRecorder')
conf$printSamplers()   ## 'lpRecorder' sampler on lpSum node

## option to add monitors for other 'logProb' nodes:
##conf$addMonitors(c('logProb_a', 'logProb_y'))

conf$printMonitors()   ## lpSum already monitored by default

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 20)

## model global loglikelihood values:
samples[, 'lpSum']

## other testing:
##head(samples)
##lpCols <- grep('^logProb_', colnames(samples))
##head(samples[, lpCols])
##lpVals <- apply(samples[, lpCols], 1, sum)
## 
##samples[, 'lpSum'] - lpVals






## very light "testing" of the parameterTransform nimbleFunction
## the draft of this function is on branch "AD-parameterTransform"

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dgamma(1, 1)
    c ~ dunif(2, 10)
    d[1:3] ~ dmnorm(mu[1:3], cov = C[1:3,1:3])
    e[1:3,1:3] ~ dwish(R = C[1:3,1:3], df = 5)
})
constants <- list(mu=rep(0,3), C=diag(3))
data <- list()
U <- matrix(c(.2,2,4,0,1,1,0,0,4), nrow=3, byrow=TRUE)
eInit <- t(U) %*% U
inits <- list(a=0, b=1, c=5, d=rep(0,3), e=eInit)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

##Rmodel$e
##chol(Rmodel$e)
##chol(Rmodel$e) - U
##(eVec <- values(Rmodel, 'e'))
##mVal <- eVec
##(node <- Rmodel$expandNodeNames('e'))
##model <- Rmodel

##(nodes <- c('a'))
##(nodes <- c('a', 'b'))
##(nodes <- c('a', 'b', 'c'))
##(nodes <- c('a', 'b', 'c', 'd'))
(nodes <- c('a', 'b', 'c', 'd', 'e'))

pt <- parameterTransform(Rmodel, nodes)

Cmodel <- compileNimble(Rmodel)
Cpt <- compileNimble(pt, project = Rmodel)##, showCompilerOutput = TRUE)

theNF <- pt
theNF <- Cpt

theNF$ptNodeNFL$contentsList
theNF$nLength
theNF$tLength
theNF$nInd
theNF$tInd

theNF$run()
theNF$getOriginalLength()
theNF$getTransformedLength()
(vals <- values(Rmodel, nodes))
(tVals <- theNF$transform(vals))
(vals2 <- theNF$inverseTransform(tVals))
vals - vals2
theNF$calcLogDetJacobian(tVals)



## drafted code for the "coding club" presentation
## at UW (March 2020), which never happened....

library(nimble)
library(basicMCMCplots)

set.seed(0)
N <- 100
p <- 5
X <- array(0, c(N,p))
X[,1] <- 1
X[,2:p] <- array(rnorm(N*(p-1)), c(N,p-1))
beta <- c(40, 20, -10, 14, -3)
sigma <- 50
y <- rnorm(N, X %*% beta, sigma)

code <- nimbleCode({
    sigma ~ dunif(0, 1000)
    for(i in 1:p) {
        beta[i] ~ dnorm(0, sd = 10000)
    }
    for(i in 1:N) {
        y[i] ~ dnorm(sum(X[i,1:p]*beta[1:p]), sd = sigma)
    }
})
constants <- list(N = N, p = p , X = X)
data <- list(y = y)
inits <- list(beta = rep(0,p), sigma = 1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel, useConjugacy = FALSE)
conf$printSamplers()
conf$printMonitors()

conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(Rmodel)

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

samples <- runMCMC(Cmcmc, 1000)

dim(samples)
colnames(samples)
head(samples, 20)
samplesSummary(samples)
samplesPlot(samples)
samplesPlot(samples, ind = 1:500)
samplesPlot(samples, ind = 1:100)

initsFunction <- function() {
    list(beta = rnorm(p, 0, 1000),
         sigma = runif(1, 0, 100))
}


nimbleOptions('MCMCuseConjugacy')
nimbleOptions(MCMCuseConjugacy = TRUE)
nimbleOptions(MCMCuseConjugacy = FALSE)

configureMCMC(Rmodel)
configureMCMC(Rmodel, useConjugacy = TRUE)
configureMCMC(Rmodel, useConjugacy = FALSE)

conf <- configureMCMC(Rmodel)
conf <- configureMCMC(Rmodel, useConjugacy = TRUE)
conf <- configureMCMC(Rmodel, useConjugacy = FALSE)






## checking that niter >= nburnin, in compiled execution
## to not crash R

library(nimble)

code <- nimbleCode({a ~ dnorm(0, 1)})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Rmcmc <- buildMCMC(Rmodel)
compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

samples <- runMCMC(Cmcmc, niter = 20, nburnin = 0)
samples

samples <- runMCMC(Cmcmc, niter = 20, nburnin = 18)
samples

samples <- runMCMC(Cmcmc, niter = 20, nburnin = 19)
samples

samples <- runMCMC(Cmcmc, niter = 20, nburnin = 20)
samples

samples <- runMCMC(Cmcmc, niter = 0, nburnin = 0)
samples

samples <- runMCMC(Cmcmc, niter = 20, nburnin = 21)
samples <- runMCMC(Cmcmc, niter = -2, nburnin = 0)
samples <- runMCMC(Cmcmc, niter = 0)
samples <- runMCMC(Cmcmc, niter = -1)
samples <- runMCMC(Cmcmc, niter = -10)
samples <- runMCMC(Cmcmc, niter = -10, nburnin = -20)


## plotting code for AB

library(ggplot2)
integrand <- function(mu, tau) mu*exp(-tau/mu)
mubar <- function(tau) -tau / log(2 * integrate(integrand, 0, 1, tau)$value)
x <- seq(1E-6, 200, length = 100)
y <- sapply(x, mubar)
ggplot(data.frame(tau=x, mubar=y), aes(x=tau, y=mubar)) + geom_line()



## example of the non-generality of BNP
## that makes it hard to use for CR methods.
## putting this example together for Chris and Claudia

library(nimble)

code <- nimbleCode({
    xi[1:N] ~ dCRP(conc = 1, size = N)
    for(i in 1:N) {
        p[i] ~ dunif(0, 1)
        x[i,1] <- 1
        y[i,1] <- 1
        for(t in 2:T) {
            x[i,t] ~ dbern(x[i,t-1] * 0.5)
            y[i,t] ~ dbern(x[i,t] * p[xi[i]])
        }
    }
})

N <- 10
T <- 5
y <- array(1, c(N, T))
constants <- list(N = N, T = T)
data <- list(y = y)
inits <- list(xi = rep(1,N), p = rep(0.5,N), x = array(1, c(N,T)))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
## [1] -57.75436

conf <- configureMCMC(Rmodel)
conf$printSamplers(byType = TRUE)
## binary sampler (40)
##   - x[]  (40 elements)
## CRP_cluster_wrapper sampler (10)
##   - p[]  (10 elements)
## CRP sampler (1)
##   - xi[1:10] 

Rmcmc <- buildMCMC(conf)
## this error *goes away* on github branch 'bnp_moreGeneral2':
## Error in samplerFunction(model = model, mvSaved = mvSaved, target = target,  :
##   sampler_CRP: At the moment, NIMBLE can only sample when there is one variable being clustered for each cluster membershipID.

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
table(samples[,grep('^xi', colnames(samples))])

library(basicMCMCplots)
samplesPlot(samples, 'p')
samplesPlot(samples, Rmodel$expandNodeNames('p[1:3]'))

## figuring out Vincent Landau's problem,
## why the discrete z is sampling with non-integer values
## using dinterval and a custom distribution

##library(devtools)
##devtools::install_gitlab("actionable-phenology/tempo")
library(tempo)
library(abind)
library(doParallel)

data <- tempo_simulate(n = 50,
                       t = 200, 
                       censor_intensity = 2,
                       beta_mu = c(-6, 1, 0.1))

str(data)

##### Function to run sampler #####
tempo_mcmc <- function(y,
                       covariates,
                       beta_start,
                       n_iter = 1000,
                       n_burnin = 1000,
                       n_chains = 3,
                       n_workers = 1,
                       pp_checks = c("mean", "variance"),
                       beta_prior_mean = NULL,
                       beta_prior_sd = NULL,
                       random_effects = NULL,
                       group_ids = NULL,
                       sd_eps_start = NULL,
                       correlated = TRUE,
                       monitor_random_effects = FALSE
                       ) {
    if (is.null(beta_prior_mean)) {
        beta_prior_mean <- rep(0, length(covariates) + 1)
    }
    if (is.null(beta_prior_sd)) {
        beta_prior_sd <- rep(1.5, length(covariates) + 1)
    }
    ## append interecept term to covariates
    covariates <- c(list(intercept = array(1, dim = dim(covariates[[1]]))),
                    covariates)
    ## get indices of random effects
    if (!is.null(random_effects)) { # TODO double check influence of order of random_effects elements
        beta_random_indices <- seq_along(covariates)[names(covariates)
                                                     %in% random_effects]
    } else {
        beta_random_indices <- NULL
    }
    
    ## convert covariates to array
    covariate_array <- do.call(abind, c(covariates, along = 3))
    dimnames(covariate_array) <- list(rownames(covariates[[2]]),
                                      NULL,
                                      names(covariates))
    ## Convert NAs in y to correct numbers
    y$c1[is.na(y$c1)] <- 0
    y$c2[is.na(y$c2)] <- dim(covariate_array)[2] + 1
    
    ## get model inputs
    inputs <- tempo:::nimble_inputs(y, covariate_array, beta_prior_mean, beta_prior_sd,
                                    beta_start, beta_random_indices, group_ids,
                                    sd_eps_start, correlated, pp_checks) 
    
    if (is.null(random_effects)) {
        monitors <- c("Beta")
    } else if (length(beta_random_indices) == 1 | !correlated) {
        monitors <- c("mean_beta", "sd_eps")
        if (monitor_random_effects) {
            monitors <- c(monitors, "eps") 
        }
    } else {
        monitors <- c("mean_beta", "sd_eps", "rho")
        if (monitor_random_effects) {
            monitors <- c(monitors, "eps") 
        }
    }
    monitors <- c(monitors, "z")
    ##suppressWarnings(registerDistributions(list(
    ##    dtvgeom_nimble = list(
    ##        BUGSdist = "dtvgeom_nimble(prob)",
    ##        pqAvail = TRUE,
    ##        discrete = TRUE,
    ##        range = c(1, Inf),
    ##        types = c("prob = double(1)")
    ##    )), verbose = F, userEnv = .GlobalEnv))
    ## Parallel or serial processing
    if (n_workers > 1) {
        cat(
            sprintf(
                "Starting up nimble to run %s chain(s) in parallel on %s workers. Progress bars not shown in parallel mode...\n",
                n_chains,
                n_workers)
        )
        message("Compiling models and running samplers in parallel. This may take a while...")
        cl <- makeCluster(n_workers)
        registerDoParallel(cl)
        out <- foreach(i = 1:n_chains,
                       .combine = list,
                       .packages = c("nimble", "tempo"),
                       .multicombine = TRUE) %dopar% {
                           pheno_model <- suppressMessages(
                               nimbleModel(code = eval(parse(text = tempo:::nimble_model(beta_random_indices,
                                                                 correlated))),
                                           name = "pheno",
                                           constants = inputs$pheno_consts,
                                           data = inputs$pheno_data,
                                           inits = inputs$pheno_inits # TODO: [[i]] # different inits for each chain
                                           )
                           )
                           suppressWarnings(registerDistributions(list(
                               dtvgeom_nimble = list(
                                   BUGSdist = "dtvgeom_nimble(prob)",
                                   pqAvail = TRUE,
                                   discrete = TRUE,
                                   range = c(1, Inf),
                                   types = c("prob = double(1)")
                               )), verbose = F, userEnv = .GlobalEnv))
                           mcmc_cfg <- configureMCMC(pheno_model,
                                                     monitors = monitors)
                           samp <- mcmc_cfg$getSamplers()
                           pheno_mcmc <- buildMCMC(mcmc_cfg)
                           compiled_mcmc <- suppressMessages(compileNimble(pheno_model,
                                                                           pheno_mcmc))
                           out_temp <- runMCMC(compiled_mcmc$pheno_mcmc,
                                               niter = n_iter + n_burnin,
                                               nburnin = n_burnin)
                           return(list(out_temp, samp))
                       }
        stopCluster(cl)
        names(out) <- paste0("chain_", seq_len(n_chains))
    } else {
        cat(sprintf("Starting up nimble to run %s chain(s) in serial\n",
                    n_chains))
        message("Building and compiling model. This may take a few minutes...")
        pheno_model <- suppressMessages(
            nimbleModel(code = eval(parse(text = tempo:::nimble_model(beta_random_indices,
                                              correlated))),
                        name = "pheno",
                        constants = inputs$pheno_consts,
                        data = inputs$pheno_data,
                        inits = inputs$pheno_inits
                        )
        )
        mcmc_cfg <- configureMCMC(pheno_model,
                                  monitors = monitors)
        pheno_mcmc <- buildMCMC(mcmc_cfg)
        compiled_mcmc <- suppressMessages(compileNimble(pheno_model, pheno_mcmc))
        out <- runMCMC(compiled_mcmc$pheno_mcmc,
                       niter = n_iter + n_burnin,
                       nburnin = n_burnin,
                       nchains = n_chains)
        if(n_chains == 1) {
            out <- list(out)
        }
        names(out) <- paste0("chain_", seq_len(n_chains))
    }
    out
}

## Problem shows up when run in parallel (n_workers > 1)
draws <- tempo_mcmc(
    y = data$y,
    covariates = data$covariates,
    beta_start = c(-5, 0, 0),
    n_iter = 100,
    n_burnin = 10,
    n_workers = 2,
    n_chains = 3,
    pp_checks = NA
)

##str(draws)
class(draws)
length(draws)

i <- 3
class(draws[[i]])
length(draws[[i]])
names(draws[[i]])

class(draws[[i]][[1]])
dim(draws[[i]][[1]])
dimnames(draws[[i]][[1]])
draws[[i]][[1]][1:100,4:50]

class(draws[[i]][[2]])
length(draws[[i]][[2]])
names(draws[[i]][[2]])

for(kk in 1:103) print(draws[[i]][[2]][[kk]]$toStr())

y = data$y
covariates = data$covariates
beta_start = c(-5, 0, 0)
n_iter = 100
n_burnin = 10
n_workers = 2
n_chains = 3
pp_checks = NA
beta_prior_mean = NULL
beta_prior_sd = NULL
random_effects = NULL
group_ids = NULL
sd_eps_start = NULL
correlated = TRUE
monitor_random_effects = FALSE

draws[[1]][,7]

## No issue when run in serial (n_workers = 1)
draws2 <- tempo_mcmc(
    y = data$y,
    covariates = data$covariates,
    beta_start = c(-5, 0, 0),
    n_iter = 100,
    n_burnin = 10,
    n_workers = 1,
    n_chains = 3,
    pp_checks = NA
)
draws2[[1]][,7]



## nimbleLists work and experimenting
## done by David Pleydell

#################
## Toy Example ##
#################

library(nimble)

toyCode <- nimbleCode({
    rate[1,1] <- state[1,1] 
    rate[2,1] <- state[1,1] + state[2,1] + state[3,1] 
    rate[3,1] <- state[1,1] + state[2,1] 
    rate[1,2] <- state[1,1] + state[2,2] + state[2,3] 
    rate[2,2] <- state[1,1] + state[3,3] 
    rate[3,2] <- state[1,1] + sum(state[3,])
})

inits    <- list(state = matrix(1:9, nrow=3))
toyModel <- nimbleModel(toyCode, inits=inits)
toyModel[["state"]]
toyModel[["rate"]]

(stateIndices <- as.vector(outer(1:dim(toyModel[["state"]])[1], 1:dim(toyModel[["state"]])[2], paste, sep=",")))
(stateNodes   <- paste0("state[",stateIndices,"]"))
(nStateNodes  <- length(stateNodes))
(stateIndices <- paste0("row",sub(",","col",stateIndices)))

(rateIndices <- as.vector(outer(1:dim(toyModel[["rate"]])[1], 1:dim(toyModel[["rate"]])[2], paste, sep=",")))
(rateNodes   <- paste0("rate[",rateIndices,"]"))
(nRateNodes  <- length(rateNodes))

## Define list structures
innerList <- nimbleList(n = integer(0), txt = character(1))
(outerDef  <- paste0("nimbleList(",paste0(stateIndices, "=innerList()", collapse=","),")"))
outerList <- eval(parse(text=outerDef))

## Create outer list
dependentRatesList <- outerList$new()

## Create inner list & fill outer list
for (iSN in 1:nStateNodes) { ## iSN=2
    (deps <- toyModel$getDependencies(stateNodes[iSN]))
    (deps <- deps[grep("rate", deps)])
    eval(parse(text=paste0("dependentRatesList$",stateIndices[iSN], " <- innerList$new(n=", length(deps),", txt=deps)"))) 
}

print(dependentRatesList)



## initial work on making a French-to-English
## vocab testing program

## I kinda got stuck with the French character encodings ....

library(stringr)
words <- readLines('~/Downloads/French vocab.txt')

replaceSet <- c('Ã¢' = 'a', 'Ã€' = 'A',
                'Ã©' = 'e', 'Ã‰' = 'E',
                'Ã¨' = 'e',
                'Ãª' = 'e',
                'Ã®' = 'i',
                'Ã¯' = 'i',
                'Ã´' = 'o',
                'Ã»' = 'u',
                'Ã‡' = 'C')

wordsNew <- str_replace_all(words, replaceSet)

for(i in seq_along(replaceSet)) {
    wordsNew <- gsub(names(replaceSet)[i], replaceSet[i], wordsNew)
}


wordsNew <- gsub('\uFFFD', '\'', wordsNew)
wordsNew <- gsub('â€™', '\'', wordsNew)

head(wordsNew, 60)
ind <- which(sapply(strsplit(wordsNew, '-'), length) > 2)
ind
wordsNew[ind]
a <- wordsNew[ind][2]

aa <- strsplit(a, '')[[1]][2]

gsub("\uFFFD", "X", aa)


str_replace_all(words[2], replaceSet)

ww <- iconv(words, to = 'ASCII//TRANSLIT')



## error when using the variable name 'class'
## for a github issue

library(nimble)

code <- nimbleCode({
    class ~ dnorm(0, 1)
})

Rmodel <- nimbleModel(code)

Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)




## comparing WAIC waic of some models

library(nimble)

code <- nimbleCode({
    mu ~ dnorm(0, sd = 100)
    sigma ~ dunif(0, 1000)
    for(i in 1:N) {
        y[i] ~ dnorm(mu, sd = sigma)
    }
})
N <- 50
set.seed(0)
y <- rnorm(N, 5, 10)
constants <- list(N = N)
data <- list(y = y)
inits <- list(mu = 0, sigma = 1)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

waic1 <- nimbleMCMC(model = Rmodel,
                    niter = 10000, nburnin = 1000, nchains = 3,
                    samples = FALSE, WAIC = TRUE)
waic1


code <- nimbleCode({
    for(i in 1:N) {
        mu[i] ~ dnorm(0, sd = 1)
        sigma[i] ~ dunif(0, 1000)
        y[i] ~ dnorm(mu[i], sd = sigma[i])
    }
})
N <- 50
set.seed(0)
y <- rnorm(N, 5, 10)
constants <- list(N = N)
data <- list(y = y)
inits <- list(mu = rep(0,N), sigma = rep(1,N))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

waic2 <- nimbleMCMC(model = Rmodel,
                    niter = 10000, nburnin = 1000, nchains = 3,
                    samples = FALSE, WAIC = TRUE)

waic2



xs <- 1:20
plot(-1, -1, xlim = c(0, 21), ylim = c(0, 1))

a <- .1
ys <- numeric(20)
ys[1] <- 1
ys[2:20] <- a / (a + 2:20 - 1)
lines(xs, ys, type = 'b', col = 'green')



library(nimble)

Rnf <- nimbleFunction(
    run = function(x = double(1)) {  ## CHANGE
        out <- is.na.vec(x)
        ##out <- any_na(x)
        print(out)
        returnType(logical())
        return(out)
    }
)

Cnf <- compileNimble(Rnf)

set.seed(0)
x <- runif(10)
x[2] <- NA

is.na.vec <- nimble:::is.na.vec

Rnf(x)
Cnf(x)




require(stats); require(graphics)
n <- 10; nn <- 100
g <- factor(round(n * runif(n * nn)))
x <- rnorm(n * nn) + sqrt(as.numeric(g))
xg <- split(x, g)
boxplot(xg, col = "lavender", notch = TRUE, varwidth = TRUE)
sapply(xg, length)
sapply(xg, mean)

### Calculate 'z-scores' by group (standardize to mean zero, variance one)
z <- unsplit(lapply(split(x, g), scale), g)

# or

zz <- x
split(zz, g) <- lapply(split(x, g), scale)

# and check that the within-group std dev is indeed one
tapply(z, g, sd)
tapply(zz, g, sd)


### data frame variation

## Notice that assignment form is not used since a variable is being added

g <- airquality$Month
l <- split(airquality, g)
l <- lapply(l, transform, Oz.Z = scale(Ozone))
aq2 <- unsplit(l, g)
head(aq2)
with(aq2, tapply(Oz.Z,  Month, sd, na.rm = TRUE))


### Split a matrix into a list by columns
ma <- cbind(x = 1:10, y = (-4:5)^2)
split(ma, col(ma))

split(1:10, 1:2)




## testing of accumulatorSummary function



library(nimble)

code <- nimbleCode({
    ##for(i in 1:3) {
    ##    a[i] ~ dnorm(i*5, sd = 3)
    ##}
    b ~ dunif(0, 1)
    ##c ~ dnorm(1000, 1)
})
constants <- list()
data <- list()
inits <- list(b = 0.5)#a = c(1,1,1))#, b = 0.5, c = 1000)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel, accumulators = c('b'))
conf

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

samplesMean <- apply(samples, 2, mean)
samplesSD <- apply(samples, 2, sd)


accumulatorSummary(Cmcmc)
rbind(samplesMean, samplesSD)

accumulatorSummary(Cmcmc) - rbind(samplesMean, samplesSD)




## testing MCMC configuration printing

library(nimble)

code <- nimbleCode({
    for(i in 1:n) 
        y[i] ~ dnorm(b0 + inprod(beta[1:p], X[i, 1:p]), 1)
    for(i in 1:p) 
        beta[i] ~ dnorm(0, 1)
    b0 ~ dnorm(0, 1)
    xx ~ dbeta(1, 1)
    yy ~ dbern(xx)
})
constants <- list(n = 5, p = 3)
data <- list(y = rnorm(constants$n),
             X = matrix(rnorm(constants$n * constants$p), constants$n),
             yy = 1)
inits <- list(b0 = 1, beta = rnorm(constants$p), xx = 0.5)
Rmodel <- nimbleModel(code, data = data, constants = constants)

conf <- configureMCMC(Rmodel)
conf$addSampler('b0', 'RW', print = TRUE)
conf$addSampler('xx', 'RW', print = TRUE)
conf$addSampler('xx', 'slice', print = TRUE)

conf$printSamplers()

conf$addSampler('y', 'RW', scalarComponents = TRUE, print = TRUE)


conf

samplerConfs <- conf$samplerConfs
ind <- seq_along(conf$getSamplers())
model <- Rmodel



library(nimble)

##nimbleOptions('verbose')
##nimbleOptions(verbose = FALSE)
##nimbleOptions(verbose = TRUE)
##nimbleOptions('verbose')

code <- nimbleCode({
    for(i in 1:100) {
        a[i] ~ dnorm(0, 1)
    }
    for(i in 1:50) {
        b[i] ~ dbern(0.5)
    }
    y ~ dnorm(sum(a[1:25]) + sum(b[1:25]), 1)
    x1[1:5] ~ ddirch(alpha[1:5])
    x2[1:5] ~ ddirch(alpha[1:5])
    x3[1:5] ~ ddirch(alpha[1:5])
    p ~ dbern(0.5)
})
constants <- list()
data <- list(y = 0)
inits <- list(
    p = 0,
    a = rep(0, 100),
    b = rep(0, 50),
    alpha = rep(1,5),
    x1 = rep(0.2, 5),
    x2 = rep(0.2, 5),
    x3 = rep(0.2, 5)
)

Rmodel <- nimbleModel(code, constants, data, inits); Rmodel$calculate()

conf <- configureMCMC(Rmodel)

conf$addSampler(type = 'RW_block', target = c('a[1]'))
conf$addSampler(type = 'RW_block', target = c('a[2]'))
conf$addSampler(type = 'RW_block', target = c('b[1]'))
conf$addSampler(type = 'RW_block', target = 'p')
conf$addSampler(type = 'RW_block', target = c('a[1]', 'a[2]'))
conf$addSampler(type = 'RW_block', target = c('a[1:2]'))
conf$addSampler(type = 'RW_block', target = c('a[1:2]', 'a[1:2]'))

conf

conf$printSamplers(byType = TRUE)
conf$printSamplers()

conf$removeSamplers()

conf$printMonitors()

conf





## working on MCMC accumulators

##ac <- Cmcmc$getAccumulators()
##act <- t(ac)
##names <- c(Rmodel$expandNodeNames(c('x', 'y', 'v', 'arr')))
##colnames(act) <- c(names, 'N')
##act
##acmv <- as.matrix(Cmcmc$mvAccumulators)
##acmv <- acmv[, names]
##acmv
##act[, names] - acmv[, names]

Cmcmc$getAccumulatorIterations()


constants <- list()
data <- list()
inits <- list(x=0, y=0, v=rep(0,2), arr=array(0, c(2,3)))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()


nfDef <- nimbleFunction(
    setup = function(model, accumulators) {
        ##accumulators <- unique(removeIndexing(accumulators))
        ##nl_checkVarNamesInModel(model, accumulators)
        ##numAccumulators <- length(values(model, accumulators))
        ##if(numAccumulators < 1) stop('something wrong with MCMC accumulators')
        ##accumulatorValues <- array(0, c(numAccumulators+1, 2))
        accumulators <- unique(removeIndexing(accumulators))
        nl_checkVarNamesInModel(model, accumulators)
        modelSymbolObjects <- model$getSymbolTable()$getSymbolObjects()
        if(!all(accumulators %in% names(modelSymbolObjects))) stop('some accumulator names are not in the model symbol table; this should never occur')
        mvAccumulatorConf <- modelValuesConf(symbolTable(symbols = modelSymbolObjects[accumulators]))
        mvAccumulators <- modelValues(mvAccumulatorConf)
        resize(mvAccumulators, 2)
        X <- 'x'
        Y <- 'y'
        V <- 'v'
        ARR <- 'arr'
    },
    run = function(niter = integer(0)) {
        for(iter in 1:niter) {
            model$x[1] <<- iter
            model$y[1] <<- iter
            for(i in 1:2) {
                model$v[i] <<- iter*i
                for(j in 1:3) {
                    model$arr[i,j] <<- iter*i + 100*iter*j
                }
            }
            ##accumulatorValues[1:numAccumulators, 1] <<- accumulatorValues[1:numAccumulators, 1] + values(model, accumulators)
            ##accumulatorValues[1:numAccumulators, 2] <<- accumulatorValues[1:numAccumulators, 2] + values(model, accumulators)^2
            ##accumulatorValues[numAccumulators+1, 1] <<- accumulatorValues[numAccumulators+1, 1] + 1
        }
        mvAccumulators[X, 1][1] <<- model[[X]]
        mvAccumulators[X, 2][1] <<- model[[X]]^2
        ##
        mvAccumulators[Y, 1][1] <<- model[[Y]]
        mvAccumulators[Y, 2][1] <<- model[[Y]]^2
        ##
        mvAccumulators[V, 1] <<- model[[V]]
        mvAccumulators[V, 2] <<- model[[V]]^2
        ##
        mvAccumulators[ARR, 1] <<- model[[ARR]]
        mvAccumulators[ARR, 2] <<- model[[ARR]]^2
    }
    ##methods = list(
    ##    getAccumulators = function() {
    ##        returnType(double(2))
    ##        return(accumulatorValues)
    ##    }
    ##)
)

accumulators <- c('x')
accumulators <- c('x', 'y')
accumulators <- c('x', 'y', 'v')
accumulators <- c('v')
accumulators <- c('arr')
accumulators <- c('x', 'y', 'v', 'arr')

Rnf <- nfDef(Rmodel, accumulators = accumulators)

Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(Rnf, project = Rmodel)

as.matrix(Rnf$mvAccumulators)
as.matrix(Cnf$mvAccumulators)

iter <- 5
Rnf$run(iter)
Cnf$run(iter)
##Rnf$getAccumulators()
##Cnf$getAccumulators()

as.matrix(Rnf$mvAccumulators)
as.matrix(Cnf$mvAccumulators)

Rnf$mvAccumulators[['x']]
Cnf$mvAccumulators[['x']]

Rnf$mvAccumulators[['y']]
Cnf$mvAccumulators[['y']]

Rnf$mvAccumulators[['v']]
Cnf$mvAccumulators[['v']]

Rnf$mvAccumulators[['arr']]
Cnf$mvAccumulators[['arr']]




## reproducible example for
## modelValues, for storing *scalar* valued model variables

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})

Rmodel <- nimbleModel(code, inits = list(a = 10))

nfDef <- nimbleFunction(
    setup = function(model) {
        modelSymbolObjects <- model$getSymbolTable()$getSymbolObjects()
        mvConf <- modelValuesConf(symbolTable(symbols = modelSymbolObjects['a']))
        mv <- modelValues(mvConf)
        resize(mv, 1)
        aVar <- 'a'
    },
    run = function(niter = integer(0)) {
        for(iter in 1:niter) {
            model$a[1] <<- iter
        }
        mv[aVar, 1] <<- model[[aVar]]
    }
)

Rnf <- nfDef(Rmodel)

Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(Rnf, project = Rmodel)

as.matrix(Rnf$mv)
##       a
## [1,] NA

as.matrix(Cnf$mv)
##       a
## [1,] NA

Rnf$run(10)
Cnf$run(10)

as.matrix(Rnf$mv)
##       a
## [1,] 10

as.matrix(Cnf$mv)   ## warning, and doesn't actually work
## Matrix copying not allowed for given indices
##        a
## [1,] 0.1


## another example of error


library(nimble)

code <- nimbleCode({
    x ~ dnorm(0, 1)
})

Rmodel <- nimbleModel(code, inits = list(x = 0))

nfDef <- nimbleFunction(
    setup = function(model) {
        modelSymbolObjects <- model$getSymbolTable()$getSymbolObjects()
        mvAccumulatorConf <- modelValuesConf(symbolTable(symbols = modelSymbolObjects['x']))
        mv <- modelValues(mvAccumulatorConf)
        resize(mv, 2)
        Xvar <- 'x'
    },
    run = function(niter = integer(0)) {
        for(iter in 1:niter) {
            model$x[1] <<- iter
        }
        mv[Xvar, 1] <<- model[[Xvar]]
        mv[Xvar, 2] <<- model[[Xvar]]^2
    }
)

Rnf <- nfDef(Rmodel)

Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(Rnf, project = Rmodel)

as.matrix(Rnf$mv)
as.matrix(Cnf$mv)

iter <- 3
Rnf$run(iter)
Cnf$run(iter)

as.matrix(Rnf$mv)
as.matrix(Cnf$mv)

Rnf$mv[['x']]
Cnf$mv[['x']]





## my own testing of RJMCMC

library(nimble)
library(testthat)

source('~/github/nimble/nimble/packages/nimble/R/MCMC_RJ2.R')

if(TRUE) {
    ##
    code <- nimbleCode({
        beta0 ~ dnorm(0, sd = 100)
        sigma ~ dunif(0, 100) 
        psi ~ dunif(0, 1)
        for(i in 1:2) {
            beta[i] ~ dnorm(0, sd = 100)
            z[i] ~ dbern(psi)
        }
        ##beta1 ~ dnorm(0, sd = 100)
        ##beta2 ~ dnorm(0, sd = 100)
        ##z1 ~ dbern(psi)
        ##z2 ~ dbern(psi)
        for(i in 1:N) {
            Ypred[i] <- beta0 + beta[1] * z[1] * x1[i] + beta[2] * z[2] * x2[i]
            ##Ypred[i] <- beta0 + beta1 * z1 * x1[i] + beta2 * z2 * x2[i]
            Y[i] ~ dnorm(Ypred[i], sd = sigma)
        }
    })
    ##
    ## simulate some data
    set.seed(1)
    N <- 100
    x1 <- runif(N, -1, 1)
    x2 <- runif(N, -1, 1) ## this covariate is not included
    Y <- rnorm(N, 1 + 2.5 * x1, sd = 1)
    ##
    ## build the model
    rIndicatorModel <- nimbleModel(code,
                                   constants = list(N = N),
                                   data = list(Y = Y, x1 = x1, x2 = x2), 
                                   inits = list(
                                       beta0 = 0, sigma = sd(Y),
                                       psi = 0.5,
                                       beta = c(0,0), z = c(1,1)))
    ##beta1=0, beta2=0, z1=0, z,0)))
    ##
    indicatorModelConf <- configureMCMC(rIndicatorModel)
    ##
    ## Add reversible jump  
    configureRJ2(conf = indicatorModelConf,
                 targetNodes = c("beta[1]", "beta[2]"),
                 ##targetNodes = "beta",
                 indicatorNodes = c("z[1]", "z[2]"),
                 ##indicatorNodes = "z",
                 control = list(mean = 0, scale = 2))
    ##
    ##indicatorModelConf$printSamplers()
    ##
    samps <- indicatorModelConf$getSamplers()
    expect_true(samps[[4]]$name == "RJ_indicator2")
    expect_true(samps[[5]]$name == "RJ_toggled2")
    expect_true(samps[[6]]$name == "RJ_indicator2")
    expect_true(samps[[7]]$name == "RJ_toggled2")
    ##
    indicatorModelConf$addMonitors("beta", "z", print = FALSE)
    ##
    rIndicatorMCMC <- buildMCMC(indicatorModelConf)
    ##
    ##cIndicatorModel <- compileNimble(rIndicatorModel)
    ##cIndicatorMCMC <- compileNimble(rIndicatorMCMC, project = rIndicatorModel)
    compiledList <- compileNimble(list(model=rIndicatorModel, mcmc=rIndicatorMCMC))
    cIndicatorModel <- compiledList$model
    cIndicatorMCMC <- compiledList$mcmc
    ##
    set.seed(1)
    samples <- runMCMC(cIndicatorMCMC, 10000, nburnin = 6000, progressBar = FALSE)
    ##
    ## posterior probability to be included in the mode
    expect_equal(mean(samples[ , "z[1]"]), 1)
    expect_equal(mean(samples[ , "z[2]"]), 0.003)
    ##
    ## posterior means when in the model
    expect_equal(mean(samples[ , "beta[1]"][samples[ , "z[1]"] != 0]), 2.464274, tol=10^-6)
    expect_equal(mean(samples[ , "beta[2]"][samples[ , "z[2]"] != 0]), 0.04106177, tol=10^-8)
    ##
    ## Linear regression with intercept and two covariates, without indicator variables
    ##
    code <- nimbleCode({
        beta0 ~ dnorm(0, sd = 100)
        beta1 ~ dnorm(0, sd = 100)
        beta2 ~ dnorm(0, sd = 100)
        sigma ~ dunif(0, 100)
        for(i in 1:N) {
            Ypred[i] <- beta0 + beta1 * x1[i] + beta2 * x2[i]
            Y[i] ~ dnorm(Ypred[i], sd = sigma)
        }
    })
    ##
    rNoIndicatorModel <- nimbleModel(code,
                                     constants = list(N = N),
                                     data = list(Y = Y, x1 = x1, x2 = x2), 
                                     inits = list(beta0 = 0, beta1 = 0, beta2 = 0, sigma = sd(Y)))
    ##
    noIndicatorModelConf <- configureMCMC(rNoIndicatorModel)
    ##
    ## Add reversible jump  
    configureRJ2(conf = noIndicatorModelConf,   ## model configuration
                 targetNodes = c("beta1", "beta2"), ## coefficients for selection   
                 priorProb = 0.5,                   ## prior probability of inclusion
                 control = list(mean = 0, scale = 2))
    ##
    ##noIndicatorModelConf$printSamplers()
    ##
    samps <- noIndicatorModelConf$getSamplers()
    expect_true(samps[[3]]$name == "RJ_prior2")
    expect_true(samps[[4]]$name == "RJ_toggled2")
    expect_true(samps[[5]]$name == "RJ_prior2")
    expect_true(samps[[6]]$name == "RJ_toggled2")
    ##
    ## add monitors
    noIndicatorModelConf$addMonitors("beta1", "beta2", print = FALSE)
    rNoIndicatorMCMC <- buildMCMC(noIndicatorModelConf) 
    ##
    ##cNoIndicatorModel <- compileNimble(rNoIndicatorModel)
    ##cNoIndicatorMCMC <- compileNimble(rNoIndicatorMCMC, project = rNoIndicatorModel)
    compiledList <- compileNimble(list(model=rNoIndicatorModel, mcmc=rNoIndicatorMCMC))
    cNoIndicatorModel <- compiledList$model
    cNoIndicatorMCMC <- compiledList$mcmc
    ##
    set.seed(1)
    samples <- runMCMC(cNoIndicatorMCMC, 10000, nburnin = 6000, progressBar = FALSE)
    ##
    ## posterior probability to be included in the mode
    expect_equal(mean(samples[ , "beta1"] != 0), 1)
    expect_equal(mean(samples[ , "beta2"] != 0), 0.0035)
    ##
    ## posterior means when in the model
    expect_equal(mean(samples[ , "beta1"][samples[ , "beta1"] != 0]), 2.467329, tol=10^-6)
    expect_equal(mean(samples[ , "beta2"][samples[ , "beta2"] != 0]),  -0.003701137, tol=10^-9)
    ##
}










## error during compilation (see below)
cf <- compileNimble(f)

out <- cf(x[1:3], param)

expect_identical(out, c(d, p, q), info = 'dt_nonstandard nf')

printErrors()

In file included from P_1_rcFun_R_GlobalEnv4.cpp:6:

    In file included from
/Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble/include/nimble/EigenTypedefs.h:33:

    /Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble/include/nimble/nimbleEigen.h:1587:1:
        error: reference member 'Arg6' binds to a temporary object whose
lifetime would be shorter than the lifetime of the constructed object

MAKE_RECYCLING_RULE_CLASS4_2scalar(pt_nonstandard, double)

^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble/include/nimble/nimbleEigen.h:1502:58:
    note: expanded from macro 'MAKE_RECYCLING_RULE_CLASS4_2scalar'

Arg1(A1), Arg2(A2), Arg3(A3), Arg4(A4), Arg5(A5), Arg6(6)             \

^

/Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble/include/nimble/nimbleEigen.h:1587:1:
    note: in instantiation of member function
'pt_nonstandardRecyclingRuleClass<long,
Eigen::Map<Eigen::Matrix<double, -1, -1, 0, -1, -1>, 0,
Eigen::Stride<-1, -1> >, int, Eigen::Map<Eigen::Matrix<double, -1, -1,
0, -1, -1>, 0, Eigen::Stride<-1, -1> >, double, bool,
bool>::pt_nonstandardRecyclingRuleClass' requested here

/Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble/include/nimble/nimbleEigen.h:1545:108:
    note: expanded from macro 'MAKE_RECYCLING_RULE_CLASS4_2scalar'

FUNNAME ## RecyclingRuleClass<IndexReturn, Derived1, Derived2,
Derived3, Derived4, Derived5, Derived6> obj(A1, A2, A3, A4, A5, A6); \


^

P_1_rcFun_R_GlobalEnv4.cpp:38:44: note: in instantiation of function
template specialization 'pt_nonstandard_RR_impl<Eigen::Matrix<double,
-1, -1, 0, -1, -1>
>::pt_nonstandard_RecyclingRule<Eigen::Map<Eigen::Matrix<double, -1,
-1, 0, -1, -1>, 0, Eigen::Stride<-1, -1> >, int,
Eigen::Map<Eigen::Matrix<double, -1, -1, 0, -1, -1>, 0,
Eigen::Stride<-1, -1> >, double, bool, bool>' requested here

Eig_pp = pt_nonstandard_RR_impl<MatrixXd>::pt_nonstandard_RecyclingRule(Eig_ARG1_x_Interm_4,
    3, Eig_ARG2_theta_Interm_5, 3.5, true, false);

^

/Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble/include/nimble/nimbleEigen.h:1587:1:
    note: reference member declared here

MAKE_RECYCLING_RULE_CLASS4_2scalar(pt_nonstandard, double)

^

/Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble/include/nimble/nimbleEigen.h:1499:20:
    note: expanded from macro 'MAKE_RECYCLING_RULE_CLASS4_2scalar'

const DerivedA6 &Arg6;\

^

/Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble/include/nimble/nimbleEigen.h:1605:1:
    error: reference member 'Arg6' binds to a temporary object whose
lifetime would be shorter than the lifetime of the constructed object

MAKE_RECYCLING_RULE_CLASS4_2scalar(qt_nonstandard, double)

^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble/include/nimble/nimbleEigen.h:1502:58:
    note: expanded from macro 'MAKE_RECYCLING_RULE_CLASS4_2scalar'

Arg1(A1), Arg2(A2), Arg3(A3), Arg4(A4), Arg5(A5), Arg6(6)             \

^

/Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble/include/nimble/nimbleEigen.h:1605:1:
    note: in instantiation of member function
'qt_nonstandardRecyclingRuleClass<long,
Eigen::Map<Eigen::Matrix<double, -1, -1, 0, -1, -1>, 0,
Eigen::Stride<0, 0> >, int, Eigen::Map<Eigen::Matrix<double, -1, -1,
0, -1, -1>, 0, Eigen::Stride<-1, -1> >, double, bool,
bool>::qt_nonstandardRecyclingRuleClass' requested here

/Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble/include/nimble/nimbleEigen.h:1545:108:
    note: expanded from macro 'MAKE_RECYCLING_RULE_CLASS4_2scalar'

FUNNAME ## RecyclingRuleClass<IndexReturn, Derived1, Derived2,
Derived3, Derived4, Derived5, Derived6> obj(A1, A2, A3, A4, A5, A6); \


^

P_1_rcFun_R_GlobalEnv4.cpp:43:44: note: in instantiation of function
template specialization 'qt_nonstandard_RR_impl<Eigen::Matrix<double,
-1, -1, 0, -1, -1>
>::qt_nonstandard_RecyclingRule<Eigen::Map<Eigen::Matrix<double, -1,
-1, 0, -1, -1>, 0, Eigen::Stride<0, 0> >, int,
Eigen::Map<Eigen::Matrix<double, -1, -1, 0, -1, -1>, 0,
Eigen::Stride<-1, -1> >, double, bool, bool>' requested here

Eig_qq = qt_nonstandard_RR_impl<MatrixXd>::qt_nonstandard_RecyclingRule(Eig_pp,
    3, Eig_ARG2_theta_Interm_6, 3.5, true, false);

^

/Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble/include/nimble/nimbleEigen.h:1605:1:
    note: reference member declared here

MAKE_RECYCLING_RULE_CLASS4_2scalar(qt_nonstandard, double)

^

/Library/Frameworks/R.framework/Versions/3.6/Resources/library/nimble/include/nimble/nimbleEigen.h:1499:20:
    note: expanded from macro 'MAKE_RECYCLING_RULE_CLASS4_2scalar'

const DerivedA6 &Arg6;\

^

1 warning and 2 errors generated.

make: *** [P_1_rcFun_R_GlobalEnv4.o] Error 1




## experimenting with errors in model$expandNodeNames()

library(nimble)

code <- nimbleCode({
    for(i in 1:3)
        mu[i] ~ dnorm(0,1)
    for(i in 1:3)
        for(j in 1:3)
            theta[i,j] ~ dnorm(0,1)
})

m <- nimbleModel(code)

m$expandNodeNames('mu[4]')
## character(0)

m$expandNodeNames('theta[4,3]')

m$expandNodeNames('x')


## trying the reversible job RJMCMC examples
## from the User Manual

library(nimble)

## Linear regression with intercept and two covariates
code <- nimbleCode({
    beta0 ~ dnorm(0, sd = 100)
    beta1 ~ dnorm(0, sd = 100)
    beta2 ~ dnorm(0, sd = 100)
    sigma ~ dunif(0, 100) 
    ##
    z1 ~ dbern(psi)  ## indicator variable associated with beta1
    z2 ~ dbern(psi)  ## indicator variable associated with beta2
    psi ~ dbeta(1, 1) ## hyperprior on inclusion probability
    for(i in 1:N) {
        Ypred[i] <- beta0 + beta1 * z1 * x1[i] + beta2 * z2 * x2[i]
        Y[i] ~ dnorm(Ypred[i], sd = sigma)
    }
})

## simulate some data
set.seed(1)
N <- 100
x1 <- runif(N, -1, 1)
x2 <- runif(N, -1, 1) ## this covariate is not included
Y <- rnorm(N, 1.5 + 2 * x1, sd = 1)


## build the model and configure default MCMC

rIndicatorModel <- nimbleModel(code, constants = list(N = N),
                               data = list(Y = Y, x1 = x1, x2 = x2), 
                               inits=  list(beta0 = 0, beta1 = 0, beta2 = 0, sigma = sd(Y),
                                   z2 = 1, z1 = 1, psi = 0.5))

rIndicatorModel$calculate()

indicatorModelConf <- configureMCMC(rIndicatorModel)

## print NIMBLE default samplers
indicatorModelConf$printSamplers()

## At this point we may want to modify the current configuration by adding a reversible jump sampler to allow for selection on `beta[1]` and `beta[2]` variables, which can be done by calling the `configureRJ` function.


debug(configureRJ)
undebug(configureRJ)

configureRJ(conf = indicatorModelConf,     
            targetNodes = c("beta1", "beta2"),
            indicatorNodes = c('z1', 'z2'),
            control = list(mean = c(1, 0), scale = 2))

## The `targetNodes` argument should indicate the nodes for which we want to do variable selection. `targetNodes` will be expanded as described in Section \@ref(sec:arbitr-coll-nodes). I.e., either `targetNodes = "beta"` or `targetNodes = c("beta[1]", "beta[2]")` will assign a RJ sampler to each of `beta[1]` and `beta[2]` if `beta` is a vector of two values.

## The same applies to `indicatorNodes`, which provides the indicator variables paired with nodes in `targetNodes`. Notice that `indicatorNodes` must be provided consistently with respect to `targetNodes`. E.g. if `targetNodes = "beta"` is a vector, then one should have `indicatorNodes = "z"` with `z` a vector as well; something like `indicatorNodes = c("z1", "z2")` will throw an error.


indicatorModelConf$printSamplers()

## An `RJ_indicator` sampler is assigned to `z[1]` and `z[2]` in place of the `binary` sampler, while the samplers for `beta[1]`and `beta[2]` have been changed to  `toggled` samplers, which still use the default `conjugate_dnorm_dnorm` sampler but only when the corresponding indicator variable is equal to $1$, thereby including the coefficient in the model. The two `RJ_indicator` samplers have different means for the proposal distribution but the same scale, as given in the `control` list of `configureRJ`.

## Notice that the order of the sampler has changed, since `configureRJ` calls `removeSampler` for nodes in `targetNodes` and `indicatorNodes`, and subsequently `addSampler`, which appends the sampler to the end of current sampler list. Order can be modified by using `setSamplers`.

## Also note that `configureRJ` modifies the existing sampler and returns `NULL`.

## Without indicator variables {#sec:rjmcmc-no-indicator}

## We consider the same regression setting, but writing the model without the use of indicator variables. 

## Linear regression with intercept and two covariates
code <- nimbleCode({
    beta0 ~ dnorm(0, sd = 100)
    beta1 ~ dnorm(0, sd = 100)
    beta2 ~ dnorm(0, sd = 100)
    sigma ~ dunif(0, 100)
    for(i in 1:N) {
        Ypred[i] <- beta0 + beta1 * x1[i] + beta2 * x2[i]
        Y[i] ~ dnorm(Ypred[i], sd = sigma)
    }
})

## build the model
rNoIndicatorModel <- nimbleModel(code, constants = list(N = N),
                                 data = list(Y = Y, x1 = x1, x2 = x2), 
                                 inits=  list(beta0 = 0, beta1 = 0, beta2 = 0, sigma = sd(Y)))

rNoIndicatorModel$calculate()

noIndicatorModelConf <- configureMCMC(rNoIndicatorModel)

## print NIMBLE default samplers
noIndicatorModelConf$printSamplers()

## In this case, since there are no indicator variables, we need to pass to `configureRJ` the prior inclusion probabilities for each node in `targetNodes`, by specifying either one common value or a vector of values for the argument `priorProb`. Notice that this case does not allow for a stochastic prior. 


configureRJ(conf = noIndicatorModelConf,     
            targetNodes = c("beta1", "beta2"),
            priorProb = 0.5,
            control = list(mean = 0, scale = 2, fixedValue = c(1.5, 0)))

## print samplers after configureRJ
noIndicatorModelConf$printSamplers()

## Since there are no indicator variables, the `RJ` sampler is assigned directly to `beta[1]` and `beta[2]` along with the `toggled` sampler. In addition in this case one can set the coefficient to a value different from $0$ via the `fixedValue` argument in the `control` list. 

## If `fixedValue` is given when using `indicatorNodes` the values provided in `fixedValue` are ignored.  However the same behavior can be obtained in this situation, using a different model specification. For example, the model in \@ref(sec:rjmcmc-indicator) can be modified to have `beta1` equal to $1.5$ when in the model as follows:

for(i in 1:N) {
    Ypred[i] <- beta0 + (1 - z1) * 1.5 * beta1 * x1[i] +
        z1 * beta1 * x1[i] + beta2 * z2 * x2[i]
    Y[i] ~ dnorm(Ypred[i], sd = sigma)
}





## testing new options for basicMCMCplots:
## handling "dropped" arrays to vectors
## only one "buffer" argumentx
## intelligent defaults for "buffer" and "jitter" arguments


library(nimble)


code <- nimbleCode({
    mu ~ dnorm(0, sd = 10)
    tau ~ dgamma(0.01, 0.01)
    sigma ~ dunif(0, 10)
    p ~ dunif(0, 1)
    for(i in 1:50) {
        x[i] ~ dnorm(0, 1)
    }
})

constants <- list()
data <- list()
inits <- list(mu = 0, tau = 1, sigma = 1, p = 0.5, x = rep(0, 50))
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)


set.seed(0)
samples <- runMCMC(Cmcmc, 10000)
samplesList <- runMCMC(Cmcmc, 1000, nchains = 3)
str(samples)
str(samplesList)

##debug(samplesPlot)
##samplesPlot(samples[, 1])

chainsSummary(samplesList)
chainsSummary(samplesList, buffer.left = 2)
chainsSummary(samplesList, buffer.left = 2, buffer.right = 1)
chainsSummary(samplesList, buffer.left = 2, buffer.r = 1)
chainsSummary(samplesList, buffer.left = 4, buffer = 1)
chainsSummary(samplesList, buffer = 4)


for(i in 1:16) {
    sl2 <- lapply(samplesList, function(el) el[, 1:i, drop = FALSE] )
    chainsSummary(sl2, buffer.right = 1)
}



## testing HMC sampler, with dynamically
## setting number of warmup iterations nwarmup

library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)

code <- nimbleCode({
    mu ~ dnorm(0, sd = 10)
    tau ~ dgamma(0.01, 0.01)
    sigma ~ dunif(0, 10)
    p ~ dunif(0, 1)
    mu4 <- mu * p
    y1 ~ dnorm(mu, tau)
    y2 ~ dnorm(mu, tau)
    y3 ~ dnorm(mu, sd = sigma)
    y4 ~ dnorm(mu4, tau)
})

constants <- list()
data <- list(y1 = 10, y2 = 9, y3 = 9, y4 = 7)
inits <- list(mu = 0, tau = 1, sigma = 1, p = 0.5)
Rmodel <- nimbleModel(code, constants, data, inits)
##conf <- configureMCMC(Rmodel, nodes = NULL)
##conf$addSampler(c('mu','tau','sigma','p'), 'HMC2')
conf <- configureMCMC(Rmodel)
conf$addSampler(c('mu','tau','sigma','p'), 'HMC', nwarmup = 49)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

set.seed(0); Cmodel$setInits(inits)
samples <- runMCMC(Cmcmc, 10000, nburnin = 0)
samples <- runMCMC(Cmcmc, 500, nburnin = 0, nchains = 3)
samples <- runMCMC(Cmcmc, 98, nburnin = 0, nchains = 3)

name <- 'warningsOrig'
name <- 'numDivergences'
name <- 'maxTreeDepth'
name <- 'numTimesMaxTreeDepth'
name <- 'nwarmupOrig'
name <- 'nwarmup'

i <- 1
i <- 5

valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[i]], name)

valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[i]], name, -1)
valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[i]], name, 3)
valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[i]], name, 4)
valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[i]], name, 5)
valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[i]], name, 6)
valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[i]], name, 10)
valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[i]], name, 50)
valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[i]], name, 100)


conf$printSamplers()
Rmcmc$samplerFunctions$contentsList
Rmcmc$samplerFunctionsHMC
Rmcmc$samplerFunctionsHMC$contentsList


Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)



conf$addSampler('mu', 'RW')
conf$addSampler('mu', 'slice')
conf$addSampler('mu', sampler_slice)
conf$addSampler('mu', 'sampler_RW')
conf$addSampler('mu', 'HMC')

conf$printSamplers()

conf$samplerConfs




## working on assignment of samplers
## to SCR model, for nimble user Colby Anton

library(nimble)

code <- nimbleCode({
    for(i in 1:M) {
        for(t in 1:Year) {
            ##activ.s[i,t] ~ dunif(1,  nPix)
            ##s.x[i,t] <-  myCalculation(Mask[,2],round(activ.s[i,t]))
            ##s.y[i,t] <-  myCalculation(Mask[,3],round(activ.s[i,t]))
            activ.s[i,t] ~ dcat(s.probs[1:nPix])
            s[i,t,1] <- myCalculation(Mask[,2], activ.s[i,t])
            s[i,t,2] <- myCalculation(Mask[,3], activ.s[i,t])
        }
    }
})

myCalculation <- nimbleFunction(
    run = function(grid = double(1), index = double()) {
        return(grid[index])
        returnType(double(0))
    })


M <- 10##180
Year <- 4
nPix <- 1000
s.probs <- rep(1/nPix, nPix)   ## uniform probabilities
Mask <- array(0, c(nPix, 3))   ## I know that 0's don't make sense here

constants <- list(M=M, Year=Year, nPix=nPix, s.probs=s.probs, Mask=Mask)

Rmodel <- nimbleModel(code, constants)  ## you probably want data and inits also

conf <- configureMCMC(Rmodel)

conf$printSamplers()

## if you see posterior_predictive samplers assigned,
## again it's because nothing in your model (in particular:
## no latent states, nor any data) *depend* on those
## nodes.  So the MCMC simply takes draws from their
## prior distribution to sample them, because in that
## case, prior = posterior.

## Anyway, let's remove whatever samplers were assigned,
## and I'd recommend either 'categorial' samplers,
## which could be slow but will sample well, or 'slice'
## samplers

## In response to this question:
## How do I keep this format when changing just the nodes activ.s[i,1] to a different sampler?
sNodes <- Rmodel$expandNodeNames(paste0('activ.s[1:',M,',1]'))
conf$removeSamplers(sNodes)
## this scalarComponents = TRUE is a very new feature,
## just added within the past month or so.  tbh I'm not
## sure if it made the most recent release or not (?)
## so let me know if it doesn't work:
conf$addSampler(target = sNodes, type = 'slice', scalarComponents = TRUE)

## to change samplers on *all* the 'activ.s' nodes,
## say to the other option of categorical samplers:
sNodesAll <- Rmodel$expandNodeNames('activ.s')
conf$removeSamplers(sNodesAll)
conf$addSampler(target = sNodesAll, type = 'categorical', scalarComponents = TRUE)

Rmcmc <- buildMCMC(conf)




## helping someone on nimble-users list
## with a ragged array structure

library(nimble)

m <- 5
ns <- c(1, 5, 10, 20, 3)   ## number of elements in each of the 'm' groups

x <- array(0, c(max(ns), m))   ## your actual data here

poisCode <- nimbleCode({
    ##
    ## Parameters with priors
    for (cell in 1:m) {
        lambda[cell] ~ dunif(0, 5000)
    }
    ##
    ## The data and its probability distributions
    for (cell in 1:m) {
        for (sample in 1:ns[cell]) {   ## changed index to 'cell', rather than 'm'
            x[sample, cell] ~ dpois(lambda = lambda[cell])
        }
    }
})

constants <- list(m = m, ns = ns)
data <- list(x = x)
inits <- list(lambda = rep(1, m))

Rmodel <- nimbleModel(poisCode, constants, data, inits)

Rmodel$calculate()
## -81.58597   ## looks good, model is fully initialized





## testing assignment of data into a node called "data"

library(nimble)

code <- nimbleCode({
    x ~ dnorm(0, 1)
    y ~ dnorm(0, 1)
    data ~ dnorm(0, 1)
    ##inits ~ dnorm(0, 1)
    ##a ~ dnorm(0, 1)
    ##b ~ dnorm(0, 1)
})

dataList <- list(x = 1, data = 2, y = 3)
##initsList <- list(a = 10, inits = 11, b = 12)

Rmodel <- nimbleModel(code, data = dataList)##, inits = initsList)

Rmodel$x
Rmodel$y
Rmodel$data

Rmodel$a
Rmodel$b
Rmodel$inits




## small logistic regression and glm()
## test case, for Justin Kitzes

set.seed(0)
N <- 1000
beta0 <- .3
beta <- .4
x <- rnorm(N)
eta <- beta0 + beta*x
p <- 1 / (1 + exp(-eta))
trials <- 10
y <- rbinom(N, size = trials, prob = p)


d <- data.frame(y = y, fail = trials-y, x = x, prop = y/trials, trials = trials)

glm(cbind(y,fail) ~ x, family = 'binomial', data = d)

glm(prop ~ x, family = 'binomial', weights = trials, data = d)






## testing code for new conf$printSamplers(byType = TRUE)
## now doing compression of scalar nodes


printSamplersByType <- function(ind) {
    if(length(ind) == 0) return(invisible(NULL))
    indent <- '  - '
    samplerTypes <- unlist(lapply(ind, function(i) samplerConfs[[i]]$name))
    uniqueSamplerTypes <- sort(unique(samplerTypes), decreasing = TRUE)
    nodesSortedBySamplerType <- lapply(uniqueSamplerTypes, function(type) sapply(samplerConfs[which(samplerTypes == type)], `[[`, 'target', simplify = FALSE))
    names(nodesSortedBySamplerType) <- uniqueSamplerTypes
    cat('\n')
    for(i in seq_along(nodesSortedBySamplerType)) {
        theseSampledNodes <- nodesSortedBySamplerType[[i]]
        cat(paste0(names(nodesSortedBySamplerType)[i], ' sampler (', length(theseSampledNodes), ')\n'))
        ##cat(paste0(theseSampledNodes, collapse = ', '))  ## prints all nodes
        anyMultivariate <- any(grepl(':', theseSampledNodes))
        anyLengthGTone <- any(lapply(theseSampledNodes, length) > 1)
        if(anyMultivariate || anyLengthGTone) {
            ## multivariate samplers:
            nodesCompressed <- sapply(theseSampledNodes, function(nns) if(length(nns)==1) nns else paste0('{ ', paste0(nns, collapse = ', '), ' }'))
            nodesCompressedIndent <- paste0(indent, nodesCompressed)
            cat(paste0(nodesCompressedIndent, collapse = '\n'))
        } else {
            ## univariate samplers:
            theseVars <- model$getVarNames(nodes = theseSampledNodes)
            nodesListByVar <- lapply(theseVars, function(var)
                unlist(theseSampledNodes[(theseSampledNodes == var) |
                                             grepl(paste0('^', var, '\\['), theseSampledNodes)]))
            if(length(unlist(nodesListByVar)) != length(theseSampledNodes)) stop('something went wrong')
            for(j in seq_along(nodesListByVar)) {
                theseNodes <- nodesListByVar[[j]]
                isIndexed <- grepl("\\[", theseNodes[1])
                if(isIndexed) {
                    numElements <- length(theseNodes)
                    sTag <- ifelse(numElements>1, 's', '')
                    cat(paste0(indent, theseVars[j], '[]  (', numElements, ' element', sTag, ')'))
                } else {
                    if(length(theseNodes) > 1) stop('something wrong')
                    cat(paste0(indent, theseNodes))
                }
                if(j < length(nodesListByVar)) cat('\n')
            }
            ##if(anyCommas) {
            ##    ## the hard case
            ##    ## really should update this!
            ##    ## punt for now, and just collapse on the *final* index location:
            ##    theseVars <- model$getVarNames(nodes = theseSampledNodes)
            ##    nodesListByVar <- lapply(theseVars, function(var) grep(paste0('^', var, '\\['), theseSampledNodes, value = TRUE))
            ##    for(j in seq_along(nodesListByVar)) {
            ##        theseNodes <- nodesListByVar[[j]]
            ##        initialIndexStrings <- gsub('^[[:alpha:]]+\\[(.*), [[:digit:]]+\\]$', '\\1', theseNodes)
            ##        uniqueInitialStrings <- unique(initialIndexStrings)
            ##        nodeListByInitString <- lapply(uniqueInitialStrings, function(uis) grep(paste0('^[[:alpha:]]+\\[',uis,', [[:digit:]]+\\]$'), theseNodes, value = TRUE))
            ##        for(k in seq_along(nodeListByInitString)) {
            ##            theseNodes <- nodeListByInitString[[k]]
            ##            numIndices <- length(strsplit(theseNodes[1], ',')[[1]])
            ##            indices <- mcmc_getIndexNumberFromNodeNames(theseNodes, numIndices)
            ##            indexRangeList <- mcmc_compressIndexRanges(indices)
            ##            printNodesList <- lapply(indexRangeList, deparse)
            ##            printNodesList <- lapply(printNodesList, function(n) paste0(theseVars[j], '[', uniqueInitialStrings[k], ',', n, ']'))
            ##            printNodesList <- sapply(printNodesList, function(n) if(grepl(':',n)) paste0('components of ',n) else n)
            ##            cat(paste0(printNodesList, collapse = ', '))
            ##            if(j < length(nodesListByVar) || k < length(nodeListByInitString)) cat(', ')
            ##        }
            ##    }
            ##} else {
            ##    ## all scalar nodes, with single numeric index:
            ##    theseVars <- model$getVarNames(nodes = theseSampledNodes)
            ##    nodesListByVar <- lapply(theseVars, function(var) grep(paste0('^', var, '\\['), theseSampledNodes, value = TRUE))
            ##    for(j in seq_along(nodesListByVar)) {
            ##        theseNodes <- nodesListByVar[[j]]
            ##        indices <- mcmc_getIndexNumberFromNodeNames(theseNodes, 1)
            ##        indexRangeList <- mcmc_compressIndexRanges(indices)
            ##        printNodesList <- lapply(indexRangeList, deparse)
            ##        printNodesList <- lapply(printNodesList, function(n) paste0(theseVars[j], '[', n, ']'))
            ##        printNodesList <- sapply(printNodesList, function(n) if(grepl(':',n)) paste0('components of ',n) else n)
            ##        cat(paste0(printNodesList, collapse = ', '))
            ##        if(j < length(nodesListByVar)) cat(', ')
            ##    }
            ##}
        }
        cat('\n\n')
    }
}

printSamplersByType(ind)

library(nimble)

code <- nimbleCode({
    phi ~ dnorm(0, 1)
    p ~ dnorm(0, 1)
    for(i in 1:10) {
        x[i] ~ dnorm(0, 1)
        y[i] ~ dnorm(0, 1)
        z[i] ~ dnorm(0, 1)
        for(j in 1:5) {
            a[i,j] ~ dnorm(0, 1)
            b[i,j] ~ dnorm(0, 1)
        }
    }
})

Rmodel <- nimbleModel(code)
model <- Rmodel
conf <- configureMCMC(Rmodel, nodes = NULL)

conf$addSampler('x[1:7]', 'RW', scalarComponents = TRUE)
conf$addSampler(c('z[1]', 'z[2]'), 'RW', scalarComponents = TRUE)
conf$addSampler(c('y[1]'), 'RW', scalarComponents = TRUE)
conf$addSampler(c('x[1]', 'x[3]', 'x[4]'), 'slice', scalarComponents = TRUE)
conf$addSampler(c('z[1]', 'z[2]'), 'RW_block')
conf$addSampler(c('z[4:5]'), 'RW_block')
conf$addSampler(c('z[1]', 'z[2]'), 'AF_slice')
conf$addSampler(c('z[1]', 'x[1]'), 'AF_slice')
conf$addSampler(c('z[1]', 'x[1]'), 'ess')
conf$addSampler('phi', 'RW')
conf$addSampler('p', 'slice')
conf$addSampler('p', 'binary')
conf$addSampler('a[1:5,1]', 'RW', scalarComponents = TRUE)

conf$printSamplers()

## working with underlying function: printSamplersByType
debug(printSamplersByType)
undebug(printSamplersByType)

ind <- seq_along(conf$getSamplers())
samplerConfs <- conf$samplerConfs
mcmc_getIndexNumberFromNodeNames <- nimble:::mcmc_getIndexNumberFromNodeNames
mcmc_compressIndexRanges <- nimble:::mcmc_compressIndexRanges

printSamplersByType(ind)



## working with conf$printSamplers(byType = TRUE)
conf$printSamplers(byType = TRUE)


## development work
theseSampledNodes <- Rmodel$getNodeNames()

theseVars <- model$getVarNames(nodes = theseSampledNodes)

theseVars



nodesListByVar <- lapply(theseVars, function(var) grep(paste0('^', var, '\\['), theseSampledNodes, value = TRUE))
names(nodesListByVar) <- theseVars


mcmc_getIndexNumberFromNodeNames(nodesListByVar[[1]], 2)

mcmc_getIndexNumberFromNodeNames(nodeNames, 0)

theseNodes <-c(paste0('x[1,1,', 1:5, ']'),
               paste0('x[2,1,', 1:5, ']'),
               paste0('x[3,3,', 1:5, ']'),
               paste0('x[3,4,', 1:5, ']'))

nodeNames <- paste0('x[1,', 1:5, ']')


theseSampledNodes <- c('x', 'y[4]', 'z[4,4:5]')

any(grepl(':', theseSampledNodes))


nums <- 5
nums <- 1:15
nums <- 3:3
nums <- 3:4
nums <- c(5,6)
(nums <- c(3,7,8,1,31,2,100,5,3,8,9,3,11,45,12,13,20,14,25,26,28,30))


(nums <- sort(unique(nums)))
(rangeStartInd <- c(1, which(diff(nums) != 1) + 1))
ranges <- vector('list', length(rangeStartInd))
for(i in seq_along(rangeStartInd)) {
    startInd <- rangeStartInd[i]
    if(i == length(rangeStartInd)) {
        if(rangeStartInd[i] == length(nums)) {
            ranges[[i]] <- nums[startInd]
        } else {
            ranges[[i]] <- substitute(START:END, list(START = as.numeric(nums[startInd]),
                                                      END = as.numeric(nums[length(nums)])))
        }
    } else {
        if(startInd+1 < rangeStartInd[i+1]) {
            ranges[[i]] <- substitute(START:END, list(START = as.numeric(nums[startInd]),
                                                      END = as.numeric(nums[rangeStartInd[i+1]-1])))
        } else ranges[[i]] <- nums[startInd]
    }
}

print(ranges)



## putting together Bayesian logistic regression
## model for Bernhard (which was taken from STAT365 materials)


setwd('~/temp')

## data generation:
set.seed(0)
np <- 10
beta0 <- .3
beta <- rep(0,np)
beta[3] <- 0.3
beta[4] <- 0.8
beta[8] <- -0.4
N <- 1000
X <- array(rnorm(N*np), c(N,np))
logitp <- as.numeric(X %*% beta + beta0)
ptrue <- 1/(1+exp(-logitp))
y <- rbinom(N, 1, ptrue)
writearray <- array(NA, c(N, np+1))
dimnames(writearray)[[2]] <- c(paste0('x',1:np), 'y')
writearray[,1:np] <- X
writearray[,np+1] <- y
head(writearray)
write.csv(writearray, row.names = FALSE, file = 'logistic_regression.csv')


library(nimble)

df <- read.csv('~/github/courses/stat365/data/logistic_regression.csv')

N <- dim(df)[1]
np <- dim(df)[2] - 1
X <- df[,1:np]
y <- df[,np+1]

code <- nimbleCode({
    beta0 ~ dnorm(0, sd = 10000)
    for(i in 1:np) {
        beta[i] ~ dnorm(0, sd = 10000)
    }
    for(i in 1:N) {
        logit(p[i]) <- beta0 + sum(X[i,1:10] * beta[1:10])
        y[i] ~ dbern(p[i])
    }
})

constants <- list(np = np, N = N, X = X)
data <- list(y = y)
inits <- list(beta0 = 0, beta = rep(0,np))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()  ## -804.5692

conf <- configureMCMC(Rmodel)
conf$printSamplers()

xconf <- configureMCMC(Rmodel)
xconf$printSamplers()

conf$printSamplers()
debug(conf$removeSamplers)
conf$removeSamplers(integer(0))
conf$removeSamplers()


Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, niter = 20000, nburnin = 10000)

round(samplesSummary(samples), 3)
##            Mean Median St.Dev. 95%CI_low 95%CI_upp
## beta[1]   0.018  0.019   0.072    -0.128     0.155
## beta[2]   0.070  0.070   0.067    -0.062     0.200
## beta[3]   0.377  0.376   0.082     0.222     0.536
## beta[4]   0.831  0.831   0.085     0.670     1.000
## beta[5]   0.073  0.074   0.068    -0.060     0.200
## beta[6]   0.063  0.062   0.069    -0.068     0.197
## beta[7]   0.085  0.087   0.072    -0.052     0.226
## beta[8]  -0.412 -0.411   0.072    -0.554    -0.269
## beta[9]   0.011  0.011   0.072    -0.124     0.151
## beta[10] -0.026 -0.028   0.071    -0.165     0.110
## beta0     0.228  0.230   0.071     0.085     0.362

library(basicMCMCplots)

basicMCMCplots::samplesPlot(samples)

basicMCMCplots::chainsPlot(samples)

## alternate:
basicMCMCplots::chainsPlot(samples, densityplot = FALSE)
basicMCMCplots::chainsPlot(samples, traceplot = FALSE)


## GitHub issue #164
## can't get getScaleHistory() to work 

library(nimble)
nimbleOptions(MCMCsaveHistory = TRUE)
code <- nimbleCode({
    y ~ dnorm(mu, 1)
    mu ~ T(dnorm(0, 1), -3, 3)
})
m <- nimbleModel(code, data = list(y = 1))
conf <- configureMCMC(m)
mcmc <- buildMCMC(conf)
cm <- compileNimble(m)
cmcmc <- compileNimble(mcmc, project = m)

set.seed(0)
cmcmc$run(1000)

valueInCompiledNimbleFunction(cmcmc$samplerFunctions[[1]], 'scaleHistory')
## 1.000000 1.695204 1.539137 1.399093 1.506234

valueInCompiledNimbleFunction(cmcmc$samplerFunctions[[1]], 'acceptanceHistory')
## 0.600 0.405 0.400 0.475 0.540





## code giving to Maud (at CEFE) for her large birds model


#' Dynamic Hidden Markov Model distribution for use in NIMBLE models
#'
#' \code{dDHMM} and \code{dDHMMo} provide Dynamic hidden Markov model
#' distributions for NIMBLE models.
#'
#' @param x vector of observations, each one a positive integer
#'     corresponding to an observation state
#'     (one value of which could can correspond to "not observed", and
#'     another value of which can correspond to "dead" or
#'     "removed from system").
#' 
#' @param init vector of initial state probabilities. Must sum to 1
#' 
#' @param probObs time-independent matrix (\code{dDHMM} and
#'     \code{rHMM}) or time-dependent 3D array (\code{dDHMMo} and
#'     \code{rHMMo}) of observation probabilities.
#'     First two dimensions of \code{probObs} are of size x (number of possible
#'     system states) x (number of possible observation classes). \code{dDHMMo}
#'     and \code{rHMMo} expects an additional third dimension of size (number of
#'     observation times)
#' 
#' @param probTrans time-dependent array of system state transition
#' probabilities. Dimension of \code{probTrans} is (number of possible
#' system states) x  (number of possible system states)
#' x (number of observation times)
#' 
#' @param len length of observations (needed for rDHMM)
#' 
#' @param log TRUE or 1 to return log probability. FALSE or 0 to return probability
#' 
#' @param n number of random draws, each returning a vector of length
#'     \code{len}. Currently only \code{n = 1} is supported, but the
#'     argument exists for standardization of "\code{r}" functions
#'
#' @details
#' 
#' These nimbleFunctions provide distributions that can be used directly in R or
#' in \code{nimble} hierarchical models (via \code{\link[nimble]{nimbleCode}}
#' and \code{\link[nimble]{nimbleModel}}).
#'
#' The probability (or likelihood) of observation \code{x[t, o]} depends on
#' the previous true latent state, the time-dependent probability of
#' transitioning to a new state \code{probTrans}, and the probability of
#' observation states given the true latent state \code{probObs}.
#'
#' The distribution has two forms, \code{dDHMM} and \code{dDHMMo}. \code{dDHMM}
#' takes a time-independent observation probability matrix with dimension
#' S x O, while \code{dDHMMo} expects a three-dimensional array of time-dependent
#' observation probabilities with dimension S x O x T, where O is the number of
#' possible occupancy states, S is the number of true latent states, and T is
#' the number of time intervals.
#'
#' \code{probTrans} has dimension S x S x (T - 1). \code{probTrans}[i, j, t] is
#' the probability that an individual in state \code{i} at time \code{t} takes on
#' state \code{j} at time \code{t+1}.
#'
#' \code{initStates} has length S. \code{initStates[i]} is the
#' probability of being in state \code{i} at the first observation time.
#'
#' Compared to writing \code{nimble} models with a discrete true latent state
#' and a separate scalar datum for each observation, use
#' of these distributions allows one to directly sum (marginalize) over the
#' discrete latent state and calculate the probability of all observations from
#' one site jointly.
#'
#' These are \code{nimbleFunction}s written in the format of user-defined
#' distributions for NIMBLE's extension of the BUGS model language. More
#' information can be found in the NIMBLE User Manual at
#' \href{https://r-nimble.org}{https://r-nimble.org}.
#'
#' When using these distributions in a \code{nimble} model, the left-hand side
#' will be used as \code{x}, and the user should not provide the \code{log}
#' argument.
#'
#' For example, in a NIMBLE model,
#'
#' \code{observedStates[1:T] ~ dDHMM(initStates[1:S],
#' observationProbs[1:S, 1:O],
#' transitionProbs[1:S, 1:S, 1:(T-1)], T)}
#'
#' declares that the \code{observedStates[1:T]} vector follows a dynamic hidden
#' Markov model distribution with parameters as indicated, assuming all the
#' parameters have been declared elsewhere in the model. In this case, \code{S}
#' is the number of system states, \code{O} is the number of observation
#' classes, and \code{T} is the number of observation occasions.This
#' will invoke (something like) the following call to \code{dDHMM} when
#' \code{nimble} uses the model such as for MCMC:
#'
#' \code{rDHMM(observedStates[1:T], initStates[1:S],
#' observationProbs[1:S, 1:O],
#' transitionProbs[1:S, 1:S, 1:(T-1)], T, log = TRUE)}
#'
#' If an algorithm using a \code{nimble} model with this declaration
#' needs to generate a random draw for \code{observedStates[1:T]}, it
#' will make a similar invocation of \code{rDHMM}, with \code{n = 1}.
#'
#' If the observation probabilities are time-dependent, one would use:
#'
#' \code{observedStates[1:T] ~
#' dDHMMo(initStates[1:S], observationProbs[1:S, 1:O, 1:(T-1)],
#' transitionProbs[1:S, 1:S, 1:(T-1)], T)}
#'
#' @references D. Turek, P. de Valpine and C. J. Paciorek. 2016. Efficient Markov chain Monte
#' Carlo sampling for hierarchical hidden Markov models. Environmental and Ecological Statistics
#' 23:549â€“564. DOI 10.1007/s10651-016-0353-z
#' 
#' @examples
#' 
#' # Set up constants and initial values for defining the model
#' dat <- c(1,2,1,1) # A vector of observations
#' init <- c(0.4, 0.2, 0.4) # A vector of initial state probabilities
#' probObs <- t(array( # A matrix of observation probabilities
#'        c(1, 0,
#'          0, 1,
#'          0.8, 0.2), c(2, 3)))
#'
#' probTrans <- array(rep(0.5, 27), # A matrix of time-indexed transition probabilities
#'             c(3,3,3))
#'
#' # Define code for a nimbleModel
#'  nc <- nimbleCode({
#'    x[1:4] ~ dDHMM(init[1:3], probObs = probObs[1:3, 1:2],
#'                   probTrans = probTrans[1:3, 1:3, 1:4], len = 4)
#'
#'    for (i in 1:3) {
#'      init[i] ~ dunif(0,1)
#'
#'      for (j in 1:3) {
#'        for (t in 1:4) {
#'          probTrans[i,j,t] ~ dunif(0,1)
#'        }
#'      }
#'
#'      probObs[i, 1] ~ dunif(0,1)
#'      probObs[i, 2] <- 1 - probObs[1,i]
#'    }
#'  })
#'
#' # Build the model, providing data and initial values
#' DHMM_model <- nimbleModel(nc,
#'                           data = list(x = dat),
#'                           inits = list(init = init,
#'                                        probObs = probObs,
#'                                        probTrans = probTrans)))
#' # Calculate log probability of x from the model
#' DHMM_model$calculate()
#' 


dDHMMo <- nimbleFunction(
    run = function(x = double(1), init = double(1), probObs = double(3), probTrans = double(3), len = double(), log = integer(0, default = 0)) {
        if(length(init) != dim(probObs)[1]) stop("Length of init does not match ncol of probObs in dDHMMo.")
        if(length(init) != dim(probTrans)[1]) stop("Length of init does not match dim(probTrans)[1] in dDHMMo.")
        if(length(init) != dim(probTrans)[2]) stop("Length of init does not match dim(probTrans)[2] in dDHMMo.")
        if(length(x) != len) stop("Length of x does not match len in dDHMM.")
        if(len - 1 != dim(probTrans)[3]) stop("dim(probTrans)[3] does not match len - 1 in dDHMMo.")
        if(len != dim(probObs)[3]) stop("dim(probObs)[3] does not match len in dDHMMo.")
        pi <- init
        logL <- 0
        nObsClasses <- dim(probObs)[2]
        lengthX <- length(x)
        for(t in 1:lengthX) {
            ##if(x[t] > nObsClasses) stop("Invalid value of x[t] in dDHMM.")
            xt <- x[t]
            Zpi <- probObs[, xt, t] * pi
            sumZpi <- sum(Zpi)
            logL <- logL + log(sumZpi)
            if(t != lengthX)   pi <- (probTrans[,,t] %*% asCol(Zpi) / sumZpi)[ ,1]
        }
        returnType(double())
        if (log) return(logL)
        return(exp(logL))
    }
)

rDHMMo <- nimbleFunction(
    run = function(n = integer(), init = double(1), probObs = double(3), probTrans = double(3), len = double()) {
        returnType(double(1))
        ans <- numeric(len)
        trueInit <- 0
        r <- runif(1, 0, 1)
        j <- 1
        while(r > sum(init[1:j])) j <- j + 1
        trueState <- j
        for(i in 1:len) {
            r <- runif(1, 0, 1)
            j <- 1
            while(r > sum(probObs[trueState, 1:j, i])) j <- j + 1
            ans[i] <- j
            if (i != len) {
                r <- runif(1, 0, 1)
                j <- 1
                while(r > sum(probTrans[trueState, 1:j, i])) j <- j + 1
                trueState <- j
            }
        }
        return(ans)
    }
)

registerDistributions(list(
    dDHMMo = list(
        BUGSdist = "dDHMMo(init, probObs, probTrans, len)",
        Rdist = "dDHMMo(init, probObs, probTrans, len)",
        discrete = TRUE,
        types = c('value = double(1)', 'init = double(1)', 'probObs = double(3)', 'probTrans = double(3)', 'len = double()'),
        mixedSizes = TRUE,
        pqAvail = FALSE)
))





## modifying map (maximum a posterior) code
## from Oliver Stone

library(nimble)

n <- 100
mu <- 4
sigma <- 1
y <- rnorm(100, mu, sigma)

map_code <- nimbleCode({
    for(i in 1:n) {
        y[i] ~ dnorm(mu, sd = sigma)
    }
    mu ~ dnorm(0, sd = 10)
    sigma ~ dexp(0.01)
})

map_constants <- list(n = n)
map_data <- list(y = y)

map_inits <- list(mu = 3, sigma = 4)

map_model <- nimbleModel(map_code, map_constants, map_data, map_inits)

stochNonDataNodes <- map_model$getNodeNames(stochOnly = TRUE, includeData = FALSE)
stochNonDataNodes
## [1] "mu"    "sigma"

map_compiled_model <- compileNimble(map_model)

map_lp <- nimbleFunction(
    setup = function(model) {
        stochNonDataNodes <- map_model$getNodeNames(stochOnly = TRUE, includeData = FALSE)
    },
    run = function(x = double(1)) {
        map_compiled_model$mu <- x[1]
        map_compiled_model$sigma <- x[2]
        negLP <- -map_compiled_model$calculate()
        returnType(double())    ## DT: was missing returnType() statement
        return(negLP)
    }
})

my_map_lp_function <- map_lp(

nlm(map_lp, c(3,4))


## removing internal print() statements,
## from conf$getSamplerDefinition(), for Cyril

library(nimble)

code <- nimbleCode({
    for(i in 1:n) 
        y[i] ~ dnorm(b0 + inprod(beta[1:p], X[i, 1:p]), 1)
    for(i in 1:p) 
        beta[i] ~ dnorm(0, 1)
    b0 ~ dnorm(0, 1)
    xx ~ dbeta(1, 1)
    yy ~ dbern(xx)
})

constants <- list(n = 5, p = 3)
data <- list(y = rnorm(constants$n),
             X = matrix(rnorm(constants$n * constants$p), constants$n),
             yy = 1)
inits <- list(b0 = 1, beta = rnorm(constants$p), xx = 0.5)

m <- nimbleModel(code, data = data, constants = constants)

conf <- configureMCMC(m)

conf$printSamplers()

conf$getSamplerDefinition(1)
d <- conf$getSamplerDefinition(1)




## (failing) example of multiple nfList inheritance
## for Perry.
## trying to get this (multiple inheritance) to work for HMC sampler

library(nimble)

baseA <- nimbleFunctionVirtual(
    methods = list(
        methodA = function() { }
    )
)

baseB <- nimbleFunctionVirtual(
    contains = baseA,
    methods = list(
        methodA = function() { },
        methodB = function() { }
    )
)



nfType1 <- nimbleFunction(
    name = 'nfType1',
    contains = baseA,
    setup = function() {
        print('building nfType1')
    },
    run = function() {
        print('run function nfType1')
    },
    methods = list(
        methodA = function() {
            print('in methodA of nfType1')
        }
    )
)

nfType2 <- nimbleFunction(
    name = 'nfType2',
    contains = baseA,
    setup = function() {
        print('building nfType2')
    },
    run = function() {
        print('run function nfType2')
    },
    methods = list(
        methodA = function() {
            print('in methodA of nfType2')
        }
    )
)

nfType3 <- nimbleFunction(
    name = 'nfType3',
    contains = baseB,
    setup = function() {
        print('building nfType3')
    },
    run = function() {
        print('run function nfType3')
    },
    methods = list(
        methodA = function() {
            print('in methodA of nfType3')
        },
        methodB = function() {
            print('in methodB of nfType3')
        }
    )
)


nfDef <- nimbleFunction(
    setup = function() {
        nflA <- nimbleFunctionList(baseA)
        nflA[[1]] <- nfType1()
        nflA[[2]] <- nfType2()
        nflA[[3]] <- nfType3()
        ##
        ## once the existing code works, I plan to add:
        nflB <- nimbleFunctionList(baseB)
        nflB[[1]] <- nfType3()
    },
    run = function() {
        print('======================================')
        print('running run() methods of nflA')
        for(i in seq_along(nflA)) {
            nflA[[i]]$run()
        }
        print('======================================')
        print('running methodA() methods of nflA')
        for(i in seq_along(nflA)) {
            nflA[[i]]$methodA()
        }
        print('======================================')
        ##
        ## once the existing code works, I plan to add:
        print('running methodA() methods of nflB')
        for(i in seq_along(nflB)) {
            nflB[[i]]$methodA()
        }
        print('======================================')        
        print('running methodB() methods of nflB')
        for(i in seq_along(nflB)) {
            nflB[[i]]$methodB()
        }
        print('======================================')        
    }
)

Rnf <- nfDef()

Cnf <- compileNimble(Rnf)

Rnf$run()
Cnf$run()






## testing the use of dsum() distribution in jags

## continuous-valued case: dsum
code <- quote({
    a ~ dnorm(5, .1)
    b ~ dnorm(9, .2)
    y ~ dsum2(a, b)
})

constants <- list()
data <- list(y = 10)
inits <- list(a = 5, b = 9)

niter <- 10000
monitorVars <- c('a', 'b')

constsAndData <- c(constants, data)
modelfile <- file.path(tempdir(), 'model.txt')
writeLines(paste0('model\n', paste0(deparse(code, width.cutoff=500L), collapse='\n')), con=modelfile)

library(rjags)
set.seed(0)
jags_mod <- jags.model(file=modelfile, data=constsAndData, inits=inits, n.chains=1, quiet=FALSE)
set.seed(0)
jags_out <- coda.samples(model=jags_mod, variable.names=monitorVars, n.iter=niter, thin=1)

list.samplers(jags_mod)

samples <- as.matrix(jags_out[[1]])

library(basicMCMCplots)
samplesPlot(samples)

head(samples)
apply(samples, 2, mean)
sum(apply(samples, 2, mean))
apply(samples, 1, sum)


## discrete-valued case:dsum
library(nimble)


code <- nimbleCode({
    a ~ dpois(50)
    b ~ dbin(0.1, 200)
    d[1] <- a
    d[2] <- b
    y ~ dsum(d[1:2])
})
constants <- list()
data <- list(y = 40)
inits <- list(a = 20, b = 20)


sampler_dsum_pair <- nimbleFunction(
    name = 'sampler_dsum_pair',
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        ## control list extraction
        adaptive      <- if(!is.null(control$adaptive))      control$adaptive      else TRUE
        adaptInterval <- if(!is.null(control$adaptInterval)) control$adaptInterval else 200
        scale         <- if(!is.null(control$scale))         control$scale         else 1
        ## node list generation
        targetAsScalar <- model$expandNodeNames(target, returnScalarComponents = TRUE)
        target1 <- targetAsScalar[1]
        target2 <- targetAsScalar[2]
        calcNodes <- model$getDependencies(target)
        ## numeric value generation
        scaleOriginal <- scale
        timesRan      <- 0
        timesAccepted <- 0
        timesAdapted  <- 0
        optimalAR     <- 0.44
        gamma1        <- 0
        ## checks
        if(length(targetAsScalar) != 2) stop()
        if(!model$isDiscrete(target1)) stop()
        if(!model$isDiscrete(target2)) stop()
        ## could check if they have a common dsum() dependent node,
        ## and could do linearity check on dependent dsum() expression
    },
    run = function() {
        currentTarget1 <- model[[target1]]
        currentTarget2 <- model[[target2]]
        currentLogProb <- model$getLogProb(calcNodes)
        prop <- rpois(n = 1, scale) + 1   ## don't allow proposals = 0
        pm <- 2 * rbinom(n = 1, size = 1, prob = 0.5) - 1  ## +1 or -1
        signedProp <- prop * pm
        model[[target1]] <<- currentTarget1 + signedProp
        model[[target2]] <<- currentTarget2 - signedProp
        probLogProb <- model$calculate(calcNodes)
        logMHR <- probLogProb - currentLogProb
        jump <- decide(logMHR)
        if(jump) nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
        else     nimCopy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE)
        if(adaptive)     adaptiveProcedure(jump)
    },
    methods = list(
        adaptiveProcedure = function(jump = logical()) {
            timesRan <<- timesRan + 1
            if(jump)     timesAccepted <<- timesAccepted + 1
            if(timesRan %% adaptInterval == 0) {
                acceptanceRate <- timesAccepted / timesRan
                timesAdapted <<- timesAdapted + 1
                gamma1 <<- 1/((timesAdapted + 3)^0.8)
                gamma2 <- 10 * gamma1
                adaptFactor <- exp(gamma2 * (acceptanceRate - optimalAR))
                scale <<- ceiling(scale * adaptFactor)
                timesRan <<- 0
                timesAccepted <<- 0
            }
        },
        reset = function() {
            scale <<- scaleOriginal
            timesRan      <<- 0
            timesAccepted <<- 0
            timesAdapted  <<- 0
            gamma1 <<- 0
        }
    )
)





dsum2 <- nimbleFunction(
    run = function(x = double(), a = double(), b = double(), log = integer(0, default = 0)) {
        returnType(double())
        if(x == a + b) return(0)
        print('warning: dsum2 density evaluation failed...')
        return(-Inf)
    }
)

rsum2 <- nimbleFunction(
    run = function(n = integer(), a = double(), b = double()) {
        print('warning: called rsum2 random generation function')
        returnType(double())
        return(1)
    }
)



registerDistributions(list(
    dsum2 = list(
        BUGSdist = 'dsum2(a, b)',
        types = c('value = double()', 'a = double()', 'b = double()')
    )
))



{
    ## dsum2 test #1
    code <- nimbleCode({
        a ~ dpois(5)
        b ~ dbin(0.4, 20)
        y ~ dsum2(a, b)
    })
    constants <- list()
    data <- list(y = 10)
    inits <- list(a = 5, b = 5)
    Rmodel <- nimbleModel(code, constants, data, inits)
    conf <- configureMCMC(Rmodel, nodes = NULL)
    conf$addSampler(target = c('a', 'b'), type = 'dsum2')
    Rmcmc <- buildMCMC(conf)
    Cmodel <- compileNimble(Rmodel)
    Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)
    set.seed(0)
    samples <- runMCMC(Cmcmc, 50000)
    if(!all(apply(samples, 2, mean) == c(3.47178, 6.52822))) stop()
    if(!all(round(apply(samples, 2, sd), 6) == c(1.404152, 1.404152))) stop()
    if(!all(as.numeric(prop.table(table(samples[, 'a']))) == c(0.00906, 0.06274, 0.17744, 0.26978, 0.25672, 0.14824, 0.05802, 0.01576, 0.00188, 0.00034, 0.00002))) stop()
    if(!all(prop.table(table(samples[, 'b'])) == c(0.00002, 0.00034, 0.00188, 0.01576, 0.05802, 0.14824, 0.25672, 0.26978, 0.17744, 0.06274, 0.00906))) stop()
    ##
    ## dsum2 test #2
    code <- nimbleCode({
        a ~ dpois(50)
        b ~ dbin(0.1, 200)
        y ~ dsum2(a, b)
    })
    constants <- list()
    data <- list(y = 40)
    inits <- list(a = 20, b = 20)
    Rmodel <- nimbleModel(code, constants, data, inits)
    conf <- configureMCMC(Rmodel, nodes = NULL)
    conf$addSampler(target = c('a', 'b'), type = 'dsum2')
    Rmcmc <- buildMCMC(conf)
    Cmodel <- compileNimble(Rmodel)
    Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)
    set.seed(0)
    samples <- runMCMC(Cmcmc, 50000)
    if(!all(apply(samples, 2, mean) == c(28.1796, 11.8204 ))) stop()
    if(!all(round(apply(samples, 2, sd), 6) == c(2.821819, 2.821819 ))) stop()
    if(!all(as.numeric(prop.table(table(samples[, 'a']))) == c(0.00012, 0.00044, 0.00114, 0.00290, 0.00582, 0.01530, 0.02662, 0.04854, 0.07084, 0.09386, 0.12614, 0.14252, 0.14390, 0.11684, 0.08894, 0.05676, 0.03418, 0.01702, 0.00498, 0.00268, 0.00040, 0.00006))) stop()
    if(!all(prop.table(table(samples[, 'b'])) == c(0.00006, 0.00040, 0.00268, 0.00498, 0.01702, 0.03418, 0.05676, 0.08894, 0.11684, 0.14390, 0.14252, 0.12614, 0.09386, 0.07084, 0.04854, 0.02662, 0.01530, 0.00582, 0.00290, 0.00114, 0.00044, 0.00012))) stop()
}



## fitting the discrete dsum example in jags
niter <- 100000
monitorVars <- c('a', 'b')

constsAndData <- c(constants, data)
modelfile <- file.path(tempdir(), 'model.txt')
writeLines(paste0('model\n', paste0(deparse(code, width.cutoff=500L), collapse='\n')), con=modelfile)

library(rjags)
set.seed(0)
jags_mod <- jags.model(file=modelfile, data=constsAndData, inits=inits, n.chains=1, quiet=FALSE)
set.seed(0)
jags_out <- coda.samples(model=jags_mod, variable.names=monitorVars, n.iter=niter, thin=1)

list.samplers(jags_mod)

samples <- as.matrix(jags_out[[1]])

head(samples)

apply(samples, 2, mean)

apply(samples, 2, sd)

prop.table(table(samples[, 'a']))

prop.table(table(samples[, 'b']))



## > apply(samples, 2, mean)
##        a        b 
## 28.16795 11.83205 
## > apply(samples, 2, sd)
##        a        b 
## 2.808224 2.808224 
## > prop.table(table(samples[, 'a']))
##  
##      17      18      19      20      21      22      23      24      25      26 
## 0.00001 0.00027 0.00091 0.00266 0.00656 0.01391 0.02780 0.04643 0.07290 0.09875 
##      27      28      29      30      31      32      33      34      35      36 
## 0.12738 0.14215 0.13829 0.11493 0.09029 0.05983 0.03288 0.01592 0.00584 0.00191 
##      37      38 
## 0.00031 0.00007 
## > prop.table(table(samples[, 'b']))
##  
##       2       3       4       5       6       7       8       9      10      11 
## 0.00007 0.00031 0.00191 0.00584 0.01592 0.03288 0.05983 0.09029 0.11493 0.13829 
##      12      13      14      15      16      17      18      19      20      21 
## 0.14215 0.12738 0.09875 0.07290 0.04643 0.02780 0.01391 0.00656 0.00266 0.00091 
##      22      23 
## 0.00027 0.00001




## making table rows for SCR paper

## for Wolverine example

df <- as.data.frame(dfRepAvg)
head(df)
dim(df)
df
for(e in 1:4) {
    start <- 3*(e-1)
    for(i in 1:3) {
        row <- start + i
        if(i == 1) cat(as.character(df[row,1]), '& ')
        val <- df[row,3]
        if(i == 1)  rnd <- 6
        if(i == 2)  rnd <- 6
        if(i == 3)  rnd <- 6
        cat(round(val,rnd))
        if(i < 3) cat(' & ')
        if(i == 3) cat(' \\\\\n')
    }
}

## for Voles example
df <- as.data.frame(dfRepAvg)
head(df)
dim(df)
df
for(e in 1:4) {
    start <- 11*(e-1)
    for(i in 1:11) {
        row <- start + i
        if(i == 1) cat(as.character(df[row,1]), '& ')
        val <- df[row,3]
        if(i == 1)  rnd <- 2
        if(i == 2)  rnd <- 2
        if(i == 3)  rnd <- 2
        if(i == 4)  rnd <- 3
        if(i == 5)  rnd <- 3
        if(i == 6)  rnd <- 2
        if(i == 7)  rnd <- 2
        if(i == 8)  rnd <- 3
        if(i == 9)  rnd <- 3
        if(i == 10) rnd <- 3
        if(i == 11) rnd <- 3
        cat(round(val,rnd))
        if(i < 11) cat(' & ')
        if(i == 11) cat(' \\\\\n')
    }
}


## loading models, to find number of nodes, for SCR paper models

model <- 'SCR1'
model <- 'SCR2'
model <- 'SCR4'
model <- 'SCR6'

modelToLoad <- if(model == 'jags') 'orig' else model
filename <- paste0('data/modelInfo_',
                   modelToLoad,
                   if(makeDatasetLarger) paste0('_i',timesMoreIndividuals,'_t',timesMoreTraps^2) else "",
                   ifelse(reduced, '_reduced', ''),
                   '.RData')
if(!filename %in% list.files(recursive = TRUE)) stop(paste0('could not find file: ', filename))
load(filename)
Rmodel <- nimbleModel(modelInfo$code, modelInfo$constants, modelInfo$data, modelInfo$inits)
Rmodel$calculate()

length(Rmodel$getNodeNames())
length(Rmodel$getNodeNames(stochOnly = TRUE, includeData = FALSE))


## trying to build the Rmd vignette for nimbleEcology package

setwd('~/github/nimble/nimbleEcology/vignettes/')
library(knitr)
knit('Introduction_to_nimbleEcology.Rmd')
knit('Introduction_to_nimbleEcology.md')



## making a general solution for
## "saving MCMC state" variables of the samplers,
## for Cyril

library(nimble)


checkModels <- function(model, model2) {
    modelVarNames <- model$getVarNames()
    for(v in modelVarNames) {
        if(!all(model[[v]] == model2[[v]])) return(FALSE)
    }
    return(TRUE)
}

checkMCMCs <- function(conf, mcmc, conf2, mcmc2) {
    mcmcState <- getMCMCstate(conf, mcmc)
    mcmcState2 <- getMCMCstate(conf2, mcmc2)
    return(identical(mcmcState, mcmcState2))
}





moreItsRan <- 0
while(all(c(checkModels(model, model2), checkMCMCs(conf, mcmc, conf2, mcmc2)))) {
    set.seed(1)
    mcmc$run(1, reset = FALSE)
    set.seed(1)
    mcmc2$run(1, reset = FALSE)
    moreItsRan <- moreItsRan + 1
    print('==================================================================')
    print(paste0('RAN ITERATION', moreItsRan))
    print('==================================================================')
}


c(checkModels(model, model2), checkMCMCs(conf, mcmc, conf2, mcmc2))


valueInCompiledNimbleFunction(mcmc$samplerFunctions[[6]],  'timesRan')
valueInCompiledNimbleFunction(mcmc2$samplerFunctions[[6]], 'timesRan')

valueInCompiledNimbleFunction(mcmc$samplerFunctions[[6]],  'empirSamp')
valueInCompiledNimbleFunction(mcmc2$samplerFunctions[[6]], 'empirSamp')

valueInCompiledNimbleFunction(mcmc$samplerFunctions[[7]],  'empirSamp')
valueInCompiledNimbleFunction(mcmc2$samplerFunctions[[7]], 'empirSamp')



## restart R here !


## load model and mcmc state lists:
stateFilename <- '~/temp/state.rds'
stateLists <- readRDS(stateFilename)
modelState <- stateLists[['modelState']]
mcmcState <- stateLists[['mcmcState']]



set.seed(1)
mcmc$run(200, reset = FALSE)
vars500 <- c(model$x, model$b, model$c, model$p)
##saveRDS(vars500, '~/temp/vars500.rds')
vars500_saved <- readRDS('~/temp/vars500.rds')
if(!all(round(vars500 - vars500_saved, 10) == 0)) stop()









## request for Perry

## how to do the same thing with Cmcmc ??
## that is, extract the value of the 'gamma1' state variable, from the nested
## nimbleFunction 'my_calcAdaptationFactor' that's within the first (RW_block)
## sampler?

library(nimble)

code <- nimbleCode({
    x[1] ~ dnorm(0, 1)
    x[2] ~ dnorm(0, 1)
})

Rmodel <- nimbleModel(code, inits = list(x=c(0,0)))
Rmodel$calculate()

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('x[1:2]', 'RW_block', control = list(adaptFactorExponent = 0.8))
conf$printSamplers()

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 1000)
samples[990:1000, ]

Rmcmc$samplerFunctions$contentsList[[1]]$my_calcAdaptationFactor$gamma1

Rmcmc$samplerFunctions$contentsList[[1]]
Rmcmc$samplerFunctions$contentsList[[1]]$my_calcAdaptationFactor
Rmcmc$samplerFunctions$contentsList[[1]]$my_calcAdaptationFactor$.CobjectInterface

model <- if(is.Cnf(mcmc)) mcmc$Robject$model$CobjectInterface else mcmc$model

identical(Cmcmc$Robject, Rmcmc)

## his answer:
valueInCompiledNimbleFunction(Rmcmc$samplerFunctions$contentsList[[1]]$my_calcAdaptationFactor$.CobjectInterface, 'gamma1')
valueInCompiledNimbleFunction(Cmcmc$Robject$samplerFunctions$contentsList[[1]]$my_calcAdaptationFactor$.CobjectInterface, 'gamma1')






## trying Perry's problem about default (length) parameters to
## custom distributions

library(nimble)

## Occupancy distribution with scalar detection prob (not time-varying)
dOcc_s <- nimbleFunction(
    run = function(x = double(1),
        probOcc = double(0),
        probDetect = double(0),
        len = integer(0, default = 0),
        log = logical(0, default = 0)) {
        if (len != 0) if (len != length(x)) stop("Argument 'len' must match length of data, or be 0.")
        returnType(double(0))
        logProb_x_given_occupied <- sum(dbinom(x, prob = probDetect, size = 1, log = TRUE))
        prob_x_given_unoccupied <- sum(x) == 0
        prob_x <- exp(logProb_x_given_occupied) * probOcc + prob_x_given_unoccupied * (1 - probOcc)
        if (log) return(log(prob_x))
        return(prob_x)
    }
)

## We need the len argument for the r function only, else it has no way to know how long the return object should be.
rOcc_s <- nimbleFunction(
    run = function(n = integer(),
        probOcc = double(0),
        probDetect = double(0),
        len = integer(0, default = 0)) {
        if (len == 0) stop("Argument 'len' must be given for rOcc_s (or if a nimble model with dOcc_s is used for simulation).")
        returnType(double(1))
        u <- runif(1, 0, 1)
        if (u > probOcc) return(numeric(0, length = len))
        return(rbinom(len, prob = probDetect, size = 1))
    }
)

## We desire to avoid requiring len if it won't be used, i.e. in MCMC only the d function is used and that doesn't need len.
registerDistributions(list(
    dOcc_s = list(
        BUGSdist = "dOcc_s(probOcc, probDetect, len)",
        Rdist = c("dOcc_s(probOcc, probDetect, len)",
            "dOcc_s(probOcc, probDetect, len = )"),
        discrete = TRUE,
        types = c('value = double(1)', 'probOcc = double(0)', 'probDetect = double(0)', 'len = integer(0)'),
        pqAvail = FALSE)))




## like here, where AFAIC see I have to include len.
occupancy_code_new <- nimbleCode({
    psi ~ dunif(0,1)
    p ~ dunif (0,1)
    for(i in 1:nSites) {
        y[i, 1:nVisits] ~ dOcc_s(probOcc = psi, probDetect = p, len = nVisits)
    }
})

## Example to use this model (a bit roundabout b/c of what I was writing):
## Make a traditional model to use for simulation
occupancy_code <- nimbleCode({
    psi ~ dunif(0,1)
    p ~ dunif (0,1)
    for(i in 1:nSites) {
        z[i] ~ dbern(psi)
        for(j in 1:nVisits) {
            y[i, j] ~ dbern(z[i] * p)
        }
    }
})

occupancy_model <- nimbleModel(occupancy_code,
                               constants = list(nSites = 50, nVisits = 5))

occupancy_model$psi <- 0.7
occupancy_model$p <- 0.15
simNodes <- occupancy_model$getDependencies(c("psi", "p"), self = FALSE)
occupancy_model$simulate(simNodes)
occupancy_model$z
head(occupancy_model$y, 10) ## first 10 rows
occupancy_model$setData('y') ##

## Then make our model of interest
occupancy_model_new <- nimbleModel(occupancy_code_new,
                                   constants = list(nSites = 50, nVisits = 5),
                                   data = list(y = occupancy_model$y),
                                   inits = list(psi = 0.7, p = 0.15))

## I know we've seen things like this before, where we need a "length" argument that is in some cases redundant.  I'm not sure we have a good way to avoid it when it's not needed.


## testing whether the 'data' list can be character string names ??
## answer: no! error!

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list('a')
inits <- list()

a <- 10

Rmodel <- nimbleModel(code, constants, data, inits)  ## error



## updates and testing to BayesNSGP package

# Generate some data: stationary/isotropic
N <- 100
coords <- matrix(runif(2*N), ncol = 2)
alpha_vec <- rep(log(sqrt(1)), N) # Log process SD
delta_vec <- rep(log(sqrt(0.05)), N) # Log nugget SD
Sigma11_vec <- rep(0.4, N) # Kernel matrix element 1,1
Sigma22_vec <- rep(0.4, N) # Kernel matrix element 2,2
Sigma12_vec <- rep(0, N) # Kernel matrix element 1,2
mu_vec <- rep(0, N) # Mean
nu <- 0.5 # Smoothness
dist_list <- nsDist(coords)
Cor_mat <- nsCorr( dist1_sq = dist_list$dist1_sq, dist2_sq = dist_list$dist2_sq, 
                  dist12 = dist_list$dist12, Sigma11 = Sigma11_vec, 
                  Sigma22 = Sigma22_vec, Sigma12 = Sigma12_vec, nu = nu )
Cov_mat <- diag(exp(alpha_vec)) %*% Cor_mat %*% diag(exp(alpha_vec))
D_mat <- diag(exp(delta_vec)^2) 
set.seed(110)
z <- as.numeric(mu_vec + t(chol(Cov_mat + D_mat)) %*% rnorm(N))
# Set up constants
constants <- list( nu = 0.5, Sigma_HP1 = 2 )
# Defaults: tau_model = "constant", sigma_model = "constant", mu_model = "constant",
# and Sigma_model = "constant"
Rmodel <- nsgpModel(likelihood = "fullGP", constants = constants, coords = coords, z = z )
conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

samples <- runMCMC(Cmcmc, niter = 200, nburnin = 100)
# Prediction
predCoords <- as.matrix(expand.grid(seq(0,1,l=10),seq(0,1,l=10)))
postpred <- nsgpPredict( model = Rmodel, samples = samples, coords.predict = predCoords )





NOTES for BayesNSGP package resubmission
Wrote package and software names in single quotes in title and description fields of DESCRIPTION file.
Provided reference to the main method in the description field of DESSCRIPTION file.
Instance of \dontrun was replaced by \donttest, since examples of this function will require >5 seconds to execute.    
Replaced instances of cat() with message().



## trying new printSamplers() method for crossLevel sampler
## on example: litters

library(nimble)

littersCode <- nimbleCode({
    for (i in 1:G) {
        for (j in 1:N) {
            r[i,j] ~ dbin(p[i,j], n[i,j])
            p[i,j] ~ dbeta(a[i], b[i]) 
        }
        a[i] ~ dgamma(1, .001)
        b[i] ~ dgamma(1, .001)
    }
})
##
G <- 2
N <- 16
n <- matrix(c(13, 12, 12, 11, 9, 10, 
              9, 9, 8, 11, 8, 10, 13, 10, 12, 9, 10, 9, 10, 5, 9, 9, 13, 
              7, 5, 10, 7, 6, 10, 10, 10, 7), nrow = 2)
r <- matrix(c(13, 12, 12, 11, 9, 10, 9, 9, 8, 10, 8, 9, 
              12, 9, 11, 8, 9, 8, 9, 4, 8, 7, 11, 4, 4, 5, 5, 3, 7, 3, 7, 0), 
            nrow = 2)
##
littersConsts <- list(G = G, N = N, n = n)
littersData <- list(r = r)
littersInits <- list( a = c(2, 2), b=c(2, 2) )
##
Rmodel <- nimbleModel(littersCode,
                      littersConsts,
                      littersData,
                      littersInits)
##
conf <- configureMCMC(Rmodel)

conf$printSamplers()

conf$addSampler(target = 'p', type = 'RW', scalarComponents = TRUE)
conf$addSampler(target = 'a', type = 'slice', scalarComponents = TRUE)

conf$addSampler(type = 'crossLevel', target = c('a[1]')) 
conf$addSampler(type = 'crossLevel', target = c('a[1]', 'b[1]')) 
conf$addSampler(type = 'crossLevel', target = c('a', 'b')) 

conf$printSamplers()

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)



## compareMCMCs installation from GitHub


library(nimble)
library(devtools)


install_github('nimble-dev/compareMCMCs', subdir = 'compareMCMCs')



littersCode <- nimbleCode({
    for (i in 1:G) {
        for (j in 1:N) {
            r[i,j] ~ dbin(p[i,j], n[i,j])
            p[i,j] ~ dbeta(a[i], b[i]) 
        }
        a[i] ~ dgamma(1, .001)
        b[i] ~ dgamma(1, .001)
    }
})

G <- 2
N <- 16
n <- matrix(c(13, 12, 12, 11, 9, 10, 
              9, 9, 8, 11, 8, 10, 13, 10, 12, 9, 10, 9, 10, 5, 9, 9, 13, 
              7, 5, 10, 7, 6, 10, 10, 10, 7), nrow = 2)
r <- matrix(c(13, 12, 12, 11, 9, 10, 9, 9, 8, 10, 8, 9, 
              12, 9, 11, 8, 9, 8, 9, 4, 8, 7, 11, 4, 4, 5, 5, 3, 7, 3, 7, 0), 
            nrow = 2)

littersConsts <- list(G = G, N = N, n = n)
littersData <- list(r = r)
littersInits <- list( a = c(2, 2), b=c(2, 2) )

Rmodel <- nimbleModel(littersCode,
                      littersConsts,
                      littersData,
                      littersInits)











## Ron Bassar model
## salmon lab and field metabolic rates

library(nimble)

code <- nimbleCode({
    ## Prior for beta
    for(j in 1:4) {
        beta[j] ~ dnorm(0, 0.0001)
    }
    ## Prior for the inverse variance
    inv.var ~ dgamma(0.01, 0.01)
    sigma <- 1/sqrt(inv.var)
    ## Priors for lab smr
    for (ii in 1:nfam) {
        alpha.smrcent[ii] ~ dnorm(0, 0.0001)
    }
    sigma.int.m ~ dunif(0, 100)   #dgamma(0.01, 0.01)
    tau.int.m <- 1/(sigma.int.m)
    ## Likelihood
    ## lab data
    for (ii in 1:nlab) {
        smrcent[ii] ~ dnorm(alpha.smrcent[lab.fam[ii]], tau.int.m)
    }
    ## for field data
    for(i in 1:nfield) {
        mu[i] <- beta[1] + beta[2]*treat[i] + beta[3]*alpha.smrcent[family[i]] + beta[4]*treat[i]*alpha.smrcent[family[i]] 
        flowindex[i] ~ dnorm(mu[i], inv.var)
    }
})

constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)
library(basicMCMCplots)
samplesPlot(samples)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()




## NIMBLE User List error ... ?

library(nimble)

simple.model <- nimbleModel(code=nimble.fixed.s, constants=constants, data=jags.onestate.data, inits=inits, check = FALSE)
Rmcmc <- buildMCMC(simple.model)
Cmodel <- compileNimble(simple.model)
Cmcmc <- compileNimble(Rmcmc, project = simple.model)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)

However if I run this code 
ms<-nimbleMCMC(simple.model, data = jags.onestate.data, inits = inits,
               monitors = c("pd_a", "psi", "phi"), thin = 10,
               niter = 10000, nburnin = 5000, nchains = 3,
               summary = TRUE, WAIC = FALSE)




## for VIBASS3 NIMBLE Workshop in Valencia
## change all [...](...) links to
## <a href="URL_GOES_HERE" target="_blank">HYPERLINK_TEXT_OPENS_IN_NEW_TAB</a>

setwd('~/github/nimble/nimble-vibass-2019')
filenames <- c(list.files(pattern = '*\\.Rmd'),
               list.files(path = 'modules', pattern = '*\\.Rmd', full.names = TRUE))
filesnamesModified <- character()

##f <- filenames[4]
##lines[linesToModify[3]]
##linesModified[linesToModify[3]]

for(f in filenames) {
    message('================================================')
    message('processing file: ', f)
    message('------------------------------------------------')
    lines <- readLines(f)
    linesToModify <- grep('\\[.+?\\]\\(.+?\\)', lines)
    if(length(linesToModify) > 0) {
        filesnamesModified <- c(filesnamesModified, f)
    }
    linesModified <- gsub('\\[(.+?)\\]\\((.+?)\\)', '<a href="\\2" target="_blank" style="color: blue">\\1</a>', lines)
    for(ind in linesToModify) {
        message(lines[ind])
        message(linesModified[ind])
        message('------------------------------------------------')
    }
    writeLines(linesModified, con = f)
    message('================================================')
}

filenames
filesnamesModified

## now, run shell command
## ./make_slides, for the filenames that were modified:
for(f in filesnamesModified) {
    message('================================================')
    message('building slides for file: ', f)
    out <- system2(command = './make_slides', args = f, stdout = TRUE, stderr = TRUE)
    message(paste0(out, collapse = '\n'))
    message('================================================')
}

## open the modified files:
for(f in filesnamesModified) {
    system2('open', args = gsub('\\.Rmd$', '_slides.html', f))
}


## fixing HK's model initialization error, sent to the nimble-users email list.


setwd('~/Downloads')

library(nimble)

dat <- read.csv("3He3Hedat.csv")

re <- as.numeric(dat$lab) # Change the label to a numeric vector
## Note that the numbers are assigned by "sorting" the labels in alphabetical order

Nre <- length(unique(dat$lab))
## Unique removes duplicated vector, we want to know how many groups of 
## data are there

N <- nrow(dat) # Total No of data sets
obsy <- dat$S    # Response variable in MeV
obsx <-  dat$E   # Predictors
erry <- dat$Stat # Error in MeV
set <- dat$lab # Get the labels as a vector

## Inputting the quoted systematic uncertainty:
syst = c(log(1.057),log(1.082),log(1.037),log(1.045),log(1.038))
## In accordance to 
## 1 Bon99 5.7%
## 2 Daw71 8.2%
## 3 Jun98 3.7%
## 4 Kra87 4.5%
## 5 Kud04 3.8%

samplerCode <-  nimbleCode({
    for (i in 1:N) {
        mut[i] <- (exp(2.429819509 * i.screen * (obsx[i]^(-1.5)))) * (alpha + beta * obsx[i] + gamma * (obsx[i]^2))
        yt[i] <- y.norm[re[i]]*mut[i]
        ## Temp mean (mut) multiplied by normalising constant, 
        ya[i] ~ dnorm(yt[i],sd = y.scat[re[i]])
        ## re[i] refers to the index that we would use to refer to the category
        obsy[i] ~ dnorm(ya[i], sd = erry[i])
        ## Propagating the errors in observations of y
    }
    ## PRIORS  
    ## polynomial parameters
    alpha ~ T(dnorm(0.0,sd=100),0,Inf)
    ## Note the difference in how we truncate the distributions
    beta ~ dnorm(0.0,sd=100)
    gamma ~ dnorm(0.0,sd=100)
    i.screen ~ T(dnorm(0.0,sd=100),0,Inf)  
    ## Some sort of hyperprior for the "grand mean"
    mt ~ T(dnorm(0.0,sd=5),0,Inf)
    for (k in 1:Nre){
        ## Systematic Uncertainty as a highly informative prior
        y.norm[k] ~ dlnorm(0,sd = syst[k])       ## DT: changed log(1.0) to 0
        y.scat[k] ~ T(dnorm(mt,sd=100),0,Inf)
    }
})

samplerData <- list(obsy = obsy)    # Response variable

samplerConst <- list(N = N, # Sample size
                     Nre = Nre, 
                     re = re, # This is used to "iterate")
                     erry = erry,
                     syst = syst,
                     obsx = obsx     # Predictors   ## DT: moved predictor/covariate obsx to "constants",
                     ## more appropriate than "data", although if you plan
                     ## on ever changing the values of obsx, then move it back to "data"
                     )

samplerInits <- list(alpha = 1, 
                     beta = -1, 
                     gamma = 1, 
                     i.screen = 1e-6,
                     mt = 1,                  ## DT: initial value added
                     ya = rep(1, N),          ## DT: initial value added
                     y.norm = rep(1, Nre),    ## DT: initial value added
                     y.scat = rep(1, Nre)     ## DT: initial value added
                     )

ourmodel <- nimbleModel(samplerCode, samplerConst, samplerData, samplerInits)

Rmodel <- ourmodel

Rmodel$calculate()     ## DT: -34550.91  good, a real number.  model is now fully initialized

conf <- configureMCMC(Rmodel)
conf$printSamplers(byType = TRUE)
conf$printMonitors()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

library(basicMCMCplots)
samplesPlot(samples, c('alpha', 'beta', 'gamma'))
samplesPlot(samples, 'y.norm')

samplesSummary(samples)





## quick demo for Richard Bischoff, regarding how to clear out the mvSamples
## MCMC samples from R memory, then continue the same MCMC run

library(nimble)

## construct model object (standard BUGS example: pump)
code <- nimbleCode({
    for(i in 1:N){
        theta[i] ~ dgamma(alpha, beta)
        lambda[i] <- theta[i] * t[i]
        x[i] ~ dpois(lambda[i])
    }
    alpha ~ dexp(1)
    beta ~ dgamma(1, 1)
}) 

constants <- list(N = 10, t = c(94.3,15.7,62.9,126,5.24,31.4,1.05,1.05,2.1,10.5))
data <- list(x = c(5,1,5,14,3,19,1,1,4,22))
inits <- list(alpha = 1, beta = 1,theta = rep(1, 10))

Rmodel <- nimbleModel(code, constants, data, inits)

## build MCMC, compile model and MCMC

conf <- configureMCMC(Rmodel)
conf$addMonitors('theta')   ## so we're monitoring a total of 12 nodes
Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

## generate initial 1000 MCMC samples
set.seed(0)
Cmcmc$run(1000)

## extract the samples from the compiled MCMC object, as an R array

samples1000 <- as.matrix(Cmcmc$mvSamples)

####################################
## save samples1000, or otherwise ##
####################################

## remove the samples1000 variable,
## reduce the internal mvSamples object to 0 rows,
## run R's garbage collector
rm('samples1000')
Cmcmc$mvSamples$resize(0)
gc()

## continue same run of the MCMC for another 1000 iterations, using reset = FALSE

Cmcmc$run(1000, reset = FALSE)

samples2000 <- as.matrix(Cmcmc$mvSamples)

## samples2000 contains the *second set of 1000 samples* from the MCMC run,
## that is, samples # 1001 - 2000.

## now you could save the 'samples2000' object,
## clear the mvSamples object again using: Cmcmc$mvSamples$resize(0)
## run R's garbage collector again: gc()
## and continue the MCMC run for longer, if desired: Cmcmc$run(10000, reset = FALSE)




##Cmcmc$run(10000, reset = FALSE)
##samples10000 <- as.matrix(Cmcmc$mvSamples)
##samples_save <- rbind(samples1000, samples2000, samples10000)
##dim(samples_save)
##set.seed(0)
##Cmcmc$run(12000)
##samples_save2 <- as.matrix(Cmcmc$mvSamples)
##dim(samples_save2)
##identical(samples_save, samples_save2)


## developing new MCMC printSamplers(byType = TRUE)

library(nimble)

N1 <- 2
N2 <- 100
##
code <- nimbleCode({
    b ~ dgamma(1, 1)
    for(i in 1:N1) {
        a[i] ~ dnorm(0, 1)
        y[i] ~ dnorm(a[i], b)
    }
    for(i in 1:N2) {
        x[i] ~ dexp(1)
        yy[i] ~ dnorm(x[i], 1)
    }
})
##
constants <- list(N1=N1, N2=N2)
data <- list(y = c(0, N1), yy = rep(0, N2))
inits <- list(b = 1, a = rep(0,N1), x = rep(1,N2))
##

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
Rmodel$initializeInfo()
##
conf <- configureMCMC(Rmodel, nodes = NULL)
conf <- configureMCMC(Rmodel)


conf$printSamplers()
conf$printSamplers(byType = TRUE)



## testing new nimble MCMC options for BayesNSGP
## MCMCmultivariateNodesAsScalars
## MCMCmultivariateNodesAsScalars

nimbleOptions(MCMCmonitorAllSampledNodes = TRUE)


library(nimble)

getNimbleOption('MCMCprogressBar')
getNimbleOption('MCMCmultivariateNodesAsScalars')
getNimbleOption('MCMCmonitorAllSampledNodes')


nimbleOptions(MCMCprogressBar = TRUE)
nimbleOptions(MCMCprogressBar = FALSE)

nimbleOptions(MCMCmultivariateNodesAsScalars = TRUE)
nimbleOptions(MCMCmultivariateNodesAsScalars = FALSE)

nimbleOptions(MCMCmonitorAllSampledNodes = TRUE)
nimbleOptions(MCMCmonitorAllSampledNodes = FALSE)


code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a, 1)
    d ~ dnorm(b, 1)
    x[1:2] ~ dmnorm(mu[1:2], cov = Sigma[1:2,1:2])
    c ~ dexp(x[1])
})
constants <- list(Sigma = diag(2), mu = c(0,0))
data <- list(c = 1, d = 1)
inits <- list(a = 0, x = c(1,1), b = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printMonitors()

Rmodel$getNodeNames()
Rmodel$getNodeNames(stochOnly = TRUE)
Rmodel$getNodeNames(stochOnly = TRUE, includeData = FALSE)


conf <- configureMCMC(Rmodel, print = TRUE)
conf <- configureMCMC(Rmodel, print = TRUE, multivariateNodesAsScalars = TRUE)
conf <- configureMCMC(Rmodel, print = TRUE, multivariateNodesAsScalars = FALSE)




Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

samples <- runMCMC(Cmcmc, 10000)
samples <- runMCMC(Cmcmc, 10000, progressBar = TRUE)
samples <- runMCMC(Cmcmc, 10000, progressBar = FALSE)

samples <- nimbleMCMC(code, constants, data, inits)
samples <- nimbleMCMC(code, constants, data, inits, progressBar = TRUE)
samples <- nimbleMCMC(code, constants, data, inits, progressBar = FALSE)

undebug(nimbleMCMC)
undebug(runMCMC)
undebug(mcmc$run)





library(basicMCMCplots)

nchains <- 2
nparams <- 3
nsamples <- 1000
samplesList <- vector('list', nchains)
for(i in 1:nchains) {
    samples <- array(NA, c(nsamples, nparams))
    colnames(samples) <- letters[1:nparams]
    for(j in 1:nparams) {
        samples[,j] <- rnorm(nsamples, j/10, j/10)
    }
    samplesList[[i]] <- samples
}
names(samplesList) <- paste0('chain', 1:nchains)
str(samplesList)

chainsPlot(samplesList, cex = 0.7)
chainsPlot(samplesList, traceplot = FALSE, cex = 0.7)
chainsPlot(samplesList, densityplot = FALSE, cex = 0.7, burnin = 100)
chainsPlot(samplesList, densityplot = FALSE, traceplot = FALSE, cex = 0.7)



samplesPlot(samplesList)
chainsSummary(samplesList, jitter = .1, buffer.left = 0.3, buffer.right = 0.3)
chainsPlot(samplesList, density = TRUE, cex = 0.7)
samplesPlot3(samplesList, ind = 1:6)




## scoping error with nimbleOptions() function,
## in the BayesNSGP package (5/3/2019)

library(BayesNSGP)

# Generate some data: stationary/isotropic
N <- 100
coords <- matrix(runif(2*N), ncol = 2)
alpha_vec <- rep(log(sqrt(1)), N) # Log process SD
delta_vec <- rep(log(sqrt(0.05)), N) # Log nugget SD
Sigma11_vec <- rep(0.4, N) # Kernel matrix element 1,1
Sigma22_vec <- rep(0.4, N) # Kernel matrix element 2,2
Sigma12_vec <- rep(0, N) # Kernel matrix element 1,2
mu_vec <- rep(0, N) # Mean
nu <- 0.5 # Smoothness
dist_list <- nsDist(coords)
Cor_mat <- nsCorr( dist1_sq = dist_list$dist1_sq, dist2_sq = dist_list$dist2_sq, 
                  dist12 = dist_list$dist12, Sigma11 = Sigma11_vec, 
                  Sigma22 = Sigma22_vec, Sigma12 = Sigma12_vec, nu = nu )
Cov_mat <- diag(exp(alpha_vec)) %*% Cor_mat %*% diag(exp(alpha_vec))
D_mat <- diag(exp(delta_vec)^2) 
set.seed(110)
z <- as.numeric(mu_vec + t(chol(Cov_mat + D_mat)) %*% rnorm(N))
# Set up constants
constants <- list( nu = 0.5, Sigma_HP1 = 2 )
# Defaults: tau_model = "constant", sigma_model = "constant", mu_model = "constant",
# and Sigma_model = "constant"
Rmodel <- nsgpModel(likelihood = "fullGP", constants = constants, coords = coords, z = z )



conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
samples <- runMCMC(Cmcmc, niter = 200, nburnin = 100)
# Prediction
predCoords <- as.matrix(expand.grid(seq(0,1,l=10),seq(0,1,l=10)))
postpred <- nsgpPredict( model = Rmodel, samples = samples, coords.predict = predCoords )





## error from Floriane Plard (14 May 2019)

library(nimble)

test_HT <- nimbleCode({
    Nad[1] <- round(0.14*inipop/2)
    mean.sad ~ dunif(0, 1)
    for(t in 2:nyears) {
        Nad[t] ~ dbin(prob = mean.sad, size = Nad[t-1])
        y[t] ~ dpois(Nad[t])
    }
})

Count <- c(44,66,66,75,65,72,94,112,90,100,96,70,104,94,105)
inipop <- Count[1]
nyears <- length(Count)

constants <- list(nyears = nyears)
data <- list(inipop = inipop, y = Count)
inits <- list(mean.sad = 0.4, Nad = rep(1,nyears))   ## possible initial value for Nad

IPMi <- nimbleModel(code = test_HT, constants = constants, data = data, inits = inits)
IPMi$calculate()

IPMi$calculate('mean.sad')
IPMi$getNodeNames()
IPMi$calculate('y')
IPMi$Nad
IPMi$logProb_Nad
IPMi$calculate('Nad[1]')
IPMi$calculate('Nad[2]')   ## -Inf


params <- c('mean.sad','Nad')
specIPMi <- configureMCMC(IPMi, monitors = params)
specIPMi$printSamplers()
specIPMi$removeSamplers('mean.sad')
specIPMi$addSampler(target = 'mean.sad', type ='slice')

IPMMCMCi <- buildMCMC(specIPMi)
CIPMi <- compileNimble(IPMi)
CIPMMCMCi <- compileNimble(IPMMCMCi, project=IPMi, resetFunctions = TRUE)

CIPMMCMCi$run(1000)

### Warning: slice sampler reached maximum number of contractions.
### Warning: slice sampler reached maximum number of contractions.
### Warning: slice sampler reached maximum number of contractions.
### Warning: slice sampler reached maximum number of contractions.
### Warning: slice sampler reached maximum number of contractions.
### Warning: slice sampler reached maximum number of contractions.
### Warning: slice sampler reached maximum number of contractions.
### .....
dim(as.matrix(CIPMMCMCi$mvSamples)  )

as.matrix(CIPMMCMCi$mvSamples)[1:50,]  
## Nad[1] Nad[2] Nad[3] Nad[4]       Nad[5] Nad[6] Nad[7] Nad[8] Nad[9] Nad[10] Nad[11] Nad[12] Nad[13] Nad[14] Nad[15]   mean.sad
## [19,]      3   -624    -17    145 -40513298256  -1240   -485   -221   -677      87   -1560    -728    -720    -476     523 -11.892531
## [20,]      3   -624    -17    145 -40513298256  -1240   -485   -221   -677      87   -1560    -728    -720    -476     523 -11.614054
## [21,]      3   -624    -17    145 -40513298256  -1240   -485   -221   -677      87   -1560    -728    -720    -476     523 -11.955403
## [22,]      3   -624    -17    145 -40513298256  -1240   -485   -221   -677      87   -1560    -728    -720    -476     523 -11.883934
## [23,]      3   -624    -17    145 -40513298256  -1240   -485   -221   -677      87   -1560    -728    -720    -476     523 -11.169499
## [24,]      3   -624    -17    145 -40513298256  -1240   -485   -221   -677



## demo of using HMC hmc sampler

remove.packages("nimble")
library(devtools)
install_github("nimble-dev/nimble", ref = "hmcAD", subdir = "packages/nimble")
library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)  ## NEED THIS LINE

code <- nimbleCode({
    sigma ~ dunif(0, 100)
    a ~ dnorm(0, 1)
    b ~ dnorm(a, sd = sigma)
})

constants <- list()
data <- list(b = 1)
inits <- list(sigma = 1, a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()   ## -6.943047

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$removeSamplers(c('sigma', 'a'))
conf$addSampler(c('sigma', 'a'), type = 'HMC')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

samples <- runMCMC(Cmcmc, 1000)



## testing inits() model$setInits() handling of un-named list elements


library(nimble)

code <- nimbleCode({ a ~ dnorm(0, 1) })

inits <- list(a = 0)   ## inits list has an unnamed element
inits <- NULL   ## inits list has an unnamed element
inits <- list()
inits <- list(0)   ## inits list has an unnamed element
inits <- list(a = 0, 1)   ## inits list has an unnamed element
inits <- list(a = 0, b = 11)   ## inits list has an unnamed element
inits <- list(b = 11, c = 4)   ## inits list has an unnamed element

Rmodel <- nimbleModel(code, inits = inits)

Rmodel$initializeInfo()
##debug(Rmodel$setInits)

Rmodel$setInits(inits)

Rmodel$a
UGcars <- read.csv('~/github/courses/stat202/data/UsedCars.csv')
cars

Power <- c('gas', 'hybrid', 'diesel', 'hybrid', 'hybrid', 'gas', 'hybrid', 'gas', 'diesel', 'diesel', 'diesel', 'hybrid', 'diesel', 'gas', 'diesel', 'hybrid', 'hybrid', 'hybrid', 'gas')

cars$Power <- as.factor(Power)

cars$Type <- as.factor(ifelse(cars$Type, 'foreign', 'domestic'))
cars$Type

## look at the data:
library(ggplot2) 
ggplot(cars, aes(x = Price, y = Power, color = Type)) + geom_point(size = 3) 

## multiple regression with (categorical) Power variable
m <- lm(Price ~ Power + Type, data = cars)    # DELETE THIS, DO IT IN CLASS
summary(m)                                    # DELETE THIS, DO IT IN CLASS

## ANOVA
anova(m)                                      # DELETE THIS, DO IT IN CLASS

## Important and interesting:
## if we *only* consider Type (foreign / domestic),
## then it's highly significant:

ggplot(cars, aes(x = Price, y = Type, color = Type)) + geom_point(size = 3)

m2 <- lm(Price ~ Type, data = cars)

summary(m2)

anova(m2)















## STAT202 Lecture #23
## ANOVA I

cars <- read.csv('~/github/courses/stat202/data/UsedCars.csv')
cars

## say there was a categorical predictor with 3 levels:
## 1 = unleaded gasoline
## 2 = diesel
## 3 = hybrid

Power <- c(rep("gas",   10),
           rep("diesel", 6),
           rep("hybrid", 3))

Power

cars$Power <- as.factor(Power)
cars
cars$Power

## let's look at the data:
library(ggplot2)

ggplot(cars, aes(x = Price, y = Power, color = Power)) + geom_point(size = 3)



## multiple regression with (categorical) Power variable
## fit multiple regression model,
## price as a function of (categorical) Power variable:

m <- lm(Price ~ Power, data = cars)

summary(m)


## ANOVA:
## anova() function

anova(m)


## let's look at the data:
library(ggplot2)
ggplot(cars, aes(x = Price, y = Power, color = Power)) + geom_point(size = 3)

## try new assignments of Power:
Power <- c('gas', 'hybrid', 'diesel', 'hybrid', 'hybrid', 'gas', 'hybrid', 'gas', 'diesel', 'diesel', 'diesel', 'hybrid', 'diesel', 'gas', 'diesel', 'hybrid', 'hybrid', 'hybrid', 'gas')
Power

cars$Power <- as.factor(Power)
cars
cars$Power

## refit model, and ANOVA:
m <- lm(Price ~ Power, data = cars)
summary(m)
anova(m)



## STAT360 Lecture #23
## Bayesian Inference

set.seed(0)

n <- 100
n

y <- rbinom(1, size = n, prob = theta_true)
y

## choose & graph some priors
par(mfrow = c(3,1), mar = c(4,2,2,1)) 
plot(-1, -1, xlim=c(0,1), ylim=c(0,9), xlab='', ylab='', main='Priors')

## prior 1: Uniform
a1 <- b1 <- 1
plot(function(x) dbeta(x, a1, b1), add = TRUE, lwd=2, col = 'blue')

## prior 2: mildly weighted towards 0.5
a2 <- b2 <- 10
plot(function(x) dbeta(x, a2, b2), add = TRUE, lwd=2, col = 'green')

## prior 3: heavily weighted towards 0.5
a3 <- b3 <- 50
plot(function(x) dbeta(x, a3, b3), add = TRUE, lwd=2, col = 'purple')

## plot likelihood:
xs <- seq(0, 1, 0.01)
plot(xs, dbinom(y, n, xs), type='l', lwd=2, xlab='', main='Likelihood')

## graph posteriors:
plot(-1, -1, xlim = c(0,1), ylim = c(0,13), xlab='', ylab='', main = 'Posteriors')
plot(function(x) dbeta(x, a1+y, b1+n-y), add = TRUE, lwd=2, col = 'blue')
plot(function(x) dbeta(x, a2+y, b2+n-y), add = TRUE, lwd=2, col = 'green')
plot(function(x) dbeta(x, a3+y, b3+n-y), add = TRUE, lwd=2, col = 'purple')

## true value:
theta_true
abline(v = theta_true, lwd=2, col = 'red')

## posterior means:
round(
    rbind((a1 + y) / (a1 + b1 + n),
          (a2 + y) / (a2 + b2 + n),
          (a3 + y) / (a3 + b3 + n)), 2)

## posterior CIs (credible intervals):
## equal-tailed BCIs
round(
    rbind(qbeta(c(0.025, 0.975), a1+y, b1+n-y),
          qbeta(c(0.025, 0.975), a2+y, b2+n-y),
          qbeta(c(0.025, 0.975), a3+y, b3+n-y)), 2)

## frequentist MLE for p
phat <- y/n
round(phat, 2)

## frequentist 95% CI for p
se <- sqrt(phat*(1-phat)/n)
round(se, 2)
round(phat + c(-1,1) * 1.96 * se, 2)

a <- a1+y
b <- b1+n-y

v <- a*b / ((a+b)^2 * (a+b+1))
sqrt(v)
se

theta_true 



summary(m)

(b)
0.6615 / 0.1578 = 4.192015
(c)
2*pt(-4.1920, 100-4) = 0.0000615418
(d)
qt(0.975, 96) = 1.984984
0.0075 + c(-1,1) *1.9849*0.0151 = (-0.0224, 0.03747)
(e) No, it doesnt
(f) s = 0.218624
(g)
1.1362 + 0.6615*3.8 + 0.3301*0.9 + 0.0075*5 = 3.98449
3.98449 + c(-1,1) * 1.9849 * 0.218624 = (3.55, 4.418)

## STAT202 Lecture 20
## Multiple Regression 1 using Cars dataset
cars <- read.csv('~/github/courses/stat202/data/UsedCars.csv')

cars <- cars[, c('Price', 'Age', 'HP')]

## pairs plot:
plot(cars)

## correlation matrix:
cor(cars)

## let's do multiple regression,
## trying to predict y = Price,
## using covariates x1 = Age, and x2 = HP
m <- lm(Price ~ Age + HP, data = cars)

## regression coefficients
m$coefficients[3]

## model summary
summary(m)

## calculating multiple-R
cor(cars$Price, m$fitted.values)^2

## plot yhat vs. y to "see" the multiple-R
plot(cars$Price, m$fitted.values)

## calculating multiple-R^2
## residuals
m$residuals

cars$Price - m$fitted.values


## calculating s = residual standard error
## sqrt(SSE / (n-p))
sqrt(sum((cars$Price - m$fitted.values)^2) / (19-3))


## R can also calculate s,
## using sigma() function:
sigma(m)



## Daily Problem 19:
df <- read.csv('~/Downloads/house_selling_prices_OR.csv')
df <- df[, c('House.Price..USD.', 'House.Size', 'Lot.Size')]
head(df)
dim(df)
par(mfrow = c(1,3))
boxplot(df$House.Price..USD.)
boxplot(df$House.Size)
boxplot(df$Lot.Size)
pairs(df)
cor(df)
m <- lm(House.Price..USD. ~ House.Size + Lot.Size, data = df)
summary(m)






library(nimble)

code <- nimbleCode({
    mu ~ dnorm(0, sd = 10000)
    sigma ~ dunif(0, 10000)
    for(i in 1:5) {
        y[i] ~ dnorm(mu, sd = sigma)
    }
    for(i in 1:N) {
        yc[i] ~ T(dnorm(mu, sd = sigma), 2, )
    }
})
N <- 2
constants <- list(N = N)
data <- list(y = c(-2, -1, 0, 1, 2))
inits <- list(mu = 0, sigma = 1, yc = rep(2.1,N))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printMonitors()
conf$addMonitors('yc')
Rmcmc <- buildMCMC(conf)

##compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
##Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

[27,]  0.4970267 1.4853999 3.211170 2.286722
[28,] -0.2797500 1.4853999 2.575468 2.102819
[29,] -0.2797500 0.2046505      Inf      Inf


set.seed(0)
Rmcmc$run(28)
samples <- as.matrix(Rmcmc$mvSamples)
samples

debug(Rmcmc$run)
Rmcmc$run(2, reset = FALSE)

debug(samplerFunctions[[ind]]$run)
ls(model$nodes)
debug(model$nodes[['yc_L6_UID_59']]$simulate)

Cmodel$yc
colnames(samples)
samples


samplesSummary(samples)
Mean       Median  St.Dev. 95%CI_low 95%CI_upp
mu    -0.01309894 -0.007039629 1.292003 -2.633911  2.569409
sigma  2.49683880  2.066773567 1.503980  1.041366  7.015764

library(basicMCMCplots)
samplesPlot(samples)

Cmodel$yc



x1 <- 12
(n1 <- x1+6+16)
x2 <- 4
(n2 <- x2+11+19)
(p1 <- x1/n1)
(p2 <- x2/n2)
(RR <- p1/p2)
(logRR <- log(RR))
(se <- sqrt(1/x1 - 1/n1 + 1/x2 - 1/n2))
##z <- qnorm(0.995)    ## 99% CI
z <- 1.96
(logCI <- logRR + c(-1,1) * z * se)
(CI <- exp(logCI))

## STAT202 Quiz #3, problem #3
n <- 93
r <- -0.3
sx <- 3.1
sy <- 0.7
(b <- sy/sx * r)
(r2 <- r^2)
r2*100    ## as a percentage
(SSE <- 100*(1 - r2))
(s <- sqrt(SSE / (n-2)))







df <- read.csv('~/Downloads/digit.csv')

labels <- df[, 1]
pixels <- df[, -1]

table(labels)   ## explortory
dim(pixels)     ## explortory

## part (a)

plotImage <- function(x) {
    x <- as.numeric(x)
    label <- ''
    if(length(x) > 784) {
        label <- x[1]
        x <- x[-1]
    }
    xMatrix <- matrix(x, nrow=28)
    xMatrix <- xMatrix[, 28:1]    ## flips image from top-to-bottom
    col <- grey(seq(0, 1, length = 256))
    image(xMatrix, main = label, col = col, xaxt = 'n', yaxt = 'n')
}

par(mfrow = c(3,4))
for(i in 1:12) plotImage(df[i,])


## part (b)

nTotal <- nrow(df)
nTrain <- nTotal / 2   ## assumes nTotal is even

set.seed(355)     ## set.seed(0) is better
trainInd <- sample(1:nTotal, nTrain)
testInd <- setdiff(1:nTrain, trainInd)

trainPixels <- pixels[trainInd, ]
testPixels <- pixels[testInd, ]

## part(c)

?prcomp

out <- prcomp(trainPixels)   ## ~ 45 seconds

evs <- out$rotation

par(mfrow = c(2,3))
for(i in 1:6) plotImage(evs[,i])   ## careful, ordering of indices here

## explain what you learned here
## PC1 for detecting 1's ?
## PC2 for detecting 9's or 0's ?
## PC3 for detecting 3's ?
## ..... ?


## part (d)

percentVariance <- out$sdev^2 / sum(out$sdev^2)

cumulativeVariance <- cumsum(percentVariance)

M <- min(which(cumulativeVariance > 0.9))   ## 87 PCs

par(mfrow = c(1,1))
plot(1:784, cumulativeVariance, type = 'l')
abline(h = 0.9, col = 'red')
segments(M, -10, M, 0.9, col = 'blue')



## part (e)

?predict.prcomp

pca <- out

Projection <- predict(pca, pixels)

dim(Projection)

proj <- Projection[,1:M]


## (f) (10pts) Use hierarchical clustering based on Euclidean distances between principal components for a random sample of 500 training images. Compare three different types of linkages; discuss pros and cons of the linkages for this data set. Also, comment on the structure you notice in the dendrograms and what does this tells you about the data (what did you learn about the data?).

set.seed(355)
randomInd <- sample(1:nTrain, 500)
randomInd <- sample(1:nTrain, 100)


## Euclidean distances b/w principle components
dm <- dist(Projection[randomInd,1:M])

cs <- hclust(dm, method = "single")
ca <- hclust(dm, method = "average")
cc <- hclust(dm, method = "complete")

par(mfrow = c(1,3))
plot(cs, labels=FALSE, xlab = "" , main = "Single")
plot(cc, labels=FALSE, xlab = "", main = "Cluster")
plot(ca, labels=FALSE, xlab = "", main = "Complete")

par(mfrow = c(1,1))
plot(cs, xlab = "" , main = "Single", labels = labels[randomInd], cex = 0.5)
plot(cc, xlab = "" , main = "Single", labels = labels[randomInd], cex = 0.5)
plot(ca, xlab = "" , main = "Single", labels = labels[randomInd], cex = 0.5)

names(cs)
cs$labels

dim(Projection)
dim(pixels)
length(labels)
table(labels)








## STAT202 Lecture 19
## Exponential Regression

## Moore's Law
x <- c(0, 3, 4, 5, 6)      ## years since 1959
y <- c(1, 7, 19, 29, 64)   ## num. of components on microchip

plot(x, y, pch=19, xlim = c(0,8), ylim = c(0,100), col = 'red', type = 'b')

m <- lm(y ~ x)

abline(m, col = 'blue')

## correlation, measure of linear relationship
cor(x, y)

## R^2 for linear regression:
cor(x, y)^2

## instead, let's plot of log(y) vs. x
plot(x, log(y), pch=19, xlim=c(0,8), ylim=c(0,6), type = 'b')


m <- lm(log(y) ~ x)   ## fit exponential regression model in R

m

abline(m, col = 'red')

## R^2 for exponential regression:
cor(x, log(y))^2

summary(m)

## let's extract the coefficients
## don't forget, these are: log(a), and log(b)
loga <- m$coefficients[1]
logb <- m$coefficients[2]

a <- exp(loga)
b <- exp(logb)

plot(x, y, pch=19, xlim = c(0,8), ylim = c(0,100), type='b')


a*b^x

points(x, a*b^x, col='red', pch=5)

xs <- seq(0, 10, by = 0.01)
xs

lines(xs, a*b^xs, col = 'red')

## extracting fitted values:
## predict() function:


library(nimble)
library(mvtnorm)

S <- 159   # number of locations
X <- rmvnorm(159, sigma = diag(rep(1, 6)))
b1 <- rep(-1, S) + rnorm(S)
b2 <- rep(2, S) + rnorm(S)
b3 <- rep(0, S)
b4 <- rep(0, S)
b5 <- rep(2, S) + rnorm(S)
b6 <- rep(1, S) + rnorm(S)
y <- 1 + b1 * X[, 1] + b2 * X[, 2] + b3 * X[, 3] + b4 * X[, 4] + b5 * X[, 5] + b6 * X[, 6] + rnorm(S) 

##(ii) spatial-specific coefficients
code <- nimbleCode({
    for (i in 1:S) {
        y[i] ~ dnorm(mu_y[i], tau = tau_y)
        mu_y[i] <-
            b0 + b[i, 1] * x1[i] + b[i, 2] * x2[i] +
            b[i, 3] * x3[i] + b[i, 4] * x4[i] + b[i, 5] * x5[i] + b[i, 6] * x6[i]
        b[i, 1:6] ~ dmnorm(mu_bm[1:6], cov = var_bm[1:6, 1:6])
    }
    var_bm[1:6, 1:6] <- diag(1/tau_bm[1:6])
    for (j in 1:6) {
        mu_bm[j] <- 0
        tau_bm[j] <- tau_bmm[ind[j] + 1]
        ind[j] ~ dbern(gamma[j])
        gamma[j] ~ dbeta(0.5, 0.5)
    }
    tau_bmm[1] <- tau_b0
    tau_bmm[2] <- tau_b0 * c
    b0 ~ dnorm(0, 1)
    tau_y ~ dgamma(1, 1)
})


constants <- list(S = 159, c = 0.0001, tau_b0 = 100)

data <- list(y = y, x1 = X[,1], x2 = X[,2], x3 =X[,3], x4 = X[,4], x5 = X[,5], x6 = X[,6])

inits <- list(tau_y = 1, tau_bm = rep(1,6), b0 = rnorm(1), ind = rep(1,6), gamma = runif(6),
              b = array(0, c(159,6)))   ## DT: added initial value for b array

Rmodel <- nimbleModel(code, constants, data, inits)

## DT: calculate() returns a real number, model is fully initialized
Rmodel$calculate()   

conf <- configureMCMC(Rmodel)

## check the sampler assignments.
## might consider whether or not you want the RW_block samplers assigned
## to each b[i, 1:6] ?
conf$printSamplers()

## add your monitors
conf$addMonitors(c("b0","b","ind","gamma","tau_y"))

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

samplesSummary(samples)

library(basicMCMCplots)
samplesPlot(samples, c('gamma', 'b0'))

#### this does the same thing:
##mcmc.out <- nimbleMCMC(code, constants, data, inits,
##                       monitors = c("b0","b","ind","gamma","tau_y"),
##                       niter = 50000, thin = 10, nchains = 1, setSeed = TRUE)






## STAT202 Lecture 18
## Regression 2

head(mtcars)
dim(mtcars)
names(mtcars)

hp <- mtcars$hp
mpg <- mtcars$mpg


plot(x = hp, y = mpg)

## lm(y ~ x)
m <- lm(mpg ~ hp)

## coefficients
## residuals
## residual standard error
## model summary

m
m$coefficients
a <- m$coefficients[1]
b <- m$coefficients[2]

e <- mpg - (a + b*hp)
e

m$residuals - e

n <- length(hp)
n

sqrt(sum(e^2) / (n-2))

summary(m) #### !!!! remember this










##-- Daily Participation Problem 18 ---

x <- c(3,8,9,10,11,20,29,30,31,31,31,34,41)  ## GDP
y <- c(1,2,4,7,8,18,12,13,11,12,16,26,26)    ## oil comsumption
(n <- length(x))    ## 13
(b <- cor(x,y) * sd(y)/sd(x))   ## 0.5464
(a <- mean(y) - b*mean(x))      ## -0.1055
yhat <- a + b*x
(SSE <- sum((y - yhat)^2))      ##  217.92
(s <- sqrt(SSE / (n-2)))    ## 4.451
(seb <- s / sqrt(sum((x-mean(x))^2)))   ## 0.1033
(T <- (b - 0)/seb)    ## 5.288
(cib <- b + c(-1,1) * qt(0.975, n-2) * seb)   ## 0.319 0.774
m <- lm(y ~ x)

summary(m)
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  -0.1055     2.6007  -0.041 0.968382    
## x             0.5464     0.1033   5.288 0.000257 ***
## Residual standard error: 4.451 on 11 degrees of freedom

e <- m$residuals

plot(x, y)

hist(e)
    










name <- '~/github/courses/stat360/data/from_textbook/textbook_datasets/Chapter 8/whales.txt'


df <- read.delim(name, header = FALSE)


dim(df)
head(df)

names(df) <- 'time'

dim(df)
head(df)

write.csv(df, '~/Downloads/whales.csv', row.names = FALSE)

df2 <- read.csv('~/Downloads/whales.csv')

dim(df2)
head(df2)




## STAT202 Lecture 17 Daily Participation Problem
## Simple Linear Regression

x <- c(3,8,9,10,11,20,29,30,31,31,31,34,41)  ## GDP

y <- c(1,2,4,7,8,18,12,13,11,12,16,26,26)    ## oil consumption 

n <- length(x)
n

(x-mean(x))/sd(x)

sum( (x-mean(x))/sd(x) * (y-mean(y)/sd(y)))


sum((x-mean(x))/sd(x) * (y-mean(y))/sd(y)) / (n-1)

cor(x, y)

plot(x, y)

lm(y ~ x)

26 - (-.1055 + 0.5464 * 34)





## for Canada, x = GDP = 34, y = oil = 26
## prediction for Canada:
-0.1055 + 0.5464 * 34

## residual for Canada:
26 - 18.4721




## Poisson data (from Example A, page 261)

x <- c(
    31, 29, 19, 18, 31, 28,
    34, 27, 34, 30, 16, 18,
    26, 27, 27, 18, 24, 22,
    28, 24, 21, 17, 24)

n <- length(x)
n

xbar <- mean(x)
xbar

xbar + c(-1,1) * 1.96 * sqrt(xbar/n)

## 22.87, 26.95

## Bootstrap CI:
N <- 100
N <- 100000

lambdaHatStar <- numeric(N)

for(i in 1:N) {
    xStar <- rpois(n, xbar)
    lambdaHatStar[i] <- mean(xStar)
}

hist(lambdaHatStar, breaks = 50)

quantile(lambdaHatStar, probs = c(0.025, 0.975))

##     2.5%    97.5% 
## 22.86957 26.95652 


## alternate: using replicate()
replicate

lambdaHatStar <- replicate(N, mean(rpois(n, xbar) ))

hist(lambdaHatStar, breaks = 50)
quantile(lambdaHatStar, probs = c(0.025, 0.975))



## secret!
lambda <- 17



## STAT 202 Lecture 14
## X^2 test for independence

data <- matrix(c(10, 40, 20, 30, 20, 40), nrow = 2)

data


chisq.test(data, correct = FALSE)

## pchisq
## 3.84
## 4.8485

pchisq(3.84, 1)

1 - pchisq(3.84, 1)

pchisq(3.84, 1, lower.tail = FALSE)

1 - pchisq(4.8485, 2)




data <- matrix(c(914, 46, 581, 735), nrow = 2)
data

chisq.test(data)



## secret value: lambda

xlim <- c(12, 25)
par(mfrow = c(2,2))
 
## generate n iid observations Poisson(lambda)
n <- 10      ## 1st time
n <- 100     ## 2nd time
 
set.seed(0)
(y <- rpois(n, lambda))

## let's now caluclate the MLE !!
(ybar <- mean(y))
 
xs <- seq(min(xlim), max(xlim), length = 10000)
ys <- sapply(xs, function(lam) prod(dpois(y, lam)))
plot(xs, ys, type='l', xlim=xlim, xlab='lambda', ylab='Likelihood', main=paste0('n = ',n))
 
abline(v = ybar, col = 'red')
 
ys2 <- sapply(xs, function(lam) sum(dpois(y, lam, log = TRUE)))
plot(xs, ys2, type='l', xlim=xlim, xlab='lambda', ylab='log-ikelihood', main=paste0('n = ',n))
 
abline(v = ybar, col = 'red')
 
(se <- sqrt(ybar/n))
ci <- ybar + c(-1,1) * 2 * se
round(ci, 1)
ci[2] - ci[1]

segments(x0=ci[1], x1=ci[2], y0=max(ys2), col='blue', lwd=2)
abline(v = ci, col = 'blue')
 
## secret value of lambda:

lambda




## doing STAT360 quiz3 question by simulation

N <- 1000000
x <- numeric(N)

th <- 1/3
for(i in 1:N) {
    x[i] <- rexp(1, th)
}

y <- x * (rbinom(N, 1, .5)*2-1)

mean(y)
var(y)

sum(y^2) / N


hist(y, breaks=100)




## STAT 202 Lecture 14
## Permutation Distribution

## preliminaries:
survey <- read.csv('~/github/courses/stat202/data/STAT202.csv')

head(survey)

gender <- survey$Gender
friends <- survey$Friends

gender
friends

length(gender)

## histogram
hist(friends)

## boxplots
boxplot(friends)
boxplot(friends ~ gender)

table(gender)

## using sample(friends)
sample(1:5)
sample(gender)

## permutation distribution
N <- 100000
difs <- numeric(N)

for(i in 1:N) {
    friendsP <- sample(friends)
    F <- friendsP[1:11]
    M <- friendsP[12:27]
    difs[i] <- mean(M) - mean(F)
}

difs

## histogram of permutation distribution
hist(difs, breaks = 100)

## confidence interval
## assuming *no assocuation*
ci <- quantile(difs, probs = c(0.025, 0.975))
abline(v = ci, col = 'blue', lwd = 2)

## the observed difference
od <- mean(friends[gender == 'male']) - mean(friends[gender == 'female'])
abline(v = od, col = 'red', lwd = 2)

## permutation p-value
pv <- mean(difs < od | difs > (-od))
pv

## two-samples t-test
## t.test(group1, group2)



females:
462
428
1338
555
0
574
721
586
316
407
1175

males:
325
531
1065
300
764
1217
100
910
970
500
201
268
0
461
280
850



## permutation distribution class problem for STAT202

y <- c(5,7,9,10,12,12,12,13,13,15,15,20,
       5,7,7,8,10,10,11,12,12,14,14,14,16,18,20,20,20,22,23,25,40)

indM <- 1:12
indF <- 13:33

y[indM]
y[indF]

N <- 100000
difs <- numeric(N)

for(i in 1:N) {
    newy <- sample(y)
    difs[i] <- mean(newy[indM]) - mean(newy[indF])
}

hist(difs, breaks = 50)

ci <- quantile(difs, c(0.025, 0.975))
ci
abline(v = ci, col = 'blue', lwd = 2)

obsdif <- mean(y[indM]) - mean(y[indF])
obsdif

abline(v = obsdif, col = 'red', lwd = 2)

mean(difs < obsdif | difs > -obsdif)

t.test(y[indM], y[indF])







library(CompGLM)
library(nimble)

ZICMPcode <- nimbleCode({
    p~dunif(0,1)
    lam~dunif(0,100)
    nu~dunif(0,100)
    for(i in 1:N) {
        y[i] ~ dZICMP(lam, nu,sumTo = 200L,zeroProb = p)
    }
})


## constants, data, and initial values
constants <- list(N = 100)

dCMP <- nimbleRcall(function(y = integer(), lam = double(), nu = double(),sumTo = integer(),logP = logical(0, default = 0)){}, Rfun = 'dcomp',
                    returnType = double() )##  DT: shouldn't need this: envir = .GlobalEnv)

rCMP <- nimbleRcall(function(n = integer(), lam = double(), nu = double(),sumTo = integer()){}, Rfun = 'rcomp',
                    returnType = integer()) ## DT: shouldn't need this: envir = .GlobalEnv)


dZICMP <- nimbleFunction(
    run = function(x = integer(), lam = double(), nu = double(),sumTo = integer(), zeroProb = double(), log = logical(0, default = 0)) {
        returnType(double())
        ## First handle non-zero data
        if(x != 0) {
            ## return the log probability if log = TRUE
            if(log) return(dCMP(x, lam, nu,sumTo = 200L, log = TRUE) + log(1-zeroProb))
            ## or the probability if log = FALSE
            else return((1-zeroProb) * dCMP(x, lam, nu,sumTo = 200L, log = FALSE))
        }
        ## From here down we know x is 0
        totalProbZero <- zeroProb + (1-zeroProb) * dCMP(0, lam, nu,sumTo = 200L, log = FALSE)
        if(log) return(log(totalProbZero))
        return(totalProbZero)
    })


rZICMP <- nimbleFunction(
    run = function(n = integer(), lam = double(), nu = double(),sumTo = integer(), zeroProb = double()) {
        returnType(integer())
        isStructuralZero <- rbinom(1, prob = zeroProb, size = 1)
        if(isStructuralZero) return(0)
        return(rCMP(1, lam,nu,sumTo = 200L))
    })


registerDistributions(list(
    dZICMP = list(
        BUGSdist = "dZICMP(lam, nu,sumTo, zeroProb)",
        discrete = TRUE,
        range = c(0, Inf),
        types = c('value = integer()', 'lam = double()', 'nu = double()','sumTo = integer()', 'zeroProb = double()')
    )))


## DT: let's include the model check.
## it's helpful, especially for such a small model:
ZICMPmodel <- nimbleModel( ZICMPcode, constants=constants) ##check = FALSE)

ZICMPmodel$calculate()   ## something is wrong here!
## DT:
## Error in if (x != 0) { : missing value where TRUE/FALSE needed
## looks like since we're missing values for y[i] (no initial values were provided)
## this first logical if() statement in your dZICMP() function definition
## is failing.  Let's provide initial values and try again:

inits <- list(p = 0.5,
              lam = 1,
              nu = 1)

data <- list(y = rep(1, constants$N))

ZICMPmodel <- nimbleModel( ZICMPcode, constants=constants, data=data, inits=inits)

ZICMPmodel$calculate()    ## -178.5251    DT: that's better

ZICMPmodel$p <- .4             
ZICMPmodel$lam <- 1.8
ZICMPmodel$nu<-0.9
ZICMPmodel$simulate('y')     

ZICMPmodel$calculate()    ##  -191.4907    DT: still good

simulatedData <- ZICMPmodel$y
simulatedData
hist(simulatedData)

ZICMPmodel$setData(list(y = simulatedData))  
cZICMPmodel <- compileNimble(ZICMPmodel)

## DT: let's configure the MCMC first, look at the samplers
## that are assigned, and the monitors.  Always
## helpful and a good idea to know what's going on in the MCMC:
##ZICMPmcmc <- buildMCMC(ZICMPmodel)   not this line, but instead:

conf <- configureMCMC(ZICMPmodel)

conf$printMonitors()
## thin = 1: p, lam, nu

conf$printSamplers()
## [1] RW sampler: p
## [2] RW sampler: lam
## [3] RW sampler: nu

## DT: now build the MCMC, and compile it:
ZICMPmcmc <- buildMCMC(conf)
    
cZICMPmcmc <- compileNimble(ZICMPmcmc, project = ZICMPmodel)

cZICMPmcmc$run(1000)   ## using fewer iterations

samples <- as.matrix(cZICMPmcmc$mvSamples)

samplesSummary(samples)
##           Mean      Median    St.Dev.    95%CI_low  95%CI_upp
## lam 0.62186699 0.560347129 0.18223126 0.4068033525 0.93935815
## nu  0.34242280 0.014943882 0.40012792 0.0026253462 0.90000000
## p   0.02232572 0.002589668 0.02716132 0.0009254903 0.05182992


library(basicMCMCplots)
samplesPlot(samples)    ## DT: looks like we need longer runs, but things are off to a good start







## creating new test for CAR conjugacy checking system,
## using skipExpansionsNode rather than old skipExpansions system

library(nimble)

code <- nimbleCode({
    S[1:N] ~ dcar_normal(adj[1:L], weights[1:L], numneighbours[1:N], 1)
    for(i in 1:K) {
        beta[i] ~ dnorm(0, 1)
    }
    for(i in 1:N){
        eta[i] <- inprod(beta[1:K], x[1:K])
        mu[i] <- S[i] + eta[i]
        y[i] ~ dnorm(mu[i], 1)
    }
})

N <- 3
L <- 4
K <- 7

constants <- list(N=N, L=L, K=K, adj=c(2,1,3,2), weights=rep(1,L), numneighbours=c(1,2,1))
data <- list(y = rep(0,N))
inits <- list(S = rep(0,N), beta = rep(0,K), x=1:K)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()

Rmcmc <- buildMCMC(conf)

class(Rmcmc) == 'MCMC'





code <- nimbleCode({
    for(sex in 1:2){
        kappa[sex] ~ dunif(0,50)
        sigma[sex] ~ dunif(0.1,20)
    }
    for(sex in 1:2){
        for(TOD in 1:2){
            lambda[TOD, sex] <- lambda0 * pow(beta[1],(TOD-1)) * pow(beta[2],(sex-1))
        }
    }
    PL ~ dunif(0.01,0.99)
    lambda0 <- -log(1-PL)
    beta[1] ~ dunif(0.1,10)
    beta[2] ~ dunif(0.1,10)
    for(sex in 1:2){
        Phi[sex] ~ dunif(0,1)
        for(k in 1:(n.prim-1)){
            phi[sex,k] <- pow(Phi[sex], dt[k])
        }
    }
    for(sex in 1:2){
        dmean[sex] ~ dunif(0,100)
        dlambda[sex] <- 1/dmean[sex]
    }
    for(i in 1:N[1]){
        z[i,first[i]] ~ dbern(1)
        S[i,1,first[i]] ~ dunif(xlow[i], xupp[i]) # Prior for the first x coordinate
        S[i,2,first[i]] ~ dunif(ylow[i], yupp[i]) # Prior for the first y coordinate
        g[i,first[i],1] <- 0
        for(r in 1:R){ # trap
            D[i,r,first[i]] <- sqrt(pow(S[i,1,first[i]]-X[r,1],2) + pow(S[i,2,first[i]]-X[r,2],2))
            g[i,first[i],r+1] <- exp(-pow(D[i,r,first[i]]/sigma[gr[i]], kappa[gr[i]])) # Trap exposure
        }
        G[i,first[i]] <- sum(g[i,first[i],1:(R+1)]) # Total trap exposure
        for(j in 1:J[i,first[i]]){
            P[i,j,first[i]] <- 1 - exp(-lambda[tod[first[i],j],gr[i]]*G[i,first[i]]) # Probability of being captured
            PPII[i,first[i],j] <- step(H[i,j,first[i]]-2)*(g[i,first[i],H[i,j,first[i]]]/(G[i,first[i]]+ 0.000000001))*P[i,j,first[i]] + (1-step(H[i,j,first[i]]-2))*(1-P[i,j,first[i]])
            Ones[i,j,first[i]] ~ dbern(PPII[i,first[i],j])
        }
    }
    for(i in (N[1]+1):N[2]){
        z[i,first[i]] ~ dbern(1)
        S[i,1,first[i]] ~ dunif(xlow[i], xupp[i]) # Prior for the first x coordinate
        S[i,2,first[i]] ~ dunif(ylow[i], yupp[i]) # Prior for the first y coordinate
        ## First primary session:
        g[i,first[i],1] <- 0
        for(r in 1:R){ # trap
            D[i,r,first[i]] <- sqrt(pow(S[i,1,first[i]]-X[r,1],2) + pow(S[i,2,first[i]]-X[r,2],2))
            g[i,first[i],r+1] <- exp(-pow(D[i,r,first[i]]/sigma[gr[i]], kappa[gr[i]])) # Trap exposure
        }
        G[i,first[i]] <- sum(g[i,first[i],1:(R+1)]) # Total trap exposure
        for(j in 1:J[i,first[i]]){
            P[i,j,first[i]] <- 1 - exp(-lambda[tod[first[i],j],gr[i]]*G[i,first[i]]) # Probability of being captured
            PPII[i,first[i],j] <- step(H[i,j,first[i]]-2)*(g[i,first[i],H[i,j,first[i]]]/(G[i,first[i]]+ 0.000000001))*P[i,j,first[i]] + (1-step(H[i,j,first[i]]-2))*(1-P[i,j,first[i]])
            Ones[i,j,first[i]] ~ dbern(PPII[i,first[i],j])
        }
        for(k in (first[i]+1):K[i]){ # primary session
            theta[i,k-1] ~ dunif(-3.141593,3.141593) # Prior for dispersal direction 
            z[i,k] ~ dbern(Palive[i,k-1])
            Palive[i,k-1] <- z[i,k-1]*phi[gr[i],k-1] # Pr(alive in primary session k) gr[i] = sex
            d[i,k-1] ~ dexp(dlambda[gr[i]])
            S[i,1,k] <- S[i,1,k-1] + d[i,k-1]*cos(theta[i,k-1])
            S[i,2,k] <- S[i,2,k-1] + d[i,k-1]*sin(theta[i,k-1])
            g[i,k,1] <- 0
            for(r in 1:R){ # trap
                D[i,r,k] <- sqrt(pow(S[i,1,k]-X[r,1],2) + pow(S[i,2,k]-X[r,2],2))  # Squared distance to trap
                g[i,k,r+1] <- exp(-pow(D[i,r,k]/sigma[gr[i]], kappa[gr[i]])) # Trap exposure
            }
            G[i,k] <- sum(g[i,k,1:(R+1)]) # Total trap exposure
            for(j in 1:J[i,k]){
                P[i,j,k] <- (1 - exp(-lambda[tod[k,j],gr[i]]*G[i,k]))*z[i,k] # Probability of being captured
                PPII[i,k,j] <- step(H[i,j,k]-2)*(g[i,k,H[i,j,k]]/(G[i,k] + 0.000000001))*P[i,j,k] + (1-step(H[i,j,k]-2))*(1-P[i,j,k])
                Ones[i,j,k] ~ dbern(PPII[i,k,j])
            }
        }
    }
})





dSCR1 <- nimbleFunction(
    run = function(x = double(2),
        first = double(), last = double(), J = double(1),
        lambda = double(1), tod = double(2),
        g = double(2), G = double(1), z = double(1),
        log = double()) {
        lp <- 0
        for(k in first:last) {    # primary session
            for(j in 1:J[k]) {    # secondary session
                logPnoCapture <- -lambda[tod[k,j]] * G[k] * z[k]
                if(x[j,k] == 1) {    # not captured
                    lp <- lp + logPnoCapture
                } else {             # captured
                    lp <- lp + log(1-exp(logPnoCapture)) + log(g[k, x[j,k]-1]) - log(G[k])
                }
            }
        }
        returnType(double())
        if(log) return(lp) else return(exp(lp))
    }
)

code_dSCR1 <- nimbleCode({
    ## space use and recapture probability parameters
    PL ~ dunif(0.01, 0.99)
    lambda0 <- -log(1-PL)
    for(sex in 1:2) {
        kappa[sex] ~ dunif(0,   50)
        sigma[sex] ~ dunif(0.1, 20)
        beta[sex]  ~ dunif(0.1, 10)    # misnomer: beta[1] is coeff of tod, beta[2] is coeff of sex
        for(TOD in 1:2) {
            lambda[TOD, sex] <- lambda0 * beta[1]^(TOD-1) * beta[2]^(sex-1)
        }
        ## survival parameters
        Phi[sex] ~ dunif(0, 1)
        for(k in 1:(nPrimary-1)) {
            phi[sex, k] <- Phi[sex]^dt[k]
        }
        ## dispersal parameters
        dmean[sex] ~ dunif(0, 100)
        dlambda[sex] <- 1/dmean[sex]
    }
    for(i in 1:nInd) {
        S[i, 1, first[i]] ~ dunif(xlow[i], xupp[i])  # initial center of activity (x)
        S[i, 2, first[i]] ~ dunif(ylow[i], yupp[i])  # initial center of activity (y)
        z[i, first[i]] <- 1
        for(k in first[i]:last[i]) {
            D[i, k, 1:R] <- sqrt((S[i, 1, k] - X[1:R, 1])^2 + (S[i, 2, k] - X[1:R, 2])^2)
            g[i, k, 1:R] <- exp(-(D[i, k, 1:R]/sigma[gr[i]])^kappa[gr[i]])  # trap exposure
            G[i, k] <- sum(g[i, k, 1:R])                                    # total trap exposure
        }
        for(k in first[i]:(last[i]-1)) {
            theta[i, k] ~ dunif(-3.141593, 3.141593)   # dispersal direction
            d[i, k] ~ dexp(dlambda[gr[i]])
            S[i, 1, k+1] <- S[i, 1, k] + d[i, k] * cos(theta[i, k])
            S[i, 2, k+1] <- S[i, 2, k] + d[i, k] * sin(theta[i, k])
            Palive[i, k] <- z[i, k] * phi[gr[i], k]
            z[i, k+1] ~ dbern(Palive[i, k])
        }
        ## likelihood
        H[i, 1:nSecondary, 1:nPrimary] ~ dSCR1(
            first = first[i], last = last[i], J = J[i,1:nPrimary],
            lambda = lambda[1:2,gr[i]], tod = tod[1:nPrimary,1:nSecondary],
            g = g[i,1:nPrimary,1:R], G = G[i,1:nPrimary], z = z[i,1:nPrimary])
    }
})







dSCR2 <- nimbleFunction(
    run = function(x = double(2),
        first = double(), last = double(), J = double(1),
        lambda = double(1), tod = double(2),
        g = double(2), G = double(1), z = double(1), phi = double(1),
        log = double()) {
        pAlive <- 1
        pDead <- 0
        lp <- 0
        ## probability of surviving from k to (k+1): phi[k]
        for(k in first:last) {
            if(z[k] == 1) {    # known to be alive
                if(k > first)           # survived
                    lp <- lp + log(phi[k-1])
                for(j in 1:J[k]) {
                    pNoCaptureGivenAlive <- exp(-lambda[tod[k,j]] * G[k])
                    if(x[j,k] == 1) {   # not captured
                        lp <- lp + log(pNoCaptureGivenAlive)
                    } else {            # captured
                        lp <- lp + log(1-pNoCaptureGivenAlive) + log(g[k, x[j,k]-1]) - log(G[k])
                    }
                }
            } else {           # could be dead or alive
                pTheseNonSightings <- 1
                for(j in 1:J[k]) {
                    pNoCaptureGivenAlive <- exp(-lambda[tod[k,j]] * G[k])
                    pTheseNonSightings <- pTheseNonSightings * pNoCaptureGivenAlive
                }
                pAlive_new <- phi[k-1] * pAlive
                pDead_new <- (1-phi[k-1]) * pAlive + pDead
                L <- pAlive_new * pTheseNonSightings + pDead_new
                pAlive <- (pAlive_new * pTheseNonSightings) / L
                pDead <- pDead_new / L
                lp <- lp + log(L)
            }
        }
        returnType(double())
        if(log) return(lp) else return(exp(lp))
    }
)

code_dSCR2 <- nimbleCode({
    ## space use and recapture probability parameters
    PL ~ dunif(0.01, 0.99)
    lambda0 <- -log(1-PL)
    for(sex in 1:2) {
        kappa[sex] ~ dunif(0,   50)
        sigma[sex] ~ dunif(0.1, 20)
        beta[sex]  ~ dunif(0.1, 10)    # misnomer: beta[1] is coeff of tod, beta[2] is coeff of sex
        for(TOD in 1:2) {
            lambda[TOD, sex] <- lambda0 * beta[1]^(TOD-1) * beta[2]^(sex-1)
        }
        ## survival parameters
        Phi[sex] ~ dunif(0, 1)
        for(k in 1:(nPrimary-1)) {
            phi[sex, k] <- Phi[sex]^dt[k]
        }
        ## dispersal parameters
        dmean[sex] ~ dunif(0, 100)
        dlambda[sex] <- 1/dmean[sex]
    }
    for(i in 1:nInd) {
        S[i, 1, first[i]] ~ dunif(xlow[i], xupp[i])  # initial center of activity (x)
        S[i, 2, first[i]] ~ dunif(ylow[i], yupp[i])  # initial center of activity (y)
        for(k in first[i]:last[i]) {
            D[i, k, 1:R] <- sqrt((S[i, 1, k] - X[1:R, 1])^2 + (S[i, 2, k] - X[1:R, 2])^2)
            g[i, k, 1:R] <- exp(-(D[i, k, 1:R]/sigma[gr[i]])^kappa[gr[i]])  # trap exposure
            G[i, k] <- sum(g[i, k, 1:R])                                    # total trap exposure
        }
        for(k in first[i]:(last[i]-1)) {
            theta[i, k] ~ dunif(-3.141593, 3.141593)   # dispersal direction
            d[i, k] ~ dexp(dlambda[gr[i]])
            S[i, 1, k+1] <- S[i, 1, k] + d[i, k] * cos(theta[i, k])
            S[i, 2, k+1] <- S[i, 2, k] + d[i, k] * sin(theta[i, k])
        }
        ## likelihood
        H[i, 1:nSecondary, 1:nPrimary] ~ dSCR2(
            first = first[i], last = last[i], J = J[i,1:nPrimary],
            lambda = lambda[1:2,gr[i]], tod = tod[1:nPrimary,1:nSecondary],
            g = g[i,1:nPrimary,1:R], G = G[i,1:nPrimary],
            z = z[i,1:nPrimary], phi = phi[gr[i],1:(nPrimary-1)])
    }
})




dSS <- nimbleFunction(
    run = function(x = double(1), S = double(1), lam = double(), log = double()) {
        dist <- sqrt(sum((x-S)^2))
        lp <- dexp(dist, rate = lam, log = TRUE) - log(dist)
        returnType(double())
        if(log) return(lp) else return(exp(lp))
    }
)

code_dSCR4 <- nimbleCode({
    ## space use and recapture probability parameters
    PL ~ dunif(0.01, 0.99)
    lambda0 <- -log(1-PL)
    for(sex in 1:2) {
        kappa[sex] ~ dunif(0,   50)
        sigma[sex] ~ dunif(0.1, 20)
        beta[sex]  ~ dunif(0.1, 10)    # misnomer: beta[1] is coeff of tod, beta[2] is coeff of sex
        for(TOD in 1:2) {
            lambda[TOD, sex] <- lambda0 * beta[1]^(TOD-1) * beta[2]^(sex-1)
        }
        ## survival parameters
        Phi[sex] ~ dunif(0, 1)
        for(k in 1:(nPrimary-1)) {
            phi[sex, k] <- Phi[sex]^dt[k]
        }
        ## dispersal parameters
        dmean[sex] ~ dunif(0, 100)
        dlambda[sex] <- 1/dmean[sex]
    }
    for(i in 1:nInd) {
        S[i, 1, first[i]] ~ dunif(xlow[i], xupp[i])  # initial center of activity (x)
        S[i, 2, first[i]] ~ dunif(ylow[i], yupp[i])  # initial center of activity (y)
        for(k in first[i]:last[i]) {
            D[i, k, 1:R] <- sqrt((S[i, 1, k] - X[1:R, 1])^2 + (S[i, 2, k] - X[1:R, 2])^2)
            g[i, k, 1:R] <- exp(-(D[i, k, 1:R]/sigma[gr[i]])^kappa[gr[i]])  # trap exposure
            G[i, k] <- sum(g[i, k, 1:R])                                    # total trap exposure
        }
        for(k in first[i]:(last[i]-1)) {
            ##theta[i, k] ~ dunif(-3.141593, 3.141593)   # dispersal direction
            ##d[i, k] ~ dexp(dlambda[gr[i]])
            ##S[i, 1, k+1] <- S[i, 1, k] + d[i, k] * cos(theta[i, k])
            ##S[i, 2, k+1] <- S[i, 2, k] + d[i, k] * sin(theta[i, k])
            S[i, 1:2, k+1] ~ dSS(S[i, 1:2, k], dlambda[gr[i]])
        }
        ## likelihood
        H[i, 1:nSecondary, 1:nPrimary] ~ dSCR2(   ## this model is SCR4
            first = first[i], last = last[i], J = J[i,1:nPrimary],
            lambda = lambda[1:2,gr[i]], tod = tod[1:nPrimary,1:nSecondary],
            g = g[i,1:nPrimary,1:R], G = G[i,1:nPrimary],
            z = z[i,1:nPrimary], phi = phi[gr[i],1:(nPrimary-1)])
    }
})
\end{verbatim}
\end{singlespace}





\begin{singlespace}
\begin{verbatim}
makeGrid <- function(xmin=0, ymin=0, xmax, ymax, resolution=1, buffer=0) {
    makeVals <- function(min, max, buf, res) {
        unique(c(rev(seq(min, min-buf, by = -res)), seq(min, max+buf, by = res)))
    }
    xvals <- makeVals(xmin, xmax, buffer, resolution)
    yvals <- makeVals(ymin, ymax, buffer, resolution)
    grid <- expand.grid(xvals, yvals)
    colnames(grid) <- c('x', 'y')
    ## unique ids:
    mult <- diff(range(grid$y/resolution)) + 1
    ids <- grid$x/resolution * mult + grid$y/resolution
    offset <- 1 - min(ids)
    require(nimble)
    makeIDdef <- substitute(
        nimbleFunction(
            run = function(xy = double(1)) {
                id <- xy[1]/RES * MULT + xy[2]/RES + OFFSET
                returnType(double())
                return(id)
            }
        ),
        list(RES = resolution,
             MULT = mult,
             OFFSET = offset))
    makeID <- eval(makeIDdef)
    ##length(grid$x/resolution * mult + grid$y/resolution)
    ##length(unique(grid$x/resolution * mult + grid$y/resolution))
    ids2 <- apply(grid, 1, function(xy) makeID(xy))
    if(!identical(ids+offset, ids2)) stop('something wrong')
    sorted <- sort(ids2, index.return = TRUE)
    gridReordered <- grid[sorted$ix, ]
    gridReordered$id <- sorted$x
    if(!all(apply(gridReordered, 1, function(row) makeID(row[1:2]) == row[3]))) stop('something wrong')
    return(list(grid = gridReordered, makeID = makeID))
}


xr <- range(constants$X[, 1])
yr <- range(constants$X[, 2])

buffer <- 40
exposureRadius = 40
resolution <- 7

makeGridReturn <- makeGrid(xmin=xr[1], xmax = xr[2], ymin=yr[1], ymax = yr[2], buffer = buffer, resolution = resolution)

grid <- makeGridReturn$grid
makeID <- makeGridReturn$makeID


findLocalTraps <- function(grid, traps, exposureRadius) {
    trtrapsBool <- apply(grid, 1, function(row) {
        apply(traps, 1, function(tp) {
            ##print(tp)
            sqrt(sum((row[1:2]-tp)^2)) <= exposureRadius
        })
    })
    trapsBool <- t(trtrapsBool)
    trapsInd <- apply(trapsBool, 1, which)
    numsTraps <- sapply(trapsInd, length)
    localTraps <- array(as.numeric(NA), c(dim(grid)[1], max(numsTraps)+1))
    for(i in seq_along(trapsInd)) {
        n <- numsTraps[i]
        localTraps[i,1] <- n
        if(n > 0)    localTraps[i, 2:(n+1)] <- trapsInd[[i]]
    }
    localTraps
}

## n = localTraps[i,1] gives the number of local traps
## localTraps[i, 2:(n+1)] gives the indices of the local traps
localTraps <- findLocalTraps(grid, constants$X, exposureRadius)   ## ~10 seconds for 9x more traps


constants$localTraps <- localTraps
constants$LTD1 <- dim(localTraps)[1]
constants$LTD2 <- dim(localTraps)[2]
constants$MaxNumberLocalTraps <- dim(localTraps)[2] - 1


code_dSCR6 <- nimbleCode({
    ## space use and recapture probability parameters
    PL ~ dunif(0.01, 0.99)
    lambda0 <- -log(1-PL)
    for(sex in 1:2) {
        kappa[sex] ~ dunif(0,   50)
        sigma[sex] ~ dunif(0.1, 20)
        beta[sex]  ~ dunif(0.1, 10)    # misnomer: beta[1] is coeff of tod, beta[2] is coeff of sex
        for(TOD in 1:2) {
            lambda[TOD, sex] <- lambda0 * beta[1]^(TOD-1) * beta[2]^(sex-1)
        }
        ## survival parameters
        Phi[sex] ~ dunif(0, 1)
        for(k in 1:(nPrimary-1)) {
            phi[sex, k] <- Phi[sex]^dt[k]
        }
        ## dispersal parameters
        dmean[sex] ~ dunif(0, 100)
        dlambda[sex] <- 1/dmean[sex]
    }
    for(i in 1:nInd) {
        S[i, 1, first[i]] ~ dunif(xlow[i], xupp[i])  # initial center of activity (x)
        S[i, 2, first[i]] ~ dunif(ylow[i], yupp[i])  # initial center of activity (y)
        Sdiscrete[i, 1, first[i]] <- round(S[i, 1, first[i]]/7) * 7            ## resolution = 7
        Sdiscrete[i, 2, first[i]] <- round(S[i, 2, first[i]]/7) * 7            ## resolution = 7
        for(k in first[i]:last[i]) {
            id[i, k] <- makeID(Sdiscrete[i,1:2,k])
            nLocalTraps[i, k] <- getNumLocalTraps6(idarg=id[i,k], localTrapNumbers = localTraps[1:LTD1,1], LTD1arg = LTD1)
            localTrapIndices[i, k, 1:MaxNumberLocalTraps] <- getLocalTrapIndices6(MAXNUM = MaxNumberLocalTraps, localTraps = localTraps[1:LTD1,1:LTD2], n = nLocalTraps[i, k], idarg = id[i,k])
            Ds[i, k, 1:MaxNumberLocalTraps] <- calcLocalTrapDists6(MAXNUM = MaxNumberLocalTraps, n = nLocalTraps[i,k], localTrapInd = localTrapIndices[i,k,1:MaxNumberLocalTraps], S = S[i,1:2,k], X = X[1:R,1:2])
            g[i, k, 1:R] <- calcLocalTrapExposure6(R = R, n = nLocalTraps[i,k], Ds = Ds[i,k,1:MaxNumberLocalTraps], localTrapInd = localTrapIndices[i,k,1:MaxNumberLocalTraps], sigma = sigma[gr[i]], kappa = kappa[gr[i]])
            G[i, k] <- sum(g[i, k, 1:R])                                    # total trap exposure
        }
        for(k in first[i]:(last[i]-1)) {
            S[i, 1:2, k+1] ~ dSS(S[i, 1:2, k], dlambda[gr[i]])
            Sdiscrete[i, 1:2, k+1] <- round(S[i, 1:2, k+1]/7) * 7            ## resolution = 7
        }
        ## likelihood
        H[i, 1:nSecondary, 1:nPrimary] ~ dSCR2(   ## this model is SCR6
            first = first[i], last = last[i], J = J[i,1:nPrimary],
            lambda = lambda[1:2,gr[i]], tod = tod[1:nPrimary,1:nSecondary],
            g = g[i,1:nPrimary,1:R], G = G[i,1:nPrimary],
            z = z[i,1:nPrimary], phi = phi[gr[i],1:(nPrimary-1)])
    }
})

getNumLocalTraps6 <- nimbleFunction(
    run = function(idarg = double(), localTrapNumbers = double(1), LTD1arg = double()) {
        if(idarg < 1)       {   return(0)   }
        if(idarg > LTD1arg) {   return(0)   }
        n <- localTrapNumbers[idarg]
        returnType(double())
        return(n)
    }
)

getLocalTrapIndices6 <- nimbleFunction(
    run = function(MAXNUM = double(), localTraps = double(2), n = double(), idarg = double()) {
        indices <- numeric(MAXNUM, 0)
        if(n > 0) {
            indices[1:n] <- localTraps[idarg, 2:(n+1)]
        }
        returnType(double(1))
        return(indices)
    }
)

calcLocalTrapDists6 <- nimbleFunction(
    run = function(MAXNUM = double(), n = double(), localTrapInd = double(1), S = double(1), X = double(2)) {
        Ds <- numeric(MAXNUM, 0)
        if(n > 0) {
            Ds[1:n] <- sqrt((S[1] - X[localTrapInd[1:n],1])^2 + (S[2] - X[localTrapInd[1:n],2])^2)
        }
        returnType(double(1))
        return(Ds)
    }
)

calcLocalTrapExposure6 <- nimbleFunction(
    run = function(R = double(), n = double(), Ds = double(1), localTrapInd = double(1), sigma = double(), kappa = double()) {
        g <- numeric(R, 0.00000000000001)      ## small value = 0.00000001
        if(n > 0) {
            g[localTrapInd[1:n]] <- exp(-(Ds[1:n]/sigma)^kappa)
        }
        returnType(double(1))
        return(g)
    }
)
\end{verbatim}
\end{singlespace}





#### secret!
##lambda <- 17
##xlim <- c(12, 25)
##par(mfrow = c(2,2))
## 
#### generate n iid observations Poisson(lambda)
##n <- 10      ## 1st time
##n <- 100     ## 2nd time
## 
##set.seed(0)
##(y <- rpois(n, lambda))
##(ybar <- mean(y))
## 
##xs <- seq(min(xlim), max(xlim), length = 10000)
##ys <- sapply(xs, function(lam) prod(dpois(y, lam)))
##plot(xs, ys, type='l', xlim=xlim, xlab='lambda', ylab='Likelihood', main=paste0('n = ',n))
## 
##abline(v = ybar, col = 'red')
## 
##ys2 <- sapply(xs, function(lam) sum(dpois(y, lam, log = TRUE)))
##plot(xs, ys2, type='l', xlim=xlim, xlab='lambda', ylab='log-ikelihood', main=paste0('n = ',n))
## 
##abline(v = ybar, col = 'red')
## 
##(se <- sqrt(ybar/n))
##ci <- ybar + c(-1,1) * 2 * se
##round(ci, 1)
## 
##segments(x0=ci[1], x1=ci[2], y0=max(ys2), col='blue', lwd=2)
##abline(v = ci, col = 'blue')
## 
#### secret value of lambda:
##lambda






library(nimble)

Rnf <- nimbleFunction(
    run = function(a = double(1)) {
        X <- diag(a)
        if(length(a) == 1) X <- array(a, c(1,1))
        return(X)
        returnType(double(2))
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

a <- c(3,4)

diag(a)
Rnf(a)
Cnf(a)





code
constants
data
inits


Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()



x1 <- c(150, 165, 135)
x2 <- c(130, 140, 120)


d <- x1 - x2

mean(d)
(s <- sd(d))
(se <- s/sqrt(3))
(t <- qt(0.975, 2))

mean(d) + c(-1,1) * t * se


(158+290) / (158+290+515+1134)
(158+515) / (158+290+515+1134)

(z <- (515-290) / sqrt(515+290))


## part (a)
R <- array(c(1, .63, .45, .63, 1, .35, .45, .35, 1), c(3,3))
Psi <- diag(c(.19, .51, .75))
L <- array(c(.9, .7, .5), c(3,1))
L %*% t(L) + Psi   ## same as R !!

## part (c)

## principal factor method:
## https://rpubs.com/aaronsc32/factor-analysis-principal-factor-method

## I'm guessing you "coded" this with her in class:

communalities <- 1 - 1/diag(solve(R))
communalities

Rstar <- R
diag(Rstar) <- communalities
Rstar   ## the "reduced correlation matrix"

eig <- eigen(Rstar)
eig     ## no longer positive definite; that's ok
eig$vectors[,1] * sqrt(eig$values[1])   ## k=1 factor model loadings

## agrees:
library(psych)
fa(R, nfactors = 1, max.iter = 1, rotate = 'none', fm = 'pa')


#### This is the "reduced correlation matrix", just L * L',
#### .... if we're using the 1-factor model given in (a)
#### communalities:
##communalities <- diag(L %*% t(L))
##communalities
#### find k-1 factor model loadings:
##Rstar <- R
##diag(Rstar) <- communalities
##eig <- eigen(Rstar)
##eig
##eig$vectors[,1] * sqrt(eig$values[1])

## experimenting with "naming" of models,
## and extracting the (informative) model names, later

library(nimble)

code <- nimbleCode({a ~ dnorm(0, 1)})
Rmodel2 <- nimbleModel(code, inits = list(a=0), name = 'my_unique_name:XXX_is_reallY_long_is_that_ok_with_NIBMEL_any_problems_with_thisc,olon:double:,Colon_more_u,dners=cores:_:_:_:_:')
Cmodel2 <- compileNimble(Rmodel2)


Rmodel2$getModelDef()$name
Cmodel2$getModelDef()$name


Cmodel <- compileNimble(Rmodel)

Rmodel$getModelDef()$name
Cmodel$getModelDef()$name

nsgpModel <- Cmodel


## trying besselK() function with different
## dimensions of array / vector / scalar arguments
library(nimble)

Rnf <- nimbleFunction(
    run = function(A = double(2), B = double(0)) {
        returnType(double(1))
        return(besselK(A, B))
    }
)

Cnf <- compileNimble(Rnf)

(A <- array(1:8, c(2,4)))
#(B <- array(1:8, c(2,4)))
(B <- 3)


besselK(A, B)
Rnf(A, B)
Cnf(A, B)

array(Cnf(A, B), c(2,4))








## experimenting with the X^2 chi-squared distribution
## of the quadratic form (x-mu)' %*% Sigma^-1 %*% (x-mu),
## which appears in the exponent of the MVN density


library(mvtnorm)
N <- 10000
df <- 5
dfGen <- 15

if(FALSE) {
    set.seed(0)
    x <- rchisq(N, df)
}

mu <- 1:dfGen

set.seed(0)
tt <- array(rnorm(dfGen^2), c(dfGen, dfGen))
ttt <- tt + t(tt)
Sigma <- ttt %*% t(ttt)
SigmaDiag <- Sigma * diag(dfGen)

set.seed(0)
X <- rmvnorm(N, mu, Sigma)

x <- apply(X, 1, function(x) ((x[1:df]-mu[1:df]) %*% solve(Sigma[1:df,1:df]) %*% t(t(x[1:df]-mu[1:df])))[1,1]  )

##x <- apply(X, 1, function(x) ((x[1:df]-mu[1:df]) %*% solve(SigmaDiag[1:df,1:df]) %*% t(t(x[1:df]-mu[1:df])))[1,1]  )


xlim <- c(0, max(x))
hist(x, prob = TRUE, breaks = 100, xlim = xlim)
xs <- seq(min(xlim), max(xlim), by = 0.01)
ys <- dchisq(xs, df)
lines(xs, ys, col = 'red', lwd = 2)




## STAT202 quiz #2
## problem #1

x <- c(0, 0, 0, 1, 2, 3, 4, 4, 13)
(n <- length(x))  ## n = 9
(xbar <- mean(x))  ## xbar = 3
(tcv <- qt(0.95, n-1))   ## t critical value (df = 8) is 1.86
(s <- sd(x))  ## s = 4.092
(se <- s / sqrt(n))  ## 1.364225
(ci <- xbar + c(-1, 1) * tcv * se)   ## 0.463, 5.537


## part (a)

score <- read.csv('~/Downloads/Scorecard.csv')

sat <- score$SAT_AVG
loan <- score$PCTFLOAN

## looks like there's one missing value... let's remove it:
any(is.na(loan))   ## TRUE
sum(is.na(loan))   ## 1

keep.index <- !is.na(loan)

sat <- sat[keep.index]
loan <- loan[keep.index]

## that's better
any(is.na(loan))   ## FALSE

plot(sat, loan, pch = 20)

## I'd add density plots (or histogram) of each variable separately.
## also explicitly show the correlation:

par(mfrow = c(1,2))
plot(density(sat), main = 'Average SAT Score')
plot(density(loan), main = 'Percent Loan')

## here: comment on their shapes, or whatever

cor(sat, loan)   ## -0.4803502

## here: comment on this high negative correlation

## makes more sense to me, to use "loan" as the x-variable (predictor),
## and "sat" as the outcome.  but do whatever....
par(mfrow = c(1,1))
m <- lm(sat ~ loan)
plot(loan, sat, pch = 20)
abline(m, col = 'red')

summary(m)

## here: discuss the slope, intercept, interpretation
## and fact that the slope is highly "significant"


## part (b)

quants <- c(.25, .5, .75, .95)
qchisq(quants, 2)

## set up the mean vector and covariance matrix
mu <- c(mean(sat), mean(loan))

##Sigma <- rbind(c(var(sat), cov(sat, loan)),
##               c(cov(sat, loan), var(loan)))

## this way easier?
Sigma <- cov(cbind(sat, loan))

## set up A as pairs of observations
A <- cbind(sat, loan)

## calculate the quadratic form for each observation in the data set

## please pre-allocate b here:
b <- numeric(nrow(A))
for(i in 1:nrow(A)){
    b[i] <- t(A[i,]-mu)%*%solve(Sigma)%*%(A[i,]-mu)
}

## find the proportion inside each contour
for(i in 1:length(quants)){
    print(sum(b <= qchisq(quants[i], 2))/length(b))
}

## part (c)

mu <- c(mean(sat), mean(loan))

s11 <- sd(sat)
s22 <- sd(loan)
rho <- cor(sat, loan)
Sigma <- matrix(c(s11^2, s11*s22*rho, s11*s22*rho, s22^2), 2, 2)
SigmaInv <- solve(Sigma)


ngrid <- 100
x1grid <- seq(.8*min(sat), 1.2*max(sat), length = ngrid)
x2grid <- seq(-.2, 1.2, length = ngrid)

Xgrid <- expand.grid(x1grid, x2grid)

qf <- matrix(apply(Xgrid, 1, function(x12) (t(x12-mu) %*% SigmaInv %*% (x12-mu))[1,1]), length(x1grid), length(x2grid))

cv <- qchisq(0.997, 2)
qf997 <- ifelse(qf > cv, 1, 0)

qfData <- apply(cbind(sat, loan), 1, function(x12) (t(x12-mu) %*% SigmaInv %*% (x12-mu))[1,1])
extremeInd <- which(qfData > cv)

plot(sat, loan, xlab ="Average SAT Score", ylab = "Pct of Undergrads Receiving Fed.", col = "blue", pch = 20, xlim = c(600, 1600), ylim = c(0, 1.2))
contour(x1grid, x2grid, qf997, col = "red", add = TRUE)
points(sat[extremeInd], loan[extremeInd], col = "red", pch = 17)

score[keep.index, "INSTNM"][extremeInd]
##  [1] Southwest Tennessee Community College    
##  [2] University of the Virgin Islands         
##  [3] CUNY Medgar Evers College                
##  [4] CUNY York College                        
##  [5] Motlow State Community College           
##  [6] CUNY New York City College of Technology 
##  [7] CUNY John Jay College of Criminal Justice
##  [8] Oakland City University                  
##  [9] Hebrew Theological College               
## [10] University of Puerto Rico-Carolina       
## [11] Southeastern Baptist Theological Seminary
## [12] College of the Ozarks                    
## [13] University of Puerto Rico-Cayey          
## [14] Vanderbilt University                    
## [15] Princeton University                     
## [16] Yale University                          
## [17] Harvard University                       
## [18] Massachusetts Institute of Technology    
## [19] California Institute of Technology       





lines(trans3d(cont[[1]]$x, cont[[1]]$y, cont[[1]]$level, pmat), col=2, lty=2)





plot(sat, loan, xlab ="Average SAT Score", ylab = "Pct of Undergrads Receiving Fed.", col = "blue", pch = 20)

mu <- c(mean(sat), mean(loan))

n <- 100
x1 <- seq(min(sat), max(sat), length = n)
x2 <- seq(min(loan), max(loan), length = n)
X <- expand.grid(x1, x2)

library(mvtnorm)
dens <- matrix(apply(X, 1, function(x) dmvnorm(x, mu, Sigma)), n, n)

cont <- contourLines(x1, x2, dens, nlevels=3)
cont
lines(cont[[1]]$x, cont[[1]]$y, col=2)




dmvnorm(X[1000,], mean = mu, sigma = Sigma)

est <- bkde2D(cbind(sat, loan), bandwidth = .5, gridsize=rep(1000,2))
contour(est$x1, est$x2, est$fhat, add=TRUE, col = "red")


cont <- contourLines(seq(-2,12,,100), seq(-2,12,,100), matrix(mvds,100,100), levels=0.05^2)
lines(trans3d(cont[[1]]$x, cont[[1]]$y, cont[[1]]$level, pmat), col=2, lty=2)



class(est$fhat)
dim(est$fhat)



library(mvtnorm)

mean(sat)
mean(loan)
mu


dens <- apply(cbind(sat, loan), 1, function(x) dmvnorm(x, mu, Sigma))
q

outer(x1, x2, function(x,y) dmvnorm(c(x,y), mu, Sigma))


dmvnorm(c(1000, 1), mu, Sigma)

dmvnorm(c(sat[1], loan[1]), mu, Sigma)


z <- mapply(sat, loan, dmvnorm, mean = mu, sigma = Sigma)


dens <- function(xx1, xx2) dmvnorm(c(xx1, xx2), mean = mu, sigma = Sigma)



z997 <- outer(x1, x2, dens)

library(lattice)
contourplot

SigmaInverse <- solve(Sigma)

x1 <- seq(min(sat), max(sat), length = length(sat))
x2 <- seq(min(loan), max(loan), length = length(sat))





x1 <- 1:5
x2 <- 1:5

outer(1:3, 1:5, function(x, y) x+y)

dens <- function(xx1, xx2) {
    ##print(x1)
    x <- c(xx1, xx2)
    as.numeric(t(x-mu) %*% SigmaInverse %*% (x-mu))
}


z997 <- outer(x1, x2, dens)

dens(x1[1], x2[1])

plot(sat, loan, xlab ="Average SAT Score", ylab = "Pct of Undergrads Receiving Fed.", col = "blue", pch = 20)

z997 <- outer(x1, x2, function(xx1, xx2) {
        term1 <- 1/(2*pi*sqrt(s11*s22*(1-rho^2)))
        term2 <-  -1/(2*(1-rho^2))
        term3 <- (xx1-mu1)^2/s11
        term4 <- (xx2-mu2)^2/s22
        term5 <- 2*rho*((xx1-mu1)*(xx2-mu2))/(sqrt(s11)*sqrt(s22))
        result <- term1*exp(term2*(term3+term4+term5))
        return(as.numeric(result > outlierNo))
    })

contour(x1, x2, z997, add = TRUE)
              



library(KernSmooth)
colpalette = c("blue", "red")

pairs(bodyfat[, c("Chest", "Waist", "Neck")],
      panel = function(x,y){
          points(x,y, col = colpalette[cut(bodyfat$Pct.BF,c(0,20,100))])
          den <- bkde2D(cbind(x,y)[cut(bodyfat$Pct.BF,c(-1,20,100)) == "(-1,20]",], bandwidth = 2)
          contour(denx1, denx2,denfhat, add = TRUE, col = "blue")
          den < âˆ’bkde2D(cbind(x, y)[cut(bodyfatPct.BF,c(-1,20,100)) == "(20,100]",], bandwidth = 2)
          contour(denx1, denx2,den$fhat,add=TRUE,col=â€˜redâ€™)
      }
      )



library(KernSmooth)
est <- bkde2D(cbind(sat, loan), bandwidth = c(1,1), gridsize=rep(1000,2))

plot(sat, loan, type = 'p')
contour(est$x1, est$x2, est$fhat, add = TRUE)

hist(loan, breaks = 50)

sat

summary(sat)
summary(loan)


dim(score)
any(is.na(score))
head(score)

complete.cases(score)

dim(score)
names(score)
summary(score)
str(score)

score$CONTROL

score$SAT_AVG
score$


a <- besselK(1,1)
b <- besselK(1,2)
c <- besselK(1,3)
vec <- c(a,b,c)

library(nimble)

code <- nimbleCode({
    a <- besselK(1, 1)
    b <- 1
    c <- besselK(b, 1)
})

Rmodel <- nimbleModel(code)


Rmodel$calculate()

Rmodel$a

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)
samplesPlot(samples)
apply(samples, 2, effectiveSize)




Rnf <- nimbleFunction(
    run = function(a = double(), b = double(2)) {
        returnType(double(1))
        d <- besselK(a, b)
        return(d)
    }
)

Cnf <- compileNimble(Rnf)

a <- 1
b <- array(1:9, c(3,3))

besselK(a, b)
Rnf(a, b)
Cnf(a, b)


Rnf2 <- nimbleFunction(
    run = function(a = double(1), b = double(1)) {
        returnType(double(1))
        d <- besselK(a, b)
        ddim <- dim(d)
        print(ddim)
        ##print('dim(d) = ', nimDim(d))
        return(d)
    }
)

Cnf2 <- compileNimble(Rnf2)







## STAT360 Lecture 8

## Poisson samples for lambda = 3, 9, 27, 81
## and overlaid Normal approximations
k <- 4
nSamples <- 10000
par(mfrow = c(k,1))
for(i in 1:k) {
    lambda <- 3^i
    x <- rpois(nSamples, lambda)
    hist(x, prob = TRUE, main = paste0('lambda = ', lambda),
         breaks = 10*sqrt(lambda), xlab = '', ylab = '')
    r <- range(x)
    xs <- seq(min(x)-2*lambda, max(x)+2*lambda, by = 0.01)
    ys <- dnorm(xs, mean(x), sd(x))
    lines(xs, ys, col = 'red', lwd = 2)
}

## *Standardized* Poisson samples (x-lambda)/sqrt(lambda)
## and overlaid *standard* N(0,1) curves
k <- 4
nSamples <- 10000
par(mfrow = c(k,1))
for(i in 1:k) {
    lambda <- 3^i
    x <- (rpois(nSamples, lambda) - lambda) / sqrt(lambda)
    hist(x, prob = TRUE, main = paste0('lambda = ', lambda),
         breaks = 6*sqrt(lambda), xlab = '', ylab = '', xlim = c(-4,4))
    r <- range(x)
    xs <- seq(min(x)-2*lambda, max(x)+2*lambda, by = 0.01)
    ys <- dnorm(xs, 0, 1)
    lines(xs, ys, col = 'red', lwd = 2)
}


n <- 9
n <- 36
n <- 144

x <- runif(n, 50, 100)
x
round(x)
sum(x - round(x))

s <- replicate(1000, {
    x <- runif(n, 50, 100)
    x
    round(x)
    sum(x - round(x))
})

hist(s)
sd(s)


## STAT202 Lecture7: Bootstrapping

## daily Williamstown MA temperatures in
## September 2018
temps <- c(87, 81, 79, 87, 75, 73, 67, 73, 75, 77,
           81, 81, 84, 88, 91, 88, 91, 86, 81, 91,
           68, 68, 76, 80, 65, 67, 72)

temps <- c(133, 120, 97, 106, 124)

temps

se <- sd(temps) / sqrt(5)
se

mean(temps) + c(-1, 1) * qt(0.975, df=4) * se

t.test(temps)

## BOOTSTRAPPING
## parameter of interest:
## Williamstown, MA
## average daily temperature in September

N <- 10000
means <- numeric(N)

## let's learn the "sample" function
sample(1:10, replace = TRUE)

for(i in 1:N) {
    bootstrap_sample <- sample(temps, replace = TRUE)
    ##means[i] <- mean(bootstrap_sample)
    means[i] <- mean(bootstrap_sample)
}

## histogram of "means" of bootstrap resamples
hist(means, xlim = range(temps), breaks = 40)
hist(means, breaks = 40)
## add original temperature data points
points(temps, rep(0, length(temps)), pch=19, col = 'blue')
## vertical red line at sample mean
abline(v = mean(temps), col = 'red', lwd = 3)

## calculate the 95% bootstrap CI
BCI <- quantile(means, probs = c(0.025, 0.975))
BCI

BCI99 <- quantile(means, probs = c(0.005, 0.995))
BCI99


## vertical purple lines showing the bootstrap CI
abline(v = BCI, col = 'purple', lwd = 3)
abline(v = BCI99, col = 'green', lwd = 3)


se <- sd(temps) / sqrt(5)
se

mean(temps) + c(-1, 1) * 1.96 * se








## do unit testing of cc_checkLinearity too

library(nimble)

code <- nimbleCode({
    for(i in 1:n) 
        y[i] ~ dnorm(b0 + inprod(beta[1:p], X[i, 1:p]), 1)
    for(i in 1:p) 
        beta[i] ~ dnorm(0, 1)
    b0 ~ dnorm(0, 1)
    xx ~ dbeta(1, 1)
    yy ~ dbern(xx)
})

constants <- list(n = 5, p = 3)
data <- list(y = rnorm(constants$n),
             X = matrix(rnorm(constants$n * constants$p), constants$n),
             yy = 1)
inits <- list(b0 = 1, beta = rnorm(constants$p), xx = 0.5)

m <- nimbleModel(code, data = data, constants = constants)

conf <- configureMCMC(m)

conf$printSamplers()

conf$getSamplerDefinition(1)
d <- conf$getSamplerDefinition(1)

expect_identical(conf$getSamplers()[[1]]$name, 'conjugate_dnorm_dnorm',
                 info = "conjugacy with inprod not detected")

conf$getSamplerDefinition(5)



$setup
function (model, mvSaved, target, control) {
    calcNodes <- model$getDependencies(target)
    calcNodesDeterm <- model$getDependencies(target, determOnly = TRUE)
    dep_dnorm_nodeNames <- control$dep_dnorm
    N_dep_dnorm <- length(control$dep_dnorm)
    dep_dnorm_values <- array(0, dim = N_dep_dnorm)
    dep_dnorm_tau <- array(0, dim = N_dep_dnorm)
    dep_dnorm_offset <- array(0, dim = N_dep_dnorm)
    dep_dnorm_coeff <- array(0, dim = N_dep_dnorm)
    contribution_mean <- 0
    contribution_tau <- 0
}

$run
function () {
    prior_mean <- model$getParam(target[1], "mean")
    prior_tau <- model$getParam(target[1], "tau")
    for (iDep in 1:N_dep_dnorm) {
        dep_dnorm_values[iDep] <<- model$getParam(dep_dnorm_nodeNames[iDep], "value")
        dep_dnorm_tau[iDep] <<- model$getParam(dep_dnorm_nodeNames[iDep], "tau")
    }
    model[[target]] <<- 0
    model$calculate(calcNodesDeterm)
    for (iDep in 1:N_dep_dnorm) dep_dnorm_offset[iDep] <<- model$getParam(dep_dnorm_nodeNames[iDep], "mean")
    model[[target]] <<- 1
    model$calculate(calcNodesDeterm)
    for (iDep in 1:N_dep_dnorm) dep_dnorm_coeff[iDep] <<- model$getParam(dep_dnorm_nodeNames[iDep], "mean") - dep_dnorm_offset[iDep]
    contribution_mean <<- 0
    contribution_tau <<- 0
    for (iDep in 1:N_dep_dnorm) {
        contribution_mean <<- contribution_mean + dep_dnorm_coeff[iDep] * (dep_dnorm_values[iDep] - dep_dnorm_offset[iDep]) * dep_dnorm_tau[iDep]
        contribution_tau <<- contribution_tau + dep_dnorm_coeff[iDep]^2 * dep_dnorm_tau[iDep]
    }
    newValue <- rnorm(1, mean = (prior_mean * prior_tau + contribution_mean)/(prior_tau + contribution_tau), sd = (prior_tau + contribution_tau)^(-0.5))
    model[[target]] <<- newValue
    calculate(model, calcNodes)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
}

$getPosteriorLogDensity
function () {
    prior_mean <- model$getParam(target[1], "mean")
    prior_tau <- model$getParam(target[1], "tau")
    for (iDep in 1:N_dep_dnorm) {
        dep_dnorm_values[iDep] <<- model$getParam(dep_dnorm_nodeNames[iDep], "value")
        dep_dnorm_tau[iDep] <<- model$getParam(dep_dnorm_nodeNames[iDep], "tau")
    }
    model[[target]] <<- 0
    model$calculate(calcNodesDeterm)
    for (iDep in 1:N_dep_dnorm) dep_dnorm_offset[iDep] <<- model$getParam(dep_dnorm_nodeNames[iDep], "mean")
    model[[target]] <<- 1
    model$calculate(calcNodesDeterm)
    for (iDep in 1:N_dep_dnorm) dep_dnorm_coeff[iDep] <<- model$getParam(dep_dnorm_nodeNames[iDep], "mean") - dep_dnorm_offset[iDep]
    contribution_mean <<- 0
    contribution_tau <<- 0
    for (iDep in 1:N_dep_dnorm) {
        contribution_mean <<- contribution_mean + dep_dnorm_coeff[iDep] * (dep_dnorm_values[iDep] - dep_dnorm_offset[iDep]) * dep_dnorm_tau[iDep]
        contribution_tau <<- contribution_tau + dep_dnorm_coeff[iDep]^2 * dep_dnorm_tau[iDep]
    }
    targetValue <- model[[target]]
    posteriorLogDensity <- dnorm(targetValue, mean = (prior_mean * prior_tau + contribution_mean)/(prior_tau + contribution_tau), sd = (prior_tau + contribution_tau)^(-0.5), log = 1)
    returnType(double())
    return(posteriorLogDensity)
}





## compare to conjugate sampler using summed contributions
mcmc <- buildMCMC(conf)
cm <- compileNimble(m)
cmcmc <- compileNimble(mcmc, project = m)
smp1 <- runMCMC(cmcmc, 50, setSeed = 1)

code <- nimbleCode({
    for(i in 1:n) 
        y[i] ~ dnorm(b0 + beta[1]*X[i,1] + beta[2]*X[i,2] + beta[3]*X[i,3], 1)
    for(i in 1:p) 
        beta[i] ~ dnorm(0, 1)
    b0 ~ dnorm(0, 1)
})
m <- nimbleModel(code, data = data, constants = constants)
conf <- configureMCMC(m)

conf$printSamplers()


mcmc <- buildMCMC(conf)
cm <- compileNimble(m)
cmcmc <- compileNimble(mcmc, project = m)
smp2 <- runMCMC(cmcmc, 50, setSeed = 1)
expect_equal(smp1, smp2, info = 'conjugate sampler with inprod does not match summation')

code <- nimbleCode({
    for(i in 1:n) 
        y[i] ~ dnorm(b0 + inprod(exp(beta[1:p]), X[i, 1:p]), 1)
    for(i in 1:p) 
        beta[i] ~ dnorm(0, 1)
    b0 ~ dnorm(0, 1)
})
constants <- list(n = 5, p = 3)
m <- nimbleModel(code, data = data, constants = constants)
conf <- configureMCMC(m)

conf$printSamplers()


code <- nimbleCode({
    for(i in 1:n) 
        y[i] ~ dnorm(b0 + inprod(beta[1:p], beta[1:p]), 1)
    for(i in 1:p) 
        beta[i] ~ dnorm(0, 1)
    b0 ~ dnorm(0, 1)
})
constants <- list(n = 5, p = 3)
m <- nimbleModel(code, data = list(), constants = constants)
conf <- configureMCMC(m)

conf$printSamplers()



expect_identical(conf$getSamplers()[[1]]$name, 'RW',
                 info = "conjugacy with inprod improperly detected")

code <- nimbleCode({
    for(i in 1:n) 
        y[i] ~ dnorm(b0 + (X[i, 1:p] %*% beta[1:p])[1,1], 1)
    for(i in 1:p) 
        beta[i] ~ dnorm(0, 1)
    b0 ~ dnorm(0, 1)
})
constants <- list(n = 5, p = 3)
m <- nimbleModel(code, data = data, constants = constants)
conf <- configureMCMC(m)

conf$printSamplers()


code <- nimbleCode({
    for(i in 1:n) 
        y[i] ~ dnorm(b0 + (beta[1:p] %*% beta[1:p])[1,1], 1)
    for(i in 1:p) 
        beta[i] ~ dnorm(0, 1)
    b0 ~ dnorm(0, 1)
})
constants <- list(n = 5, p = 3)
m <- nimbleModel(code, constants = constants)
conf <- configureMCMC(m)

conf$printSamplers()





expect_identical(conf$getSamplers()[[1]]$name, 'conjugate_dnorm_dnorm',
                 info = "conjugacy with inprod not detected")


mcmc <- buildMCMC(conf)
cm <- compileNimble(m)
cmcmc <- compileNimble(mcmc, project = m)
smp3 <- runMCMC(cmcmc, 50, setSeed = 1)
expect_equal(smp1, smp3, info = 'conjugate sampler with matrix mult. does not match summation')

check <- nimble:::cc_checkLinearity(quote(b0 + (X[1, 1:3] %*% structureExpr(beta[1], beta[2], beta[3]))[1,1]), 'beta[1]')


expect_identical(check, list(offset = quote(b0 + X[1, 1:3] * structureExpr(beta[1], beta[2], beta[3])),
                             scale = quote(X[1, 1:3])))

check <- nimble:::cc_checkLinearity(quote(b0 + inprod(structureExpr(beta[1], beta[2], beta[3]), X[1, 1:3])), 'beta[1]')
expect_identical(check, list(offset = quote(b0 + structureExpr(beta[1], beta[2], beta[3]) * X[1, 1:3]),
                             scale = quote(X[1, 1:3])))





## looking into discrapancy between runMCMC and MCMCsuite,
## sent to NIMBLE Users List by Hamze, 2/4/2019


library(nimble)

Pf <- matrix(c(4729.4, 236.58, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 
 236.6,  46.49, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 
   0.0,   0.00, 4.850e+03, 5.205e+02, 6.785e-59, 7.285e-60, 
   0.0,   0.00, 5.205e+02, 1.310e+02, 7.283e-60, 1.829e-60, 
   0.0,   0.00, 6.785e-59, 7.283e-60, 4.850e+03, 5.207e+02, 
   0.0,   0.00, 7.285e-60, 1.829e-60, 5.207e+02, 1.312e+02), nrow=6)

mu.f <- c(156.69,  17.29, 192.87,  18.98, 192.87,  18.98)

multiVarCode <- nimbleCode({
    X.mod[1:6] ~ dmnorm(mean = muf[1:6], cov = pf[1:6, 1:6])
})


data.tobit <- list(muf = as.vector(mu.f), pf = Pf)

inits.pred <- list(X.mod = as.vector(mu.f))

model_pred <- nimbleModel(multiVarCode,
                          data = data.tobit,
                          inits = inits.pred,
                          name = 'base')

conf <- configureMCMC(model_pred, print=TRUE)

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(model_pred)

Cmcmc <- compileNimble(Rmcmc, project=model_pred)

set.seed(0)  ## NEW

## REMOVED samplesAsCodaMCMC = TRUE
## only running one chain: nchains = 1
## *** no longer using any thinning ***
samples.list <- runMCMC(Cmcmc, niter = 5e6, nburnin=1e3, nchains = 1) ###, thin=500)
## runMCMC's handling of nburnin changed in nimble version 0.6-11. Previously, nburnin samples were discarded *post-thinning*.  Now nburnin samples are discarded *pre-thinning*.  The number of samples returned will be floor((niter-nburnin)/thin).

dim(samples.list)
## [1] 4999000       6

head(samples.list)
##      X.mod[1]  X.mod[2] X.mod[3]  X.mod[4] X.mod[5] X.mod[6]
## [1,] 127.7350  8.356631 251.3212 29.933980 322.9569 36.80349
## [2,] 179.4979  7.172669 287.5826 35.495803 289.0479 30.53078
## [3,] 240.9356 30.002400 280.4888 36.332780 275.9722 23.82634
## [4,] 101.2537  8.974672 202.8461  7.639825 136.1938 12.02671
## [5,] 162.2434 17.101117 144.8465  4.340241 269.7960 17.10870
## [6,] 187.9230 21.628204 186.4102 14.226036 231.5753 23.53257

round(cov(samples.list), 3)
##          X.mod[1] X.mod[2] X.mod[3] X.mod[4] X.mod[5] X.mod[6]
## X.mod[1] 4724.721  236.376    0.992    0.312    1.147    0.120
## X.mod[2]  236.376   46.503   -0.063   -0.011    0.206    0.004
## X.mod[3]    0.992   -0.063 4856.244  521.009    0.314    0.031
## X.mod[4]    0.312   -0.011  521.009  131.064    0.436    0.028
## X.mod[5]    1.147    0.206    0.314    0.436 4846.351  520.523
## X.mod[6]    0.120    0.004    0.031    0.028  520.523  131.229


## *** no longer using any thinning ***
output <- MCMCsuite(multiVarCode,
                    data=data.tobit,
                    inits = inits.pred,
                    niter = 5e6,
                    burnin = 1e3,
                    ##thin = 500,
                    monitors = 'X.mod',
                    MCMCs = c('nimble'),
                    makePlot = FALSE)


dim(t(output$samples[1,,]))
## [1] 4999000       6

head(t(output$samples[1,,]))
##      X.mod[1]  X.mod[2] X.mod[3]  X.mod[4] X.mod[5] X.mod[6]
## [1,] 127.7350  8.356631 251.3212 29.933980 322.9569 36.80349
## [2,] 179.4979  7.172669 287.5826 35.495803 289.0479 30.53078
## [3,] 240.9356 30.002400 280.4888 36.332780 275.9722 23.82634
## [4,] 101.2537  8.974672 202.8461  7.639825 136.1938 12.02671
## [5,] 162.2434 17.101117 144.8465  4.340241 269.7960 17.10870
## [6,] 187.9230 21.628204 186.4102 14.226036 231.5753 23.53257

round(cov(t(output$samples[1,,])), 3)
##          X.mod[1] X.mod[2] X.mod[3] X.mod[4] X.mod[5] X.mod[6]
## X.mod[1] 4724.721  236.376    0.992    0.312    1.147    0.120
## X.mod[2]  236.376   46.503   -0.063   -0.011    0.206    0.004
## X.mod[3]    0.992   -0.063 4856.244  521.009    0.314    0.031
## X.mod[4]    0.312   -0.011  521.009  131.064    0.436    0.028
## X.mod[5]    1.147    0.206    0.314    0.436 4846.351  520.523
## X.mod[6]    0.120    0.004    0.031    0.028  520.523  131.229





## writing ns_dist_3d function for NNGP project

ns_dist <- function( coords, scale_factor = NULL ){
    N <- nrow(coords)
    ## Calculate distances
    dists1 <- as.matrix(dist(coords[,1], upper = T, diag = T))
    dists2 <- as.matrix(dist(coords[,2], upper = T, diag = T))
    temp1 <- matrix(coords[,1], nrow = N, ncol = N) 
    temp2 <- matrix(coords[,2], nrow = N, ncol = N) 
    sgn_mat1 <- ( temp1 - t(temp1) >= 0 )
    sgn_mat1[sgn_mat1 == FALSE] <- -1 
    sgn_mat2 <- ( temp2 - t(temp2) >= 0 )
    sgn_mat2[sgn_mat2 == FALSE] <- -1 
    dist1_sq <- dists1^2
    dist2_sq <- dists2^2
    dist12 <- sgn_mat1*dists1*sgn_mat2*dists2
    if( !is.null(scale_factor) ){
        dist1_sq <- dist1_sq/scale_factor  
        dist2_sq <- dist2_sq/scale_factor  
        dist12 <- dist12/scale_factor     
    }
    dimnames(dist1_sq) <- NULL
    dimnames(dist2_sq) <- NULL
    dimnames(dist12) <- NULL
    return(list(dist1_sq = dist1_sq, dist2_sq = dist2_sq, 
                dist12 = dist12, scale_factor = scale_factor))
}


ns_dist_3d <- function(coords, nID, scale_factor = NULL) {
    N <- nrow(coords)
    k <- ncol(nID)
    dist1_3d <- array(0, c(N, k+1, k+1))
    dist2_3d <- array(0, c(N, k+1, k+1))
    dist12_3d <- array(0, c(N, k+1, k+1))
    for(i in 2:N) {
        if(i<=k)     nNei <- i-1      else      nNei <- k
        ind <- c( nID[i,1:nNei], i )
        thisN <- nNei + 1
        theseCoords <- coords[ind, ]
        dists1 <- as.matrix(dist(theseCoords[,1]))
        dists2 <- as.matrix(dist(theseCoords[,2]))
        temp1 <- matrix(theseCoords[,1], nrow = thisN, ncol = thisN) 
        temp2 <- matrix(theseCoords[,2], nrow = thisN, ncol = thisN) 
        sgn_mat1 <- ( temp1 - t(temp1) >= 0 )
        sgn_mat1[sgn_mat1 == FALSE] <- -1 
        sgn_mat2 <- ( temp2 - t(temp2) >= 0 )
        sgn_mat2[sgn_mat2 == FALSE] <- -1 
        dist1_3d[i, 1:thisN, 1:thisN] <- dists1^2
        dist2_3d[i, 1:thisN, 1:thisN] <- dists2^2
        dist12_3d[i, 1:thisN, 1:thisN] <- sgn_mat1*dists1*sgn_mat2*dists2
    }
    if(!is.null(scale_factor)) {
        dist1_3d  <- dist1_3d  / scale_factor  
        dist2_3d  <- dist2_3d  / scale_factor  
        dist12_3d <- dist12_3d / scale_factor     
    } 
    return(list(dist1_3d  = dist1_3d,
                dist2_3d  = dist2_3d, 
                dist12_3d = dist12_3d,
                scale_factor = scale_factor))
}



dists1 <- as.matrix(dist(coords[,1]))
dists2 <- as.matrix(dist(coords[,2]))

temp1 <- matrix(coords[,1], nrow = N, ncol = N) 
temp2 <- matrix(coords[,2], nrow = N, ncol = N) 

sgn_mat1 <- ( temp1 - t(temp1) >= 0 )
sgn_mat1[sgn_mat1 == FALSE] <- -1 

sgn_mat2 <- ( temp2 - t(temp2) >= 0 )
sgn_mat2[sgn_mat2 == FALSE] <- -1 

dist1_sq <- dists1^2
dist2_sq <- dists2^2
dist12 <- sgn_mat1*dists1*sgn_mat2*dists2








set.seed(0)
N <- 30
k <- 10
coords <- array(runif(2*N), c(N,2))
nID <- determineNeighbors(coords, k)


dists_sqs <- ns_dist(coords)
dist1_sq <- dists_sqs$dist1_sq
dist2_sq <- dists_sqs$dist2_sq
dist12 <- dists_sqs$dist12

dists_3d <- ns_dist_3d(coords, nID)
dist1_3d <- dists_3d$dist1_3d
dist2_3d <- dists_3d$dist2_3d
dist12_3d <- dists_3d$dist12_3d





for(i in 2:N) {
    if(i<=k)     nNei <- i-1      else      nNei <- k
    ind <- c( nID[i,1:nNei], i )
    ## old
    d1_old <- dist1_sq[ind,ind]
    d2_old <- dist2_sq[ind,ind]
    d12_old <- dist12[ind,ind]
    ## new
    d1 <- dist1_3d[i,1:(nNei+1),1:(nNei+1)]
    d2 <- dist2_3d[i,1:(nNei+1),1:(nNei+1)]
    d12 <- dist12_3d[i,1:(nNei+1),1:(nNei+1)]
    ## check
    id <- identical(d1_old, d1) & identical(d2_old, d2) & identical(d12_old, d12)
    if(!id) { message('i = ', i, ': WRONG') }
}




df <- data.frame(y = c(7,8,9, 5,7,9, 5, 5, 8),
                 x = as.factor(rep(1:3, each=3)))

df
df$x
mean(df$y)
m <- lm(df$y ~ df$x)
anova(m)



library(nimble)

code <- nimbleCode({
  # these are priors for weights, each location gets its own weights
  theta1 ~ dnorm(0, sd  = 10000)
  theta2 ~ dgamma(0.001, 0.001)
  theta3 ~ dnorm(0, sd = 10000)
  theta4 ~ dnorm(0, sd = 10000)
  # for data error
  sigma_e ~ dunif(0, 10000)
  # for the parameters in linear model of mean
  beta0 ~ dnorm(0, sd = 10000)
  beta1 ~ dnorm(0, sd = 10000)
  beta2 ~ dnorm(0, sd = 10000)
  beta3 ~ dnorm(0, sd = 10000)
  for(i in 1:N){
   # # turn parameters for IDE into vectors
    ## could make these a function of time
    #theta1[i] <- mu_theta1
    #theta2[i] <- mu_theta2
    #theta3[i] <- mu_theta3
    #theta4[i] <- mu_theta4
    for(t in 1:nT){
      # make the mean PDSI as a function of ENSO and latitude
      mu[i,t] <- beta0 + soi[t]*beta1 + lat[i]*beta2 + soi[t]*lat[i]*beta3
    }
  }
  # for first time point in model, model latent state as a function of mean
  for(i in 1:N){
    y[i, 1] <- mu[i, 1]
    z[i, 1] ~ dnorm(y[i,1], sd = sigma_e)
  }
  # now make rest of latent states an autoregressive
  # process, a function of others
  for(i in 1:N){
    for(j in 1:N){
      # weight for each location i, relative to each other locaiton j
      m[i, j] <- theta1*exp(-(1/theta2^2)*((s[i,1]-theta3-s[j,1])^2
                                           +(s[i,2]-theta4-s[j,2])^2))
    }
  }
  for(i in 1:N){
    for(t in 2:nT){
      y[i, t] <- sum(m[i, 1:N]*y[1:N, (t-1)])
      z[i, t] ~ dnorm(mu[i,t]+y[i,t], sd = sigma_e)
    }
  }
})


constants <- list(nT = 5, N = 10)

Rmodel <- nimbleModel(code, constants)

Rmodel$z   ## make sure they all have real numbers
Rmodel$mu   ## make sure they all have real numbers
Rmodel$m   ## make sure they all have real numbers
Rmodel$y   ## make sure they all have real numbers
Rmodel$logProb_z

Rmodel$calculate('beta0')     ## should be real
Rmodel$calculate('beta1')     ## should be real
Rmodel$calculate('beta2')     ## should be real
Rmodel$calculate('beta3')     ## should be real
Rmodel$calculate('theta1')     ## should be real
Rmodel$calculate('theta2')     ## should be real
Rmodel$calculate('theta3')     ## should be real
Rmodel$calculate('theta4')     ## should be real
Rmodel$calculate('sigma_e')     ## should be real
Rmodel$calculate('mu')     ## should be 0
Rmodel$calculate('m')     ## should be 0
Rmodel$calculate('y')     ## should be 0
Rmodel$calculate('z')     ## should be real

## this will get you the logProbs for each element of z:



N <- 10
nT <- 4
lat <- 1:N
soi <- 1:nT
s <- array(0, c(N,2))
z <- array(0, c(N, nT))

constants <- list(s = s, soi = soi, N = N, nT = nT, lat = lat)

# data to be orginaized by space and time: 88 x 1005
data <- list(z = z)
inits <- list(beta0=0, beta1=0, beta2=0, beta3=0, theta1=0, theta2=1, theta3=0, theta4=0, sigma_e=1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()



library(nimble)

## Argument sm1: A (n1 x 3) sparse representation of x, where column #1 contains row indices,
##               column #2 contains column indices, and column #3 contains the
##               values of the non-zero entries of the first matrix argument x.
## Argument sm2: A (n2 x 3) sparse representation of y, the second matrix argument,
##               with the same column structure as sm1.
## Argument transpose: If 0 (default) computes t(x) %*% y, if 1 computes x %*% t(y).
## Argument returnMatrix: If 1, returns the full matrix crossproduct (sort of...),
##                        if 0 (default) returns a sparse (nx x 3) representation.
## Note: Does *not* accept the dimensions of the full x, y matrices, and hence
##       this function does *not* check that the x, y matrices are conformable.
sparse_crossprod <- nimbleFunction(
    run = function(sm1 = double(2), sm2 = double(2), transpose = double(0, default=0), returnMatrix = double(0, default=0)) {
        n1 <- dim(sm1)[1]     ## number of non-zero elements in sm1
        n2 <- dim(sm2)[1]     ## number of non-zero elements in sm2
        N <- max(n1, n2) * 1      ## can adjust resizing factor here
        maxRows <- N
        smx <- array(-1, c(N,3))    ## sparse matrix crossprod
        nx <- 0                     ## number of non-zero elements in smx
        if(transpose) {   ## transpose both x and y: swap the row & column indices
            temp1 <- sm1[1:n1, 1]
            sm1[1:n1, 1] <- sm1[1:n1, 2]
            sm1[1:n1, 2] <- temp1
            temp2 <- sm2[1:n2, 1]
            sm2[1:n2, 1] <- sm2[1:n2, 2]
            sm2[1:n2, 2] <- temp2
        }
        for(i in 1:n1) {
            for(j in 1:n2) {
                if(sm1[i,1] == sm2[j,1]) {
                    if(nx == maxRows) {   ## resize smx larger
                        smxTemp <- smx
                        setSize(smx, maxRows+N, 3)
                        smx[1:maxRows, 1:3] <- smxTemp
                        smx[(maxRows+1):(maxRows+N), 1:3] <- -1
                        maxRows <- maxRows + N
                    }
                    nx <- nx + 1
                    smx[nx,1] <- sm1[i,2]
                    smx[nx,2] <- sm2[j,2]
                    smx[nx,3] <- sm1[i,3] * sm2[j,3]
                }
            }
        }
        i <- 1
        while(i < nx) {    ## combine common cells
            j <- i + 1
            while(j <= nx) {
                if(smx[i,1]==smx[j,1] & smx[i,2]==smx[j,2]) {
                    smx[i,3] <- smx[i,3] + smx[j,3]
                    smx[j, 1:3] <- smx[nx, 1:3]
                    smx[nx, 1:3] <- -1
                    nx <- nx - 1
                } else j <- j + 1
            }
            i <- i + 1
        }
        if(returnMatrix) {
            if(nx == 0) return(array(0, c(1,1)))
            nr <- max(smx[1:nx,1])
            nc <- max(smx[1:nx,2])
            mat <- array(0, c(nr, nc))
            for(i in 1:nx)   mat[smx[i,1], smx[i,2]] <- smx[i,3]
            return(mat)
        }
        if(nx < maxRows) {     ## drop extra unused rows in smx
            if(nx == 0) { setSize(smx, 0, 3)
                      } else { smxTemp <- smx[1:nx, 1:3]
                               smx <- smxTemp }
        }
        returnType(double(2))
        return(smx)
    }
)

Csparse_crossprod <- compileNimble(sparse_crossprod, showCompilerOutput = TRUE)

for(func in 1:2) {
    if(func == 1) {
        message('uncompiled...')
        sparse_crossprod_USE <- sparse_crossprod
    }
    if(func == 2) {
        message('NOW COMPILED...')
        sparse_crossprod_USE <- Csparse_crossprod
    }
    ##
    for(test in 1:7) {
        message('beginning test #', test)
        ##
        if(test==1) {
            x <- matrix(1:6, nrow=3, ncol=2, byrow = TRUE)
            r1 <- rep(1:3, each=2)
            c1 <- rep(1:2, 3)
            v1 <- 1:6
            sm1 <- cbind(r1,c1,v1)
            y <- matrix(1:21, nrow=3, ncol=7, byrow=TRUE)
            r2 <- rep(1:3, each=7)
            c2 <- rep(1:7, 3)
            v2 <- 1:21
            sm2 <- cbind(r2,c2,v2)
        }
        if(test == 2) {
            x <- array(0, c(10,5))
            r1 <- c(2,5,8,3)
            c1 <- c(5,5,5,2)
            v1 <- c(1,2,3,5)
            sm1 <- cbind(r1,c1,v1)
            for(i in seq_along(r1)) x[r1[i],c1[i]] <- v1[i]
            y <- array(0, c(10,3))
            r2 <- c(2,  2, 3, 7, rep(10,3), 8, 8)
            c2 <- c(3,  1, 2, 2, 1:3,       1, 2)
            v2 <- c(10,.1, 4, 7, 1:3,       3, 9)
            sm2 <- cbind(r2,c2,v2)
            for(i in seq_along(r2)) y[r2[i],c2[i]] <- v2[i]
        }
        if(test == 3) {
            N <- 3
            x <- matrix(1:N^2, nrow=N, ncol=N, byrow = TRUE)
            r1 <- rep(1:N, each=N)
            c1 <- rep(1:N, N)
            v1 <- 1:N^2
            sm1 <- cbind(r1,c1,v1)
            y <- matrix((1:N^2)+7, nrow=N, ncol=N, byrow=TRUE)
            r2 <- rep(1:N, each=N)
            c2 <- rep(1:N, N)
            v2 <- (1:N^2)+7
            sm2 <- cbind(r2,c2,v2)
        }
        if(test == 4) {
            N <- 7
            x <- matrix(1:N^2, nrow=N, ncol=N, byrow = TRUE)
            r1 <- rep(1:N, each=N)
            c1 <- rep(1:N, N)
            v1 <- 1:N^2
            sm1 <- cbind(r1,c1,v1)
            y <- matrix((1:N^2)+7.3, nrow=N, ncol=N, byrow=TRUE)
            r2 <- rep(1:N, each=N)
            c2 <- rep(1:N, N)
            v2 <- (1:N^2)+7.3
            sm2 <- cbind(r2,c2,v2)
        }
        if(test == 5) {
            r1 <- 1:4
            c1 <- 1:4
            v1 <- 1:4
            sm1 <- cbind(r1,c1,v1)
            x <- array(0, c(4, 4))
            for(i in seq_along(r1)) x[r1[i],c1[i]] <- v1[i]
            r2 <- c(2, 2)
            c2 <- c(1, 4)
            v2 <- c(.2, .7)
            sm2 <- cbind(r2,c2,v2)
            y <- array(0, c(4, 4))
            for(i in seq_along(r2)) y[r2[i],c2[i]] <- v2[i]
        }
        if(test == 6) {
            r1 <- 1:4
            c1 <- 1:4
            v1 <- 1:4
            sm1 <- cbind(r1,c1,v1)
            x <- array(0, c(4, 4))
            for(i in seq_along(r1)) x[r1[i],c1[i]] <- v1[i]
            r2 <- 2
            c2 <- 3
            v2 <- .5
            sm2 <- cbind(r2,c2,v2)
            y <- array(0, c(4, 4))
            for(i in seq_along(r2)) y[r2[i],c2[i]] <- v2[i]
        }
        if(test == 7) {
            r1 <- 2
            c1 <- 3
            v1 <- 7
            sm1 <- cbind(r1,c1,v1)
            x <- array(0, c(4, 4))
            for(i in seq_along(r1)) x[r1[i],c1[i]] <- v1[i]
            r2 <- 1
            c2 <- 4
            v2 <- 11
            sm2 <- cbind(r2,c2,v2)
            y <- array(0, c(4, 4))
            for(i in seq_along(r2)) y[r2[i],c2[i]] <- v2[i]
        }
        ##sparse_crossprod(sm1, sm2)
        if(nrow(x) == nrow(y)) {
            t(x) %*% y
            a1 <- crossprod(x, y)
            a2 <- Matrix::crossprod(x, y)
            a3temp <- sparse_crossprod_USE(sm1, sm2, returnMatrix = 1)
            a3 <- array(0, dim(a1))
            a3[1:nrow(a3temp), 1:ncol(a3temp)] <- a3temp
            if(!all(abs(a1-a2) < 0.00000001)) stop('disagree')
            if(!all(abs(a1-a3) < 0.00000001)) stop('disagree')
        } else message('... skipping check')
        if(ncol(x) == ncol(y)) {
            y %*% t(x)
            a1 <- tcrossprod(x, y)
            a2 <- Matrix::tcrossprod(x, y)
            a3temp <- sparse_crossprod_USE(sm1, sm2, returnMatrix = 1, transpose = 1)
            a3 <- array(0, dim(a1))
            a3[1:nrow(a3temp), 1:ncol(a3temp)] <- a3temp
            if(!all(abs(a1-a2) < 0.00000001)) stop('disagree in transpose check')
            if(!all(abs(a1-a3) < 0.00000001)) stop('disagree in transpose check')
        } else message('... skipping transpose check')
    }
}


smx



sparse_crossprod(sm1, sm2)
sparse_crossprod(sm1, sm2, returnMatrix = 1)
crossprod(x, y)
t(x) %*% y





set.seed(0)
N <- 8
k <- 3
s <- array(runif(2*N), c(N,2))

ret <- ns_dist(s)
ret

Nei <- determineNeighbors(s, k)
Nei



library(nimble)
library(coda)
nimbleOptions(verbose = FALSE)
setwd('~/github/nngp/analysis')
source('../bayes_nsgp/source_nimble.R')
source('../code/nngp4.R')

COprecip <- read.csv('../fullGP_nonstationary/Final_data.csv')
coords <- as.matrix(COprecip[,c('Longitude', 'Latitude')])
dist_list <- ns_dist(coords)
Xmat <- unname(lm(logPrecip ~ Zelevation*Zslope10, x = TRUE, data = COprecip)$x)
p <- dim(Xmat)[2]
z <- COprecip$logPrecip

constants <- list(X_tau = Xmat, X_sigma = Xmat, X_Sigma = Xmat, X_mu = Xmat,
                  p_tau = p, p_sigma = p, p_Sigma = p, p_mu = p,
                  dist1_sq = dist_list$dist1_sq, dist2_sq = dist_list$dist2_sq, dist12 = dist_list$dist12)

niter <- 1000

Rmodel <- nsgp_model(likelihood = 'fullGP', constants = constants, z = z)
system.time(samples <- nimbleMCMC(model = Rmodel, niter = niter, progressBar = FALSE, setSeed = TRUE))
samplesList <- list(fullGP = samples)







## STAT202
## ANOVA
cars <- read.csv('~/github/courses/stat202/data/UsedCars.csv')
cars

## say there was a categorical predictor with 3 levels:
## 1 = unleaded gasoline
## 2 = diesel
## 3 = hybrid
Power <- c(rep("gas",10), rep("diesel",6), rep("hybrid",3))

Power
cars$Power <- as.factor(Power)
cars
cars$Power

str(cars)

## multiple regression with (categorical) Power variable

m <- lm(Price ~ Power, data = cars)
summary(m)

## ANOVA
anova(m)









## STAT365
## Hamiltonian Monte Carlo (HMC) sampling
## ** currently under development, experimental **


remove.packages("nimble")
library(devtools)
install_github("nimble-dev/nimble", ref = "hmcAD", subdir = "packages/nimble")
library(nimble)






## data generation
n <- 20
a <- 6
b <- 0.8
sigmaPN <- 2
sigmaOE <- 4
set.seed(0)
x <- numeric(n)
y <- numeric(n)
x[1] <- 1
y[1] <- rnorm(1, x[1], sigmaOE)
for(i in 2:n) {
    x[i] <- rnorm(1, a+b*x[i-1], sigmaPN)
    y[i] <- rnorm(1, x[i], sigmaOE)
}


library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)

code <- nimbleCode({
    a ~ dnorm(0, sd = 10000)
    b ~ dnorm(0, sd = 10000)
    sigmaPN ~ dunif(0, 10000)
    sigmaOE ~ dunif(0, 10000)
    x[1] ~ dnorm(0, sd = 10000)
    y[1] ~ dnorm(x[1], sd = sigmaOE)
    for(t in 2:N) {
        x[t] ~ dnorm(a + b*x[t-1], sd = sigmaPN)
        y[t] ~ dnorm(x[t], sd = sigmaOE)
    }
})

constants <- list(N = length(y))
data <- list(y = y)
inits <- list(a=0, b=0, sigmaOE=1, sigmaPN=1, x=y)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()   ## 24 stochastic nodes being sampled

topParams <- Rmodel$getNodeNames(topOnly = TRUE)
topParams
conf$removeSamplers(topParams)
conf$addSampler(topParams, "HMC")
conf$printSamplers()   ## HMC sampler

Rmcmc <- buildMCMC(conf)
compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

set.seed(0)
samples <- runMCMC(Cmcmc, 2000, nburnin = 200)

library(coda)
apply(samples[, topParams], 2, function(s) effectiveSize(s))








library(nimble)

## data generation
n <- 100
a <- 6
b <- 0.8
sigmaPN <- 2
sigmaOE <- 4
set.seed(0)
x <- numeric(n)
y <- numeric(n)
x[1] <- 1
y[1] <- rnorm(1, x[1], sigmaOE)
for(i in 2:n) {
    x[i] <- rnorm(1, a+b*x[i-1], sigmaPN)
    y[i] <- rnorm(1, x[i], sigmaOE)
}
head(y,5)  ## 6.051817 11.466730 15.121451  9.976140 16.339817

##
library(nimble)
library(basicMCMCplots)
library(coda)
code <- nimbleCode({
    a ~ dnorm(0, sd = 10000)
    b ~ dnorm(0, sd = 10000)
    sigmaPN ~ dunif(0, 10000)
    sigmaOE ~ dunif(0, 10000)
    x[1] ~ dnorm(0, sd = 10000)
    y[1] ~ dnorm(x[1], sd = sigmaOE)
    for(t in 2:N) {
        x[t] ~ dnorm(a + b*x[t-1], sd = sigmaPN)
        y[t] ~ dnorm(x[t], sd = sigmaOE)
    }
})
constants <- list(N = length(y))
data <- list(y = y)
inits <- list(a=0, b=0, sigmaOE=1, sigmaPN=1, x=y)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()    ## -41884.41


params <- Rmodel$getNodeNames(topOnly = TRUE)
params
calcNodes1 <- Rmodel$getDependencies(params, determOnly = TRUE)

calcNodes2 <- Rmodel$getDependencies('x[1:100]', determOnly = TRUE)
calcNodes2

Rmodel$sigma <- .....
Rmodel$calculate(calcNodes1)
Rmodel$simulate('x[2:99]')
Rmodel$calculate(calcNodes2)
Rmodel$simulate('y')


params <-

Rmodel$calculate()



conf <- configureMCMC(Rmodel, enableWAIC = TRUE)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc






## EXAMPLE 1

code <- nimbleCode({
    logit(x) ~ a
})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
## defining model...
## Error in code[[3]][[1]] : object of type 'symbol' is not subsettable

## EXAMPLE 2
## even more obscure...

code <- nimbleCode({
    logit(x) ~ a + b
})
constants <- list()
data <- list()
inits <- list(a = 0, b = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
## defining model...
## Registering the following user-provided distributions: + .
## Error in FUN(X[[i]], ...) : 
##   checkDistributionFunctions: density function for + is not available.  It must be a nimbleFunction (with no setup code).


Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)
samplesPlot(samples)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()



seeds <- read.csv('~/Downloads/seeds (1).csv')

seeds$plot <- 1:21
seeds

seeds$fertilizer <- as.numeric(seeds$fertilizer) - 1
seeds$plant <- as.numeric(seeds$plant) - 1

code.seed <- nimbleCode({
  b0 ~ dnorm(0, sd = 10000)
  b.t ~ dnorm(0, sd = 10000)
  b.f ~ dnorm(0, sd = 10000)
  for (i in 1:N) {
    logit(p[i]) <- b0 + b.t*type[i] + b.f*fertilizer[i]
    y[i] ~ dbern(p[i])
  }
})

constants.seed <- list(N = nrow(seeds), type = seeds$plant), fertilizer = seeds$fertilizer)

data.seed <- list(y = seeds$germinations)

inits.seed <- list(b0 = 0, b.t = 0, b.f = 0)

Rmodel.seed <- nimbleModel(code.seed, constants.seed, data.seed, inits.seed)

scallops <- read.csv("~/github/courses/stat365/data/scallops.csv")

code <- nimbleCode({
  mu0 ~ dnorm(0, sd=sigma_max)
  sigma ~ dunif(0, 10000)
  rho ~ dunif(0, 10000)
  for (i in 1:N) {
    mu[i] <- mu0
    for (j in 1:N) {
      Sigma[i,j] <- sigma^2*exp(-dist[i,j]/rho)
    }
  }
  x[1:N] ~ dmnorm(mu[1:N], cov=Sigma[1:N,1:N])
  for (i in 1:N) {
    y[i] ~ dpois(exp(x[i]))
  }
})

constants <- list(N=80, sigma_max=100000, dist=as.matrix(dist(scallops[1:2])))
data <- list(y=scallops$scallops)
inits <- list(mu0=0, sigma=1, rho=1, x=rep(0,80))

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$y[80] <- 4


Rmodel$calculate()

conf <- configureMCMC(Rmodel)

conf$printSamplers()

conf$addSampler(c("rho", "sigma"), "RW_block")
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
t <- system.time(sample <- runMCMC(Cmcmc, nburnin=4000, 10000))
effectiveSize(sample)
t[3]


a =nimbleFunction(
    setup = function() {
        p <- 1
        out <- numeric(p+1)
    },
    run = function() {
        for(i in 1:p)
            out[i] <<- 3.0
        returnType(double(1))
        return(out)
    })

ra = a()

ra$out

ca=compileNimble(ra)


help(dcar_normal)
help(dcar_proper)


library(nimble)

set.seed(0)
k <- 10
n <- 75
phi <- .9
p <- 0.5
y <- array(NA, c(n,k))

for(i in 1:n) {
    y[i,1] <- 1
    x <- 1
    for(t in 2:k) {
        x <- rbinom(1, prob=x*phi, size=1)
        y[i,t] <- rbinom(1, prob=p*x, size=1)
    }
}

code <- nimbleCode({
  p ~ dbeta(1,1)
  phi ~ dbeta(1,1)
  for (i in 1:n) {
    x[i,1] <- 1
    y[i,1] <- 1
    for (j in 2:k) {
        x[i,j] ~ dbern(phi * x[i,j-1])
        y[i,j] ~ dbern(p * x[i,j])
    }
  }
})

constants <- list(n = 75, k = 10)
data <- list(y = y)
inits <- list(p = .5, phi = .5, x = matrix(1, 75, 10))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()   ## -935.7487

conf <- configureMCMC(Rmodel, enableWAIC = TRUE)
conf$addMonitors('x')

Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Cmodel)

Cmodel$calculate()
Cmcmc$run(5000)
Cmodel$calculate()
Cmcmc$calculateWAIC()
Cmodel$calculate()


runMCMC(Cmcmc, niter = 5000, nchains = 1, samples = FALSE, WAIC = TRUE)  ## fine

Cmodel$calculate()

Cmcmc$calculateWAIC()

runMCMC(Cmcmc, niter = 5000, nchains = 2, samples = FALSE, WAIC = TRUE)
## NaN

Cmodel$calculate()
Cmcmc$calculateWAIC()





library(nimble)

code <- nimbleCode({
    mu ~ dnorm(0, sd = 10000)
    log(sigma) ~ dnorm(0, 0.0001)
    sigma2 <- sigma^2
    for(i in 1:4) {
        y[i] ~ dnorm(mu, var = sigma2)
    }
    p[1:4] ~ ddirch(a[1:4])
})

inits <- list(mu = 0, log_sigma = 0, y = 1:4, a = 11:14, p = rep(1/4,4))
Rmodel <- nimbleModel(code, inits = inits)
Rmodel$calculate()

Rmodel$getNodeNames()
Rmodel$getVarNames(includeLogProb = TRUE)

(allVarsIncludingLogProb <- Rmodel$getVarNames(includeLogProb = TRUE))
(vals <- values(Rmodel, allVarsIncludingLogProb))

Rmodel$y <- 11:14
Rmodel$mu <- 5
Rmodel$log_sigma <- 10
Rmodel$a <- 101:104
Rmodel$p <- c(0.7, 0.1, 0.1, 0.1)
Rmodel$calculate()

values(Rmodel, allVarsIncludingLogProb)
values(Rmodel, allVarsIncludingLogProb) <- vals
values(Rmodel, allVarsIncludingLogProb) - vals


args(Rmodel$getNodeNames)
args(Rmodel$getVarNames)

values(Rmodel)


df <- read.csv('~/Downloads/FrenchCuisine.csv')
df
m <- lm(Rating ~ City, data = df)
summary(m)
anova(m)


df <- read.csv('~/Downloads/AnorexiaLong.csv')
df
m <- lm(change ~ therapy, data = df)
summary(m)
anova(m)



-3 Please include indexing for the mathematical models, to make them entirely precise and formal (e.g., .... for i = 1,.... 34).  It is actually really important to get these right, because that's how you communicate to others the structure of your model.

-2 Problem 1 (c) much more specifically it's because it's a *conjugate* sampler.'

Good understanding of the a[] parameters to the dirichlet, and the analogy to the beta(a, b) parameters.

cars <- read.csv('~/github/courses/stat202/data/UsedCars.csv')
head(cars)
## multiple regression using Age and (categorical) Type
m <- lm(Price ~ Age + Type, data = cars)
summary(m)
## multiple regression using Age and (categorical) Type
## including interaction term:


## STAT202
## Quiz #4, problem 2
n <- 30
p <- 4
SST <- 100
SSE <- 74
(SSR <- SST - SSE)   ## 26
(df1 <- p-1)    ## 3
(df2 <- n-p)    ## 26
(MSR <- SSR / df1)   ## 8.666667
(MSE <- SSE / df2)   ## 2.846154
(F <- MSR / MSE)   ## 3.045045






## multiple linear regression,
## matrix algebra for exact beta_hat coefficients

df <- read.csv('~/github/courses/stat365/data/UsedCars.csv')
head(df)
price <- df$Price
age <- df$Age
hp <- df$HP
type <- df$Type

m <- lm(price ~ age + hp + type)
summary(m)
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 18531.38    3788.51   4.891 0.000196 ***
## age         -1108.50     336.26  -3.297 0.004894 ** 
## hp             15.23      21.45   0.710 0.488751    
## type        -2171.16    1151.05  -1.886 0.078776 .  

X <- cbind(1, age, hp, type)
Y <- cbind(price)
(beta <- solve(t(X)%*%X) %*% t(X) %*% Y)



## testing autoBlock ??

library(nimble)

code <- nimbleCode({
    mu ~ dnorm(0, sd = 10000)
    sigma ~ dunif(0, 10000)
    for(i in 1:4) {
        y[i] ~ dnorm(mu, sd = sigma)
    }
})

constants <- list()
data <- list(y = c(100, 110, 112, 118))
inits <- list(mu = 0, sigma = 1)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmcmc <- buildMCMC(Rmodel, autoBlock = TRUE)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Cmodel)

set.seed(0)
out <- runMCMC(Cmcmc, 10000)



## writing MMD maximum-minimum distance coordinate ordering
## for NNGP work with Mark Risser


## data generation
set.seed(0)
N <- 100
s <- cbind(runif(N), runif(N))

require(FNN)

orderCoordinatesMMD <- function(s, exact = FALSE) {
    ## input s: an Nx2 array of spatial coordinates
    ## data generation, for testing
    ## N <- 10
    ## set.seed(0)
    ## s <- cbind(runif(N), runif(N))
    N <- dim(s)[1]
    if(N < 3) return(s)
    if(!exact) {       ## approximate MMD ordering
        initialOrdering <- sample(1:N)
        orderedIndices <- c(initialOrdering, rep(NA, 3*N))
        indexLookupVector <- order(initialOrdering)
        maxNeighbors <- floor(sqrt(N))
        NN <- FNN::get.knn(s, k = maxNeighbors)$nn.index
        nextSpot <- N+1
        cycleCheckIndex <- -1
        for(i in 2:(3*N)) {
            (targetIndex <- orderedIndices[i])
            if(cycleCheckIndex == targetIndex) break
            if(cycleCheckIndex == -1) cycleCheckIndex <- targetIndex
            targetNeighbors <- NN[targetIndex, 1:min(maxNeighbors, round(N/(i+N-nextSpot)))]
            targetNeighborLocs <- indexLookupVector[targetNeighbors]
            if(min(targetNeighborLocs) < i) {   ## relocate this index to the back
                orderedIndices[nextSpot] <- targetIndex
                orderedIndices[i] <- NA
                indexLookupVector[targetIndex] <- nextSpot
                nextSpot <- nextSpot + 1
            } else cycleCheckIndex <- -1
        }
        orderedIndicesNoNA <- orderedIndices[!is.na(orderedIndices)]
        orderedS <- s[orderedIndicesNoNA,]
    } else {           ## exact MMD ordering
        availableIndices <- 1:N
        orderedS <- array(NA, c(N,2))
        sbar <- apply(s, 2, mean)   ## group centroid
        iNext <- which.min(sapply(1:N, function(i) sum((s[i,] - sbar)^2)))
        orderedS[1,] <- s[iNext,]
        availableIndices <- setdiff(availableIndices, iNext)
        for(i in 2:N) {
            aIndNext <- which.max(    ## this indexes the availableIndices vector
                sapply(1:(N-i+1), function(j) {
                    min(sapply(1:(i-1), function(k) sum((s[availableIndices[j],] - orderedS[k,])^2)))
                }))
            iNext <- availableIndices[aIndNext]   ## this indexes rows of the original s[] array
            orderedS[i,] <- s[iNext,]
            availableIndices <- setdiff(availableIndices, iNext)
        }
        ##if(length(availableIndices) > 0) stop('something wrong with MMD algorithm')
        ##if(any(is.na(orderedS)))         stop('something wrong with MMD algorithm')
    }
    return(orderedS)
}


orderedS <- orderedIndices[!is.na(orderedIndices)]


## data generation
set.seed(0)
N <- 1
s <- cbind(runif(N), runif(N))

s

system.time(os <- order_maxmin(s))

system.time(os <- orderCoordinatesMMD(s))
os

par(mfrow = c(3,3))

for(i in 1:9) {
    ind <- 1:(10*i)
    plot(os[ind,1], os[ind,2], pch='x')
}


determineNeighbors <- function(dst, k) {
    N <- dim(dst)[1]
    if(k+2 > N) stop()
    nID <- array(-1, c(N,k))     ## populate unused values with -1, to prevent a warning from NIMBLE
    for(i in 2:(k+1))     nID[i, 1:(i-1)] <- as.numeric(1:(i-1))
    for(i in (k+2):N)     nID[i, 1:k] <- as.numeric(order(dst[i,1:(i-1)])[1:k])
    return(nID)
}

system.time(dst <- as.matrix(dist(os)))

system.time(nID <- determineNeighbors(dst, k = 30))


require(FNN)
locs <- s
n <- nrow(locs)

## m is number of neighbors to search over
## get the past and future nearest neighbors
(m <- round(sqrt(n)))

NNall <- FNN::get.knn(locs, k = m)$nn.index

## pick a random ordering
set.seed(0)
(index_in_position <- c( sample(n), rep(NA,1*n) ))   ## WHY 1*n ????
(position_of_index <- order(index_in_position[1:n]))

## loop over the first n/4 locations
## move an index to the end if it is a
## near neighbor of a previous location
curlen <- n
##curpos <- 1
nmoved <- 0
j <- 2

## [10,]    5    7    4

j
index_in_position[j]
NNall[index_in_position[j],]
(nneigh <- round( min(m,n/(j-nmoved+1)) ))
(neighbors <- NNall[index_in_position[j],1:nneigh])
index_in_position

min( position_of_index[neighbors], na.rm = TRUE ) < j

(nmoved <- nmoved+1)
(curlen <- curlen + 1)
(position_of_index[ index_in_position[j] ] <- curlen)
(index_in_position[curlen] <- index_in_position[j])
index_in_position[j] <- NA
index_in_position
position_of_index

(j <- j+1)



order_maxmin <- function(locs) {
    ##n <- nrow(locs)
    ##ee <- min(apply( locs, 2, stats::sd ))
    ##locs <- locs + matrix( ee*1e-4*stats::rnorm(n*ncol(locs)), n, ncol(locs) )    
    n <- nrow(locs)
    m <- round(sqrt(n))
    ## m is number of neighbors to search over
    ## get the past and future nearest neighbors
    NNall <- FNN::get.knn( locs, k = m )$nn.index
    ## pick a random ordering
    set.seed(0)
    index_in_position <- c( sample(n), rep(NA,1*n) )
    position_of_index <- order(index_in_position[1:n])
    ## loop over the first n/4 locations
    ## move an index to the end if it is a
    ## near neighbor of a previous location
    curlen <- n
    nmoved <- 0
    for(j in 2:(2*n) ){
        nneigh <- round( min(m,n/(j-nmoved+1)) )
        neighbors <- NNall[index_in_position[j],1:nneigh]
        if( min( position_of_index[neighbors], na.rm = TRUE ) < j ){
            nmoved <- nmoved+1
            curlen <- curlen + 1
            position_of_index[ index_in_position[j] ] <- curlen
            index_in_position[curlen] <- index_in_position[j]
            index_in_position[j] <- NA
        }
    }
    ord <- index_in_position[ !is.na( index_in_position ) ]
    orderedS <- locs[ord,]
    return(orderedS)
    ##return(index_in_position)
}

order_maxmin(s)




## WAIC
## STAT 365 lecture fall 2018

library(nimble)

code <- nimbleCode({
    mu ~ dnorm(0, sd = 10000)
    sigma ~ dunif(0, 10000)
    for(i in 1:4) {
        y[i] ~ dnorm(mu, sd = sigma)
    }
})

constants <- list()
data <- list(y = c(100, 110, 112, 118))
inits <- list(mu = 0, sigma = 1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()   ## -24307.02

## need to "enable" WAIC calculations!
## this can be done either in configureMCMC(),
## or buildMCMC():
conf <- configureMCMC(Rmodel, enableWAIC = TRUE)      ## enable WAIC here,
Rmcmc <- buildMCMC(conf, enableWAIC = TRUE)           ## or here

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

## then, specify WAIC = TRUE, to return the WAIC value:
set.seed(0)
out <- runMCMC(Cmcmc, 50000, WAIC = TRUE, samples=FALSE)

str(out)
out$WAIC   ## 32.29415

out

## say we forgot to specify WAIC = TRUE, for a long run.
## don't worry, there's still a way to calculate it:
out <- runMCMC(Cmcmc, 50000)

Cmcmc$calculateWAIC()

Cmcmc$run(10000, reset = FALSE)


## finally, let's see this using nimbleMCMC() again:
args(nimbleMCMC)

out <- nimbleMCMC(code, constants, data, inits, niter=10000, nchains=3, WAIC = TRUE, samplesAsCodaMCMC=TRUE )

str(out)

out$samples
out$WAIC



## STAT 202
## example of exponential regression:
## Moore's Law
## fall 2018

x <- c(0, 3, 4, 5, 6)      ## years since 1959
y <- c(1, 7, 19, 29, 64)   ## num. of components on microchip

plot(x, y, pch=19, xlim = c(0,8), ylim = c(0,100))


## R^2 for linear regression
m <- lm(y ~ x)
abline(m, col = 'red')
cor(x, y)^2   ## R^2 = 0.7193205


## instead, let's plot of log(y) vs. x
plot(x, log(y), pch=19, xlim=c(0,8), ylim=c(0,6))
abline(m, col = 'red')

##ytransformed <- log(y)

## fit exponential regression model in R
m <- lm(log(y) ~ x)
m

## R^2 for exponential regression
cor(log(y), x)^2  ## R^2 = 0.9945088

## let's extract the coefficients
## don't forget, these are: log(a), and log(b)
coef(m)
loga <- m$coef[1]
logb <- m$coef[2]
loga
logb
a <- exp(loga)
b <- exp(logb)
a
b

## y = a*b^x
## y = 1*2^x = 2^x

plot(1:20, 1:20, pch=1:20)

plot(x, y, pch=19, xlim = c(0,8), ylim = c(0,100))

points(x, a*b^x, pch=20, col = 'red')

exp(m$fitted.values)

a*b^x

xs <- seq(0, 10, by = 0.1)
xs

lines(xs, a*b^xs, col = 'red')

plot(function(x) 2^x, xlim=c(0,10))

plot(function(x) 2^x, xlim=c(0,10), add=TRUE)

## extracting fitted values

yhat <- m$fitted.values

epsilons <- y - yhat

m$residuals

y - exp(m$fitted.values)

## predict() function






set.seed(0)
N <- 10
s <- cbind(runif(N), runif(N))

##system.time(s2 <- orderCoordinatesMMD(s))
s2 <- orderCoordinatesMMD(s)
s2


library(GpGp)

ord <- order_maxmin(s)
ss <- s[ord,]


plot(ss[,1], ss[,2], pch = 19)
for(i in 1:dim(ss)[1]) {
    text(x=ss[i,1], y=ss[i,2]+0.02, label=i, cex=0.5)
}



nvec <- c(50,50)
locs <- as.matrix( expand.grid( 1:nvec[1]/nvec[1], 1:nvec[2]/nvec[2] ) )
ord <- order_maxmin(locs)
par(mfrow=c(1,3))
plot( locs[ord[1:100],1], locs[ord[1:100],2], xlim = c(0,1), ylim = c(0,1) )
plot( locs[ord[1:300],1], locs[ord[1:300],2], xlim = c(0,1), ylim = c(0,1) )
plot( locs[ord[1:900],1], locs[ord[1:900],2], xlim = c(0,1), ylim = c(0,1) )


library(nimble)
library(basicMCMCplots)


N <- dim(df)[1]
school <- df$School
private <- ifelse(school=="private", 1, 0)
public  <- ifelse(school=="public", 1, 0)

df <- read.csv("~/github/courses/stat365/data/school_awards.csv")
school_awards <- df
school_awards$private <- ifelse(school_awards$School=="private", 1, 0)
school_awards$public <- ifelse(school_awards$School=="public"|school_awards$School=="charter", 1, 0)

code <- nimbleCode({
    b0~dnorm(0,sd=10000)
    bmath~dnorm(0,sd=10000)
    bprivate~dnorm(0,sd=10000)
    bpublic~dnorm(0,sd=10000)
    for(i in 1:N) {
        log(lambda[i]) <- b0+bmath*math[i]+bprivate*private[i]+bpublic*public[i]
        y[i] ~ dpois(lambda[i])
    }
})

constants <- list(N=200, math=school_awards$Avg_Math,private=school_awards$private,public=school_awards$public)
data <- list(y=school_awards$Num_Awards)
inits <- list(b0 = 0, bmath = 0, bprivate = 0, bpublic = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate("")

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000, nburnin=2000)

colnames(samples)
samplesSummary(samples)



Rmodel.school$calculate('')
Rmodel.school$initializeInfo()







library(nimble)
df <- read.csv('~/github/courses/stat365/data/seizures.csv')
age <- df$age
N <- length(age)
logbaseline <- df$logbaseline
treatment <- df$treatment
y <- array(0, c(N,4))
y[,1] <- df$s1
y[,2] <- df$s2
y[,3] <- df$s3
y[,4] <- df$s4

##
code <- nimbleCode({
    b0    ~ dnorm(0, sd = 10000)
    bbase ~ dnorm(0, sd = 10000)
    bage  ~ dnorm(0, sd = 10000)
    btrt  ~ dnorm(0, sd = 10000)
    sigma         ~ dunif(0, 10000)
    sigma_patient ~ dunif(0, 10000)
    for(i in 1:N) {
        g[i] ~ dnorm(0, sd = sigma_patient)
        for(j in 1:4) {
            eps[i,j] ~ dnorm(0, sd = sigma)
            log(lambda[i,j]) <- b0 + bbase*logbaseline[i] + bage*age[i] + btrt*treatment[i] + eps[i,j] + g[i]
            y[i,j] ~ dpois(lambda[i,j])
        }
    }
})
##
constants <- list(N=N, logbaseline=logbaseline, age=age, treatment=treatment)
data <- list(y = y)
inits <- list(b0=1, bbase=0, bage=0, btrt=0, sigma=1, sigma_patient=1, g=rep(0,N), eps = array(0,c(N,4)))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
t <- system.time(samples <- runMCMC(Cmcmc, 10000, nburnin=2000))
t[3]
colnames(samples)

library(coda)
apply(samples, 2, effectiveSize)



library(nimble)
df <- read.csv('~/github/courses/stat365/data/surgeries.csv')
N <- dim(df)[1]
Surgeries <- df$Surgeries
Mortalities <- df$Mortalities

##
## part (a)
##
code <- nimbleCode({
    for(i in 1:N) {
        p[i] ~ dbeta(1, 1)
        y[i] ~ dbinom(size = n[i], prob = p[i])
    }
})
constants <- list(N = N, n = Surgeries)
data <- list(y = Mortalities)
inits <- list(p = rep(0.5, N))
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
samples <- runMCMC(Cmcmc, 10000)
library(basicMCMCplots)
samplesPlot(samples)
samplesSummary(samples)
## Hospital 8 has highest mortality rate
## Hospital 1 has the lowest mortality rate
cbind(df, rate = Mortalities / Surgeries)
sum <- samplesSummary(samples)
sum[,5] - sum[,4]





## rats growth data generation for STAT365 final exam
## Fall 2018

fracT








## CI for log(RR) log relative risk
## for STAT202 Quiz #3, problem #1

x1 <- 19845
(n1 <- x1+7086)
x2 <- 29
(n2 <- x2+98)
(p1 <- x1/n1)
(p2 <- x2/n2)
(RR <- p1/p2)
(logRR <- log(RR))
(se <- sqrt(1/x1 - 1/n1 + 1/x2 - 1/n2))
z <- qnorm(0.995)    ## 99% CI
(logCI <- logRR + c(-1,1) * z * se)
(CI <- exp(logCI))

## STAT202 Quiz #3, problem #3
n <- 93
r <- -0.3
sx <- 3.1
sy <- 0.7
(b <- sy/sx * r)
(r2 <- r^2)
r2*100    ## as a percentage
(SSE <- 100*(1 - r2))
(s <- sqrt(SSE / (n-2)))




## working on making elections88 problem for STAT365 (large GLM model)
## below here is creating the **simplified** version of the "elections88"
## model, and putting that dataset together for STAT365
## Fall 2018
rrr
load('~/github/public/HMCcomparisons/election88_clean/data_setup_from_stan_github/data_setup_from_stan_github.RData')
library(nimble)
election88_BUGS <- readBUGSmodel("17.4_Bugs_codes.bug", dir = '~/github/public/HMCcomparisons/election88_clean/models', returnComponents = TRUE)
code <- election88_BUGS
code
BUGSdataList.1 <- dataList.1
names(BUGSdataList.1) <- tolower( gsub("_",".",names(BUGSdataList.1)))
election88_BUGS$data <- BUGSdataList.1
election88_BUGS$inits <- election.inits
code <- election88_BUGS$code
yInd <- which(names(election88_BUGS$data) == 'y')
constants <- election88_BUGS$data[-yInd]
data <- election88_BUGS$data[yInd]
inits <- election88_BUGS$inits
str(constants)
str(data)
inits <- list(b.0=rnorm(1), b.female=rnorm(1), b.black=rnorm(1),
              b.female.black=rnorm(1),
              b.age=rnorm(n.age), b.edu=rnorm(n.edu),
              b.age.edu=array(rnorm(n.age*n.edu),
                  c(n.age,n.edu)), b.state=rnorm(n.state), b.v.prev=rnorm(1), 
              b.region=rnorm(n.region), sigma.age=runif(1),
              sigma.edu=runif(1), sigma.age.edu=runif(1),
              sigma.state=runif(1), sigma.region=runif(1))
cold <- constants
dold <- data
iold <- inits

str(cold)
str(dold)
str(iold)

df <- as.data.frame(cbind(vote = dold$y, female = cold$female, married = cold$black, age=cold$age, edu=cold$edu, state = cold$state, vote.prev = cold$v.prev[cold$state]))
dim(df)
head(df)

write.csv(df, file = '~/github/courses/stat365/data/elections.csv', row.names=FALSE)



## Now, loading and fitting that model,
## as for a HW problem in STAT365 (Fall 2018)
library(nimble)
library(basicMCMCplots)
library(coda)
df <- read.csv('~/github/courses/stat365/data/elections.csv')
dim(df)
head(df)
N <- dim(df)[1]
constants <- list(N=N, female=df$female, married=df$married, age=df$age, edu=df$edu, income = df$income, state = df$state)
data <- list(vote = df$vote)
inits <- list(b.0=0, b.female=0, b.married=0, b.female.married=0, b.age=rep(0,4), b.edu=rep(0,4), sigma.age=1, sigma.edu=1, b.income=0, sigma.state=1, b.state=rep(0,51))

code <- nimbleCode({
    b.0 ~ dnorm(0, sd = 10000)
    b.female ~ dnorm(0, sd = 10000)
    b.married ~ dnorm(0, sd = 10000)
    b.female.married ~ dnorm(0, sd = 10000)
    b.income ~ dnorm(0, sd = 10000)
    sigma.age ~ dunif(0, 10000)
    sigma.edu ~ dunif(0, 10000)
    sigma.state ~ dunif(0, 10000)
    for(i in 1:4) {
        b.age[i] ~ dnorm(0, sd = sigma.age)
        b.edu[i] ~ dnorm(0, sd = sigma.edu)
    }
    for(i in 1:51) {
        b.state[i] ~ dnorm(0, sd = sigma.state)
    }
    for(i in 1:N) {
        vote[i] ~ dbern(p[i])
        logit(p[i]) <- b.0 + b.female * female[i] + b.married * married[i] + 
            b.female.married * female[i] * married[i] + b.age[age[i]] +
                b.edu[edu[i]] + b.state[state[i]] + b.income * income[i]
    }
})


Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()  ## -1529.186

conf <- configureMCMC(Rmodel)
conf$printSamplers()   ## 67 nodes being sampled (8 top-level, 59 latent states)
Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
samplesSummary(samples)[,c(1,4,5)]
##                            Mean        95%CI_low       95%CI_upp
## b.0               0.70832547912 -0.2711797124498  3.013624804998
## b.female         -0.09316467376 -0.2895890028345  0.103016510616
## b.female.married -0.23113105207 -1.0874219665944  0.586217683648
## b.income          0.00000292566 -0.0000001080459  0.000007129485
## b.married        -1.61797873240 -2.3100376637121 -0.948194249314
## sigma.age         0.87519231167  0.0255492939338  5.330009804043
## sigma.edu         0.46513523945  0.0542670635124  2.384110880547
## sigma.state       0.46008131925  0.2950763506548  0.666614312897
samplesPlot(samples)
apply(samples, 2, effectiveSize)
##             b.0         b.female b.female.married         b.income 
##        5.877285       725.854427       539.409725        72.046173 
##       b.married        sigma.age        sigma.edu      sigma.state 
##      590.946470        20.004868        59.080406       285.335308 





## deriving iat auto correlation plots acf
## and integrated correlation time

acfplot(as.mcmc(samples[,1]), lag.max = 300)

m <- 1000
xs <- 0:m
acfs <- numeric(m+1)
acfs[1] <- 1
for(i in 1:m) {
    acfs[i+1] <- cor(x[1:(N-i)], x[(i+1):N])
}

dev.new(); plot(xs, acfs, type = 'l')

ess <- effectiveSize(x)
ess

iat <- sum(acfs)
iat
N / iat


## STAT 365 lecture, lecture demo 15 2018
## on ess effective sample size, and auto-correlation

library(nimble)

df <- read.csv('~/github/courses/stat365/data/UsedCars.csv')

code <- nimbleCode({
    b0    ~ dnorm(0, sd = 10000)
    bage  ~ dnorm(0, sd = 10000)
    bhp   ~ dnorm(0, sd = 10000)
    btype ~ dnorm(0, sd = 10000)
    sigma ~ dunif(0, 10000)
    for(i in 1:N) {
        mu[i] <- b0 + bage*age[i] + bhp*hp[i] + btype*type[i]
        y[i] ~ dnorm(mu[i], sd = sigma)
    }
})


constants <- list(N = dim(df)[1], age=df$Age, hp=df$HP, type=df$Type)
data <- list(y = df$Price)
inits <- list(b0=0, bage=0, bhp=0, btype=0, sigma = 1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()  ## -851490867

conf <- configureMCMC(Rmodel, thin = 2)
conf$printSamplers()
conf$printMonitors()
Rmcmc <- buildMCMC(Rmodel)

## one compilation call:
compiledList <- compileNimble(list(Rmodel, Rmcmc))
Cmcmc <- compiledList[[2]]

set.seed(0)
samples <- runMCMC(Cmcmc, 10000, thin=10)
dim(samples)

## samplesPlot(), samplesSummary():
library(basicMCMCplots)

samplesSummary(samples)
samplesPlot(samples)

## using summary = TRUE from runMCMC():

out <- runMCMC(Cmcmc, 10000, samples=FALSE, summary=TRUE)
out

out <- runMCMC(Cmcmc, 10000, summary=TRUE)
str(out)
out$samples
out$summary

## alternative:
## nimbleMCMC()  !
## let's quickly look at NIMBLE User Manual...

out <- nimbleMCMC(code, constants, data, inits)
head(out)

args(runMCMC)
args(nimbleMCMC)

## library: coda

library(coda)

## auto-correlation function, p(h), that's rho(h)
## from coda: acfplot()
## requires coda mcmc object!
## lag.max, thin

acfplot(as.mcmc(samples))
acfplot(as.mcmc(samples), lag.max = 100)
acfplot(as.mcmc(samples), lag.max = 100, thin=10)


## integrated auto-correlation time

## effective sample size (ESS)
## from coda: effectiveSize()

dim(samples)

effectiveSize(samples[,1])

apply(samples, 2, effectiveSize)

##       b0      bage       bhp     btype     sigma 
## 70.98230  96.24737 222.42808 507.34821  19.34069 



## doing used cars linear regression MCMC

library(nimble)
library(basicMCMCplots)

df <- read.csv('~/github/courses/stat202/data/UsedCars.csv')

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)
##compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
##Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

colnames(samples)
apply(samples, 2, mean)

samplesSummary(samples)
samplesPlot(samples)

library(coda)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()



## CI for log(RR) log relative risk

x1 <- 914
(n1 <- x1+581)
x2 <- 46
(n2 <- x2+735)
(p1 <- x1/n1)
(p2 <- x2/n2)
(RR <- p1/p2)
(logRR <- log(RR))
(se <- sqrt(1/x1 - 1/n1 + 1/x2 - 1/n2))
(logCI <- logRR + c(-1,1) * 1.96 * se)
(CI <- exp(logCI))




df <- read.csv('~/github/courses/stat202/data/CO2-and-DIJA.csv')
head(df)


df <- read.csv('~/github/courses/stat202/data/Kentucky_Derby.csv')
head(df)


df <- read.csv('~/github/courses/stat202/data/not_used_yet/sat2010.csv')
head(df)
dim(df)
df

survey <- read.csv('~/github/courses/stat202/data/STAT202.csv')
head(survey)
minutes <- survey$Minutes
friends <- survey$Friends
cor(minutes, friends)
m <- lm(friends ~ minutes)
summary(m)
coefficients(m)
plot(minutes, friends)
resid <- residuals(m)
plot(minutes, resid)
r <- cor(minutes, friends)
r^2
m
summary(m)
382.2 / sqrt(sum((minutes-mean(minutes))^2))


data <- matrix(c(10, 40, 20, 30, 20, 40), nrow = 2)
data

chisq.test(data, correct = FALSE)

pchisq(4.8485, 2)

1 - pchisq(4.8485, 2)


df <- read.csv('~/github/courses/stat202/data/trial_verdicts.csv')
trial <- df
gender <- df$Gender
verdict <- df$Verdict
chisq.test(table(gender, verdict), correct=FALSE)

gender<-trial$Gender
verdict<-trial$Verdict

table<-table(gender, verdict)
table

(49+170)/(373) * (49+31)/(373) * 373

(49+170)*(49+31) / 373


## confidence interval for the mean of used cars

df <- read.csv('~/github/courses/stat202/data/UsedCars.csv')
head(df)
summary(df)
dim(df)
hours <- df$Price
price <- df$Price
type <- df$Type
n <- length(hours)
n
hist(hours)
mean(hours)
se <- sd(hours)/sqrt(n)
z <- qnorm(0.975)
ci <- mean(hours) + c(-1,1) * z * se
ci

iter <- 100000
means <- numeric(iter)
for(i in 1:iter) {
    bootstrapSample <- sample(hours, replace=TRUE)
    means[i] <- mean(bootstrapSample)
}
hist(means, breaks=50)
abline(v=ci, lwd=2, col='red')


bootstrapCI <- quantile(means, c(0.025, 0.975))
bootstrapCI
ci
abline(v=bootstrapCI, lwd=2, col='blue')

iter <- 100000
means <- numeric(iter)
for(i in 1:iter) {
    bootstrapSample <- sample(hours, replace=TRUE)
    means[i] <- mean(bootstrapSample)
}
hist(means, breaks=50)
abline(v=ci, lwd=2, col='red')


bootstrapCI <- quantile(means, c(0.025, 0.975))
bootstrapCI
ci
abline(v=bootstrapCI, lwd=2, col='blue')


dom <- price[type==1]
fr <- price[type==0]
type
dom
fr
nd <- length(dom)
nf <- length(fr)

mean(fr) - mean(dom)

N <- 100000
means <- numeric(N)
for(i in 1:N) {
    newD <- sample(dom, replace=TRUE)
    newF <- sample(fr, replace=TRUE)
    means[i] <- mean(newD) - mean(newF)
}

bootstrapCI <- quantile(means, c(0.025, 0.975))
bootstrapCI
hist(means, breaks=50)
abline(v=bootstrapCI, lwd=2, col='red')


N <- 100000
means <- numeric(N)
for(i in 1:N) {
    newPrice <- sample(price)
    newD <- newPrice[1:nd]
    newF <- newPrice[(nd+1):(nd+nf)]
    means[i] <- mean(newD) - mean(newF)
}

bootstrapCI <- quantile(means, c(0.025, 0.975))
bootstrapCI
hist(means, breaks=50)
abline(v=bootstrapCI, lwd=2, col='red')

d <- mean(dom) - mean(fr)
d
abline(v=d, col='blue')


bootstrapCI <- quantile(means, c(0.025, 0.975))
bootstrapCI
hist(means, breaks=50)
abline(v=bootstrapCI, lwd=2, col='red')


## relative risk RR confidence interval


x1 <- 616
n1 <- 839
x2 <- 693
n2 <- 1086
p1 <- x1/n1
p2 <- x2/n2
p1
p2

se <- sqrt(p1*(1-p1)/n1 + p2*(1-p2)/n2)
se
ci <- p1-p2 + c(-1,1) * 1.96*se
ci

pp <- (x1+x2)/(n1+n2)
pp
se0 <- sqrt((1/n1+1/n2)*pp*(1-pp))
se0
p1-p2
T <- (p1-p2)/se0
T
prop.test(c(x1,x2), c(n1,n2), correct=FALSE)
sqrt(20.083)


data <- matrix(c(10, 40, 20, 30, 20, 40), nrow = 2)
data
chisq.test(data)
pchisq(3.84, 1)
1 - pchisq(3.84, 1)
pchisq(4.8485, 2)
1 - pchisq(4.8485, 2)



x <- c(480,450,490,440,460,510,460,450,540)
n <- length(x)
n
xbar <- mean(x)
xbar
s <- sd(x)
se <- s/sqrt(n)
se
T <- (xbar-500)/se
T
pvalue <- pt(T,8) * 2
pvalue
t.test(x, mu=500)





## demo for STAT365 lecture 14, Fall2018

library(nimble)

df <- read.csv('~/github/courses/stat365/data/UsedCars.csv')
head(df)

code <- nimbleCode({
    b0    ~ dnorm(0, sd = 10000)
    bage  ~ dnorm(0, sd = 10000)
    bhp   ~ dnorm(0, sd = 10000)
    btype ~ dnorm(0, sd = 10000)
    sigma ~ dunif(0, 10000)
    for(i in 1:N) {
        mu[i] <- b0 + bage*age[i] + bhp*hp[i] + btype*type[i]
        y[i] ~ dnorm(mu[i], sd = sigma)
    }
})

constants <- list(N = dim(df)[1], age=df$Age, hp=df$HP, type=df$Type)
data <- list(y = df$Price)
inits <- list(b0=0, bage=0, bhp=0, btype=0, sigma = 1)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()  ## -851490867

Rmodel$calculate('y')
Rmodel$y

Rmodel$calculate('b0')
Rmodel$calculate('bage')
Rmodel$calculate('bhp')
Rmodel$calculate('sigma')
Rmodel$sigma

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printMonitors()
Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(Rmodel, Rmcmc))   ## NEW compilation
class(compiledList)   # a list
length(compiledList)  # length 2
Cmodel <- compiledList[[1]]
Cmcmc <- compiledList[[2]]

samples <- runMCMC(Cmcmc, 10000)
head(samples)
dim(samples)

## 1.
## thinning (BRIEFLY)
Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printMonitors()
## set thinning interval:
conf$setThin(10)
conf$printMonitors()

Rmcmc <- buildMCMC(conf)

compiledList <- compileNimble(list(Rmodel, Rmcmc))   ## NEW compilation
Cmcmc <- compiledList[[2]]

samples <- runMCMC(Cmcmc, 10000)
dim(samples)

## 2.
## burn-in
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()  ## -851490867
Rmcmc <- buildMCMC(Rmodel)   ## NEW: skipping MCMC configuration step
compiledList <- compileNimble(list(Rmodel, Rmcmc))   ## NEW compilation
Cmcmc <- compiledList[[2]]

## could do:
dim(samples)
dim(samples[501:1000, ])    ## manual burn-in first 500

args(runMCMC)

samples <- runMCMC(Cmcmc, 10000, nburnin=1000)   ## NEW: burn-in

samples <- runMCMC(Cmcmc, 10000, nburnin=1000, thin=10)   ## NEW: burn-in


dim(samples)

## 3.
## samplesSummary()

samples <- runMCMC(Cmcmc, 10000)
dim(samples)
head(samples)

## mean for sigma
mean(samples[, 'sigma'])

## samplesSummary() provided with NIMBLE:
samplesSummary(samples)

apply(samples, 2, mean)

## traceplots
plot(1:10000, samples[,'sigma'], type='l')

## density plots
plot(density(samples[3000:10000,'sigma']))

## 4.
## samplesPlot()
## from R package: basicMCMCplots (also mine)
install.packages('basicMCMCplots')
library(basicMCMCplots)

samplesPlot(samples)

samplesPlot(samples, 'bhp')
samplesPlot(samples, c('bhp','bage'))

samplesPlot(samples, 'sigma')

samplesPlot(samples, 'sigma', 1:2000)
samplesPlot(samples, 'sigma', 3001:10000)

samplesPlot(samples, 'sigma', burnin=200)

samplesPlot(samples, 'sigma')

samplesPlot(samples, 'sigma', file = '~/Downloads/plots.pdf')



args(samplesPlot)


## use this example data (from a real project)
## to show more samplesPlots:

load("~/github/courses/stat365/lectures/samplesPlot_Ex_data.RData")

samplesPlot(samples)

samplesPlot(samples, 'bp')

samplesPlot(samples, 'HR')

## alternative: bayesplots package






O <- matrix(c(914, 581,46, 735), 2, byrow = TRUE)
E <- matrix(c(630.58,  864.42,329.42,  451.58), 2, byrow=TRUE)
O
E
X2 <- sum((O-E)^2/E)
X2


library(nimble)
set.seed(0)
N <- 10
## default in R for dnorm is **sd** !!!
y <- rnorm(N, 40, 10)
code <- nimbleCode({
    mu ~ dnorm(0, 0.001)
    sigma ~ dunif(0, 100000)
    for(i in 1:N) {
        y[i] ~ dnorm(mu, sd = sigma)
    }
    yp ~ dnorm(mu+10, sd = sigma)
})
constants <- list(N = 10)
data <- list(y = y)
inits <- list(mu = 0, sigma = 2, yp = -100)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$yp
Rmodel$initializeInfo()
Rmodel$plotGraph()
conf <- configureMCMC(Rmodel)
conf$printMonitors()
conf$addMonitors('yp')
conf$addMonitors('sdfsdf')
conf$addMonitors('lifted_mu_plus_10')
conf$printSamplers()
conf$removeSamplers('sigma')
conf$printSamplers()
conf$addSampler('sigma', 'RW')
conf$printSamplers()
conf$addSampler('sigma', 'RW', scale = 5)
conf$addSampler('sigma', 'RW', scale = 5, adaptive = FALSE)
conf$printSamplers()


Rmcmc <- buildMCMC(conf)

Rmcmc

Cmodel <- compileNimble(Rmodel)

Cmcmc <- compileNimble(Rmcmc, project=Rmodel)


samples <- runMCMC(Cmcmc, 10000)
samples <- runMCMC(Cmcmc, 5000000, progressBar = FALSE)

dim(samples)

head(samples)

apply(samples, 2, mean)


Rmodel$mu
set.seed(0)
samples <- runMCMC(Rmcmc, 10)
samples
Rmodel$mu

Cmodel$mu
set.seed(0)
samples <- runMCMC(Cmcmc, 10)
samples
Cmodel$mu










df <- read.csv('~/github/courses/stat202/data/trial_verdicts.csv')
gender <- df$Gender
verdict <- df$Verdict
table(gender, verdict)
nmale <- sum(gender == 'Male')
nfemale <- sum(gender == 'Female')
r <- (49/nfemale) / (31/nmale)
logr <- log(r)
se <- sqrt(1/49 - 1/nfemale + 1/31 - 1/nmale)
se
ci <- logr + c(-1,1) * 1.96 * se
ci


survey <- read.csv('~/github/courses/stat202/data/STAT202.csv')
gender <- survey$Gender
chocolate <- survey$Chocolate
table(gender, chocolate)
n <- dim(survey)[1]
nmale <- sum(gender == 'male')
nfemale <- sum(gender == 'female')
N <- 10000
difs <- numeric(N)
for(i in 1:N) {
    chocolateP <- sample(chocolate)
    pf <- mean(chocolateP[1:nfemale] == 'dark')
    pm <- mean(chocolateP[(nfemale+1):n] == 'dark')
    difs[i] <- pf - pm
}

ci <- quantile(difs, c(0.025, 0.975))
ci
tab <- table(gender, chocolate)
d <- tab['female','dark']/nfemale - tab['male','dark']/nmale
d
hist(difs, breaks = 50)
abline(v=ci, lwd=2, col = 'blue')
abline(v=d,  lwd=2, col = 'red')
pval <- mean(difs < d | difs > -d)
pval


head(scale(samples))
##install.packages('devtools')

library(devtools)
has_devel()

## equivalent:
pkgbuild::has_compiler(TRUE)  ## Mac
pkgbuild::has_rtools()        ## Windows


##install.packages('nimble')




## STAT202 Permutation Distribution

survey <- read.csv('~/github/courses/stat202/data/STAT202.csv')
head(survey)

gender <- survey$Gender
friends <- survey$Friends

gender
friends



friendsM <- friends[gender == "male"]
friendsF <- friends[gender == "female"]
d <- mean(friendsM) - mean(friendsF)
d

## histogram
hist(friends, breaks = 10)

## boxplots
boxplot(friends ~ gender)

## permutation distribution
N <- 100000
difs <- numeric(N)
for(i in 1:N) {
    ##genderP <- sample(gender)
    ##friendsM <- friends[genderP == "male"]
    ##friendsF <- friends[genderP == "female"]
    #### alternate:
    ##friendsP <- sample(friends)
    ##friendsM <- friendsP[gender == "male"]
    ##friendsF <- friendsP[gender == "female"]
    ## another alternate:
    friendsP <- sample(friends)
    friendsM <- friendsP[1:16]
    friendsF <- friendsP[17:25]
    difs[i] <- mean(friendsM) - mean(friendsF)
}

hist(difs, breaks=100)
abline(v = 0, lwd=2)
ci <- quantile(difs, c(0.025, 0.975))
ci
abline(v=ci, lwd=2, col = 'blue')
abline(v=d, lwd=2, col = 'red')
## permutation p-value
mean(difs < d | difs > -d)
sum(difs < d | difs > -d) / N

## two-sample t-test

t.test(friendsM, friendsF)



## doing MH Metropolis-Hastings live coding
## example for STAT365
## Oct 2018

## data:
y <- 140
n <- 200

## prior: p ~ Beta(a0, b0)
a0 <- 20  ## successes
b0 <- 40
  ## failures

## likelihood: y ~ Binomial(n, p)

## ===> we know p|y ~ Beta(a0+y, b0+n-y)



plt <- function() {

    ymax <- 20
    plot(function(p) dbeta(p, a0, b0), ylim = c(0,ymax), lwd=2)

    xs <- seq(0, 1, by = 0.001)
    ys <- dbinom(y, n, xs)
    lines(xs, ys/max(ys)*ymax/2, col = 'blue', lwd=2)   ## rescale
    abline(v = y/n, col = 'blue', lty = 2)

    ys <- dbeta(xs, y+a0, n-y+b0)
    lines(xs, ys, col = 'red', lwd=2)

    legend(x = 'topright', legend = c('prior', 'likelihood', 'posterior'), lwd=2, col = c('black', 'blue', 'red'))

}


## Metropolis-Hastings!!

N <- 100000
p <- 0.6
sp <- 0.1

samples <- numeric(N)
##samples

for(i in 1:N) {
    pstar <- rnorm(1, p, sp)  ## Normal around p
    ##message('proposed: ', pstar)
    if(pstar < 0 | pstar > 1) {
        ##message('pstar out of bounds')
        samples[i] <- p
        next
    }
    loga <- dbeta(pstar,a0,b0,log=TRUE) +
        dbinom(y, n, pstar, log=TRUE) -
            dbeta(p, a0, b0, log=TRUE) -
                dbinom(y, n, p, log=TRUE)
    if(loga > 0) {
        ##message('accepted!!')
        p <- pstar
        samples[i] <- pstar
    }
    if(loga < 0) {
        if(log(runif(1)) < loga) {
            ##message('accepted!!')
            p <- pstar
            samples[i] <- pstar
        } else {
            ##message('rejected =(')
            samples[i] <- p
        }
    }
}

plt()
hist(samples, freq=FALSE, add=TRUE, breaks=100)

m <- 10
m <- 100
m <- 1000
m <- 10000
m <- 100000
plot(1:m, samples[1:m], type = 'l', ylim=c(0,1))





## updated usage of has_devel():

library(devtools)
has_devel
has_devel()
pkgbuild::has_build_tools
pkgbuild::has_compiler()
pkgbuild::has_compiler(TRUE)  ## Mac
pkgbuild::has_rtools()        ## Windows


## Tor's data?

load('~/Downloads/samples_firstRun.RData')
ls()
library(basicMCMCplots)
basicMCMCplots::samplesPlot(samples)
samplesList <- samples
samples <- samplesList[[1]]

chainsPlot(samplesList)
samplesPlot(samples)
samplesPlot(samples, 'bp')


## testing NIMBLE output options,
## specifically the output from model building nimbleModel()

library(nimble)
nimbleOptions(verbose = FALSE)
nimbleOptions(verbose = TRUE)
code <- nimbleCode({ a ~ dnorm(0, 1) })
Rmodel <- nimbleModel(code, inits = list(a=0))
Cmodel <- compileNimble(Rmodel)

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    for(i in 1:10) {
        b[i] <- c[i]
    }
    d[1:10] <- c[1:10]
})

Rmodel <- nimbleModel(code, inits = list(a = 0), constants = list(c=1:10))
Rmodel <- nimbleModel(code)


## Used Cars linear regression problem for STAT365

library(nimble)
library(basicMCMCplots)
df <- read.csv('~/github/courses/stat365/data/UsedCars.csv')

code <- nimbleCode({
    b0 ~ dnorm(0, sd=10000)
    bage ~ dnorm(0, sd=10000)
    bhp ~ dnorm(0, sd=10000)
    btype ~ dnorm(0, sd=10000)
    sigma ~ dunif(0, 10000)
    for(i in 1:N) {
        mu[i] <- b0 + bage*age[i] + bhp*hp[i] + btype*type[i]
        y[i] ~ dnorm(mu[i], sd = sigma)
    }
})
constants <- list(N = dim(df)[1], age=df$Age, hp=df$HP, type=df$Type)
data <- list(y = df$Price)
inits <- list(b0=0, bage=0, bhp=0, btype=0, sigma = 1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()  ## [1] -851490867

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)
cor(samples)

samplesSummary(samples)
samplesPlot(samples, scale=TRUE)

chainsPlot(list(a=samples, b=samples), nrows=1, jitter=.2, scale=TRUE)


cbind(1:40, sapply(1:40, function(x) min(ceiling(x/7), 3)))

samplesBIG <- cbind(samples, samples, samples, samples, samples, samples)
dim(samplesBIG)
dimnames(samplesBIG)[[2]] <- paste0('a', 1:dim(samplesBIG)[2])
dimnames(samplesBIG)

samplesPlot(samplesBIG)

chainsPlot(samplesBIG[,1:6])
chainsPlot(samplesBIG[,1:13], 'a44')

head(samples)
samplesPlot(samples, 'b0')
samplesPlot(samples, 'aa')

## Diamonds problem for STAT202

df <- read.csv('~/Downloads/Diamonds.csv')
dim(df)
head(df)

diamonds_table <- table(df)
prop.table(diamonds_table, 1)
barplot(t(diamonds_table), beside=TRUE, legend=levels(df$Clarity))

## two-sample internet hours problem for STAT202

df <- read.csv('~/github/courses/stat202/data/Web_Hours.csv')
head(df)
summary(df)
dim(df)
hours <- df$hours
n <- length(hours)
n
hist(hours)
mean(hours)
se <- sd(hours)/sqrt(n)
z <- qnorm(0.975)
ci <- mean(hours) + c(-1,1) * z * se
ci

iter <- 10000
means <- numeric(iter)
for(i in 1:iter) {
    bootstrapSample <- sample(hours, replace=TRUE)
    means[i] <- mean(bootstrapSample)
}
hist(means)
abline(v=ci, lwd=2, col='red')


bootstrapCI <- quantile(means, c(0.025, 0.975))
bootstrapCI
ci
abline(v=bootstrapCI, lwd=2, col='blue')

hours <- df$hours
gender <- df$gender
boxplot(hours ~ gender)
t.test(hours ~ gender)

male <- hours[gender == 'male']
female <- hours[gender == 'female']
mean(male)
mean(female)
length(male)
length(female)
table(gender)
mean(male)
mean(female)
mean(female) - mean(male)

se <- sqrt(sd(female)^2/length(female) + sd(male)^2/length(male))
se
ci <- mean(female) - mean(male) + c(-1, 1) * 1.96 * se
ci
t.test(hours ~ gender)

N <- 10000
dif <- numeric(N)
for(i in 1:N) {
    dif[i] <- mean(sample(female, replace=TRUE)) - mean(sample(male, replace=TRUE))
}
hist(dif)
bci <- quantile(dif, c(0.025, 0.975))
bci

prop.test(c(277,461), c(603,730))
prop.test(c(277,461), c(603,730), correct = FALSE)

## permutation distribution class problem for STAT202

indM <- 1:12
indF <- 13:33

y <- c(5,7,9,10,12,12,12,13,13,15,15,20,
       5,7,7,8,10,10,11,12,12,14,14,14,16,18,20,20,20,22,23,25,40)

y[indM]
y[indF]

N <- 100000
dif <- numeric(N)
for(i in 1:N) {
    newy <- sample(y)
    dif[i] <- mean(newy[indM]) - mean(newy[indF])
}

hist(dif)
obsdif <- mean(y[indM]) - mean(y[indF])
obsdif
abline(v = obsdif, col = 'red', lwd = 2)
mean(dif < obsdif | dif > (-obsdif))




## recreating calculations of WAIC

library(nimble)

set.seed(0)
N <- 10
y <- rnorm(N, 30, 4)
x <- rnorm(N)
beta <- 0.4

code <- nimbleCode({
    mu ~ dnorm(0, sd = 10000)
    beta ~ dnorm(0, sd = 10000)
    sigma ~ dunif(0, 10000)
    for(i in 1:N) {
        y[i] ~ dnorm(mu+beta*x[i], sd = sigma)
    }
})

constants <- list(N = N, x = x)
data <- list(y = y)
inits <- list(mu = 0, beta = 0, sigma = 1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel, enableWAIC = TRUE)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

set.seed(0)
samples <- runMCMC(Cmcmc, 2000)
(waic <- Cmcmc$calculateWAIC())

##library(basicMCMCplots)
##samplesPlot(samples)

(S <- dim(samples)[1])
(dataNodes <- Rmodel$getNodeNames(dataOnly = TRUE))
(paramNames <- colnames(samples))
(nDataNodes <- length(dataNodes))

lps <- array(0, c(nDataNodes, S))
for(s in 1:S) {
    values(Cmodel, paramNames) <- samples[s,]
    Cmodel$calculate()
    for(i in 1:nDataNodes) {
        lps[i,s] <- Cmodel$getLogProb(dataNodes[i])
    }
}

ps <- exp(lps)
(lppd <- sum(log(apply(ps, 1, mean))))
(pwaic2 <- sum(apply(lps, 1, var)))

-2*(lppd - pwaic2)
-2*(lppd - pwaic2) - waic   ## agrees!




## grading comments for STAT365 problem set 3 pset3

Nice explanation for Problem 2 part (b).  Just what I was looking for.

-5 Problem 2 part (b), looking for essentially we have a prior belief corresponding to a sample of 5 past observations, and a sum total count of 3 from those 5 observations (e.g. seen data: 0,0,0,1,2)

-3 Problem 2 part (d) looking for a much more detailed exploration of the behavior at different (including log) scales, and associated plots.

Good investigation in Problem 2 part (d)

-2 Problem 5 part (4) Looking for actual posterior density function inside the double-integral

-3 Problem 10.4 prior dist is wrong, should be Gamma(r=9, v=1.5), such that mean=9/1.5=6, and sd=sqrt(var)=sqrt(18/1.5)=sqrt(4)=2



## helping NIMBLE user re-use compiled MCMC

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    y ~ dnorm(a, sd = sigma)
})

nReps <- 10 

## separately generate response *data*,
## and *inits* for covariates, etc
data.gen <- function() {
    y <- rnorm(1)
    sigma <- runif(1)
    list(data = list(y=y),
         inits = list(sigma = sigma))
}

simu.data <- as.list(rep(NA, nReps))
for(i in 1:nReps) {
    simu.data[[i]] <- data.gen()
}

## supply data here, so that 'y' is specified as data,
## and no MCMC sampler is placed on y.
## we'll change the value of the data later, inside the loop
constants <- list()
Rmodel <- nimbleModel(code, constants, data = simu.data[[1]]$data)

conf <- configureMCMC(Rmodel, enableWAIC = TRUE)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

sim4.new <- function(dat) {
    data.new <- dat$data
    inits.new <- dat$inits
    Cmodel$setData(data.new)
    Cmodel$setInits(inits.new)
    mcmc.out <- runMCMC(Cmcmc, nchains = 1, nburnin = 1000, niter = 10000,
                        summary = TRUE, WAIC = TRUE)
    list(mcmc.out$summary, mcmc.out$WAIC)
}

Result4 <- lapply(simu.data[1:nReps], sim4.new)

str(Result4)








## someone's NIMBLE error for using dmnorm()

library(nimble)
library(mvtnorm)

## generate data
Y <- rmvnorm(25,c(300,350),sigma=matrix(c(9,0,0,9),2,2,byrow=TRUE))

## specify the model
nModel <- nimbleCode({
    for(j in 1:N){
        ##Y[j,1:2] ~ dmnorm(mu[1:2],cholesky=SIGMA[1:2,1:2],prec_param=0)
        Y[j,1:2] ~ dmnorm(mu[1:2],cov=SIGMA[1:2,1:2])
    }
## priors and derivations
    sig ~ dgamma(3,1)
    for(i in 1:2){
        for(k in 1:2){
            SIGMA[i,k] <- equals(i,k)*sig*sig
        }
        mu[i] ~ dnorm(300,1/25)
    }
})

## monitors, constants, data, inits, and mcmc controls
parameters <- c("mu","sig") 
nConsts <- list(N=25)
nInits <- function(){list(mu=rnorm(2,300,10),
               sig=rgamma(1,3,1),
               SIGMA=diag(rgamma(2,3,1)**2))}
nData <- list(Y=Y)
ni <- 300
nt <- 3
nb <- 100
nc <- 3

## compile and run the model
nim <- nimbleModel(code=nModel,name='nim',constants=nConsts,data=nData)

nim$initializeInfo()

nimMCconf <- configureMCMC(nim,monitors=parameters,thin=nt)
nimMC <- buildMCMC(nimMCconf)
Cnim <- compileNimble(nim)
CnimMC <- compileNimble(nimMC,project=nim)
out <- runMCMC(CnimMC,niter=ni+nb,nburnin=nb,nchains=nc,inits=nInits)



## budget additions for Hellman Fellowship

2000 + 500 + 80*12 + 60*7 + 1200*12 + 100*7

80*12

## looking at Beta(1/2, 1/2) prior effects:
plot(function(x) dbeta(x,1,1), xlim = c(0,1), ylim = c(0,3))
text(.05, 0.9, 'Beta(1, 1)', cex=.8)

plot(function(x) dbeta(x, 1/2, 1/2), add = TRUE, col = 'red')
text(.9, 2.5, 'Beta(1/2, 1/2)', cex=.8, col = 'red')

plot(function(x) dbeta(x, 1/2, 3/2), add = TRUE, col = 'blue')
text(.15, 2.4, 'Beta(1/2, 3/2)', cex=.8, col = 'blue')

plot(function(x) dbeta(x, 1/2, 5/2), add = TRUE, col = 'orange') 
text(.22, 1.9, 'Beta(1/2, 5/2)', cex=.8, col = 'orange') 

plot(function(x) dbeta(x, 3/2, 3/2), add = TRUE, col = 'purple')
text(.6, 1.35, 'Beta(3/2, 3/2)', cex=.8, col = 'purple')

plot(function(x) dbeta(x, 3/2, 5/2), add = TRUE, col = 'green')
text(.25, 1.7, 'Beta(3/2, 5/2)', cex=.8, col = 'green')

plot(function(x) dbeta(x, 5/2, 5/2), add = TRUE, col = 'grey')
text(.6, 1.75, 'Beta(5/2, 5/2)', cex=.8, col = 'grey')

points(0.25, 1.1, lwd=3)
text(0.35, 1.1, 'what\'s going on?', cex=0.7)



lambda_true <- 3


set.seed(0)
n <- 3
y <- rpois(n, lambda_true)
y

## priors:
## prior 1: Jeffreys'
## prior 2: Flat
## prior 3: common choice: Gamma(0.001, 0.001)

par(mfrow = c(2,1), mar = c(4,2,2,1))
xmax <- lambda_true * 2.5
ymax <- 2

## graph priors:
plot(-1, -1, xlim = c(0,xmax), ylim = c(0,ymax), xlab='', ylab='', main = 'Priors & Likelihood')
xs <- seq(0, xmax, length = 100000)

## prior 1: Jeffreys (blue)
ys <- 1/sqrt(xs)
lines(xs, ys, lwd=2, col = 'blue')

## prior 2: Flat (green)
lines(xs, xs/xs, lwd=2, col = 'green')

## prior 3: common Gamma(0.001, 0.001) (red)
lines(xs, dgamma(xs, 0.001, 0.001), lwd=2, col = 'red')

legend(x = 'topright', legend = c('Jeffreys', 'flat', 'Gamma(0.001,0.001)'), col = c('blue', 'green', 'red'), lwd=2)

## Likelihood
likelihood <- sapply(xs, function(x) prod(dpois(y,x)))
likelihood_scaled <- likelihood/max(likelihood) * ymax/2
lines(xs, likelihood_scaled, lwd = 2)

## Posteriors:
plot(-1, -1, xlim = c(0,xmax), ylim = c(0,ymax), xlab='', ylab='', main = 'Posteriors')

## prior 1: Jeffreys (blue)
ys <- dgamma(xs, 1/2+sum(y), 0+n)
thismax <- max(ys) * 1.3
lines(xs, ys/thismax*ymax, lwd=2, col = 'blue')

## prior 2: Flat (green)
ys <- dgamma(xs, 1+sum(y), 0+n)
lines(xs, ys/thismax*ymax, lwd=2, col = 'green')

## prior 3: common Gamma(0.001, 0.001) (red)
ys <- dgamma(xs, 0.001+sum(y), 0.001+n)
lines(xs, ys/thismax*ymax, lwd=2, col = 'red')

legend(x = 'topright', legend = c('Jeffreys', 'flat', 'Gamma(0.001,0.001)'), col = c('blue', 'green', 'red'), lwd=2)

abline(v = lambda_true, lwd = 2)
mean(y)
abline(v = mean(y), col = 'purple', lwd = 2)






## daily Williamstown MA temperatures in September:
temps <- c(87, 81, 79, 87, 75, 73, 67, 73, 75, 77,
           81, 81, 84, 88, 91, 88, 91, 86, 81, 91,
           68, 68, 76, 80, 65, 67, 72)

length(temps)

## histogram
hist(temps, xlab='Daily High Temperature (F)', breaks = 15)

## time series
plot(1:length(temps), temps, type='l', xlab='September Day', ylab='Daily High Temperature (F)')
points(1:length(temps), temps, pch=20)

## BOOTSTRAPPING
## parameter of interest:
## average daily temperature in September







## example of dmnorm() issues
## on branch ADcleanup

library(nimble)
set.seed(0)

s1 <- 4
s2 <- 7
rho <- 0.6
Sigma <- array(0, c(2,2))
Sigma[1,1] <- s1^2
Sigma[2,2] <- s2^2
Sigma[1,2] <- s1*s2*rho
Sigma[2,1] <- s1*s2*rho

code <- nimbleCode({ x[1:2] ~ dmnorm(mu[1:2], cov = Sigma[1:2,1:2]) })
constants <- list(Sigma = Sigma, mu = c(0,0))
data <- list()
inits <- list(x = c(0,0))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()   ## -4.946938

Cmodel <- compileNimble(Rmodel)
Cmodel$calculate()   ## -8.502286 ... why different ???

N <- 10000
x <- array(0, c(N,2))
for(i in 1:N) {
    Cmodel$simulate()
    x[i,1:2] <- Cmodel$x
}

cov(x)    ## why not equal to Sigma ???
##          [,1]     [,2]
## [1,] 251.0265  269.636
## [2,] 269.6360 2703.788

## here's what I think these values are approaching:
s1^4
s2^4
s1^2 * s2^2 * rho^2





## testing modelling correlated random effects
## for Chloe Nater trout model

s1 <- 13
s2 <- .5
rho <- -0.4
n <- 1000000
x1 <- rnorm(n)
x2 <- rho*x1 + sqrt(1-rho^2)*rnorm(n)
z1 <- s1*x1
z2 <- s2*x2
c(sd(z1), sd(z2), cor(z1, z2))


library(nimble)

code <- nimbleCode({
    Sigma[1,1] <- s1^2
    Sigma[2,2] <- s2^2
    Sigma[1,2] <- s1*s2*rho
    Sigma[2,1] <- s1*s2*rho
    mu[1] <- 0
    mu[2] <- 0
    x[1:2] ~ dmnorm(mu[1:2], cov = Sigma[1:2,1:2])
    t[1] ~ dnorm(0, 1)
    t[2] ~ dnorm(0, 1)
    t[3] <- rho*t[1] + sqrt(1-rho^2)*t[2]
    y[1] <- s1*t[1]
    y[2] <- s2*t[3]
})
constants <- list(s1 = 4, s2 = 7, rho = 0.6)
data <- list()
inits <- list()
Rmodel <- nimbleModel(code, constants, data, inits)

Cmodel <- compileNimble(Rmodel)

N <- 20000
x <- array(0, c(N,2))
y <- array(0, c(N,2))
for(i in 1:N) {
    Cmodel$simulate()
    x[i,1:2] <- Cmodel$x
    y[i,1:2] <- Cmodel$y
}

c(sd(x[,1]), sd(x[,2]), cor(x[,1], x[,2]))
c(sd(y[,1]), sd(y[,2]), cor(y[,1], y[,2]))

Cmodel$Sigma
cov(x)
cov(y)


library(MASS)
n <- 10000
z <- array(0, c(n,2))
for(i in 1:n) {
    z[i,1:2] <- mvrnorm(1, mu=c(0,0), Sigma = Cmodel$Sigma)
}

cov(z)
sd(z[,1])
sd(z[,2])
cor(z[,1], z[,2])




## working out the two-sample Poisson (gamma ray emissions) from
## STAT365 problem set 3

theta_true
na <- 27
ya <- 341

nb <- 43
yb <- 467


prior_shape <- 0.5
prior_rate <- 0

n <- 100000
lambdaa <- rgamma(n, prior_shape+ya, prior_rate+na)
lambdab <- rgamma(n, prior_shape+yb, prior_rate+nb)

mean(lambdaa > lambdab)
dif <- lambdaa - lambdab
quantile(dif, c(0.025, 0.975))


theta_true <- 0.8


## testing removing extra logH() calculations from HMC sampler

library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)

{
    set.seed(0)
    ##load('~/github/public/HMCcomparisons/election88_clean/data_setup_from_stan_github/data_setup_from_stan_github.RData')
    ##election.inits <- list(b.0=rnorm(1), b.female=rnorm(1), b.black=rnorm(1),
    ##                       b.female.black=rnorm(1),
    ##                       b.age=rnorm(n.age), b.edu=rnorm(n.edu),
    ##                       b.age.edu=array(rnorm(n.age*n.edu),
    ##                           c(n.age,n.edu)), b.state=rnorm(n.state), b.v.prev=rnorm(1), 
    ##                       b.region=rnorm(n.region), sigma.age=runif(1),
    ##                       sigma.edu=runif(1), sigma.age.edu=runif(1),
    ##                       sigma.state=runif(1), sigma.region=runif(1))
    ##election88_BUGS <- readBUGSmodel("17.4_Bugs_codes.bug", dir = '~/github/public/HMCcomparisons/election88_clean/models', returnComponents = TRUE)
    ##BUGSdataList.1 <- dataList.1
    ##names(BUGSdataList.1) <- tolower( gsub("_",".",names(BUGSdataList.1)))
    ##election88_BUGS$data <- BUGSdataList.1
    ##election88_BUGS$inits <- election.inits
    ##code <- election88_BUGS$code
    ##yInd <- which(names(election88_BUGS$data) == 'y')
    ##constants <- election88_BUGS$data[-yInd]
    ##data <- election88_BUGS$data[yInd]
    ##inits <- election88_BUGS$inits
    ##
    Rmodel <- nimbleModel(code, constants, data, inits)
    params <- Rmodel$getNodeNames(topOnly = TRUE, stochOnly = TRUE)
    params
    conf <- configureMCMC(Rmodel)
    conf$removeSamplers(params)
    conf$addSampler(params, 'HMCexp', control = list(printTimesRan = TRUE, printJ = TRUE))
    conf$addSampler(params, 'HMCexp', control = list(printTimesRan = TRUE, printGradient = TRUE))
    conf$addSampler(params, 'HMCexp')
    conf$printSamplers()
    ##
    Rmcmc <- buildMCMC(conf)
    ##Cmodel <- compileNimble(Rmodel)
    ##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)##, showCompilerOutput = TRUE)
    compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
    Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
    ##
    ##niter <- 6
    ##set.seed(0)
    ##samples <- runMCMC(Cmcmc, niter)
    ##
    niter <- 50
    set.seed(0)
    samples <- runMCMC(Cmcmc, niter)
    correct <- all(round(as.numeric(samples[niter,]),8) - c(1.77494912, -1.73251839, -0.20268596, -0.07397942, -1.53999753, 1.82689544, 1.14479358, 1.01806362, 0.36902964, 0.40369282) == 0)
    if(correct)  message('looks good')
    if(!correct) message('WRONG')
}


## testing basicMCMCplots package on CRAN (!)

library(basicMCMCplots)

remove.packages('basicMCMCplots')

install.packages('basicMCMCplots')

samplesPlot
chainsPlot

?samplesPlot
?chainsPlot

samples <- cbind(rnorm(1000), rgamma(1000, 1))
colnames(samples) <- c('alpha', 'beta')
samplesPlot(samples)


samples1 <- cbind(rnorm(1000, 1), rgamma(1000, 1), rpois(1000, 1))
colnames(samples1) <- c('alpha', 'beta', 'gamma')
samples2 <- cbind(rnorm(1000, 2), rgamma(1000, 2), rpois(1000, 2))
colnames(samples2) <- c('alpha', 'beta', 'gamma')
samplesList <- list(chain1 = samples1, chain2 = samples2)
chainsPlot(samplesList, nrow = 1, jitter = .3, buffer.left = .5, buffer.right = .5)


## testing rev() reversing a vector via indexing in NIMBLE


library(nimble)

Rnf <- nimbleFunction(
    run = function(a = double(1)) {
        L <- dim(a)[1]
        print('length = ', L)
        index <- numeric(L)
        for(i in 1:L) {
            index[i] <- L + 1 - i
        }
        b <- a[index]
        print('reversed vector: ', b)
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

f <- 1:5
Rnf(f)
Cnf(f)



## discrepancy in grad calculations between R and C ???
## this actually just results from derivatives for dmnorm()
## distributions not being implemented yet.

library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)
code <- nimbleCode({
    mu ~ dnorm(0, sd = 10)
    tau ~ dgamma(0.01, 0.01)
    sigma ~ dunif(0, 10)
    p ~ dbeta(5, 2)
    mu4 <- mu * p
    mean[1] <- mu
    mean[2] <- mu4
    mean[3] <- p
    z[1:3] ~ dmnorm(mean[1:3], cov = C[1:3,1:3])
    y1 ~ dnorm(mu, tau)
    y2 ~ dnorm(mu, tau)
    y3 ~ dnorm(mu, sd = sigma)
    y4 ~ dnorm(mu4, tau)
    y5 ~ dnorm(z[1], sd = 3)
    y6 ~ dnorm(z[2], sd = sigma)
    y7 ~ dnorm(z[3], sd = sigma)
})
A <- array(c(1,2,3,0,1,2,0,0,3), c(3,3))
C <- t(A) %*% A
constants <- list(C = C)
data <- list(y1 = 10, y2 = 9, y3 = 9, y4 = 7, y5 = 6, y6 = 5, y7 = 1)
inits <- list(mu = 0, tau = 1, sigma = 1, p = 0.5, z = rep(0,3))
Rmodel <- nimbleModel(code, constants, data, inits)
targetNodes <- c('mu','p','sigma','tau','z')
values(Rmodel, targetNodes) <- c(9.3431655, 0.5562529, 2.5712827, 1.5704631, 6.3846235, 4.6843885, 0.8661889)

Rmodel$calculate()
## [1] -29.31628

nfDef <- nimbleFunction(
    setup = function(model, targetNodes) {
        calcNodes <- model$getDependencies(targetNodes)
        d <- 7
    },
    run = function() {
        q <- values(model, targetNodes)
        print('q = ', q)
        derivsOutput <- derivs(model$calculate(calcNodes), order = 1, wrt = targetNodes)
        grad <- numeric(d)
        grad[1:d] <- derivsOutput$jacobian[1, 1:d]
        print('grad = ', grad)
    }
)

Rnf <- nfDef(Rmodel, targetNodes)

Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(Rnf, project = Rmodel)

set.seed(0); Rnf$run()
## q = 9.343165 0.5562529 2.571283 1.570463 6.384624 4.684388 0.8661889
## grad = 1.727865 58.76335 -1.152893 -1.584963 1.786945 -2.892227 0.1160966

set.seed(0); Cnf$run()
## q =  9.34317
## 0.556253
##  2.57128
##  1.57046
##  6.38462
##  4.68439
## 0.866189
## 1 
## grad = -54.3675
## -223.241
## -1.15289
## -1.58496
##  42.6896
##  24.4203
##  26.9343
## NULL






## testing error catching in buildtree/leapfrog routines
## of HMC sampler, to better handle NaNs.


library(nimble)
source('~/temp/temp.R')
is.na.vec <- nimble:::is.na.vec
is.nan.vec <- nimble:::is.nan.vec
isValid <- nimble:::isValid
nimbleOptions(experimentalEnableDerivs = TRUE)
code <- nimbleCode({
    mu ~ dnorm(0, sd = 10)
    tau ~ dgamma(0.01, 0.01)
    sigma ~ dunif(0, 10)
    p ~ dbeta(5, 2)
    mu4 <- mu * p
    mean[1] <- mu
    mean[2] <- mu4
    mean[3] <- p
    z[1:3] ~ dmnorm(mean[1:3], cov = C[1:3,1:3])
    y1 ~ dnorm(mu, tau)
    y2 ~ dnorm(mu, tau)
    y3 ~ dnorm(mu, sd = sigma)
    y4 ~ dnorm(mu4, tau)
    y5 ~ dnorm(z[1], sd = 3)
    y6 ~ dnorm(z[2], sd = sigma)
    y7 ~ dnorm(z[3], sd = sigma)
})
A <- array(c(1,2,3,0,1,2,0,0,3), c(3,3))
C <- t(A) %*% A
constants <- list(C = C)
data <- list(y1 = 10, y2 = 9, y3 = 9, y4 = 7, y5 = 6, y6 = 5, y7 = 1)
inits <- list(mu = 0, tau = 1, sigma = 1, p = 0.5, z = rep(0,3))
Rmodel <- nimbleModel(code, constants, data, inits)
values(Rmodel, c('mu','p','sigma','tau','z')) <- c(9.3431655, 0.5562529, 2.5712827, 1.5704631, 6.3846235, 4.6843885, 0.8661889)
Rmodel$calculate()

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler(c('mu','tau','sigma','p','z'), 'HMCexp',
                control=list(printTimesRan=TRUE, printJ = TRUE, printEpsilon = TRUE))
conf$printSamplers()
conf$addMonitors('z')
Rmcmc <- buildMCMC(conf)

##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

iter <- 1
set.seed(0); Rsamples <- runMCMC(Rmcmc, iter)
set.seed(0); Csamples <- runMCMC(Cmcmc, iter)

##samples <- runMCMC(Cmcmc, 40000, nburnin=10000)

debug(Rmcmc$samplerFunctions$contentsList[[1]]$run)
debug(Rmcmc$samplerFunctions$contentsList[[1]]$initializeEpsilon)
debug(buildtree)
debug(leapfrog)
debug(gradient)









Rmodel <- nimbleModel(code, constants, data, inits)
values(Rmodel, c('mu','p','sigma','tau','z')) <- c(9.3431655, 0.5562529, 2.5712827, 1.5704631, 6.3846235, 4.6843885, 0.8661889)
Rmodel$calculate()
conf <- configureMCMC(Rmodel, multivariateNodesAsScalars = TRUE)
conf$addMonitors('z')
Rmcmc <- buildMCMC(conf)
compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
set.seed(0)
samples <- runMCMC(Cmcmc, 100000, nburnin = 10000)
means <- apply(samples, 2, mean)

meansHMC <- apply(samples, 2, mean)



## testing how logical scalars end up in C++
## and how to modify them

library(nimble)

nfDef <- nimbleFunction(
    setup = function() {
        warn <- TRUE
    },
    run = function() {
        for(i in 1:10) {
            print(warn)
            if(i == 4) {
                warn <<- FALSE
                print('changed warn to 0')
            }
            if(warn) print('warning appears to be true')
        }
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


## testing how model$getBounds() works,
## specifically with dynamic bounds ...?

library(nimble)

code <- nimbleCode({
    x ~ dgamma(1, 1)
    u ~ dunif(0, 1)
    u2 ~ dunif(a, b)
})
constants <- list()
data <- list(x=1, u=0.5, u2=11)
inits <- list(a = 10, b = 15)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
Rmodel$getBound('x', 'lower')   ## getBound() always returns a scalar value 
Rmodel$getBound('x', 'upper')   ## getBound() always returns a scalar value 
Rmodel$getBound('u', 'lower')   ## getBound() always returns a scalar value 
Rmodel$getBound('u', 'upper')   ## getBound() always returns a scalar value 
Rmodel$getBound('u2', 'lower')
Rmodel$getBound('u2', 'upper')
Rmodel$a <- 2
Rmodel$getBound('u2', 'lower')
Rmodel$getBound('u2', 'upper')


## testing if I can manipulate correct derivatives,
## using transformations of parameters

library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)
code <- nimbleCode({
    tau ~ dgamma(0.001, 0.001)
    y ~ dnorm(mu, tau)
})
constants <- list()
data <- list(y = 2)
inits <- list(tau = 1, mu = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel)
if(FALSE) {
    conf <- configureMCMC(Rmodel, nodes = NULL)
    conf$addSampler('tau', 'slice')
    conf$addSampler('tau', 'HMCexp')
}
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)
set.seed(0)
samples <- runMCMC(Cmcmc, 100000)
samplesSummary(samples[10001:100000,1,drop=FALSE])
##           Mean   Median   St.Dev.   95%CI_low 95%CI_upp
## tau 0.251606 0.114134 0.3575026 0.000246870  1.268310   ## conjugate
## tau 0.254423 0.119158 0.3563201 0.000288046  1.270125   ## slice
## tau 0.245664 0.111876 0.3415829 0.000206702  1.256368   ## HMCexp

## now with 2 parameters
library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)
code <- nimbleCode({
    mu ~ dnorm(0, sd = 10)
    tau ~ dgamma(0.001, 0.001)
    y1 ~ dnorm(mu, tau)
    y2 ~ dnorm(mu, tau)
})
constants <- list()
data <- list(y1 = 10, y2 = 9)
inits <- list(tau = 1, mu = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel)
if(FALSE) {
    conf <- configureMCMC(Rmodel, nodes = NULL)
    conf$addSampler('mu', 'slice')
    conf$addSampler('tau', 'slice')
    conf$addSampler(c('mu','tau'), 'HMCexp2')
}
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)
set.seed(0)
samples <- runMCMC(Cmcmc, 100000)
samples <- runMCMC(Cmcmc, 50000)
samples <- runMCMC(Cmcmc, 500)
samplesSummary(samples[10001:100000,])
samplesSummary(samples)
##           Mean    Median   St.Dev. 95%CI_low 95%CI_upp
## mu  9.067989 9.4229964 2.461264 2.66626006 12.247801   ## conjugate
## mu  9.079892 9.4254463 2.426710 2.95274037 12.229024   ## slice
## mu  9.074890 9.4185149 2.347057 2.94519150 12.074267   ## HMCexp2
## tau 2.008685 0.9428221 2.808015 0.00549257  9.986771   ## conjugate
## tau 2.032073 0.9563287 2.811927 0.00574744  9.962286   ## slice
## tau 2.036056 0.9775714 2.757554 0.00584911  9.817346   ## HMCexp2

## now with 3 parameters
library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)
code <- nimbleCode({
    mu ~ dnorm(0, sd = 10)
    tau ~ dgamma(0.001, 0.001)
    p ~ dbeta(2, 5)
    mu4 <- mu * p
    y1 ~ dnorm(mu, tau)
    y2 ~ dnorm(mu, tau)
    y3 ~ dnorm(mu, tau)
    y4 ~ dnorm(mu4, tau)
})
constants <- list()
data <- list(y1 = 10, y2 = 9, y3 = 9, y4 = 7)
inits <- list(tau = 1, mu = 0, p = 0.5)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

##conf <- configureMCMC(Rmodel)
if(FALSE) {
    conf <- configureMCMC(Rmodel, nodes = NULL)
    conf$addSampler('mu', 'slice')
    conf$addSampler('tau', 'slice')
    conf$addSampler('p', 'slice')
    conf$addSampler(c('mu','tau','p'), 'HMCexp3')  # TEMP
    conf$addSampler(c('mu','tau','p'), 'HMCtransform')
}

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler(c('mu','tau','p'), 'HMCtransform')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

##set.seed(0)
##is.na.vec <- nimble:::is.na.vec
##is.nan.vec <- nimble:::is.nan.vec
##samples <- runMCMC(Rmcmc, 10)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)
set.seed(0)
samples <- runMCMC(Cmcmc, 100000)
samplesSummary(samples[10001:100000,])
samples <- runMCMC(Cmcmc, 50000)        ## HMC
samplesSummary(samples[10001:50000,])   ## HMC
## mu  9.4060075 9.4273463 1.1635534 6.97332757 11.520919   ## RW/conj
## mu  9.4131210 9.4294717 1.1324758 7.06077706 11.4911946  ## slice
## mu  9.4028371 9.4227690 1.1829225 6.98780585 11.5523091  ## HMCexp3
## mu  9.3978445 9.4325028 1.2342564 6.86568865 11.5515871  ## HMCtransform

## p   0.5924447 0.6403939 0.1636843 0.17500541  0.799216   ## RW/conj
## p   0.5923666 0.6395908 0.1629878 0.17411109  0.7990433  ## slice
## p   0.5896037 0.6397684 0.1652728 0.16956480  0.7986725  ## HMCexp3
## p   0.5868099 0.6347775 0.1649532 0.16629817  0.7962955  ## HMCtransform

## tau 2.1022980 1.1052671 2.6385379 0.03382642  9.583375   ## RW/conj
## tau 2.0887282 1.1067871 2.6218130 0.03464545  9.3844037  ## slice
## tau 2.0765938 1.0904541 2.5352347 0.03248994  9.4888141  ## HMCexp3
## tau 2.0513042 1.0384825 2.6320516 0.03166425  9.5019526  ## HMCtransform




## now with 3 parameters, testing transform for Uniform bounds
library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)
isValid <- nimble:::isValid
code <- nimbleCode({
    mu ~ dnorm(0, sd = 10)
    tau ~ dgamma(0.001, 0.001)
    p ~ dunif(5, 15)
    mu4 <- mu * p/10
    y1 ~ dnorm(mu, tau)
    y2 ~ dnorm(mu, tau)
    y3 ~ dnorm(mu, tau)
    y4 ~ dnorm(mu4, tau)
})
constants <- list()
data <- list(y1 = 10, y2 = 9, y3 = 9, y4 = 7)
inits <- list(tau = 1, mu = 0, p = 10)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

if(FALSE) {
    conf <- configureMCMC(Rmodel)
    conf <- configureMCMC(Rmodel, nodes = NULL)
    conf$addSampler('mu', 'slice')
    conf$addSampler('tau', 'slice')
    conf$addSampler('p', 'slice')
}

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler(c('mu','tau','p'), 'HMCtransform')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 100000)
samplesSummary(samples[10001:100000,])
samples <- runMCMC(Cmcmc, 50000)        ## HMC
samplesSummary(samples[10001:50000,])   ## HMC

##         Mean   Median  St.Dev. 95%CI_low 95%CI_upp
## mu  9.246505 9.292683 0.661311 7.8100406  10.34957  ## RW/conj
## mu  9.241313 9.291300 0.678531 7.7739379  10.35707  ## slice
## mu  9.237858 9.289663 0.675398 7.7544097  10.34117  ## HMCtransform

## p   7.674516 7.564176 1.136271 5.7600260  10.54681  ## RW/conj
## p   7.678420 7.556258 1.151561 5.7414989  10.61413  ## slice
## p   7.684359 7.554083 1.165708 5.7211875  10.68479  ## HMCtransform

## tau 3.101249 2.177604 3.021380 0.1241092  11.22292  ## RW/conj
## tau 3.091093 2.184328 3.010798 0.1235070  11.21699  ## slice
## tau 3.060351 2.161488 2.937445 0.1152577  10.97772  ## HMCtransform



## now with 4 nodes, including dmnorm()
library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)
code <- nimbleCode({
    mu ~ dnorm(0, sd = 10)
    tau ~ dgamma(0.001, 0.001)
    p ~ dbeta(2, 5)
    mu4 <- mu * p
    mean[1] <- mu
    mean[2] <- mu4
    mean[3] <- p
    z[1:3] ~ dmnorm(mean[1:3], cov = C[1:3,1:3])
    y1 ~ dnorm(mu, tau)
    y2 ~ dnorm(mu, tau)
    y3 ~ dnorm(mu, tau)
    y4 ~ dnorm(mu4, tau)
    y5 ~ dnorm(z[1], sd = 3)
    y6 ~ dnorm(z[2], sd = 3)
    y7 ~ dnorm(z[3], sd = 3)
})
constants <- list()
data <- list(y1 = 10, y2 = 9, y3 = 9, y4 = 7, y5 = 6, y6 = 5, y7 = 1)
inits <- list(tau = 1, mu = 0, p = 0.5, C = diag(3), z = rep(0,3))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

##conf <- configureMCMC(Rmodel)
if(FALSE) {
    conf <- configureMCMC(Rmodel, multivariateNodesAsScalars = TRUE)
    conf <- configureMCMC(Rmodel, nodes = NULL)
    conf$addSampler('mu', 'slice')
    conf$addSampler('tau', 'slice')
    conf$addSampler('p', 'slice')
    conf$addSampler('z[1]', 'slice')
    conf$addSampler('z[2]', 'slice')
    conf$addSampler('z[3]', 'slice')
    conf$addSampler(c('mu','tau','p','z'), 'HMCtransform')
}

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler(c('mu','tau','p'), 'HMCtransform')
conf$printSamplers()
conf$addMonitors('z')
Rmcmc <- buildMCMC(conf)

##set.seed(0)
##is.na.vec <- nimble:::is.na.vec
##is.nan.vec <- nimble:::is.nan.vec
##samples <- runMCMC(Rmcmc, 10)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)
set.seed(0)
samples <- runMCMC(Cmcmc, 100000)
samplesSummary(samples[10001:100000,])
samples <- runMCMC(Cmcmc, 25000)        ## HMC
samplesSummary(samples[5001:25000,])    ## HMC

##           Mean    Median   St.Dev.   95%CI_low  95%CI_upp
## mu   9.2310748 9.3343084 0.9126479  6.9766853 10.7395677   ## RW/conj
## mu   9.2241751 9.3262938 0.8924737  6.9256212 10.7072499   ## slice
## mu   9.2304496 9.3278235 0.8529397  7.0709701 10.6669807   ## HMCtransform

## p    0.6027851 0.6404274 0.1468743  0.2272824  0.7955066   ## RW/conj
## p    0.6038997 0.6423155 0.1472301  0.2300131  0.7979917   ## slice
## p    0.6043166 0.6423192 0.1474197  0.2308217  0.7988566   ## HMCtransform

## tau  2.1030108 1.1326969 2.5892453  0.0430473  9.3521521   ## RW/conj
## tau  2.1245912 1.1502844 2.6450259  0.0433303  9.5458740   ## slice
## tau  2.1409269 1.1746608 2.6292376  0.0452026  9.2273808   ## HMCtransform

## z[1] 8.9262457 8.9720816 1.2562765  6.2992350 11.2346828   ## RW/conj
## z[1] 8.8993040 8.9467441 1.2464464  6.2689713 11.1867286   ## slice
## z[1] 8.9082048 8.9422400 1.2113777  6.4197096 11.1398199   ## HMCtransform

## z[2] 5.5270078 5.7276932 1.6278032  1.7937395  8.2404773   ## RW/conj
## z[2] 5.5339427 5.7306211 1.6243782  1.7958342  8.2285731   ## slice
## z[2] 5.5461423 5.7421786 1.6093514  1.8411145  8.2670835   ## HMCtransform

## z[3] 0.6513794 0.6574419 0.9581042 -1.2177687  2.5388174   ## RW/conj
## z[3] 0.6473852 0.6468320 0.9584760 -1.2323195  2.5294749   ## slice
## z[3] 0.6138076 0.6095436 0.9405947 -1.2191588  2.4925398   ## HMCtransform








is.na.vec <- nimble:::is.na.vec
is.nan.vec <- nimble:::is.nan.vec
isValid <- nimble:::isValid

conf$printMonitors()
conf$printSamplers()

Rmcmc$samplerFunctions$contentsList[[1]]$transformInfo

debug(Rmcmc$samplerFunctions$contentsList[[1]]$run)
debug(Rmcmc$samplerFunctions$contentsList[[1]]$initializeEpsilon)
debug(buildtree)
debug(leapfrog)
debug(gradient)
set.seed(0)
samples <- runMCMC(Rmcmc, 100)    ## XXXXXXXXXXXXXXXXXXXX



##conf <- configureMCMC(Rmodel)
if(FALSE) {
    conf$addSampler('mu', 'slice')
    conf$addSampler('tau', 'slice')
    conf$addSampler('p', 'slice')
    conf$addSampler('z[1]', 'slice')
    conf$addSampler('z[2]', 'slice')
    conf$addSampler('z[3]', 'slice')
    conf$addSampler(c('mu','tau','p','z'), 'HMCtransform')
}

conf <- configureMCMC(Rmodel, nodes = NULL)

conf$printSamplers()

##set.seed(0)
##is.na.vec <- nimble:::is.na.vec
##is.nan.vec <- nimble:::is.nan.vec
##samples <- runMCMC(Rmcmc, 10)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)
samplesSummary(samples[10001:100000,])
samples <- runMCMC(Cmcmc, 25000)        ## HMC
samplesSummary(samples[5001:25000,])    ## HMC








## looking at numerical divergence between
## different lp adjustment addition schemes in the HMC sampler
{
    library(nimble)
    nimbleOptions(experimentalEnableDerivs = TRUE)
    code <- nimbleCode({
        mu ~ dnorm(0, sd = 10)
        tau ~ dgamma(0.001, 0.001)
        p ~ dbeta(2, 5)
        mu4 <- mu * p
        y1 ~ dnorm(mu, tau)
        y2 ~ dnorm(mu, tau)
        y3 ~ dnorm(mu, tau)
        y4 ~ dnorm(mu4, tau)
    })
    constants <- list()
    data <- list(y1 = 10, y2 = 9, y3 = 9, y4 = 7)
    inits <- list(tau = 1, mu = 0, p = 0.5)
    samplesList <- list()
    for(i in 1:2) {
        Rmodel <- nimbleModel(code, constants, data, inits)
        Rmodel$calculate()
        conf <- configureMCMC(Rmodel, nodes = NULL)
        if(i == 1) conf$addSampler(c('mu','tau','p'), 'HMCexp3')
        if(i == 2) conf$addSampler(c('mu','tau','p'), 'HMCtransform')
        conf$printSamplers()
        Rmcmc <- buildMCMC(conf)
        Cmodel <- compileNimble(Rmodel)
        Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)
        set.seed(0)
        samplesList[[i]] <- runMCMC(Cmcmc, 100000)
    }
}    


samplesSummary(samplesList[[1]])
samplesSummary(samplesList[[2]])

iter <- 7
param <- 'p'
samplesList[[1]][iter,param] == samplesList[[2]][iter,param]

iter <- 990:1000
for(param in c('mu', 'tau', 'p')) {
    print(all(samplesList[[1]][iter,param] == samplesList[[2]][iter,param]))
    print(cbind(samplesList[[1]][iter,param], samplesList[[2]][iter,param]))
}



## understanding nimDeriv() and what it's doing

library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)
code <- nimbleCode({
    mu ~ dnorm(0, 0.0001)
    sigma ~ dunif(0, 1000)
    y ~ dnorm(mu, sd = sigma)
    x[1:N] ~ dmnorm(mean[1:N], cov = C[1:N,1:N])
    ##z[1:N,1:N] ~ dwish(R = C[1:N,1:N], df = df)   ## Wishart not working on AD branches
})
N <- 3
constants <- list(N = N, mean = 1:N, C = diag(N), df = 2*N)
data <- list()
inits <- list(mu = 0, sigma = 1, y = 3, x = rep(0,N), z=diag(N))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)


nfDef <- nimbleFunction(
    setup = function(model) {
        muNode <- 'mu'
        sigmaNode <- 'sigma'
        muSigma <- c()
        yNode <- 'y'
        muSigmaY <- c(muNode, sigmaNode, yNode)
        wrt <- c('mu', 'sigma', 'y', 'x')
        calcNode <- c('y', 'x')
    },
    run = function() {
        deriv <- nimDerivs(model$calculate(calcNode), order = 1, wrt = wrt)
        ##val <- deriv$value
        ##print(val)
        grad <- deriv$jacobian   ## jacobian is a vector of first derivatives,
        print(grad)              ## using the wrt nodes, which also can include y
    }
)

Rnf <- nfDef(Rmodel)


compiledList <- compileNimble(list(model=Rmodel, nf=Rnf), showCompilerOutput = TRUE)
Cmodel <- compiledList$model; Cnf <- compiledList$nf

Rnf$run()
Cnf$run()

Cmodel$y <- 4;
Cmodel$sigma <- 1
Cmodel$sigma <- 2
Cmodel$sigma <- .1

Cmodel$mu <- 0; Cnf$run()
Cmodel$mu <- 1; Cnf$run()
Cmodel$mu <- 2; Cnf$run()
Cmodel$mu <- 3; Cnf$run()
Cmodel$mu <- 4; Cnf$run()
Cmodel$mu <- 5; Cnf$run()
Cmodel$mu <- 6; Cnf$run()
Cmodel$mu <- 7; Cnf$run()


mu <- -1
y <- 3.1
sigma <- 2.5
Cmodel$mu <- mu
Cmodel$y <- y
Cmodel$sigma <- sigma
(y-mu)^2 / sigma^3 - 1/sigma
Cnf$run()


## new trying 2nd order derivatives with nimDeriv

library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)
code <- nimbleCode({
    mu ~ dnorm(0, 0.0001)
    sigma ~ dunif(0, 1000)
    y ~ dnorm(mu, sd = sigma)
})
constants <- list()
data <- list()
inits <- list(mu = 0, sigma = 1, y = 3)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
nfDef <- nimbleFunction(
    setup = function(model) {
        muNode <- 'mu'
        sigmaNode <- 'sigma'
        muSigma <- c(muNode, sigmaNode)
        yNode <- 'y'
        muSigmaY <- c(muNode, sigmaNode, yNode)
    },
    run = function() {
        deriv <- nimDerivs(model$calculate(yNode), order = 2, wrt = muSigmaY)
        ##val <- deriv$value
        ##print(val)
        grad <- deriv$jacobian   ## jacobian is a vector of first derivatives,
        print(grad)              ## using the wrt nodes, which also can include y
        hess <- deriv$hessian
        x <- hess[,,1]   ## hessian is a 3D array, at least in this case
        print(x)         ## the hess[,,1] is a 3x3 array of the second partials
    }
)
Rnf <- nfDef(Rmodel)
compiledList <- compileNimble(list(model=Rmodel, nf=Rnf), showCompilerOutput = TRUE)
Cmodel <- compiledList$model; Cnf <- compiledList$nf

y <- 4; Cmodel$y <- y
sigma <- 1;  Cmodel$sigma <- sigma 
sigma <- 2;  Cmodel$sigma <- sigma 
sigma <- .1; Cmodel$sigma <- sigma 

mu <- 0; Cmodel$mu <- mu; Cnf$run(); print(1/sigma^2 - 3/sigma^4*(y-mu)^2) 
mu <- 1; Cmodel$mu <- mu; Cnf$run(); print(1/sigma^2 - 3/sigma^4*(y-mu)^2) 
mu <- 2; Cmodel$mu <- mu; Cnf$run(); print(1/sigma^2 - 3/sigma^4*(y-mu)^2) 
mu <- 3; Cmodel$mu <- mu; Cnf$run(); print(1/sigma^2 - 3/sigma^4*(y-mu)^2) 
mu <- 4; Cmodel$mu <- mu; Cnf$run(); print(1/sigma^2 - 3/sigma^4*(y-mu)^2) 
mu <- 5; Cmodel$mu <- mu; Cnf$run(); print(1/sigma^2 - 3/sigma^4*(y-mu)^2) 
mu <- 6; Cmodel$mu <- mu; Cnf$run(); print(1/sigma^2 - 3/sigma^4*(y-mu)^2) 
mu <- 7; Cmodel$mu <- mu; Cnf$run(); print(1/sigma^2 - 3/sigma^4*(y-mu)^2) 


mu <- -1
y <- 3.2
sigma <- 2.9
Cmodel$mu <- mu
Cmodel$y <- y
Cmodel$sigma <- sigma
(y-mu)^2 / sigma^3 - 1/sigma
Cnf$run()



## compiling a dwish wishart node, when experimentalEnableDerivs = TRUE ??
## gives an error?

library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)

code <- nimbleCode({
    X[1:N,1:N] ~ dwish(R = C[1:N,1:N], df = df)
})
N <- 3
constants <- list(N = N, C = diag(N), df = 2*N)
data <- list()
inits <- list(X = diag(N))
Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()

Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)



## trying out the 'rats' model with HMC sampler

library(nimble)
library(coda)
nimbleOptions(experimentalEnableDerivs = TRUE)
modelInfo <- readBUGSmodel('rats', dir = getBUGSexampleDir('rats'), returnComponents = TRUE)
code <- modelInfo$code
constants <- modelInfo$data[c('x', 'N', 'T')]
data <- modelInfo$data['Y']
inits <- modelInfo$inits
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

set.seed(0)
time <- system.time(samples <- runMCMC(Cmcmc, 50000))[3]
samplesSummary(samples[10001:50000,])
##                Mean    Median  St.Dev  95%CI_low  95%CI_upp
## alpha.c   242.47194 242.47158 2.73198 237.052296 247.838545   ## conjugate
## beta.c      6.18481   6.18521 0.10738   5.971983   6.396892   ## conjugate
## tau.alpha   0.00495   0.00483 0.00135   0.002688   0.007934   ## conjugate
## tau.beta    4.10793   3.85167 1.52061   1.922180   7.760095   ## conjugate
## tau.c       0.02748   0.02728 0.00413   0.019950   0.036098   ## conjugate


eff <- apply(samples[10001:50000,], 2, effectiveSize) / (time*4/5)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
params <- Rmodel$getNodeNames(topOnly = TRUE, stochOnly = TRUE)
allStochNodes <- Rmodel$getNodeNames(stochOnly = TRUE, includeData = FALSE)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$addMonitors(allStochNodes)
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)
set.seed(0)
samples <- runMCMC(Cmcmc, 50000)

means <- apply(samples[10001:50000,], 2, mean)
meansSorted <- means[allStochNodes]
orderedNodeNames <- c(params, 'alpha', 'beta')

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
values(Rmodel, orderedNodeNames) <- meansSorted
Rmodel$calculate()
conf <- configureMCMC(Rmodel)
conf$removeSamplers(params)
conf$addSampler(params, 'HMCexp')    ## XXXXXXXXXXXXXXX
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)
set.seed(0)
time <- system.time(samples <- runMCMC(Cmcmc, 10000))[3]   ## XXXXX

valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[61]], 'epsilon')

samplesSummary(samples[1001:10000,])
##               Mean    Median  St.Dev  95%CI_low  95%CI_upp
## alpha.c   242.6290 242.56569 2.85729 237.035696 247.742833   ## HMC
## beta.c      6.1884   6.18589 0.11082   5.971183   6.419550   ## HMC
## tau.alpha   0.0049   0.00485 0.00132   0.002645   0.007856   ## HMC
## tau.beta    4.3298   3.85421 2.19385   1.896545  10.326020   ## HMC
## tau.c       0.0274   0.02717 0.00409   0.019956   0.035797   ## HMC


effHMC <- apply(samples[1001:10000,], 2, effectiveSize) / (time*9/10)

effHMC

effHMC / eff




## testing HMC sampler on election88 example ??

library(nimble)
library(testthat)
nimbleOptions(experimentalEnableDerivs = TRUE)
##
## OLD: setwd('~/Downloads/election88_clean')
setwd('~/github/public/HMCcomparisons/election88_clean')
load(file.path('data_setup_from_stan_github', 'data_setup_from_stan_github.RData'))
election.inits <- list(b.0=rnorm(1), b.female=rnorm(1), b.black=rnorm(1),
                       b.female.black=rnorm(1),
                       b.age=rnorm(n.age), b.edu=rnorm(n.edu),
                       b.age.edu=array(rnorm(n.age*n.edu),
                           c(n.age,n.edu)), b.state=rnorm(n.state), b.v.prev=rnorm(1), 
                       b.region=rnorm(n.region), sigma.age=runif(1),
                       sigma.edu=runif(1), sigma.age.edu=runif(1),
                       sigma.state=runif(1), sigma.region=runif(1))
election88_BUGS <- readBUGSmodel("17.4_Bugs_codes.bug", dir = 'models', returnComponents = TRUE)
BUGSdataList.1 <- dataList.1
names(BUGSdataList.1) <- tolower( gsub("_",".",names(BUGSdataList.1)))
election88_BUGS$data <- BUGSdataList.1
election88_BUGS$inits <- election.inits
code <- election88_BUGS$code
yInd <- which(names(election88_BUGS$data) == 'y')
constants <- election88_BUGS$data[-yInd]
data <- election88_BUGS$data[yInd]
inits <- election88_BUGS$inits

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

## for RW sampling:
conf <- configureMCMC(Rmodel)
conf$printSamplers()

## for HMC sampling:
params <- Rmodel$getNodeNames(topOnly = TRUE, stochOnly = TRUE)
params
conf <- configureMCMC(Rmodel)
conf$removeSamplers(params)
conf$addSampler(params, 'HMC')
conf$printSamplers()

Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)#, showCompilerOutput = TRUE)

## for RW sampling:
set.seed(0)
time <- system.time(samples <- runMCMC(Cmcmc, 10000))[3]
time   ## 22 seconds
apply(samples, 2, mean)
##           b.0        b.black       b.female b.female.black       b.v.prev 
##   -2.01094388    -1.68358119    -0.08451803    -0.18613569     4.40117854 
##     sigma.age  sigma.age.edu      sigma.edu   sigma.region    sigma.state 
##    0.20280733     0.13805601     0.41859886     0.62938776     0.22989215 
library(coda)
eff <- apply(samples, 2, effectiveSize) / time
eff
##           b.0        b.black       b.female b.female.black       b.v.prev 
##     0.2560023     21.4737754     20.6357122     20.4321349      0.3126023 
##     sigma.age  sigma.age.edu      sigma.edu   sigma.region    sigma.state 
##     6.1055591      1.7805222      2.4554082     10.2584719      4.1224752 

## for HMC sampling:
set.seed(0)
timeHMC <- system.time(samplesHMC <- runMCMC(Cmcmc, 2000))[3]
timeHMC    ## 234 seconds
apply(samplesHMC, 2, mean)
##           b.0        b.black       b.female b.female.black       b.v.prev 
##    0.53133224    -1.72864436    -0.09042266    -0.17668885     0.27606201 
##     sigma.age  sigma.age.edu      sigma.edu   sigma.region    sigma.state 
##    0.26993093     0.21950758     0.50187485     0.69488580     0.29405414 

library(coda)
effHMC <- apply(samplesHMC, 2, effectiveSize) / timeHMC
effHMC
as.numeric(effHMC / eff)

chainsPlot(list(RW = samples, HMC = samplesHMC), nrows = 1)








## testing and debugging HMC sampler
## on branch hmcAD

library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)

code <- nimbleCode({
    a[1] ~ dnorm(0, 1)
    a[2] ~ dnorm(a[1]+1, 1)
    a[3] ~ dnorm(a[2]+1, 1)
    d ~ dnorm(a[3], sd=2)
})
constants <- list()
data <- list(d = 5)
inits <- list(a = rep(0, 3))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('a', 'HMC')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
set.seed(0)
samples <- runMCMC(Cmcmc, 10000)
apply(samples, 2, mean)
##      a[1]      a[2]      a[3] 
## 0.4503489 1.8820432 3.3086721 
apply(samples, 2, sd)
##      a[1]      a[2]      a[3] 
## 0.9148666 1.2039168 1.2923344 

## conjugate MCMC
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc
set.seed(0)
conjSamples <- runMCMC(Cmcmc, 100000)
apply(conjSamples, 2, mean)
##        a         b         c 
##0.4245868 1.8505314 3.2806820 
apply(conjSamples, 2, sd)
##        a         b         c 
##0.9244013 1.1914049 1.3050024 



debug(initializeEpsilon)
debug(leapfrog)
debug(gradient)
debug(buildtree)

undebug(initializeEpsilon)
undebug(leapfrog)
undebug(gradient)
undebug(buildtree)


## testing sampler_RW_wishart against conjugate wishart updates

library(nimble)

set.seed(0)
trueCor <- matrix(c(1, .3, .7, .3, 1, -0.2, .7, -0.2, 1), 3)
covs <- c(3, 2, .5)
trueCov = diag(sqrt(covs)) %*% trueCor %*% diag(sqrt(covs))
Omega = solve(trueCov)
n = 20
R = diag(rep(1,3))
mu = 1:3
Y = mu + t(chol(trueCov)) %*% matrix(rnorm(3*n), ncol = n)
M = 3
data <- list(Y = t(Y), n = n, M = M, mu = mu, R = R)
code <- nimbleCode( {
    for(i in 1:n) {
        Y[i, 1:M] ~ dmnorm(mu[1:M], Omega[1:M,1:M])
    }
    Omega[1:M,1:M] ~ dwish(R[1:M,1:M], 4)
})
newDf = 4 + n
newR = R + tcrossprod(Y- mu)
OmegaTrueMean = newDf * solve(newR)
wishRV <- array(0, c(M, M, 10000))
for(i in 1:10000) {
    z <- t(chol(solve(newR))) %*% matrix(rnorm(3*newDf), ncol = newDf)
    wishRV[ , , i] <- tcrossprod(z)
}
OmegaSimTrueSDs = apply(wishRV, c(1,2), sd)
allData <- data
constants <- list(n = allData$n, M = allData$M, mu = allData$mu, R = allData$R)
data <- list(Y = allData$Y)
inits <- list(Omega = OmegaTrueMean)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
samplesConj <- runMCMC(Cmcmc, 50000)

all(abs(as.numeric(apply(samples, 2, mean)) - as.numeric(OmegaTrueMean)) < 0.05)
all(abs(as.numeric(apply(samples, 2, sd)) - as.numeric(OmegaSimTrueSDs)) < 0.06)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('Omega', 'RW_wishart')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
##debug(Rmcmc$samplerFunctions[[1]]$run)
##set.seed(0)
##samples <- runMCMC(Rmcmc, 3)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
samples <- runMCMC(Cmcmc, 100000)

all(abs(as.numeric(apply(samples, 2, mean)) - as.numeric(OmegaTrueMean)) < 0.05)
all(abs(as.numeric(apply(samples, 2, sd)) - as.numeric(OmegaSimTrueSDs)) < 0.06)

as.numeric(apply(samples, 2, mean)) - as.numeric(OmegaTrueMean)
as.numeric(apply(samples, 2, sd)) - as.numeric(OmegaSimTrueSDs)

apply(samples, 2, mean)
OmegaTrueMean



## testing new ... args to configureMCMC() and conf$addSampler()
library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a, 1)
    c ~ dexp(a^2 + 1)
    d ~ dexp(c)
})
constants <- list()
data <- list(b = 2, d = 1)
inits <- list(a = 1, c = 2)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel, control = list(adaptive = FALSE))
conf <- configureMCMC(Rmodel, control = list(adaptive = FALSE, xyz = 10))
conf <- configureMCMC(Rmodel, control = list(xyz = 10), adaptive = FALSE)
conf <- configureMCMC(Rmodel, adaptive = FALSE, control = list(xyz = 10))
conf <- configureMCMC(Rmodel, adaptive = FALSE, xyz = 10, control = list(abc = 3))

conf$printSamplers()

conf$addSampler('a', 'RW', anotherArg = 'hello')
conf$addSampler('a', 'RW', anotherArg = 'hello', control = list(controlArg = TRUE))
conf$printSamplers()

Rmcmc <- buildMCMC(conf)



## testing use of logdet() in nimbleFunctionsa

library(nimble)
L <- array(c(1,0,0,2,1,0,3,2,4), c(3,3))
A <- t(L) %*% L
A
det(A)

nfDef <- nimbleFunction(
    setup = function() {},
    run = function(A = double(2)) {
        x <- logdet(A)
        print(x)
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run(A)
Cnf$run(A)
det(A)
log(det(A))

Rnf$run(L)
Cnf$run(L)
det(L)
log(det(L))


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()

## testing of chol() for use in RW_wishart sampler

library(nimble)

nfDef <- nimbleFunction(
    setup = function(A) {},
    run = function() {
        L <- chol(A)
        print(L)
    }
)

L <- array(c(1,2,3,0,1,2,0,0,1), c(3,3))
L
A <- L %*% t(L)
A
chol(A)

Rnf <- nfDef(A)
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()




## more testing of initializeModel, using scr voles model

library(nimble)
getwd()

reduced <- FALSE
model <- 'SCR6'
makeDatasetLarger <- FALSE
filename <- paste0('data/modelInfo_',
                   modelToLoad,
                   if(makeDatasetLarger) paste0('_i',timesMoreIndividuals,'_t',timesMoreTraps^2) else "",
                   ifelse(reduced, '_reduced', ''),
                   '.RData')
if(!filename %in% list.files(recursive = TRUE)) stop(paste0('could not find file: ', filename))
load(filename)



RHSonlyNodes <- rhs

RHSonlyVarNames <- nimble:::removeIndexing(RHSonlyNodes)
RHSonlyVarNamesUnique <- unique(RHSonlyVarNames)
RHSonlyNodesListedByVariable <- lapply(RHSonlyVarNamesUnique, function(var) RHSonlyNodes[RHSonlyVarNames==var])
if(length(RHSonlyNodesListedByVariable) > 0) {
    lengths <- sapply(RHSonlyNodesListedByVariable, length)
    if(any(lengths == 0)) stop('something went wrong in RHS node model initialization')
    if(sum(lengths) != length(RHSonlyNodes)) stop('something went wrong in RHS node model initialization')
}





Rmodel <- nimbleModel(modelInfo$code, modelInfo$constants, modelInfo$data, modelInfo$inits)

Rinit <- initializeModel(Rmodel)

Cmodel <- compileNimble(Rmodel)
Cinit <- compileNimble(Rinit, project = Rmodel)

length(Rinit$initFunctionList$contentsList)
Rinit$initFunctionList$contentsList[1:10]
Rinit$initFunctionList$contentsList[[1]]$nodes
Rinit$initFunctionList$contentsList[[2]]$nodes
Rinit$initFunctionList$contentsList[[3]]$nodes
Rinit$initFunctionList$contentsList[[4]]$nodes
Rinit$initFunctionList$contentsList[[5]]$nodes
Rinit$initFunctionList$contentsList[[6]]$nodes


model <- Rmodel

rhs <- Rinit$initFunctionList$contentsList[[1]]$nodes

for(node in rhs) {
    ##print(node)
    val <- values(Rmodel, node)
    if(any(is.na(val))) print(node)
}

Rmodel$g[48,1,]
Rmodel$z[67,]

Rmodel$getDependencies('z[67,1]')
Rmodel$getDependencies('H[67, 1:5, 1:4]')


modelInfo$constants

set.seed(0)
Rinit$run()

set.seed(0)
Cinit$run()


## testing new initialize model

library(nimble)

code <- nimbleCode({
    x[1] <- 0.5
    x[2] <- x[1] + 1
    x[3] <- x[2]^2
    x[4] ~ dnorm(x[3], 1)
    x[5] <- x[4] + 1
    y ~ dexp(x[5])
    x[7] <- y + 1
    x[8] <- x[7] + 10
    x[9] ~ dexp(x[8])
    x[10] ~ dnorm(x[9], 1)
    d[1] <- Ra
    d[2] <- Rb[1]
    d[3] <- Rb[2]
})

Rmodel <- nimbleModel(code, data=list(y = 5, Ra=1, Rb=c(2,NA)), calculate = FALSE)

Rinit <- initializeModel(Rmodel)

##Rmodel$calculate()
##Rmodel$getNodeNames()
##Rmodel$getNodeNames(topOnly = TRUE)

length(Rmodel$x)
Rmodel$x
Rmodel$y

Cmodel <- compileNimble(Rmodel)
Cinit <- compileNimble(Rinit, project = Rmodel)

set.seed(0)
Rmodel$d
Rinit$run()
Rmodel$d
Rmodel$x

set.seed(0)
Cmodel$d
Cinit$run()
Cmodel$d
Cmodel$x

##debug(initializeModel)

Rinit <- initializeModel(Rmodel)
Rinit$initFunctionList$contentsList
Rinit$initFunctionList$contentsList[[1]]$node
Rinit$initFunctionList$contentsList[[2]]$node
Rinit$initFunctionList$contentsList[[3]]$node
Rinit$initFunctionList$contentsList[[4]]$node
Rinit$initFunctionList$contentsList[[4]]$depDetermNodes
Rinit$initFunctionList$contentsList[[5]]$node
Rinit$initFunctionList$contentsList[[5]]$depDetermNodes
Rinit$initFunctionList$contentsList[[6]]$node
Rinit$initFunctionList$contentsList[[6]]$depDetermNodes
Rinit$initFunctionList$contentsList[[7]]$node
Rinit$initFunctionList$contentsList[[7]]$depDetermNodes

Cmodel$a
Cmodel$b
Cmodel$c

set.seed(0)
samples <- runMCMC(Cmcmc, 10)


Rinit <- initializeModel(Rmodel)
Cmodel <- compileNimble(Rmodel); Cinit <- compileNimble(Rinit, project = Rmodel)

set.seed(0)
##debug(Rinit$run)
Rinit$run()

Rmodel$a
Rmodel$b
Rmodel$c

set.seed(0)
Cinit$run()

Cmodel$a
Cmodel$b
Cmodel$c


Rmodel$calculate()
Cmodel$calculate()



## issues with initializeModel() model initialization,
## when there are deterministic dependents between latent states

library(nimble)


set.seed(0)
nCues=2
nTrials=44*nCues
validity=0.75
t=1
changeT<-round(runif(nCues-1,40,48))
changeT<-cumsum(changeT)
genTarget=sign(runif(nTrials,-10000,10000))
genTarget[genTarget==0]<-1
relCue=rep(NA,nTrials)
opt=c(1,2,3)
relCue[1]=sample(opt,size = 1)
genCues<-matrix(rep(NA,nTrials*3),nrow=nTrials)
genCues[1,relCue[1]]<-genTarget[1]
other=c(1,2,3)
other<-other[other!=relCue[1]]
genCues[1,other]<-sample(c(-1,1),1)
genCues[1,genCues[1,]==0]<-1
##
for (t in 2:nTrials){
    relCue[t]=relCue[t-1]
    if (max(t==changeT)==1) {
        if (length(opt)>1) {relCue[t]=sample(opt,size = 1)} else {relCue[t]=opt}
    }
    valid=runif(1,0,1)
    if (valid<=validity){genCues[t,relCue[t]]<-genTarget[t]} else {genCues[t,relCue[t]]<--1*genTarget[t]}
    genCues[t,-relCue[t]]<-sign(runif(2,-10000,10000))
}
##
test=data.frame(genCues,genTarget,relCue)
##
stateSpaceCode<-nimbleCode({
    gamma<-0.75
    tau<-0.9
    x[1]~dcat(lambda[1,1:3])
    lambda[1,1]<-1/3
    lambda[1,2]<-1/3
    lambda[1,3]<-1/3
    prob[1]<-equals(cues[1,x[1]],1)*gamma+equals(cues[1,x[1]],-1)*(1-gamma)
    target[1]~dbern(prob[1])
    for (t in 2:nTrials){
        x[t]~dcat(lambda[t,1:3])
        lambda[t,1]<-equals(x[t-1],1)*tau+(1-equals(x[t-1],1))*(1-tau)/2
        lambda[t,2]<-equals(x[t-1],2)*tau+(1-equals(x[t-1],2))*(1-tau)/2
        lambda[t,3]<-equals(x[t-1],3)*tau+(1-equals(x[t-1],3))*(1-tau)/2
        prob[t]<-equals(cues[t,x[t]],1)*gamma+equals(cues[t,x[t]],-1)*(1-gamma)
        target[t]~dbern(prob[t])
    }
})
##
datalist<-list(cues=as.matrix(test[,1:3]),target=as.numeric((test[,4]==1)))
inits<-list(gamma=0.75, tau=0.9)
constants<-list(nTrials=length(datalist$target))

set.seed(0)
stateSpaceModel<-nimbleModel(code = stateSpaceCode,data = datalist,constants=constants,inits = inits, calculate = FALSE)

bootstrapFilter<-buildBootstrapFilter(stateSpaceModel,nodes='prob',control=list(saveAll=T))

compiledList<-compileNimble(stateSpaceModel,bootstrapFilter)

set.seed(0)
bootstrapFilter$run(10)

set.seed(0)
compiledList$bootstrapFilter$run(10)


## model$getNodeNames() arguments:
##determOnly = FALSE, stochOnly = FALSE, includeData = TRUE, dataOnly = FALSE, includeRHSonly = FALSE,
##topOnly = FALSE, latentOnly = FALSE, endOnly = FALSE, returnType = 'names', returnScalarComponents = FALSE

stateSpaceModel$getNodeNames()
stateSpaceModel$getNodeNames(includeRHSonly = TRUE)

stateSpaceModel$getNodeNames(topOnly = TRUE)

setdiff(stateSpaceModel$getNodeNames(includeRHSonly = TRUE), stateSpaceModel$getNodeNames())
## cues is the only RHS only

model <- stateSpaceModel

RHSonlyNodes <- model$getMaps('nodeNamesRHSonly')
RHSonlyNodes

model$cues

stochNonDataNodes <- model$getNodeNames(stochOnly = TRUE, includeData = FALSE)
stochNonDataNodes

for(i in seq_along(stochNonDataNodes))
    initFunctionList[[iter + i - 1]] <- stochNodeInit(model, stochNonDataNodes[i], silent)

determDepsOfRHSonly <- model$getDependencies(model$getMaps('nodeNamesRHSonly'), determOnly = TRUE)
determDepsOfRHSonly

setdiff(
model$getDependencies(model$getMaps('nodeNamesRHSonly'), determOnly = TRUE),
model$getDependencies(stochNonDataNodes, determOnly = TRUE))

model$prob

Rmodel <- stateSpaceModel
Rinit <- initializeModel(Rmodel)
Cmodel <- compileNimble(Rmodel)
Cinit <- compileNimble(Rinit, project = Rmodel)

set.seed(0)
##debug(Rinit$run)
Rinit$run()

set.seed(0)
Cinit$run()

Rmodel$calculate()
Cmodel$calculate()



## looking at sampler assignments to non-conjugate Wishart
## and inverse-Wishart nodes

library(nimble)

N <- 12
code <- nimbleCode({
    for(i in 1:N) {
        a[i] ~ dnorm(0, 1)
    }
    ##b[1:N,1:N] ~ dwish(C[1:N,1:N], df = 20)
    b[1:N,1:N] ~ dinvwish(C[1:N,1:N], df = 20)
    d ~ dnorm(b[1,1], sd = 1)
})
constants <- list(N = N, C = diag(N))
data <- list(d = 0)
inits <- list(a = rep(0, N), b = diag(N))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

##Rmodel$expandNodeNames('a')

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$addSampler('a', 'RW_block')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

conf$getSamplerDefinition(1)



## trying to track down ISEC attendance for Len Thomas
## (at ISEC 2018, St Andrews)

isec <- data.frame(date = seq(2008, 2018, by=2),
                   location = c('St. Andrews', 'Canterbury', 'Norway', 'Montpellier', 'Seattle', 'St. Andrews'),
                   number = as.numeric(NA))

## 2012
## source: https://www.mn.uio.no/cees/english/research/news/events/isec2012/announcements/countries.html
isec[isec$date==2012,]$number <- 227

## 2014
## source: https://isec2014.sciencesconf.org/
isec[isec$date==2014,]$number <- 350

## 2016
## source: http://depts.washington.edu/uwconf/wordpress/isec2016/
isec[isec$date==2016,]$number <- 300


isec


install.packages("pdftools")
library(pdftools)
## 10MB PDF file, download takes a minute:
filename <- paste(tempdir(), 'isec2016.pdf', sep = .Platform$file.sep)
download.file('http://depts.washington.edu/uwconf/isec/ISEC2016_ABSRACT_BOOKLET.pdf',
              filename)
pdftext <- pdftools::pdf_text(filename)
lines <- unlist(strsplit(pdftext, '\n'))
lines <- as.

grep('[[:upper:]]\\.? ?[[:digit:]]{1,2} . ', lines, value = TRUE)

idlines <- grep('\\(ID [[:digit:]]+\\)', lines, value = TRUE)
ids <- as.numeric(gsub('.*\\(ID ([[:digit:]]+)\\).*', '\\1', idlines))
max(ids)


as.character(substr(grep('C\\.[[:digit:]]{1,2}', lines, value = TRUE)[15], 9,9))

isec


page <- readLines('https://isec.conference-services.net/programme.asp?conferenceID=2792&action=prog_presenters')
authors <- grep('<strong>.*</strong>', page, value = TRUE)
authorNames <- gsub('.*<strong>(.*)</strong>.*', '\\1', authors)
authorNames


isec




## playing with the nimble-demos submission
## regarding Bayesian pooling of financial defaults data

set.seed(10)

# simulate data for industries A, B, C, ... , E
d_A<-rbinom(n = 10,  size = 1, prob =  .2)
d_B<-rbinom(n = 100, size = 1, prob = .3)
d_C<-rbinom(n = 10, size = 1, prob = .45)
d_D<-rbinom(n = 100, size = 1, prob = .7)
d_E<-rbinom(n = 10, size = 1, prob = .8)

# stack, combine data and industry indicators
complete <- c(d_A, d_B, d_C, d_D, d_E)
industry <- as.factor(c(rep('A', 10),  rep('B', 100), rep('C', 10),
              rep('D', 100), rep('E', 10) ))

# obtain the model matrix consisting of industry indicators and a constant.
# industry A is the reference for the other indicators.
mod_mat <- model.matrix(lm(complete ~ industry))

# data to feed into Nimble
d_list <- list(X = mod_mat, 
               complete = complete)

# constants to feed into Nimble
p <- ncol(mod_mat)
n <- nrow(mod_mat)

# glimpse of the data
data.frame(complete, industry)[105:120,]




## testing new method: conf$printSamplers(type = ...)

library(nimble)

code <- nimbleCode({
    for(i in 1:3) {
        a[i] ~ dnorm(0, 1)
        b[i] ~ dnorm(0, 1)
    }
})
constants <- list()
data <- list()
inits <- list(a = rep(0,5), b = rep(0, 5))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Rmodel$getVarNames


conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$removeSamplers(c('a[2]', 'a[3]', 'b[2]', 'b[3]'))
conf$printSamplers()
conf$addSampler('a[2]', 'RW')
conf$addSampler('b[2]', 'RW')
conf$addSampler('a[3]', 'slice')
conf$addSampler('b[3]', 'slice')
conf$printSamplers()

conf$printSamplers('a')
conf$printSamplers('b')
conf$printSamplers(c('a', 'b'))

conf$printSamplers(type = 'slice')
conf$printSamplers(type = 'R')
conf$printSamplers(type = c('post', 'W'))
conf$printSamplers(type = c('post', 'W', 'slice'))

conf$printSamplers('a', type = c('post', 'W', 'slice'))
conf$printSamplers('b', type = c('post', 'W', 'slice'))

conf$printSamplers('a[1]', type = c('post', 'W', 'slice'))
conf$printSamplers('a[2]', type = c('post', 'W', 'slice'))

conf$printSamplers(c('a', 'b[3]'), type = c('post', 'W', 'slice'))
conf$printSamplers(c('a', 'b[3]'), type = c('post', 'slice'))
conf$printSamplers(c('a', 'b[3]'), type = c('slice'))

conf$printSamplers(c('a', 'b[3]'), type = c('zzz'))


Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)
##Cmcmc$run(10000)
##samples <- as.matrix(Cmcmc$mvSamples)

colnames(samples)
apply(samples, 2, mean)

samplesSummary(samples)
samplesPlot(samples)

library(coda)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        returnType()
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


Rnf <- nimbleFunction(
    run = function() {
        returnType()
    }
)

Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf()
Cnf()



## testing whether removing posterior_predictive samplers
## changes the posterior..... (for work w/ Ben Letcher)

library(nimble)

code <- nimbleCode({
    a ~ dexp(2)
    b ~ dexp(3)
    x ~ dnorm(a, sd = b)
    y ~ dnorm(a+4, sd = b)
    z ~ dnorm(a, sd = b+1)
})

code <- nimbleCode({
    a ~ dexp(2)
    b ~ dexp(3)
    x ~ dnorm(a, sd = b)
    y ~ dnorm(a+4, sd = b)
})

constants <- list()
data <- list(x = 4, y = 7)
inits <- list(a = 1, b = 1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
Rmodel$calculate('z')

conf <- configureMCMC(Rmodel)

conf$printSamplers()
conf$removeSamplers('z')
conf$printSamplers()

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Cmodel$z
Cmodel$getNodeNames(dataOnly = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 1000000, nburnin = 50000)

Cmodel$z
Cmodel$getNodeNames(dataOnly = TRUE)

samplesSummary(samples)

## with posterior_predictive sampler on 'z'
Mean    Median   St.Dev. 95%CI_low 95%CI_upp
a 2.5232518 2.8237549 1.0181654 0.2266626  3.878514
b 0.9605417 0.8150415 0.5391889 0.3079454  2.270492

## not including 'z' in the model at all
       Mean    Median   St.Dev. 95%CI_low 95%CI_upp
a 2.5601519 2.8582259 1.0005812 0.2350451  3.884631
b 0.9405567 0.7927426 0.5319256 0.3047788  2.255075


## removing the posterior_predictive sampler on 'z'
       Mean    Median   St.Dev. 95%CI_low 95%CI_upp
a 2.8759682 3.0911494 0.7992810 0.6274089  3.906653
b 0.7742836 0.6484452 0.4378095 0.2843118  1.944413



## drafting a conjugate invgamma - dmnorm conjugate sampler for Zach Horton

library(nimble)

conjugate_invgamma_dmnorm <- nimbleFunction(
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        calcNodes <- model$getDependencies(target)
        depNode <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
        n <- length(model$expandNodeNames(depNode, returnScalarComponents = TRUE))
        if(model$getDistribution(target) != 'dinvgamma')   stop('this sampler only for dinvgamma nodes')
        if(length(depNode) > 1) stop('currently only written for one dependent node')
        if(model$getDistribution(depNode) != 'dmnorm')   stop('dependent node must be dmnorm')
        paramExpr <- model$getParamExpr(depNode, 'cov')
        paramExprExpanded <- nimble:::cc_expandDetermNodesInExpr(model, paramExpr)
        linearityCheck <- nimble:::cc_checkLinearity(paramExprExpanded, target)
        if(linearityCheck$offset != 0) stop('not multiplicative')
        coeffExpr <- linearityCheck$scale
        coeffNodeName <- deparse(coeffExpr)
    },
    run = function() {
        priorShape <- model$getParam(target, 'shape')
        posteriorShape <- priorShape + n/2
        priorRate <- model$getParam(target, 'rate')
        coeffMatrixValue <- model[[coeffNodeName]]
        depNodeValue <- model[[depNode]]
        depNodeMean <- model$getParam(depNode, 'mean')
        ## write the next line, defining posteriorRate, something involving:
        ## -- solve(coeffMatrixValue), or perhaps coeffMatrixValue
        ## -- depNodeValue
        ## -- depNodeMean
        ## NOTE: when you do matrix multiplication, you'll then have to "demote" the 1x1 matrix into a scalar, e.g.:
        ##       (t(vector) %*% matrix %*% vector)[1,1]     ## see [1,1] indexing at the end
        posteriorRate <- 1  #### CHANGE THIS
        newValue <- rinvgamma(1, shape = posteriorShape, rate = posteriorRate)
        model[[target]] <<- newValue
        model$calculate(calcNodes)
        nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
    },
    ## every sampler needs a reset method.
    ## for conjugate samplers, it doesn't need to do anything.
    methods = list(
        reset = function() {}
    )
)


library(nimble)

code <- nimbleCode({
    sig2 ~ dinvgamma(shape = a, rate = b)
    C[1:N,1:N] <- sig2*P[1:N,1:N]
    y[1:N] ~ dmnorm(mean = mu[1:N], cov = C[1:N,1:N])
})
N <- 5
set.seed(0)
mu <- 1:N
m <- array(rnorm(N*N), c(N,N))
P <- m %*% t(m)
y <- rnorm(5, mu, 1)
constants <- list(N = N, a = 2, b = 3, mu = mu, P = P)
data <- list(y = y)
inits <- list(sig2 = 0.5)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
##conf$removeSamplers('sig2')
##conf$addSampler('sig2', 'conjugate_invgamma_dmnorm', print = TRUE)
Rmcmc <- buildMCMC(conf)

model <- Rmodel
target <- 'sig2'
control <- list()
control$dep_dmnorm <- Rmodel$getDependencies(target, stochOnly = TRUE, self = FALSE)
determineNodeIndexSizes <- nimble:::determineNodeIndexSizes
iDep <- 1

conf$getSamplerDefinition(1)

calcNodes <- model$getDependencies(target)
calcNodesDeterm <- model$getDependencies(target, determOnly = TRUE)
dep_dmnorm_nodeNames <- control$dep_dmnorm
N_dep_dmnorm <- length(control$dep_dmnorm)
dep_dmnorm_nodeSizes <- sapply(dep_dmnorm_nodeNames, function(node) max(determineNodeIndexSizes(node)), USE.NAMES = FALSE)
if (length(dep_dmnorm_nodeSizes) == 1) 
    dep_dmnorm_nodeSizes <- c(dep_dmnorm_nodeSizes, -1)
dep_dmnorm_nodeSizeMax <- max(dep_dmnorm_nodeSizes)
dep_dmnorm_values <- array(0, dim = c(N_dep_dmnorm, dep_dmnorm_nodeSizeMax))
dep_dmnorm_k <- array(0, dim = N_dep_dmnorm)
dep_dmnorm_offset <- array(0, dim = N_dep_dmnorm)
dep_dmnorm_coeff <- array(0, dim = N_dep_dmnorm)
contribution_shape <- 0
contribution_scale <- 0

prior_shape <- model$getParam(target[1], "shape")
prior_scale <- model$getParam(target[1], "scale")
for (iDep in 1:N_dep_dmnorm) {
    thisNodeSize <- dep_dmnorm_nodeSizes[iDep]
    dep_dmnorm_values[iDep, 1:thisNodeSize] <- model$getParam(dep_dmnorm_nodeNames[iDep], "value")
    dep_dmnorm_k[iDep] <- model$getParam(dep_dmnorm_nodeNames[iDep], "k")
}
model[[target]] <- 0
model$calculate(calcNodesDeterm)
for (iDep in 1:N_dep_dmnorm) dep_dmnorm_offset[iDep] <- model$getParam(dep_dmnorm_nodeNames[iDep], "cov")
model[[target]] <- 1
model$calculate(calcNodesDeterm)
for (iDep in 1:N_dep_dmnorm) dep_dmnorm_coeff[iDep] <- model$getParam(dep_dmnorm_nodeNames[iDep], "cov") - dep_dmnorm_offset[iDep]
contribution_shape <<- 0
contribution_scale <<- 0
for (iDep in 1:N_dep_dmnorm) {
    contribution_shape <<- contribution_shape + (1/2 + dep_dmnorm_k[iDep])
    contribution_scale <<- contribution_scale + 1
}
newValue <- rinvgamma(1, shape = prior_shape + contribution_shape, 
                      rate = 1/(prior_scale + contribution_scale))
model[[target]] <<- newValue
calculate(model, calcNodes)
nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)


Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

set.seed(0)
samples <- runMCMC(Cmcmc, 100)
head(samples)


## experimenting with thin rates for Ben Letcher


library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dexp(a*a+4)
    c ~ dgamma(abs(a)+1, abs(b))
})
constants <- list()
data <- list(c = 1)
inits <- list(a = 0, b=5)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

thin <- 5
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$setThin(thin)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

samples <- runMCMC(Cmcmc, 100, summary = FALSE)

dim(samples)


## trying to fix samplerExecutionOrder:
## removing the default value (-1) for a dimension-1 runtime argument

library(nimble)

nfDef1 <- nimbleFunction(
    setup = function() {
        b <- c(7,7,7)
    },
    run = function(a = double(default=1)) {
        print(a)
        print(b)
    },
    methods = list(
        setB = function(newB = integer(1)) {
            b <<- c(newB, 99, 100)
        }
    )
)

Rnf1 <- nfDef1()
Rnf1$run()

Cnf1 <- compileNimble(Rnf1)
Cnf1$run()

th <- numeric()
Rnf1$setB(th)
Cnf1$setB(th)

Rnf1$run()
Cnf1$run()



nfDef2 <- nimbleFunction(
    setup = function() {
        nf1 <- nfDef1()
    },
    run = function() {
        nf1$run()
        ##nf1$run(a = 100)
    }
)

Rnf2 <- nfDef2()
Rnf2$run()
Cnf2 <- compileNimble(Rnf2, showCompilerOutput = TRUE)
Cnf2$run()


## testing using mcmc$run inside another nimbleFunction

library(nimble)

nfDef <- nimbleFunction(
    setup = function(model) { mcmc <- buildMCMC(model) },
    run = function() { mcmc$run(10) }
)

code <- nimbleCode({ a ~ dnorm(0, 1) })
Rmodel <- nimbleModel(code, inits = list(a = 0))

Rnf <- nfDef(Rmodel)

Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(Rnf, project = Rmodel, showCompilerOutput = TRUE)






## testing implied priors from re-parameterization of d and theta
## from SCR movement models

library(nimble)

rSS <- nimbleFunction(
    run = function(n = integer(), lam = double()) {
        returnType(double(1))
        return(c(0,0))
    }
)
 
dSS <- nimbleFunction(
    run = function(x = double(1), lam = double(), log = double()) {
        dist <- sqrt(sum(x^2))
        lp <- dexp(dist, rate = lam, log = TRUE) - log(dist)  ## change of variables
        returnType(double())
        if(log) return(lp) else return(exp(lp))
    }
)
 
registerDistributions(list(
    dSS = list(BUGSdist = 'dSS(lam)', types = c('value = double(1)', 'lam = double()'))))

code <- nimbleCode({
    theta ~ dunif(0, 2*3.141592653589793116)
    d ~ dexp(rate = lam)
    y[1] <- d * cos(theta)
    y[2] <- d * sin(theta)
    z[1:2] ~ dSS(lam)
})

constants <- list(lam = 3)
data <- list()
inits <- list(theta = 3, d = 1, z = c(.1,.1))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$addMonitors('y','z')
conf$removeSamplers('z')
conf$addSampler('z', 'RW_block')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 50000)
samplesSummary(samples[,c('y[1]','z[1]')])
samplesSummary(samples[,c('y[1]','y[2]', 'z[1]','z[2]')])


sy <- samples[,c('y[1]','y[2]')]
dist <- apply(sy, 1, function(x) sqrt(sum(x^2)))

hist(dist, breaks=300, prob=TRUE)
mean(dist)
xs <- seq(0,6, by=0.0001)
ys <- dexp(xs, 3)
lines(xs, ys, col = 'red')

apply(samples, 2, mean)

samplesPlot(samples, 'theta')
samplesPlot(samples, 'd')
samplesPlot(samples, 'x')
samplesPlot(samples, 'y')
samplesPlot(samples, c('x','y'))
samplesPlot(samples, c('y','z'))
samplesPlot(samples, c('y[1]','z[1]'))

colnames(samples)
samplesSummary(samples[,c('y[1]','z[1]')])
##         Mean   Median  St.Dev. 95%CI_low 95%CI_upp
##y[1] 0.495931 0.504629 0.576998 -0.682585   1.66814
##y[2] 0.500805 0.501296 0.581499 -0.684521   1.68344



##> HDInterval:::hdiVector

function (object, credMass = 0.95, ...) 
{
    result <- c(NA_real_, NA_real_)
    if (is.numeric(object)) {
        attributes(object) <- NULL
        x <- sort.int(object, method = "quick")
        n <- length(x)
        if (n > 0) {
            exclude <- n - floor(n * credMass)
            low.poss <- x[1:exclude]
            upp.poss <- x[(n - exclude + 1):n]
            best <- which.min(upp.poss - low.poss)
            result <- c(low.poss[best], upp.poss[best])
        }
    }
    names(result) <- c("lower", "upper")
    return(result)
}



## simulating random walk flat prior of 1...k candidate models
    
k <- 10
cur <- 1
N <- 5000000
hist <- numeric(N)
hist[1] <- cur
for(i in 2:N) {
    if(cur == 1) {
        ##cur <- 2
        cur <- 1 + rbinom(1, 1, .5)
    } else if(cur == k) {
        ##cur <- k-1
        cur <- cur - rbinom(1, 1, .5)
    } else {
        cur <- cur + 2*rbinom(1, 1, .5) - 1
    }
    hist[i] <- cur
}

table(hist) / N
1/k

## prep for STAT359 class

plotErrorRates()

plotErrorRates(xlim = c(-55, -40), ylim = c(0, 0.05))



boxplot(BF ~ spam, data = df, ylab = 'Bayes Factor')

boxplot(BF ~ spam, data = df, ylim = c(-400,400), ylab = 'Bayes Factor')


emails <- readEmailDirectory("data/messages/spam")

emails[[529]]

length(emails[[529]])

head(emails[[529]], 30)

split <- splitMessage(emails[[529]])

split$header

head(split$body, 15)




plotErrorRates()


emailsAll[[6031]]


emails <- readEmailDirectory("data/messages/spam_2")

length(emails)

emails[[1380]]

bodyText <- lapply(emails, extractBodyText)
bodyText[[1380]]

words <- lapply(bodyText, extractWords)

words[[1380]]

lst <- nsgp_model(likelihood = "NNGP", constants = constants, z = z,
                  k = k, nID = nID,
                  tau_model 3= "logLinReg", sigma_model = "logLinReg", 
                  Sigma_model = "covReg", mu_model = "linReg",
                  Sigma_HP1 = 10, Sigma_HP2 = 5,
                  returnModelComponents = TRUE)

lst$code



## testing new enableWAIC as a property of the MCMC configuration


library(nimble)

tst <- function(conf, mcmc) print(c(conf$enableWAIC, mcmc$enableWAIC))
tst1 <- function(mcmc) print(mcmc$enableWAIC)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a*a, 1)
    c ~ dnorm(a^3, 1)
    d ~ dnorm(b + c, 2)
})
constants <- list()
data <- list(d = 1)
inits <- list(a = 0, b = 0, c = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)
tst(conf, Rmcmc)

nimbleOptions(enableWAIC = TRUE)
conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)
tst(conf, Rmcmc)
nimbleOptions(enableWAIC = FALSE)

conf <- configureMCMC(Rmodel, enableWAIC = TRUE)
Rmcmc <- buildMCMC(conf)
tst(conf, Rmcmc)

conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf, enableWAIC = TRUE)
tst(conf, Rmcmc)

conf <- configureMCMC(Rmodel)
conf$setEnableWAIC(TRUE)
Rmcmc <- buildMCMC(conf)
tst(conf, Rmcmc)

conf <- configureMCMC(Rmodel)
conf$setEnableWAIC(1)
Rmcmc <- buildMCMC(conf)
tst(conf, Rmcmc)

conf <- configureMCMC(Rmodel)
conf$setEnableWAIC()
Rmcmc <- buildMCMC(conf)
tst(conf, Rmcmc)

conf <- configureMCMC(Rmodel)
conf$setEnableWAIC(FALSE)
Rmcmc <- buildMCMC(conf)
tst(conf, Rmcmc)

conf <- configureMCMC(Rmodel)
conf$setEnableWAIC(0)
Rmcmc <- buildMCMC(conf)
tst(conf, Rmcmc)

Rmcmc <- buildMCMC(Rmodel)
tst1(Rmcmc)

nimbleOptions(enableWAIC = TRUE)
Rmcmc <- buildMCMC(Rmodel)
tst1(Rmcmc)
nimbleOptions(enableWAIC = FALSE)

Rmcmc <- buildMCMC(Rmodel, enableWAIC = TRUE)
tst1(Rmcmc)

Rmcmc <- buildMCMC(Rmodel, enableWAIC = 1)
tst1(Rmcmc)

Rmcmc <- buildMCMC(Rmodel, enableWAIC = FALSE)
tst1(Rmcmc)

Rmcmc <- buildMCMC(Rmodel, enableWAIC = 0)
tst1(Rmcmc)

nimbleMCMC(Rmodel, niter = 7)

nimbleMCMC(Rmodel, niter = 7, WAIC = TRUE)






## testing MCMCs that use monitors2, for updates to runMCMC()

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a*a, 1)
    c ~ dnorm(a^3, 1)
    d ~ dnorm(b + c, 2)
})
constants <- list()
data <- list(d = 1)
inits <- list(a = 0, b = 0, c = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printMonitors()
conf$setSamplerExecutionOrder(c(1,2,1,3,1))
conf$printSamplers()
conf$printSamplers(executionOrder = TRUE)
conf$addMonitors2('b', 'c')
conf$setThin2(3)
Rmcmc <- buildMCMC(conf, enableWAIC = TRUE)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)


runMCMC(Cmcmc, 7)
runMCMC(Cmcmc, 7, summary = TRUE)
runMCMC(Cmcmc, 7, summary = TRUE, samplesAsCodaMCMC = TRUE)
runMCMC(Cmcmc, 7, WAIC = TRUE)
runMCMC(Cmcmc, 7, summary = TRUE, WAIC = TRUE)
runMCMC(Cmcmc, 7, samples = FALSE, summary = TRUE)
runMCMC(Cmcmc, 7, samples = FALSE, WAIC = TRUE)
runMCMC(Cmcmc, 7, samples = FALSE, summary = TRUE, WAIC = TRUE)

runMCMC(Cmcmc, 7, nchains = 2)
runMCMC(Cmcmc, 7, nchains = 2, summary = TRUE)
runMCMC(Cmcmc, 7, nchains = 2, summary = TRUE, samplesAsCodaMCMC = TRUE)
runMCMC(Cmcmc, 7, nchains = 2, WAIC = TRUE)
runMCMC(Cmcmc, 7, nchains = 2, summary = TRUE, WAIC = TRUE)
runMCMC(Cmcmc, 7, nchains = 2, samples = FALSE, summary = TRUE)
runMCMC(Cmcmc, 7, nchains = 2, samples = FALSE, WAIC = TRUE)
runMCMC(Cmcmc, 7, nchains = 2, samples = FALSE, summary = TRUE, WAIC = TRUE)

Rmcmc$samplerExecutionOrderFromConfPlusTwoZeros
Cmcmc$samplerExecutionOrderFromConfPlusTwoZeros


runMCMC(Cmcmc, 40, thin=1, thin2=1, nburnin = 10)
runMCMC(Cmcmc, 40, thin=1, thin2=10, nburnin = 10)
runMCMC(Cmcmc, 40, thin=1, thin2=6, nburnin = 10)
runMCMC(Cmcmc, 40, thin=1, thin2=3, nburnin = 10)


## testing runtime samplerExecutionOrder argument to runMCMC():
## THIS IS NO LONGER SUPPORTED, UNTIL WE
## SUPPORT NON-SCALAR DEFAULT RUNTIME ARGUMENTS:

runMCMC(Cmcmc, 2)
x <- runMCMC(Cmcmc, 2, samplerExecutionOrder = c(1,1,1))
x <- runMCMC(Cmcmc, 2, samplerExecutionOrder = 3)
x <- runMCMC(Cmcmc, 2, samplerExecutionOrder = numeric(0))
x <- runMCMC(Cmcmc, 2)

Rmcmc$run(7)
Cmcmc$run(7)



a <- c(0,0)
a[-c(length(a)-1, length(a))]

set.seed(0)
samples <- runMCMC(Cmcmc, 1000)
dim(samples)
dim(as.matrix(Cmcmc$mvSamples2))
Cmcmc$mvSamples2

Rmcmc$monitors
Rmcmc$monitors2

Cmcmc$monitors
Cmcmc$monitors2

Cmcmc$Robject$monitors
Cmcmc$Robject$monitors2

mcmc <- Rmcmc
mcmc <- Cmcmc

length(if(is.Cnf(mcmc)) mcmc$Robject$monitors2 else mcmc$monitors2) > 0

as.matrix(Rmcmc$mvSamples)
as.matrix(Rmcmc$mvSamples2)
as.matrix(Cmcmc$mvSamples)
as.matrix(Cmcmc$mvSamples2)



## testing risk difference model with Bernhard

library(nimble)
?buildMCMC

x <- matrix(c(0, 432 ,   0,  142, 
1, 375 ,   0,  125, 
0, 80  ,   0,  40 , 
0, 248 ,   1,  84 , 
0, 50  ,   0,  24 , 
2, 251 ,   0,  85 , 
5, 605 ,   1,  209, 
1, 466 ,   2,  231, 
4, 525 ,   0,  175, 
1, 80  ,   0,  40 , 
1, 375 ,   0,  124, 
0, 189 ,   0,  59 , 
0, 55  ,   0,  27 , 
0, 110 ,   0,  56 , 
0, 85  ,   0,  85 , 
0, 170 ,   0,  85 , 
4, 1030,   1,  350 ), ncol = 4, byrow = TRUE)
## y1 n1    y2 n2
colnames(x) <- c('y1', 'n1',    'y2', 'n2')
x <- as.data.frame(x)
n <- cbind(x$n1, x$n2)
y <- cbind(x$y1, x$y2)
K <- dim(x)[1]

code <- nimbleCode({
    for(j in 1:2) {
        alpha[j] ~ dgamma(shape = 0.01, scale = 0.01)
        beta[j] ~ dgamma(shape = 10, scale = 0.01)
        for(i in 1:K) {
            p[i,j] ~ dbeta(shape1 = alpha[j], shape2 = beta[j])
            y[i,j] ~ dbinom(size = n[i,j], prob = p[i,j])
        }
    }
    delta <- alpha[1]/(alpha[1]+beta[1]) - alpha[2]/(alpha[2]+beta[2])
})

constants <- list(K = K)
data <- list(y = y)
inits <- list(n = n, alpha = c(0.0001, 0.0001), beta = c(0.1, 0.1), p = (y+1)/(n+2))

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()

box <- list( list(c('alpha','beta'), c(0, Inf)))

MCEM <- buildMCEM(Rmodel, 'p', boxConstraints = box)

MLEs <- MCEM$run(initM = 1000)
## Iteration Number: 57.
## Current number of MCMC iterations: 3032.
## Parameter Estimates: 
##   alpha[1]   alpha[2]    beta[1]    beta[2] 
##   2.420032   1.166802 663.450866 436.657505 
## Convergence Criterion: 0.003097695.
## |-------------|-------------|-------------|-------------|
## |-------------------------------------------------------|

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel, control = list(logScale = TRUE))
conf$printSamplers()
conf$addMonitors('delta')
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
out <- runMCMC(Cmcmc, 500000, summary = TRUE, samples = TRUE)
str(out)
##                Mean     Median     St.Dev.   95%CI_low  95%CI_upp
## alpha[1] 0.02659941 0.02520186 0.010162037 0.010768589 0.05027605
## alpha[2] 0.01306957 0.01174824 0.007026915 0.003889054 0.02918123
## beta[1]  0.13008978 0.12698964 0.034063940 0.071855816 0.20450840
## beta[2]  0.11812389 0.11514613 0.033170417 0.063142584 0.19148567

library(coda)
samples <- out$samples
codamcmc <- coda::as.mcmc(out$samples)
class(codamcmc)
str(codamcmc)
samplesSummary(codamcmc)
##               Mean     Median     St.Dev.    95%CI_low  95%CI_upp
##alpha[1] 0.02676965 0.02545632 0.010068985  0.011020752 0.05016782
##alpha[2] 0.01302527 0.01174159 0.006685269  0.003971777 0.02941060
##beta[1]  0.13011633 0.12695276 0.034108937  0.072633702 0.20565336
##beta[2]  0.11755000 0.11437664 0.033024751  0.062356236 0.19101432
##delta    0.07158604 0.07127036 0.074219690 -0.074787529 0.21973711

samplesPlot(codamcmc)   ## looks good


install.packages('HDInterval')
library(HDInterval)
HDInterval::hdi
?HDInterval::hdi

?HDInterval::hdi

head(samples)
dim(samples)

HDInterval::hdi(samples)


HDInterval:::hdi.list
HDInterval:::hdi.bugs
HDInterval:::hdi.function
HDInterval:::hdi.matrix
HDInterval:::checkCredMass
HDInterval:::hdiVector


## testing the HDInterval:::hdiVector() function:

n <- 1000000
##x <- rnorm(n)      ### modify
##density <- dnorm   ### modify
##xlim <- c(-4, 4)
x <- rbeta(n, 2, 60)      ### modify
density <- function(x) dbeta(x, 2, 60)   ### modify
xlim <- c(0, 1)
##x <- rgamma(n, 10, 1)      ### modify
##density <- function(x) dgamma(x, .2, .2)   ### modify
##xlim <- c(0, 20)
plot(density, col = 'red', xlim = xlim)
int <- hdi(as.matrix(x))
hist(x, add = TRUE, freq = FALSE, breaks = 100)
lu <- as.numeric(int)
abline(v = lu, col = 'blue')
height <- density(lu[1])
abline(h = height, col = 'green')
equaltailed <- quantile(x, probs = c(0.025, 0.975))
abline(v = lu, col = 'purple')

equaltailed
lu
#> equaltailed
#       2.5%       97.5% 
#0.003958617 0.087976130 
#> lu
#[1] 0.0009586269 0.0760303896


## makes 2D plots of HPD region:
?emdbook::HPDregionplot
emdbook::HPDregionplot(codamcmc)    ## new
a <- emdbook::HPDregionplot(codamcmc)    ## new
class(a)
names(a)
length(a)
names(a[[1]])
names(a[[1]])
class(a[[1]]$level)
a[[1]]$level
##[1] 115.7915
a[[1]]$x


a <- emdbook::HPDregionplot(codamcmc, vars = 3:4)    ## new
head(codamcmc,2)
##         alpha[1] alpha[2] beta[1]   beta[2]       delta
##[1,] 0.0003535852   0.0001     0.1 0.1513828 0.002863253
a <- emdbook::HPDregionplot(codamcmc, vars = 3:4)    ## new
a <- emdbook::HPDregionplot(codamcmc, vars = 1)    ## new




## testing with different character encodings for spam classification project

filenames <- list.files(dirs[4], full.names = TRUE)


i <- 753


email <- readLines(filenames[i])
msg <- email
split <- splitMessage(email)
header <- split$header
body <- split$body

hasAttachment(header)
email
bodyText <- extractBodyText(msg)
header
body
words <- extractWords(body)

body

body[15]

nchar(body[15])

cc <- substr(body[15], 4,4)
class(cc)

nchar(cc)
storage.mode(cc)

cc
print(cc)
cat(cc)

intToUtf8('sdfdsf')
intToUtf8(body[15])

toUtf8

textConnection('sdf')

encoding(cc)

deparse(cc)

grepl('[[:digit:]]', cc)

gsub('\\d{3}', '', body[15])
gsub('\301', '', body[15], fixed = TRUE)

grepl('\301', body[15], fixed = TRUE)
grepl('\\\d\d\d', body[15])

for(i in 530:1000) {
    print('==========================================')
    print(i)
    email <- readLines(filenames[i])
    msg <- email
    split <- splitMessage(email)
    header <- split$header
    body <- split$body
    hasAttachment(header)
    bodyText <- extractBodyText(msg)
    words <- extractWords(body)
    print(words)
}




529

length(emailsAll)

for(i in 4500:4600) {
    print(i)
    print(emailsAll[[i]])
}

which(sapply(emailsAll, function(x) length(grep('\001', x))>0))

emailsAll[[7100]]



head(sort(bow), 100)


setwd('~/github/courses/stat359/projects/spam_classification')

files <- list.files("data/messages/hard_ham", full.names = TRUE)
fileEmails <- lapply(files, readLines)

email <- fileEmails[[270]]
msg <- email

fileEmails[[1]]

extractBodyText <- function(email) {
  bodyatt <- splitMessage(email)$body
  if (hasAttachment(splitMessage(email)$header) == FALSE) {
    return(bodyatt)
  }
  else {
    boundary <- getBoundary(email)
    boundaryind <- grep(boundary, bodyatt, fixed = TRUE)
    if (length(boundaryind) == 1) {
      body <- bodyatt[-c(boundaryind)]
    }
    else if (length(boundaryind) > 1) {
      start <- boundaryind[1] + 1
      end <- boundaryind[2] - 1
      body <- bodyatt[start:end]
    }
    return(body)
  }
}

n <- length(fileEmails)

filetxt <- lapply(fileEmails, extractBodyText)

filetxt <- lapply(1:n, function(i) {
    email <- fileEmails[[i]]
    print(i)
    extractBodyText(email)
})





filetxt[[1]]

filetxt <- lapply(filetxt, paste, collapse = " ")

filetxt[[1]]

hard_hamlengths <- lapply(filetxt, function(x) length(strsplit(x, " ")[[1]]))

hard_hamlengths[[1]]

hard_hamlengths <- log(unlist(hard_hamlengths))

hard_hamlengths


## testing new runtime thin functionality
## and also new native burnin for mcmc$run

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

samples <- nimbleMCMC(Rmodel, niter = 1000); dim(samples)   ## 1000
samples <- nimbleMCMC(Rmodel, niter = 1000, thin = 10); dim(samples)   ## 100
samples <- nimbleMCMC(Rmodel, niter = 1000, thin = 9); dim(samples)   ## 111

burnin <- 81
samples <- nimbleMCMC(Rmodel, nburnin=burnin, niter = 1000); dim(samples)   ## 919
samples <- nimbleMCMC(Rmodel, nburnin=burnin, niter = 1000, thin = 10); dim(samples)   ## 91
samples <- nimbleMCMC(Rmodel, nburnin=burnin, niter = 1000, thin = 9); dim(samples)   ## 102



code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)


burnin <- 40
samples <- runMCMC(Rmcmc, nburnin=burnin, niter = 100); dim(samples)   ## 60
samples <- runMCMC(Rmcmc, nburnin=burnin, niter = 100, thin = 10); dim(samples)   ## 6
samples <- runMCMC(Rmcmc, nburnin=burnin, niter = 100, thin = 7); dim(samples)   ## 8
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
samples <- runMCMC(Cmcmc, nburnin=burnin, niter = 1000); dim(samples)   ## 960
samples <- runMCMC(Cmcmc, nburnin=burnin, niter = 1000, thin = 10); dim(samples) ## 96
samples <- runMCMC(Cmcmc, nburnin=burnin, niter = 1000, thin = 7); dim(samples)  ## 137




code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel, thin = 7)
conf$printSamplers()
conf$printMonitors()
Rmcmc <- buildMCMC(conf)

burnin <- 0
samples <- runMCMC(Rmcmc, nburnin=burnin, niter = 700); dim(samples)   ## 100
samples <- runMCMC(Rmcmc, nburnin=burnin, niter = 706); dim(samples)   ## 100
samples <- runMCMC(Rmcmc, nburnin=burnin, niter = 100, thin = 10); dim(samples)   ## 10
samples <- runMCMC(Rmcmc, nburnin=burnin, niter = 100, thin = 3); dim(samples)   ## 33
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
samples <- runMCMC(Cmcmc, nburnin=burnin, niter = 700); dim(samples)   ## 100
samples <- runMCMC(Cmcmc, nburnin=burnin, niter = 706); dim(samples)   ## 100
samples <- runMCMC(Cmcmc, nburnin=burnin, niter = 100, thin = 10); dim(samples)   ## 10
samples <- runMCMC(Cmcmc, nburnin=burnin, niter = 100, thin = 3); dim(samples)   ## 33

burnin <- 35
samples <- runMCMC(Rmcmc, nburnin=burnin, niter = 700); dim(samples)   ## 95
samples <- runMCMC(Rmcmc, nburnin=burnin, niter = 706); dim(samples)   ## 95
samples <- runMCMC(Rmcmc, nburnin=burnin, niter = 100, thin = 10); dim(samples)   ## 6
samples <- runMCMC(Rmcmc, nburnin=burnin, niter = 100, thin = 3); dim(samples)   ## 21
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
samples <- runMCMC(Cmcmc, nburnin=burnin, niter = 700); dim(samples)   ## 95
samples <- runMCMC(Cmcmc, nburnin=burnin, niter = 706); dim(samples)   ## 95
samples <- runMCMC(Cmcmc, nburnin=burnin, niter = 100, thin = 10); dim(samples)   ## 6
samples <- runMCMC(Cmcmc, nburnin=burnin, niter = 100, thin = 3); dim(samples)   ## 21


code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel, thin = 7, thin2 = 3)
conf$printSamplers()
conf$printMonitors()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Rmcmc$run(100); dim(as.matrix(Rmcmc$mvSamples))   ## 14
Rmcmc$run(100, thin = 1); dim(as.matrix(Rmcmc$mvSamples))   ## 100
Rmcmc$run(100, thin = 2); dim(as.matrix(Rmcmc$mvSamples))   ## 50
Rmcmc$run(100, thin = 17); dim(as.matrix(Rmcmc$mvSamples))   ## 5

Cmcmc$run(1000); dim(as.matrix(Cmcmc$mvSamples))   ## 142
Cmcmc$run(1000, thin = 1); dim(as.matrix(Cmcmc$mvSamples))   ## 1000
Cmcmc$run(1000, thin = 2); dim(as.matrix(Cmcmc$mvSamples))   ## 500

Cmcmc$run(1000, thin = 17); dim(as.matrix(Cmcmc$mvSamples))   ## 58
Cmcmc$run(10, thin = 1, reset = FALSE); dim(as.matrix(Cmcmc$mvSamples))   ## 68
Cmcmc$run(10, thin = 2, reset = FALSE); dim(as.matrix(Cmcmc$mvSamples))   ## 73
Cmcmc$run(10, thin = 3, reset = FALSE); dim(as.matrix(Cmcmc$mvSamples))   ## 76
Cmcmc$run(10, thin = 3, nburnin = 1, reset = FALSE); dim(as.matrix(Cmcmc$mvSamples))   ## error

Cmcmc$run(1000, thin = 17, nburnin = -1); dim(as.matrix(Cmcmc$mvSamples))   ## error

Cmcmc$run(1000, thin = 1, nburnin = 500.1); dim(as.matrix(Cmcmc$mvSamples))   ## 



## testing samplerExecutionOrder

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a+10, 1)
    c ~ dexp(abs(b))
})
constants <- list()
data <- list()
inits <- list(a = 0, b = 10, c = 3)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)

conf$addSampler('b', 'slice')
conf$removeSamplers('b')
conf$setSamplers(c(1,1,1,2,2,1))


order <- c(1,1,1,2,3,4)
order <- rep(4,10)
order <- 3
order <- numeric()
conf$setSamplerExecutionOrder(order)
conf$setSamplerExecutionOrder(order, print = TRUE)


conf$setSamplerExecutionOrder()

conf$getSamplerExecutionOrder()
conf$printSamplers()
conf$printSamplers(executionOrder = TRUE)

nodes <- c('a', 'c')
conf$printSamplers(nodes)
conf$printSamplers(nodes, executionOrder = TRUE)




Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Rmcmc$run(1)
Cmcmc$run(1)

ord <- 1:2
ord <- c(1,1,1,4,3)
ord <- 4
ord <- numeric()
ord <- c(1,1,-1,4,3)
Rmcmc$run(1, samplerExecutionOrder = ord)
Cmcmc$run(1, samplerExecutionOrder = ord)



## testing seq_along use in a nimbleFunction

library(nimble)

nfDef <- nimbleFunction(
    setup = function() {
    },
    run = function(o = integer(1, default = -1)) {
        o <- numeric(0)
        print('length(o) = ', dim(o)[1])
        print('o = ', o)
        if(dim(o)[1] > 0 & o[1] == -1) {
            print('o[1] is -1')
        } else {
            print('BEGIN SEQ_ALONG')
            for(i in seq_along(o)) {
                print('i = ', i, ', and o[i] = ', o[i])
                ##print('i = ', i)
            }
        }
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)

Rnf$run()
Cnf$run()

o <- c(1,1,1,3,3,4,5,1)
o <- 1
o <- numeric()
Rnf$run(o)
Cnf$run(o)




## debugging error in compilation using NNGP ns_corr in model

setwd('~/github/nngp/analysis')

library(nimble)
source('../bayes_nsgp/source_nimble.R')
source('../code/nngp4.R')

COprecip <- read.csv('../fullGP_nonstationary/Final_data.csv')
coords <- as.matrix(COprecip[,c('Longitude', 'Latitude')])
dist_list <- ns_dist(coords)
Xmat <- unname(lm(logPrecip ~ Zelevation*Zslope10, x = TRUE, data = COprecip)$x)
p <- dim(Xmat)[2]
z <- COprecip$logPrecip
##
constants <- list(X_tau = Xmat, X_sigma = Xmat, X_Sigma = Xmat, X_mu = Xmat,
                  p_tau = p, p_sigma = p, p_Sigma = p, p_mu = p,
                  dist1_sq = dist_list$dist1_sq, dist2_sq = dist_list$dist2_sq, dist12 = dist_list$dist12)
##
k <- 15
dst <- unname(as.matrix(dist(coords)))
nID <- determineNeighbors(dst, k)

Rmodel <- nsgp_model(likelihood = 'NNGP', constants = constants, z = z, k = k, nID = nID)
Rmodel$calculate()

Cmodel <- compileNimble(Rmodel)





## pulling apart Mark Risser's ns_corr() function, trying to find
## a more efficient way of writing the calculateAD_ns() function

N <- 20
set.seed(0)
coords <- cbind(runif(N,0,10), runif(N,0,10))
dist_list <- ns_dist(coords)
dist1_sq <- dist_list$dist1_sq
dist2_sq <- dist_list$dist2_sq
dist12 <- dist_list$dist12
scale_factor <- dist_list$scale_factor
Sigma11 <- seq(5,6,length.out=N)
Sigma22 <- seq(3,4,length.out=N)
Sigma12 <- seq(1,2,length.out=N)
log_sigma_vec <- 1:N
log_tau_vec <- sqrt(1:N)

if(FALSE) {
    dist1_sq <- dist1_sq[ind, ind]
    dist2_sq <- dist2_sq[ind, ind]
    dist12   <- dist12  [ind, ind]
    Sigma11 <- Sigma11[ind]
    Sigma22 <- Sigma22[ind]
    Sigma12 <- Sigma12[ind]
    log_sigma_vec <- log_sigma_vec[ind]
    log_tau_vec <- log_tau_vec[ind]
    N <- length(ind)
}

dist1_sq
dist2_sq
dist12
Sigma11
Sigma22
Sigma12
log_sigma_vec
log_tau_vec
N

## Calculate the scale matrix 
det1 <- Sigma11*Sigma22 - Sigma12^2
det1

mat11_a <- matrix(Sigma11, nrow = N, ncol = N) 
mat22_a <- matrix(Sigma22, nrow = N, ncol = N)
mat12_a <- matrix(Sigma12, nrow = N, ncol = N)
mat11_a
mat22_a
mat12_a

mat11 <- 0.5*(mat11_a + t(mat11_a))
mat22 <- 0.5*(mat22_a + t(mat22_a))
mat12 <- 0.5*(mat12_a + t(mat12_a))
mat11
mat22
mat12

det12 <- mat11*mat22 - mat12^2
det12

Scale.mat <- diag(sqrt(sqrt(det1))) %*% sqrt(1/det12) %*% diag(sqrt(sqrt(det1)))
Scale.mat

## Calculate the distance matrix
inv11 <- mat22/det12
inv22 <- mat11/det12
inv12 <- -mat12/det12
inv11
inv22
inv12

Dist.mat <- sqrt( inv11*dist1_sq + 2*inv12*dist12 + inv22*dist2_sq )
Dist.mat

## Combine 
Unscl.corr <- exp(-Dist.mat) # Here is the exponential correlation
Unscl.corr

NS_corr <- Scale.mat*Unscl.corr
ns <- NS_corr

ns_corr(dist1_sq, dist2_sq, dist12, Sigma11, Sigma22, Sigma12) - ns

## say we want i = 19 and neighbor IDs = c(2,5,10)
ind <- c(2,5,10,19)
## or, ind = 1
ind <- 1

nsBlock <- ns[ind, ind]
nsBlock - ns




Cor <- ns_corr(dist1_sq[1:N,1:N], dist2_sq[1:N,1:N], dist12[1:N,1:N], Sigma11[1:N], Sigma22[1:N], Sigma12[1:N])
Cov <- diag(exp(log_sigma_vec[1:N])) %*% Cor[1:N,1:N] %*% diag(exp(log_sigma_vec[1:N]))
C <- Cov[1:N, 1:N] + diag(exp(log_tau_vec[1:N]^2))

C[1,1]
exp(log_sigma_vec[1])^2
exp(log_tau_vec[1]^2)
exp(log_sigma_vec[1])^2 + exp(log_tau_vec[1]^2)

det12 <- Sigma11[1]*Sigma22[1] - Sigma12[1]^2
det1 <- same thing
Scale.mat[1,1] is just 1!
Uncsl.corr[1,1] is exp(-0) = 1
NS_corr[1,1] is always 1 * 1 = 1
##That is:
Cor[1,1] is 1


inv11[1,1] <- Sigma22[1] / (Sigma11[1]*Sigma22[1] - Sigma12[1]^2)
inv22[1,1] <- Sigma11[1] / (Sigma11[1]*Sigma22[1] - Sigma12[1]^2)
inv12[1,1] <- - Sigma12[1] / (Sigma11[1]*Sigma22[1] - Sigma12[1]^2)


## say we want i = 19 and neighbor IDs = c(2,5,10)
ind <- c(2,5,10,19)

Cblock <- C[ind, ind]
Cblock

Cblock - C


Rmodel$Sigma11 <- rep(2, 217)
Rmodel$Sigma22 <- rep(9, 217)
Rmodel$Sigma11
Rmodel$calculate()




##return(NS_corr)




## testing new setSeed functions for runMCMC()

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a+10, 1)
    c ~ dexp(abs(b))
})
constants <- list()
data <- list(c = 3)
inits <- list(a = 0, b = 10)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel)
conf$addMonitors('a', 'b')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

runMCMC(Cmcmc, niter=2, nchains=3, inits=inits, setSeed = TRUE)

runMCMC(Cmcmc, niter=2, nchains=3, setSeed = TRUE)



## testing new pre-thinning burnin in NIMBLE,
## for MCMCsuite, runMCMC, and nimbleMCMC

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a+10, 1)
    c ~ dexp(abs(b))
})
constants <- list()
data <- list(c = 3)
inits <- list(a = 0, b = 10)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

## testing new burnin for runMCMC()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

samples <- runMCMC(Cmcmc, 1000, nburnin = 100)
dim(samples)
## [1] 900   1

samplesList <- runMCMC(Cmcmc, 1000, nburnin = 200, nchains = 3)
lapply(samplesList, dim)
## $chain1
## [1] 800   1

conf <- configureMCMC(Rmodel)
conf$setThin(10)
conf$printSamplers()
conf$printMonitors()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

samples <- runMCMC(Cmcmc, 1000)
dim(samples)
## [1] 100   1

samples <- runMCMC(Cmcmc, 1000, nburnin = 100)
dim(samples)
## [1] 90  1

samples <- runMCMC(Cmcmc, 1000, nburnin = 500)
dim(samples)
## [1] 50  1

samples <- runMCMC(Cmcmc, 1009)
dim(samples)
## [1] 100   1

samples <- runMCMC(Cmcmc, 1010, nburnin = 20)
dim(samples)
## [1] 99  1

samples <- runMCMC(Cmcmc, 1009, nburnin = 9)
dim(samples)
## [1] 100   1

samples <- runMCMC(Cmcmc, 1023, nburnin = 0)
dim(samples)
## [1] 102   1

samples <- runMCMC(Cmcmc, 1023, nburnin = 1)
dim(samples)
## [1] 102   1

samples <- runMCMC(Cmcmc, 1023, nburnin = 2)
dim(samples)
## [1] 102   1

samples <- runMCMC(Cmcmc, 1023, nburnin = 3)
dim(samples)
## [1] 102   1

samples <- runMCMC(Cmcmc, 1023, nburnin = 4)
dim(samples)
## [1] 101   1

samples <- runMCMC(Cmcmc, 1023, nburnin = 5)
dim(samples)
## [1] 101   1

samplesList <- runMCMC(Cmcmc, 1023, nburnin = 3, nchains = 3)
lapply(samplesList, dim)
## $chain1
## [1] 102   1

samplesList <- runMCMC(Cmcmc, 1023, nburnin = 4, nchains = 3)
lapply(samplesList, dim)
## $chain1
## [1] 101   1



## testing new burnin for nimbleMCMC()


samples <- nimbleMCMC(code, constants, data, inits, niter = 1000, nburnin = 100)
dim(samples)
## [1] 900   1

samplesList <- nimbleMCMC(code, constants, data, inits, niter = 1000, nburnin = 200, nchains = 3)
lapply(samplesList, dim)
## $chain1
## [1] 800   1

samples <- nimbleMCMC(code, constants, data, inits, niter = 1000, thin = 10)
dim(samples)
## [1] 100   1

samples <- nimbleMCMC(code, constants, data, inits, niter = 1000, nburnin = 100, thin = 10)
dim(samples)
## [1] 90  1

samples <- nimbleMCMC(code, constants, data, inits, niter = 1000, nburnin = 500, thin = 10)
dim(samples)
## [1] 50  1

samples <- nimbleMCMC(code, constants, data, inits, niter = 1009, thin = 10)
dim(samples)
## [1] 100   1

samples <- nimbleMCMC(code, constants, data, inits, niter = 1010, nburnin = 20, thin = 10)
dim(samples)
## [1] 99  1

samples <- nimbleMCMC(code, constants, data, inits, niter = 1009, nburnin = 9, thin = 10)
dim(samples)
##[1] 100  1

samples <- nimbleMCMC(code, constants, data, inits, niter = 1025, nburnin = 4, thin = 10)
dim(samples)
##[1] 102  1

samples <- nimbleMCMC(code, constants, data, inits, niter = 1025, nburnin = 5, thin = 10)
dim(samples)
##[1] 102  1

samples <- nimbleMCMC(code, constants, data, inits, niter = 1025, nburnin = 6, thin = 10)
dim(samples)
##[1] 101  1


## now, some tests of the *exact* post thinning and burning samples,
## that they agree with just running the MCMC straight.

## case 1
thin <- 8
burnin <- 96

## case 2
thin <- 1
burnin <- 96

## case 3
thin <- 7
burnin <- 0


Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$addMonitors('a', 'b')
conf$printMonitors()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0); Rmcmc$run(500)
set.seed(0); Cmcmc$run(500)

Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)

if(burnin>0) Rsamples <- Rsamples[-(1:burnin), ]
if(burnin>0) Csamples <- Csamples[-(1:burnin), ]

ind <- seq(thin, dim(Rsamples)[1], by = thin)
Rsamples <- Rsamples[ind,]
Csamples <- Csamples[ind,]

dim(Rsamples)
dim(Csamples)
all(Rsamples - Csamples == 0)

RsampleFinal <- Rsamples
CsampleFinal <- Csamples

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$setThin(thin)
conf$addMonitors('a', 'b')
conf$printMonitors()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0); Rsamples <- runMCMC(Rmcmc, 500, nburnin = burnin)
set.seed(0); Csamples <- runMCMC(Cmcmc, 500, nburnin = burnin)

dim(Rsamples)
dim(Csamples)
all(Rsamples - Csamples == 0)

RsampleFinal - Rsamples
CsampleFinal - Csamples

all(RsampleFinal - Rsamples == 0)
all(CsampleFinal - Csamples == 0)


## now, testing the "exact samples" against nimbleMCMC()

## case 1
thin <- 8
burnin <- 96

## case 2
thin <- 1
burnin <- 96

## case 3
thin <- 7
burnin <- 0


Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$addMonitors('a', 'b')
conf$printMonitors()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0); Rmcmc$run(500)
set.seed(0); Cmcmc$run(500)

Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)

if(burnin>0) Rsamples <- Rsamples[-(1:burnin), ]
if(burnin>0) Csamples <- Csamples[-(1:burnin), ]

ind <- seq(thin, dim(Rsamples)[1], by = thin)
Rsamples <- Rsamples[ind,]
Csamples <- Csamples[ind,]

dim(Rsamples)
dim(Csamples)
all(Rsamples - Csamples == 0)

RsampleFinal <- Rsamples
CsampleFinal <- Csamples

set.seed(0); samples <- nimbleMCMC(code, constants, data, inits,
                                    monitors = c('a', 'b'), thin = thin, niter = 500, nburnin = burnin)

dim(samples)
RsampleFinal - samples
CsampleFinal - samples

all(RsampleFinal - samples == 0)
all(CsampleFinal - samples == 0)




## repeated birthdays calculations for STAT101

## 2 students, probability of different birthdays:
364/365 * 100 

## 3 students, probability of having different birthdays:
364/365 * 363/365 * 100 

## 4 students, probability of having different birthdays:
364/365 * 363/365 * 362/365 * 100 

## 5 students, probability of having different birthdays:
364/365 * 363/365 * 362/365 * 361/365 * 100 


## n students, probability of having different birthdays:
n <- 50 
prod(seq(364, by = -1, length = n-1)) / 365^(n-1) * 100 

## complement: at least 2 students have the same birthday:
100 - prod(seq(364, by = -1, length = n-1)) / 365^(n-1) * 100 









mortality2 <- mortality
population2 <- population
countryCodes2 <- countryCodes

load('data/global.RData')

identical(mortality2   , mortality   ) 
identical(population2  , population  ) 
identical(countryCodes2, countryCodes) 


head(latlong_old)
dim(latlong_old)

head(latlong)
dim(latlong)
str(latlong)

old <- latlong_old$iso.3166.country
old <- old[!is.na(old)]

old

new <- latlong$iso3166
new

length(old)
length(new)

setdiff(old, new)
setdiff(new, old)

latlong_old[latlong_old$iso.3166.country %in% c('EU','AP'), ]

latlong <- read.csv(latlongURL)
latlong

table(latlong_raw$Alpha.2.code)
sort(table(latlong_raw$Alpha.2.code))
sum(table(latlong_raw$Alpha.2.code) > 1)
which(table(latlong_raw$Alpha.2.code) > 1)
names(which(table(latlong_raw$Alpha.2.code) > 1))
reps <- names(which(table(latlong_raw$Alpha.2.code) > 1))
reps
length(reps)
latlong_raw[latlong_raw$Alpha.2.code %in% reps,]

setdiff(old, new)
setdiff(new, old)

reps

## trying to find a replacement for Google Finance API

install.packages('quantmod')

install.packages('xts')
library(xts)
library(quantmod)

getSymbols("GOOG", src = "google")

install.packages('BatchGetSymbols')
library(BatchGetSymbols)

Sys.Date() - 1



first.date <- Sys.Date() - 60
last.date <- Sys.Date()

##tickers <- c('FB','NYSE:MMM','PETR4.SA','ab
cdef')
tickers <- c('KO', 'PEP')

out <- BatchGetSymbols(tickers = tickers, 
                       first.date = first.date,
                       last.date = last.date
                       ##cache.folder = file.path(tempdir(), 'BGS_Cache')
                       )


class(out)
names(out)
str(out)
out$df.control
out$df.tickers
as.character(out$df.tickers$ref.date)
class(as.character(out$df.tickers$ref.date))
class(out$df.tickers$ref.date)
head(out$df.tickers)
out$df.tickers$price.adjusted



stock <- 'KO'

start <- 2010
nyears <- 1

downloadPriceDF <- function(stock, start = 2010, nyears = 1) {
    require(BatchGetSymbols)
    startdate <- paste(start, '01', '01', sep = '-')
    enddate <- paste(start+nyears, '01', '01', sep = '-')
    out <- BatchGetSymbols(tickers = stock, first.date = startdate, last.date = enddate)
    cat('\n')
    if(out$df.control$download.status != 'OK') stop(paste0('something went wrong downloading ', stock, 'prices'))
    stockDF <- data.frame(date = as.character(out$df.tickers$ref.date),
                          price = out$df.tickers$price.adjusted)
    return(stockDF)
}




## playing with SQL fetching

library(RSQLite)
driver <- dbDriver("SQLite")
con <- dbConnect(driver,  dbname = "~/github/courses/stat359/projects/baseball_statistics/data/lahman2016.sqlite")

query <- 'SELECT "year.key" AS year, games
          FROM HomeGames'

### send the query to the SQL connection
results <- dbSendQuery(con, query)

### fetch the first 5 results
fetch(results, 5)

### fetch 10 more results
fetch(results, 10)

### this tells how many rows we've retrieved
dbGetRowCount(results)

### this queries whether we've retrieved all the results
dbHasCompleted(results)

### fetch(results, -1) will fetch all remaining results
dim(fetch(results, -1))

dbGetRowCount(results)
dbHasCompleted(results)

### clear the results object before sending another query
dbClearResult(results)

dbGetQuery(con, 'SELECT * FROM HomeGames WHERE "year.key" IN (1900, 1950, 2000) LIMIT 15')



## testing passing extraneous constants / inits values into nimbleModel()

library(nimble)

code <- nimbleCode({
    a ~ dnorm(A, C)
    b ~ dnorm(B, D)
})

constants <- list(A = 1, X = 5, ZZZZ = 1000)      ## no warning from extraneous 'constants'
data <- list(C = 5) ### gives a warning: , YY = 10)
inits <- list(B = 10)  ## warning: , ZZZ = 100)

Rmodel <- nimbleModel(code, constants, data, inits)


## testing many levels of nested braces in nimble model code

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    for(i in 1:5) {
        b[i] ~ dnorm(0, 1)
    }
    {
        c ~ dnorm(0, 1)
        {
            {
                {
                    g ~ dnorm(0, 1)
                }
            }
        }
    }
    {
        {
            d ~ dnorm(0, 1)
            for(i in 1:5) {
                f[i] ~ dnorm(0, 1)
            }
        }
        e ~ dnorm(0, 1)
    }
})

Rmodel <- nimbleModel(code)

Rmodel$getNodeNames()


## testing adding more points to a plot in ggplot

df <- data.frame(x = 1:10, y = 1:10)
library(ggplot2)
p <- ggplot(df, aes(x=x, y=y)) + geom_point()

df2 <- data.frame(o = c(3,3,4), xs = c(1,6,9))

p + geom_point(aes(x=xs, y=o), df2)



## factorization() function for STAT359 problem set pset1

factorization <- function(x) {
    if(x == 1) return(1)
    factors <- numeric()
    d <- 2
    while(x > 1) {
        while(x%%d == 0) {
            x <- x/d
            factors <- c(factors, d)
        }
        d <- d + 1
    }
    return(factors)
}

factorization(1)
factorization(2)
factorization(3)
factorization(4)
factorization(5)
factorization(6)
factorization(7)
factorization(8)
factorization(9)
factorization(10)
factorization(20)
factorization(60)
factorization(100)
factorization(13431)   
factorization(1343113431)   
factorization(134311343113431)   




library(gapminder)
data(gapminder)

g <- gapminder

c <- unique(as.character(gapminder$country))
c <- tolower(unique(as.character(gapminder$country)))

c <- "Canada"
c <- "morocco"
c <- "maaa"
c <- "zorocco"
c <- "Morocco"
c <- "orocco"

c <- c("Morocco", "morocco")
c <- c("Moro", "moro")

grep("([a-z]).*\\1", c("Moro", "moro"), value=TRUE)
grep("([a-z]).*\\1", c("Moro", "moro"), value=TRUE, perl=TRUE)

grep("([[:lower:]]).*\\1", c("Moro", "moro"), value=TRUE)

c <- c("Moro", "moro", "aMoro", "AxMoro")

grep("([a-z]).*\\1", c, value=TRUE)


grep("([[:lower:]]).*\\1", c, value=TRUE)


grep("([a-z]).*\\1.*\\1+", c, value=TRUE)
grep("([[:lower:]]).*\\1.*\\1+", c, value=TRUE)

grep("([[:lower:]]).*\\1", c("$^$^", "qrer"), value=TRUE)

grep("([[:lower:]]).*\\1.*\\1", c("$^$^", "Rodfdodfdo", "Morocco", "morocco"), value=TRUE)


gsub("([a-z]).*\\1.*\\1+", "X", c)


grep("[a-z]", c("", "a", "A", "$%"), value = TRUE)





## github issue about accesing non-scalar nodes in NFs, as model[['nodeName']]

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    for(i in 1:5) {
        b[i] ~ dnorm(0, 1)
        for(j in 1:3)
            c[i,j] ~ dnorm(0, 1)
    }
})
constants <- list()
data <- list()
inits <- list(a = 0, b=rep(1,5), c = array(2, c(5,3)))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

nfDef <- nimbleFunction(
    setup = function(model) {
        aNode <- 'a'
        bNode <- 'b'
        cNode <- 'c'
    },
    run = function() {
        a <- model[[aNode]]
        b <- model[[bNode]]
        c <- model[[cNode]]
        print('a = ', a)
        print('b = ', b)
        print('c = ', c)
    }
)

Rnf <- nfDef(Rmodel)

Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(Rnf, project = Rmodel)

Rnf$run()

## a = 0
## b = 1 1 1 1 1
## c =      [,1] [,2] [,3]
## [1,]    2    2    2
## [2,]    2    2    2
## [3,]    2    2    2
## [4,]    2    2    2
## [5,]    2    2    2

Cnf$run()

## a = 0
## b = 1
## 1
## 1
## 1
## 1
## c = 2 2 2
## 2 2 2
## 2 2 2
## 2 2 2
## 2 2 2
## NULL




code <- nimbleCode({
    a ~ dnorm(0, 1)
    for(i in 1:5) {
        b[i] ~ dnorm(0, 1)
        for(j in 1:3)
            c[i,j] ~ dnorm(0, 1)
    }
})
constants <- list()
data <- list()
inits <- list(a = 0, b=rep(1,5), c = array(2, c(5,3)))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

nfDef <- nimbleFunction(
    setup = function(model) { },
    run = function() {
        a <- model[['a']] 
        b <- model[['b']] 
        c <- model[['c']] 
        print('a = ', a)
        print('b = ', b)
        print('c = ', c)
    }
)

Rnf <- nfDef(Rmodel)

Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(Rnf, project = Rmodel)

Rnf$run()

## a = 0
## b = 1 1 1 1 1
## c =      [,1] [,2] [,3]
## [1,]    2    2    2
## [2,]    2    2    2
## [3,]    2    2    2
## [4,]    2    2    2
## [5,]    2    2    2

Cnf$run()

## a = 0
## b = 1
## c = 2
## NULL





## github issue: copy() doesn't work from compiled model to compiled MCMC

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)


Cmcmc$mvSaved
## CmodelValues object with variables: a, logProb_a.

nimCopy(from = Rmodel, to = Rmcmc$mvSaved, row = 1)  ## works

nimCopy(from = Cmodel, to = Cmcmc$mvSaved, row = 1)  ## FAILS




## saving samples from restarting NIMBLE MCMC vignette

save(samples, modelVariables, samplerStates, file = '~/temp/restart.Rmd')
load('~/temp/restart.Rmd')

samplerStates
modelVariables
tail(samples)


Cmodel$beta
modelVariables$beta
Cmodel$beta <- modelVariables$beta

head(samples_continued)

Cmodel$mu <- modelVariables$mu
Cmodel$sigma <- modelVariables$sigma
Cmodel$epsilon <- modelVariables$epsilon



qqq

library(nimble)
load('~/temp/restart.Rmd')

code <- nimbleCode({
    mu ~ dnorm(0, 0.0001)
    sigma ~ dunif(0, 1000)
    for(i in 1:2) {
        beta[i] ~ dnorm(0, 0.0001)
    }
    for(i in 1:N) {
        epsilon[i] ~ dnorm(mu, sd = sigma)
        log(lambda[i]) <- beta[1] + beta[2] * x[i] + epsilon[i]
        y[i] ~ dpois(lambda[i])
    }
})
set.seed(0)
N <- 5
x <- rnorm(N, 0, 2)
lambda <- exp(4 + .3*x)
y <- rpois(N, lambda)
constants <- list(N = N, x = x)
data <- list(y = y)
inits <- list(mu = 0, sigma = 1, beta = rep(0,2), epsilon = rep(0, N))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
conf <- configureMCMC(Rmodel)
conf$removeSamplers("sigma")
conf$addSampler("sigma", "slice")
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Rmodel$beta
modelVariables$beta
Rmodel$beta <- modelVariables$beta
Rmodel$beta
Rmodel$mu <- modelVariables$mu
Rmodel$sigma <- modelVariables$sigma
Rmodel$beta <- modelVariables$beta
Rmodel$epsilon <- modelVariables$epsilon
Rmodel$calculate()
for(i in 2:8) {
    Rmcmc$samplerFunctions[[i]]$scale <- samplerStates[[i]]$scale
    Rmcmc$samplerFunctions[[i]]$timesRan <- samplerStates[[i]]$timesRan
    Rmcmc$samplerFunctions[[i]]$timesAccepted <- samplerStates[[i]]$timesAccepted
    Rmcmc$samplerFunctions[[i]]$timesAdapted <- samplerStates[[i]]$timesAdapted
    Rmcmc$samplerFunctions[[i]]$gamma1 <- samplerStates[[i]]$gamma1
}
Rmcmc$samplerFunctions[[9]]$width <- samplerStates[[9]]$width
Rmcmc$samplerFunctions[[9]]$timesRan <- samplerStates[[9]]$timesRan
Rmcmc$samplerFunctions[[9]]$timesAdapted <- samplerStates[[9]]$timesAdapted
Rmcmc$samplerFunctions[[9]]$sumJumps <- samplerStates[[9]]$sumJumps

## new line!
nimCopy(from = Rmodel, to = Rmcmc$mvSaved, row = 1, logProb = TRUE)
Rmcmc$mvSaved$mu
Rmcmc$mvSaved$epsilon

Cmcmc$mvSaved, row = 1, logProb = TRUE)
Cmcmc$mvSaved[['mu']]
Cmcmc$mvSaved[['logProb_epsilon']]
Cmcmc$mvSaved
nimCopy(from = Cmodel, to = Cmcmc$mvSaved, row = 1, logProb = TRUE)
Rmcmc$mvSaved$mu
Rmcmc$mvSaved$epsilon


load('~/temp/restartMCMC.RData')

for(name in names(Cmcmc$mvSaved$sizes)) {
print(name)
Cmcmc$mvSaved[[name]] <- Cmodel[[name]]
}

Cmodel[['beta']]
Cmodel[['logProb_beta']]
Cmodel$calculate()

Cmcmc$mvSaved[['beta']]
Cmcmc$mvSaved[['logProb_beta']]
Cmcmc$mvSaved[['logProb_beta']] <- Cmodel[['logProb_beta']]
Cmcmc$mvSaved[['logProb_beta']]


getsize(Rmcmc$mvSamples)
resize(Rmcmc$mvSamples, 0)
getsize(Rmcmc$mvSamples)

Rmodel$beta
conf$printSamplers()

##undebug(Rmcmc$samplerFunctions$contentsList[[2]]$run)
##debug(Rmcmc$samplerFunctions$contentsList[[2]]$run)
debug(Rmcmc$samplerFunctions$contentsList[[2]]$run)
debug(Rmcmc$samplerFunctions$contentsList[[3]]$run)

set.seed(0)
Rmcmc$run(2, reset = FALSE)

set.seed(0)
Rmcmc$run(20, reset = FALSE)
samples_continued <- as.matrix(Rmcmc$mvSamples)

head(samples_continued)

Rmodel$mu
Rmodel$beta
Rmodel$epsilon
Rmodel$y

Rmodel$getNodeNames()
Rmodel$nodes[['beta[1]']]
ls(Rmodel$nodes)
Rmodel$nodes$beta_L4_UID_6$calculate
ls(Rmodel$nodesFunctions)

conf$printSamplers()
i <- 3

Rmcmc$samplerFunctions$contentsList[[i]]$target
Rmcmc$samplerFunctions$contentsList[[i]]$scale
Rmcmc$samplerFunctions$contentsList[[i]]$gamma1
Rmcmc$samplerFunctions$contentsList[[i]]$timesAdapted
Rmcmc$samplerFunctions$contentsList[[i]]$run

## example of extracting MCMC summaries for Ben Letcher

code <- nimbleCode({
    a ~ dnorm(0, 1)
    for(i in 1:10)
        lengthExp[i] ~ dnorm(0, 1)
    for(i in 1:2)
        for(j in 1:3)
            for(k in 1:4)
                A[i,j,k] <- i + 10*j + 100*k
})
constants <- list()
data <- list()
inits <- list(a = 0, lengthExp = rep(0, 10))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

samples <- nimbleMCMC(model = Rmodel, nchains = 3, summary = TRUE, monitors = c('a', 'lengthExp', 'A'))

## extract the posterior median for all parameters from the summary array
posteriorMedian <- samples$summary$all.chains[, 'Median']

## extract just those parameters beginning with 'lengthExp'
lengthExp <- unname(posteriorMedian[grep('lengthExp', names(posteriorMedian), value = TRUE)])
lengthExp

A <- array(posteriorMedian[grep('A', names(posteriorMedian), value = TRUE)], c(2,3,4))
A








## working on "linear reproductive function" in NIMBLE,
## for Floraine Plard's revision of the IPM manuscript

old: log(Rlambda[1:Ndatarec]) <- Rp[1] + Rp[2]*RX[1:Ndatarec] + Rp[3]*RCOV[1:Ndatarec] + Rp[4]*RX[1:Ndatarec]*RCOV[1:Ndatarec]
new:     Rlambda[1:Ndatarec]  <- Rp[1] + Rp[2]*RX[1:Ndatarec] + Rp[3]*RCOV[1:Ndatarec] + Rp[4]*RX[1:Ndatarec]*RCOV[1:Ndatarec]

old: Rdata[1:Ndatarec] ~  dRdataPois(Rlambda[1:Ndatarec], RdataLength)
new: Rdata[1:Ndatarec] ~ T(ddatanorm(Rlambda[1:Ndatarec], Tau_R, RdataLength), 0, 100)


ddataNorm <- nimbleFunction(
    run = function(x = double(1), mean = double(1), tau = double(), length = integer(), log.p = double()) {
        sd <- 1/sqrt(tau)
        ll <- 0
        for(i in 1:length) {
            if(mean[i] < 0) {
                if(log.p) return(-Inf) else return(0)
            }
            ll <- ll + dnorm(x[i], mean[i], sd=sd, log=TRUE)
        }
        returnType(double())
        if(log.p) return(ll) else return(exp(ll))
    }
)

rdataNorm <- nimbleFunction(
    run = function(n = integer(), mean = double(1), tau = double(), length = integer()) {
        print('this should never run')
        ##x <- numeric(length)
        declare(x, double(1, length))
        returnType(double(1))
        return(x)
    }
)

registerDistributions(list(
    ddataNorm = list(
        BUGSdist = 'ddataNorm(mean, tau, length)',
        types    = c('value = double(1)', 'mean = double(1)', 'tau = double()', 'length = integer()')
    )
))

##pdataNorm <- nimbleFunction(
##    run = function(q = double(), mean = double(1), tau = double(), length = integer(), lower.tail = integer(0, default = 1), log.p = integer(0, default = 0)) {
##        returnType(double())
##        sd <- 1/sqrt(tau)
##        lp <- 0
##        for(i in 1:length) {
##            lp <- lp + pnorm(x[i], mean[i], sd=sd, log=TRUE)
##        }
##        
##        if(!lower.tail) {
##            logp <- -rate * q
##            if(log.p) return(logp) else return(exp(logp))
##        } else {
##            p <- 1 - exp(-rate * q)
##            if(!log.p) return(p) else return(log(p))
##        }
##    }
##)
## 
##qdataNorm <- nimbleFunction(
##    run = function(p = double(), mean = double(1), tau = double(), length = integer(), lower.tail = integer(0, default = 1), log.p = integer(0, default = 0)) {
##        if(log.p) p <- exp(p)
##        if(!lower.tail) p <- 1 - p
##        returnType(double())
##        return(-log(1 - p) / rate)
##    }
##)




## David Pleydell's problem with using rinvgamma()

remove.packages('nimble')
install.packages('nimble')

library(nimble)

test4 <- nimbleCode({
    hyperSh <- hyperShape
    hyperSc <- hyperScale
    Shp ~ dinvgamma(shape=hyperSh, scale=hyperSc)
})

Inits <- list(hyperShape=1.0, hyperScale=1.0, Shp=1.0)

mod4 <- nimbleModel(test4, inits=Inits)

mod4$Shp

simNodes <- mod4$getDependencies(c("hyperShape","hyperScale"), downstream=TRUE, self=FALSE)
simNodes

mod4$simulate(nodes=simNodes)

mod4$Shp

## testing use of log=TRUE in using dexp() in a nimbleFunction

library(nimble)

nfDef <- nimbleFunction(
    setup = function() {},
    run = function(x = double(), lambda = double()) {
        lp <- dexp(x, lambda, log=TRUE)
        returnType(double())
        return(lp)
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

x <- 4
lambda = 1/3
Rnf$run(x=x, lambda=lambda)
Cnf$run(x=x, lambda=lambda)

dexp(x, rate=lambda, log=TRUE)




## testing HMC (NUTS) sampler
## one dimension at a time
## R execution (un-compiled) only!
## build and run on branch hmcAD
library(nimble)
library(numDeriv)
nimbleOptions(experimentalEnableDerivs = TRUE)

## model option #1
code <- nimbleCode({
    a1 ~ dnorm(0, 1)
    y1 ~ dnorm(a1, 1)
    a2 ~ dnorm(0, 1)
    y2 ~ dnorm(a2, 1)
})
constants <- list()
data <- list(y1 = 2, y2 = 2)
inits <- list(a1 = 0, a2 = 0)

## model option #2
code <- nimbleCode({
    a1 ~ dnorm(0, 1)
    y1[1] ~ dnorm(a1, 1)
    y1[2] ~ dnorm(a1^2, 1)
    a2 ~ dnorm(0, 1)
    y2[1] ~ dnorm(a2, 1)
    y2[2] ~ dnorm(a2^2, 1)
})
constants <- list()
data <- list(y1 = c(2, 4), y2 = c(2, 4))
inits <- list(a1 = 0, a2 = 0)

## model option #3
code <- nimbleCode({
    a1 ~ dnorm(0, sd=10)
    y1 ~ dnorm(a1^2, sd=1)
    a2 ~ dnorm(0, sd=10)
    y2 ~ dnorm(a2^2, sd=1)
})
constants <- list()
data <- list(y1 = 2, y2 = 2)
inits <- list(a1 = 0, a2 = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('a1', 'HMC')
##conf$addSampler('a2', 'slice')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

debug(Rmcmc$samplerFunctions$contentsList[[1]]$run)
debug(Rmcmc$samplerFunctions$contentsList[[1]]$buildtree)
##debug(Rmcmc$samplerFunctions$contentsList[[1]]$logH)
##debug(Rmcmc$samplerFunctions$contentsList[[1]]$gradient)
##debug(Rmcmc$samplerFunctions$contentsList[[1]]$leapfrog)
##debug(Rmcmc$samplerFunctions$contentsList[[1]]$initializeEpsilon)

set.seed(0); Rmcmc$run(5)


n <- 5000
set.seed(0)
system.time(Rmcmc$run(n))

samples <- as.matrix(Rmcmc$mvSamples)
samples

samplesSummary(samples)
samplesPlot(samples)

library(coda); apply(samples, 2, effectiveSize)



## testing langevin sampler performance on litters model
library(nimble)
library(numDeriv)
nimbleOptions(experimentalEnableDerivs = TRUE)

load('~/github/hybridBlockSamplers/data/model_litters.RData')

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)



conf <- configureMCMC(Rmodel, nodes = 'p')
conf$addSampler(c('a[1]','b[1]'), 'langevin')
conf$addSampler(c('a[2]','b[2]'), 'langevin')

conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)

Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
n <- 50000
system.time(Cmcmc$run(n, time = TRUE))
timing <- Cmcmc$getTimes()
names(timing) <- lapply(conf$getSamplers(), function(x) x$name)
timing
samples <- as.matrix(Cmcmc$mvSamples)
samplesSummary(samples[, c('a[1]','a[2]','b[1]','b[2]')])

            Mean       Median      St.Dev.  95%CI_low   95%CI_upp
a[1] 1510.715273 1276.0661682 1047.9102473 91.6910587 4194.675441
a[2]    3.388279    2.7441406    2.3218974  0.8759159    9.858006
b[1]  171.251570  147.4867210  113.4322513 10.6720555  457.704183
b[2]    1.056575    0.8790824    0.6601463  0.3226588    2.842398


library(coda); ess <- apply(samples, 2, effectiveSize)
ess


## testing langevin sampler - on two dimensions!
## R execution (un-compiled) only!
## build and run on branch hmcAD
library(nimble)
library(numDeriv)
nimbleOptions(experimentalEnableDerivs = TRUE)

code <- nimbleCode({
    a1 ~ dnorm(0, 1)
    b1 ~ dgamma(1, 1)
    y1 ~ dnorm(a1+b1, 1)
    a2 ~ dnorm(0, 1)
    b2 ~ dgamma(1, 1)
    y2 ~ dnorm(a2+b2, 1)
})
constants <- list()
data <- list(y1 = 2, y2 = 2)
inits <- list(a1 = 0, b1 = 1, a2 = 0, b2 = 1)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel, nodes = c('a1', 'b1'))
conf$addSampler(c('a2','b2'), 'langevin')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)

Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
n <- 50000
system.time(Cmcmc$run(n, time = TRUE))
timing <- Cmcmc$getTimes()
names(timing) <- lapply(conf$getSamplers(), function(x) x$name)
timing
samples <- as.matrix(Cmcmc$mvSamples)
samplesSummary(samples[, c('a1','a2')])
samplesPlot(samples, c('a1','a2'))
samplesSummary(samples[, c('b1','b2')])
samplesPlot(samples, c('b1','b2'))
library(coda); ess <- apply(samples, 2, effectiveSize)
ess



##n <- 2000
##set.seed(0)
##system.time(Rmcmc$run(n))
##samples <- as.matrix(Rmcmc$mvSamples)
## 
##Rmcmc$samplerFunctions$contentsList[[2]]$scale
##Rmcmc$samplerFunctions$contentsList[[3]]$scaleVec
##Rmcmc$samplerFunctions$contentsList[[3]]$epsilonVec




## testing langevin sampler - one dimension at a time
## R execution (un-compiled) only!
## build and run on branch hmcAD
library(nimble)
library(numDeriv)
nimbleOptions(experimentalEnableDerivs = TRUE)

## model option #1
code <- nimbleCode({
    a1 ~ dnorm(0, 1)
    y1 ~ dnorm(a1, 1)
    a2 ~ dnorm(0, 1)
    y2 ~ dnorm(a2, 1)
})
constants <- list()
data <- list(y1 = 2, y2 = 2)
inits <- list(a1 = 0, a2 = 0)

## model option #2
code <- nimbleCode({
    a1 ~ dnorm(0, 1)
    y1[1] ~ dnorm(a1, 1)
    y1[2] ~ dnorm(a1^2, 1)
    a2 ~ dnorm(0, 1)
    y2[1] ~ dnorm(a2, 1)
    y2[2] ~ dnorm(a2^2, 1)
})
constants <- list()
data <- list(y1 = c(2, 4), y2 = c(2, 4))
inits <- list(a1 = 0, a2 = 0)

## model option #3
code <- nimbleCode({
    a1 ~ dnorm(0, sd=10)
    y1 ~ dnorm(a1^2, sd=1)
    a2 ~ dnorm(0, sd=10)
    y2 ~ dnorm(a2^2, sd=1)
})
constants <- list()
data <- list(y1 = 2, y2 = 2)
inits <- list(a1 = 0, a2 = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('a1', 'slice')
conf$addSampler('a2', 'langevin')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)

##options(error = recover)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
n <- 50000
system.time(Cmcmc$run(n, time = TRUE))
timing <- Cmcmc$getTimes()
names(timing) <- lapply(conf$getSamplers(), function(x) x$name)
timing
samples <- as.matrix(Cmcmc$mvSamples)
samplesSummary(samples)
samplesPlot(samples)
library(coda); ess <- apply(samples, 2, effectiveSize)
ess
eff <- ess / timing; names(eff) <- names(timing)
eff

samples <- runMCMC(Cmcmc, 10000, nchains = 1)
samplesPlot(samples)

samplesList <- runMCMC(Cmcmc, 10000, nchains = 3)
chainsPlot(samplesList)

##n <- 5000
##set.seed(0)
##system.time(Rmcmc$run(n))
##samples <- as.matrix(Rmcmc$mvSamples)
##Rmcmc$samplerFunctions$contentsList[[1]]$scale
##Rmcmc$samplerFunctions$contentsList[[2]]$scaleVec
##Rmcmc$samplerFunctions$contentsList[[2]]$epsilonVec

## minimally reproducible example of error with dexp() distribution
library(nimble)
nimbleOptions(experimentalEnableDerivs = TRUE)

code <- nimbleCode({
    a ~ dgamma(1, 1)
})
Rmodel <- nimbleModel(code, inits = list(a = 1))
Rmodel$calculate()

Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)



## minimally reproducible example of error when compiling langevin sampler:

library(nimble)
library(numDeriv)
nimbleOptions(experimentalEnableDerivs = TRUE)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    y ~ dnorm(a, 1)
})
constants <- list()
data <- list(y = 2)
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler('a', 'langevin')
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

##compiling... this may take a minute. On some systems there may be some compiler warnings that can be safely ignored.
##Error in as.vector(x, "character") : 
##  cannot coerce type 'environment' to vector of type 'character'


## testing derivatives

library(nimble)
library(numDeriv)
nimbleOptions(experimentalEnableDerivs = TRUE)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    y[1] ~ dnorm(a, 1)
    y[2] ~ dnorm(a^2, 1)
})
constants <- list()
data <- list(y = c(2, 4))
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
xs <- seq(-4, 4, by=0.01)
ys <- numeric()
for(i in seq_along(xs)) {
    Rmodel$a <- xs[i]
    ys[i] <- Rmodel$calculate()
}
plot(xs, ys, type = 'l')

Rmodel$a <- 2
Rmodel$calculate()

nodes <- Rmodel$getDependencies('a')
nodes

f <- function(x) {
    Rmodel$a <- x
    dOut <- nimDerivs(Rmodel$calculate(), order=1, wrt='a')
    ##dOut <- nimDerivs(Rmodel$calculate(nodes), order=1, wrt='a')
    d <- dOut$gradient[1,1]
    return(d)
}

ds <- numeric()
for(i in seq_along(xs)) {
    x <- xs[i]
    ds[i] <- f(x)
}

plot(xs, ys, type = 'l', ylim = range(c(ys,ds)))
lines(xs, ds, col = 'blue')

nfDef <- nimbleFunction(
    setup = function(model, node) {
        ##calcNodes <- model$getDependencies(node)
    },
    run = function(x = double()) {
        model[[node]] <<- x
        derivsOutput <- derivs(model$calculate(), order = 1, wrt = node)
        ##derivsOutput <- derivs(model$calculate(calcNodes), order = 1, wrt = node)
        d <- derivsOutput$gradient[1,1]
        returnType(double(0))
        return(d)
    }
)

Rnf <- nfDef(Rmodel, 'a')

Rnf$run(-3)
Rnf$run(-1)
Rnf$run(0)
Rnf$run(1)
Rnf$run(2)

ds <- numeric()
for(i in seq_along(xs)) {
    x <- xs[i]
    ##ds[i] <- f(x)
    ds[i] <- Rnf$run(x)
}

plot(xs, ys, type = 'l', ylim = range(c(ys,ds)))
lines(xs, ds, col = 'blue')



## solving problem from Micah:
## x^7 - 7 roots
roots <- 7^(1/7) * exp((0:6)/7 * 2*pi*1i)

out <- numeric()
c <- 1
for(i in 1:7) {
    for(j in i:7) {
        out[c] <- roots[i] + roots[j]
        c <- c + 1
    }
}

p <- prod(out)
p/7^1
p/7^2
p/7^3
p/7^4
p/7^4/2^7
2^7 * 7^4
p




## Mark Risser's code, problem running on LBNL supercomputer??

library(nimble)
library(methods)
M <- 237 # Sample size

nim_code <- nimbleCode({
    ## likelihood
    for(i in 1:M){
        z[i] ~ dbinom(size = N, prob = expit(tau*logit_p[i] + mu))
        logit_p[i] ~ dnorm( mean = 0, sd = 1 )
    }
    ## hyperparameters
    mu ~ dnorm(0, sd = 100)
    tau ~ dunif(0, 100)
})

nim_model <- nimbleModel(
    code = nim_code, constants = list(M = M),
    inits = list(mu = 0, tau = 1, logit_p = rep(0,M) ), 
    data = list(z = rbinom(M, 100, 0.1), N = 100)
)

Rmodel <- nim_model

calculate(Rmodel)
Rmodel$mu
Rmodel$tau
Rmodel$logit_p
Rmodel$z

Cmodel <- compileNimble(Rmodel)

calculate(Cmodel)
Cmodel$mu
Cmodel$tau
Cmodel$logit_p
Cmodel$z



## birthdays problem for STAT101

## 2 students, probability of different birthdays:
364/365 * 100

## 3 students, probability of having different birthdays:
364/365 * 363/365 * 100

## 4 students, probability of having different birthdays:
364/365 * 363/365 * 362/365 * 100

## n students, probability of having different birthdays:
n <- 50
prod(seq(364, by = -1, length = n-1)) / 365^(n-1) * 100

## complement: at least 2 students have the same birthday:
100 - prod(seq(364, by = -1, length = n-1)) / 365^(n-1) * 100








## running through all tests of runMCMC() and nimbleMCMC()

library(nimble)

code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:10)
        x[i] ~ dnorm(mu, sd = sigma)
})
data <- list(x = c(2, 5, 3, 4, 1, 0, 1, 3, 5, 3))
initsFunction <- function() list(mu = rnorm(1,0,1), sigma = runif(1,0,10))

## execute one MCMC chain, monitoring the "mu" and "sigma" variables,
## with thinning interval 10.  fix the random number seed for reproducible
## results.  by default, only returns posterior samples.
mcmc.out <- nimbleMCMC(code = code, data = data, inits = initsFunction,
                       monitors = c("mu", "sigma"), thin = 10,
                       niter = 20000, nchains = 1, setSeed = TRUE)

class(mcmc.out)
str(mcmc.out)
head(mcmc.out)

## note that the inits argument to nimbleModel must be a list of
## initial values, whereas nimbleMCMC can accept inits as a function
## for generating new initial values for each chain.
initsList <- initsFunction()
Rmodel <- nimbleModel(code, data = data, inits = initsList)

## using the existing Rmodel object, execute three MCMC chains with 
## specified burn-in.  return samples, summary statistics, and WAIC.
mcmc.out <- nimbleMCMC(model = Rmodel,
                       niter = 20000, nchains = 3, nburnin = 2000,
                       summary = TRUE, WAIC = TRUE)

class(mcmc.out)
names(mcmc.out)
str(mcmc.out)
mcmc.out$summary
mcmc.out$WAIC

## run ten chains, generating random initial values for each
## chain using the inits function specified above.
## only return summary statistics from each chain; not all the samples.
mcmc.out <- nimbleMCMC(model = Rmodel, nchains = 10, inits = initsFunction,
                       samples = FALSE, summary = TRUE)

class(mcmc.out)
names(mcmc.out)
mcmc.out


Rmodel$calculate()
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)


## run a single chain, and return a matrix of samples
mcmc.out <- runMCMC(Cmcmc)

class(mcmc.out)
head(mcmc.out)


mcmc.out <- runMCMC(Cmcmc, WAIC = TRUE)

class(mcmc.out)
names(mcmc.out)
head(mcmc.out$samples)
mcmc.out$WAIC


## run three chains of 10,000 samples, discard a burn-in of 1,000,
## and return of list of sample matrices
mcmc.out <- runMCMC(Cmcmc, niter = 10000, nburnin = 1000, nchains = 3)

class(mcmc.out)
names(mcmc.out)
lapply(mcmc.out, head)



## run three chains, returning posterior samples, summary statistics,
## and the WAIC value for each chain
mcmc.out <- runMCMC(Cmcmc, nchains = 3, summary = TRUE, WAIC = TRUE)

class(mcmc.out)
names(mcmc.out)
lapply(mcmc.out$samples, head)
mcmc.out$summary
mcmc.out$WAIC


## run two chains, and specify the initial values for each
initsList <- list(list(mu = 1, sigma = 1),
                  list(mu = 2, sigma = 10))
mcmc.out <- runMCMC(Cmcmc, nchains = 2, inits = initsList)

class(mcmc.out)
names(mcmc.out)
lapply(mcmc.out, head)


## run ten chains of 100,000 iterations each, using a function to 
## generate initial values and a fixed random number seed for each chain.
## only return summary statistics from each chain; not all the samples.
initsFunction <- function()
    list(mu = rnorm(1,0,1), sigma = runif(1,0,100))

mcmc.out <- runMCMC(Cmcmc, niter = 100000, nchains = 10,
                    inits = initsFunction, setSeed = TRUE,
                    samples = FALSE, summary = TRUE)

mcmc.out

mcmc.out <- runMCMC(Cmcmc, niter = 100000, nchains = 10,
                    inits = initsFunction, setSeed = TRUE,
                    samples = FALSE, WAIC = TRUE)

mcmc.out


## nimble-dev error message email from Luiz F Carvalho

##The following code is carbon copy of the example ZIP code.

set.seed(666)
library(nimble)

Clustercode <- nimbleCode({
    R0 ~ dbeta(2, 2)
    omega ~ dunif(0.1, 10)
    for(i in 1:N)
        y[i] ~ dR0(r = R0, w = omega) ## Note NIMBLE allows R-like named-parameter syntax
})

dR0 <- nimbleFunction(
    run = function(x = integer(), r = double(), w = double(), log = logical(0, default = 0)) {
        returnType(double())
        c1 = lgamma(w*x + x-1)
        c2 = lgamma(w*x)
        c3 = lgamma(x+1)
        c4 = (x-1) * (log(r) - log(w))
        c5 = (w*x + x -1) * log(1 + r/w)
        dens = c1 - (c2+c3) + (c4-c5)
        if(log) {
            ans <- dens
        } else ans <- exp(dens)
        return(ans)
    })

rR0 <- nimbleFunction(
    run = function(n = integer(), r = double(), w = double()){
        returnType(integer())
        ##UpperBound = 1e4
        ys <- numeric(1e4)
        for(i in 1:1e4) ys[i] <- i
        ##ys <- 1:UpperBound
        c1 <- lgamma(w*ys  + ys -1)
        c2 <- lgamma(w*ys )
        c3 <- lgamma(ys +1)
        c4 <- (ys -1) * (log(r) - log(w))
        c5 <- (w*ys  + ys  -1) * log(1 + r/w)
        Ps <- exp( c1 - (c2+c3) + (c4-c5) )
        return(sample(ys, 1, prob = Ps))
        ##return(1)
    })

registerDistributions(list(
    dR0 = list(
        BUGSdist = "dR0(r, w)",
        discrete = TRUE,
        range = c(1, Inf),
        types = c('value = integer()', 'r = double()', 'w = double()')
    )))

Clustermodel <- nimbleModel(Clustercode, constants = list(N = 100), check = FALSE, inits = list(omega=0.5, R0=0.8))
Clustermodel <- nimbleModel(Clustercode, constants = list(N = 100), inits = list(omega=0.5, R0=0.8))

Clustermodel$y

##Then doing

Clustermodel$omega <- .5  ## Choose values of R0 and omega
Clustermodel$R0 <- .8
Clustermodel$simulate('y')       ## Simulate values of y[1]...y[100]
simulatedData <- Clustermodel$y
simulatedData
calculate(Clustermodel)

##produces nice output
##[49]  1 69  1  1  1  3  1  2  1  8  1  1  3  4  1  2 12  1  1 10  1 11  1  1  2  7  1  3  1  1 45 11  3  3  1  5  1  3 30  1  2  7  1  1  1  1  1  1
##[97]  3  1  2  3
##But then

Clustermodel$setData(list(y = simulatedData))  ## Set those values as data in the model
cClustermodel <- compileNimble(Clustermodel, showCompilerOutput = TRUE)       ## Compile the model

##compiling... this may take a minute. On some systems there may be some compiler warnings that can be safely ignored.
##Error in code$args[[1]]$type == "nimbleList" : 
##  comparison (1) is possible only for atomic and list types
##I cant rule out the possibility of there being a silly typo somewhere, but I think the error message is also not very helpful.
## 
##session info
## 
##R version 3.4.2 (2017-09-28)
##Platform: x86_64-pc-linux-gnu (64-bit)
##Running under: Ubuntu 16.04.3 LTS
## 
##Matrix products: default
##BLAS: /usr/lib/libblas/libblas.so.3.6.0
##LAPACK: /usr/lib/lapack/liblapack.so.3.6.0
## 
##locale:
## [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C               LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8     LC_MONETARY=en_GB.UTF-8   
## [6] LC_MESSAGES=en_GB.UTF-8    LC_PAPER=en_GB.UTF-8       LC_NAME=C                  LC_ADDRESS=C               LC_TELEPHONE=C            
##[11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C       
## 
##attached base packages:
##[1] stats     graphics  grDevices utils     datasets  methods   base     
## 
##other attached packages:
##[1] nimble_0.6-6
## 
##loaded via a namespace (and not attached):
##[1] compiler_3.4.2   magrittr_1.5     tools_3.4.2      igraph_1.1.0     coda_0.19-1      codetools_0.2-15 grid_3.4.2       pkgconfig_2.0.1 
##[9] lattice_0.20-35 
##Thanks


## test case of dcar_proper()

1

library(nimble)

build(browser)

code <- nimbleCode({
    mu0 ~ dnorm(0, 0.0001)
    tau ~ dgamma(0.001, 0.001)
    gamma ~ dunif(-1, 1)
    s[1:N] ~ dcar_proper(mu[1:N], adj=adj[1:L], num=num[1:N], tau=tau, gamma=gamma)
    for(i in 1:N) {
        mu[i] <- mu0
        logit(p[i]) <- s[i]
        y[i] ~ dbern(p[i])
    }
})

adj <- c(2, 1, 3, 2, 4, 3)
num <- c(1, 2, 2, 1)
constants <- list(adj = adj, num = num, N = 4, L = 6)
data <- list(y = c(1, 0, 1, 1))
inits <- list(mu0 = 0, tau = 1, gamma = 0, s = rep(0, 4))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)

Rmcmc <- buildMCMC(conf)

set.seed(0)
Rmcmc$run(1)




## testing expansion of vars / node names for use in samplesPlot() and/or chainsPlot()

target <- 'a'
target <- 'b'
target <- 'sigm'
nodeNames <- c(letters, paste0('b[', 1:5, ']'), 'bbb', 'b.sigma', 'aaa', 'a.tau.')
l <- length(nodeNames)

samples <- array(1, dim=c(2,l))
colnames(samples) <- nodeNames
samples

var <- c('a', 'b', 'd', 'dfdfd')

unlist(lapply(var, function(n) grep(paste0('^', n,'(\\[.+\\])?$'), colnames(samples), value=TRUE)))


grep(paste0('^', target,'(\\[.+\\])?$'), nodeNames, value=TRUE)


## testing new chainsPlot() function

set.seed(0)
s <- list()
nchains <- 3
nparams <- 10
nparamnames <- 20
for(i in 1:nchains) {
    samples <- matrix(rnorm(100*nparams), ncol=nparams)
    colnames(samples) <- sort(sample(c(paste0('b[', 1:nparamnames, ']'), 'beta', letters), nparams))
    s[[i]] <- samples
}
names(s) <- paste0('chain', 1:nchains)
samplesList <- s

nrows=3; width=7; height=min(1+3*nrows, 10); legend=!is.null(names(samplesList)); legend.location='topright'; jitter.factor=1; buffer.right=0; buffer.left=0


chainsPlot(samplesList, buffer.right = 0.7, nrows=1)
chainsPlot(samplesList, buffer.right = 2, nrows=1, cex=0.8, jitter=3)
chainsPlot(samplesList, buffer.right = 1, height=5, nrows=2, cex = 0.7, var=c('b',letters))
chainsPlot(samplesList, buffer.right = 1, height=5, nrows=2, cex = 0.7)
chainsPlot(samplesList, buffer.right = 0.7, nrows=3)

samplesPlot(samplesList[[1]])
samplesPlot(samplesList[[1]], file = '~/temp/FILEX.pdf')
samplesPlot(samplesList[[1]], c('b', 'beta'))

1

##dimnames(summary)
##dim(summary)
##summary
##sampleSummaries <- lapply(samplesList, function(s) rbind(
##    mean = apply(s, 2, mean),
##    low  = apply(s, 2, function(x) quantile(x, 0.025)),
##    upp  = apply(s, 2, function(x) quantile(x, 0.975))))


pdf.f(f, file=file.path(save.dir, "../figures/allparams.pdf"), height=10, width=8.5)



## testing samplers used by JAGS for non-conjugate Wishart nodes

##remove.packages('rjags')
##install.packages('rjags')
library(rjags)
library(nimble)


code <- quote({
    x[1:3, 1:3] ~ dwish(A[1:3, 1:3], 5)
    y[1] ~ dnorm(x[1,1], 1)
    y[2] ~ dnorm(x[1,2], 3)
})
constants <- list(A = diag(3))
data <- list(y = 1:2)
inits <- list(x = diag(3))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$getDistribution('x[1:3, 1:3]')
Rmodel$calculate()
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

niter <- 100
monitorVars <- c('x')
constsAndData <- c(constants, data)
modelfile <- file.path(tempdir(), 'model.txt')
writeLines(paste0('model\n', paste0(deparse(code, width.cutoff=500L), collapse='\n')), con=modelfile)
set.seed(0); jags_mod <- jags.model(file=modelfile, data=constsAndData, inits=inits, n.chains=1, quiet=FALSE, n.adapt=40)

set.seed(0); jags_out <- rjags::coda.samples(model=jags_mod, variable.names=monitorVars, n.iter=niter, thin=1)

list.samplers(jags_mod)





## trying Lauren's occupancy modelling, and my new crossLevel_binary_DT sampler
library(nimble)
setwd('~/github/occupancy/analysis/multiSpp-singleSea')

code <- ms.ss.occ
constants <- model.input$constants
data <- model.input$data
inits <- model.input$inits

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()

base.names <- c('a1', 'a2', 'a3', 'a4', 'b1', 'b2', 'u.cato', 'u.fcw', 'v.cato', 'v.fcw')
conf$removeSamplers('Z')
conf$removeSamplers(base.names)
conf$printSamplers()

conf$addSampler(target = base.names, type ='sampler_crossLevel_binary_DT', print = TRUE)
conf$printSamplers()

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 1000)

## testing my new crossLevel_binary_DT sampler

library(nimble)

binary2_virtual <- nimbleFunctionVirtual(
    run = function() { },
    methods = list(
        calcBinaryPosteriorLogDensity = function() { returnType(double()) },
        reset = function() { }
    )
)
sampler_binary2 <- nimbleFunction(
    name = 'sampler_binary2',
    contains = binary2_virtual,
    setup = function(model, mvSaved, target, control) {
        ## node list generation
        targetAsScalar <- model$expandNodeNames(target, returnScalarComponents = TRUE)
        calcNodes <- model$getDependencies(target)
        ## checks
        if(length(targetAsScalar) > 1)  stop('cannot use binary sampler on more than one target node')
        if(!model$isBinary(target))     stop('can only use binary sampler on discrete 0/1 (binary) nodes')
    },
    run = function() {
        currentLogProb <- getLogProb(model, calcNodes)
        model[[target]] <<- 1 - model[[target]]
        otherLogProb <- calculate(model, calcNodes)
        acceptanceProb <- 1/(exp(currentLogProb - otherLogProb) + 1)
        if(!is.nan(acceptanceProb) & runif(1,0,1) < acceptanceProb)
            nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
        else
            nimCopy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE)
    },
    methods = list(
        calcBinaryPosteriorLogDensity = function() {
            currentValue <- model[[target]]
            currentLogProb <- getLogProb(model, calcNodes)
            model[[target]] <<- 1 - currentValue
            otherLogProb <- calculate(model, calcNodes)
            model[[target]] <<- currentValue
            calculate(model, calcNodes)
            lp <- -log(exp(otherLogProb - currentLogProb) + 1)
            returnType(double(0))
            return(lp)
        },
        reset = function() { }
    )
)
sampler_crossLevel_binary_DT <- nimbleFunction(
    name = 'sampler_crossLevel_binary_DT',
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        ## control list extraction
        adaptive <- if(!is.null(control$adaptive)) control$adaptive else TRUE
        ## node list generation
        target       <- model$expandNodeNames(target)
        lowNodes     <- model$getDependencies(target, self = FALSE, stochOnly = TRUE, includeData = FALSE)
        lowCalcNodes <- model$getDependencies(lowNodes)
        calcNodes    <- model$getDependencies(c(target, lowNodes))
        ## nested function and function list definitions
        mvInternal <- modelValues(model)
        topRWblockSamplerFunction <- sampler_RW_block(model, mvInternal, target, control)
        lowBinarySamplerFunctions <- nimbleFunctionList(binary2_virtual)
        for(iLN in seq_along(lowNodes)) {
            lowNode <- lowNodes[iLN]
            if(!model$isBinary(lowNode))     stop('non-binary lowNode \'', lowNode, '\' in crossLevel_binary_DT sampler')
            lowBinarySamplerFunctions[[iLN]] <- sampler_binary2(model, mvSaved, lowNode, control)
        }
        my_setAndCalculateTop <- setAndCalculate(model, target)
        my_decideAndJump <- decideAndJump(model, mvSaved, calcNodes)
    },
    run = function() {
        modelLP0 <- getLogProb(model, calcNodes)
        propLP0 <- 0
        for(iSF in seq_along(lowBinarySamplerFunctions))  { propLP0 <- propLP0 + lowBinarySamplerFunctions[[iSF]]$calcBinaryPosteriorLogDensity() }
        propValueVector <- topRWblockSamplerFunction$generateProposalVector()
        topLP <- my_setAndCalculateTop$run(propValueVector)
        if(is.na(topLP))
            jump <- my_decideAndJump$run(-Inf, 0, 0, 0)
        else {
            for(iSF in seq_along(lowBinarySamplerFunctions))
                lowBinarySamplerFunctions[[iSF]]$run()
            modelLP1 <- calculate(model, calcNodes)
            propLP1 <- 0
            for(iSF in seq_along(lowBinarySamplerFunctions))
                propLP1 <- propLP1 + lowBinarySamplerFunctions[[iSF]]$calcBinaryPosteriorLogDensity()
            jump <- my_decideAndJump$run(modelLP1, modelLP0, propLP1, propLP0)
    	}
        if(adaptive)     topRWblockSamplerFunction$adaptiveProcedure(jump)
    },
    methods = list(
        reset = function() {
            topRWblockSamplerFunction$reset()
            for(iSF in seq_along(lowBinarySamplerFunctions)) {
                lowBinarySamplerFunctions[[iSF]]$reset()
            }
        }
    )
)

code <- nimbleCode({
    for(i in 1:2) {
        a[i] ~ dunif(0, 1)
        b[i] ~ dunif(0, 1)
        x[i, 1] ~ dbern(a[i])
        x[i, 2] ~ dbern(sqrt(b[i]))
        x[i, 3] ~ dbern(a[i] * b[i])
        y[i, 1] ~ dnorm(x[i, 1], sd = 1)
        y[i, 2] ~ dnorm(x[i, 2], sd = 2)
        y[i, 3] ~ dnorm(x[i, 3], sd = 3)
    }
})

constants <- list()
y <- c(1, 2, 3)
ymatrix <- rbind(y, y)
data <- list(y = ymatrix)
x <- c(1, 0, 0)
xmatrix <- rbind(x, x)
inits <- list(a = rep(1/2,2), b = rep(1/2,2), x = xmatrix)

Rmodel <- nimbleModel(code, constants, data, inits)
calculate(Rmodel)

conf <- configureMCMC(Rmodel)
conf$getMonitors()
conf$printSamplers()
conf$removeSamplers(c('a[2]', 'b[2]', 'x[2, 1]', 'x[2, 2]', 'x[2, 3]'))
conf$printSamplers()
conf$addSampler(c('a[2]', 'b[2]'), type = sampler_crossLevel_binary_DT)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 100000)

colnames(samples)
apply(samples, 2, mean)

samplesPlot(samples, c('a[1]', 'a[2]'))
samplesPlot(samples, c('b[1]', 'b[2]'))


## Perry's example spatial capture-recapture model
## maybe an idea for ISEC

tr<-seq(15,85, length=10)
X<-cbind(rep(tr,each=length(tr)),rep(tr,times=length(tr))) # 100 coord. traps
plot(X, xlim=c(0,100), ylim=c(0,100), pch=3, cex=0.75)

set.seed(10)
xlim <- c(0,100); ylim <- c(0,100)      # Area 100*100=1e4
A <- (xlim[2]-xlim[1])*(ylim[2]-ylim[1])/10000
mu <- 50                 # Density
N <- rpois(1, mu*A) ;N   # Generate population
## 50

s <- cbind(runif(N, xlim[1], xlim[2]), runif(N, ylim[1], ylim[2]))
points(s, pch=16, col=2)

sigma <- 5
lambda0 <- 0.4
J <- nrow(X)
K <- 5
yy <- array(NA, c(N, J, K))

for(j in 1:J) {
    dist <- sqrt((X[j,1]-s[,1])^2 + (X[j,2]-s[,2])^2)
    lambda <- lambda0*exp(-dist^2/(2*sigma^2))
    for(k in 1:K) {
        yy[,j,k] <- rpois(N, lambda)
    }
}

n <- apply(yy, c(2,3), sum)

## Plot capture events
tot<-apply(n, 1,sum)
symbols(X, circles=tot, inches=F, bg="#00000022", add=TRUE)
points(X, pch=3, cex=0.75); points(s, pch=16, col=2)

## Case 1
## Model in BUGS, as written, using NIMBLE
## First Iâ€™ll run in NIMBLE â€˜out of the boxâ€™.

library(nimble)

## DT's questions:
## - lack of identifiability between M (possible) individuals?
## - missing sqrt() on the dist term in model code

code <- nimbleCode({
    sigma ~ dunif(0,10)
    lam0 ~ dunif(0,5)
    psi ~ dbeta(1,1)
    for(i in 1:M) {
        z[i] ~ dbern(psi)
        s[i,1] ~ dunif(xlim[1], xlim[2])
        s[i,2] ~ dunif(ylim[1], ylim[2])
        for(j in 1:J) {# Number of traps
            dist[i,j] <- (s[i,1] - X[j,1])^2 + (s[i,2] - X[j,2])^2
            lam[i,j] <- lam0*exp(-dist[i,j]/(2*sigma^2))*z[i]
        }
    }
    for(j in 1:J){
        bigLambda[j] <- sum(lam[1:M,j])
        for(k in 1:K) {
            n[j,k] ~ dpois(bigLambda[j])
        }
    }
    N <- sum(z[1:M])
})


M<-200

constants <- list(M = M, K=K, J=J)
n1<-apply(n,1,sum)
data<-list(n=n, X=X, xlim=xlim, ylim=ylim)
s<-cbind(runif(M, xlim[1], xlim[2]), runif(M, ylim[1], ylim[2]))
z<-rep(1,M)
inits <- list (sigma=0.5, lam0=0.1, s=s, z=z)

##Rmodel <- nimbleModel(code=code, constants=constants, data=data, inits=inits)
Rmodel <- nimbleModel(code=code, constants=constants, data=data, inits=inits, check=FALSE) ## check=FALSE is faster
##calculate(Rmodel)

mcmcspec<-configureMCMC(Rmodel, print=TRUE, monitors = c("N", "lam0", "psi", "sigma"))
scrMCMC <- buildMCMC(mcmcspec)
Cmodel <- compileNimble(Rmodel) 
CscrMCMC <- compileNimble(scrMCMC, project = Rmodel)

## It's pretty slow to run so I just want an execution time first for a small sample
t1_100 <- system.time(CscrMCMC$run(100))
t1_100

## And now I want a decent sample
t1_20k <- system.time(CscrMCMC$run(20000))
t1_20k_samples <- as.matrix(CscrMCMC$mvSamples)
save(t1_100, t1_20k, t1_20k_samples, file = "case1results.Rdata")






## testing whether / why nimbleOptions() now prints the result??

library(nimble)

nimbleOptions('allowDynamicIndexing')
nimbleOptions(allowDynamicIndexing = FALSE)
nimbleOptions(allowDynamicIndexing = TRUE)
nimbleOptions('allowDynamicIndexing')


## making the distributions_implementations example for dcar_proper()

library(nimble)
##dcar_proper(mu, C, adj, num, M, tau, gamma, evs)

x <- c(1, 3, 3, 4)
mu <- rep(3, 4)
adj <- c(2, 1,3, 2,4, 3)
num <- c(1, 2, 2, 1)

## omitting C and M uses all weights = 1
dcar_proper(x, mu, adj=adj, num=num, tau=1, gamma=0)

## equivalent to above: specifying all weights = 1,
## then using as.carCM to generate C and M arguments
weights <- rep(1, 6)
CM <- as.carCM(adj, weights, num)
C <- CM$C
M <- CM$M
dcar_proper(x, mu, C, adj, num, M, tau=1, gamma=0)

## now using non-unit weights
weights <- c(2, 2, 3, 3, 4, 4)
as.carCM(adj, weights, num)
CM2 <- as.carCM(adj, weights, num)
C2 <- CM2$C
M2 <- CM2$M
dcar_proper(x, mu, C2, adj, num, M2, tau=1, gamma=0)





C <- c(1, 0.2, 0.8, 0.6, 0.4, 1)
M <- 1/num
tau <- 1

## note that in the case of a row-normalised C matrix, and M
## taking values 1/num, the bounds on gamma are necessarily [-1, 1].
gamma <- (carMinBound(C, adj, num, M) + carMaxBound(C, adj, num, M)) / 2

lp <- dcar_proper(x, mu, C, adj, num, M, tau, gamma)


library(nimble)

code1 <- nimbleCode({
    z ~ dnorm(0,1)
})

m1 <- nimbleModel(code1)
cm1 <- compileNimble(m1)
cm1$z
cm1$z <- 4
cm1$z

code2 =nimbleCode({
    y ~ dnorm(0,1)
})

m2 = nimbleModel(code2)
cm2 <- compileNimble(m2)

cm2$y
cm2$y <- 1
cm2$y 

cm1$z <- 3




## testing new CAR_calcM() function for dcar_proper()

library(nimble)

C <- c(1, 0.2, 0.8, 0.6, 0.4, 1)
C <- c(1,.5,.5,.5,.5,1)

adj <- c(2, 1, 3, 2, 4, 3)
num <- c(1, 2, 2, 1)
##M <- CAR_calcM(C, adj, num)
M <- rep(1, 4)
M
Cmatrix <- CAR_calcCmatrix(C, adj, num)
Cmatrix

diag(1/M) %*% Cmatrix
t(diag(1/M) %*% Cmatrix)
diag(1/M) %*% Cmatrix - t(diag(1/M) %*% Cmatrix)

t(diag(1/M) %*% Cmatrix)
t(Cmatrix) %*% diag(1/M)

Cnf <- compileNimble(CAR_calcM)
Cnf(C, adj, num)

Cmatrix <- CAR_calcCmatrix(C, adj, num)

code <- nimbleCode({
    mu0 ~ dnorm(0, 0.0001)
    tau ~ dgamma(0.001, 0.001)
    gamma ~ dunif(-1, 1)
    s[1:N] ~ dcar_proper(mu[1:N], C[1:L], adj[1:L], num[1:N], tau = tau, gamma = gamma)
    for(i in 1:N) {
        mu[i] <- mu0
        logit(p[i]) <- s[i]
        y[i] ~ dbern(p[i])
    }
})

C <- c(1, 0.2, 0.8, 0.6, 0.4, 1)
adj <- c(2, 1, 3, 2, 4, 3)
num <- c(1, 2, 2, 1)

constants <- list(C = C, adj = adj, num = num, N = 4, L = 6)

data <- list(y = c(1, 0, 1, 1))

inits <- list(mu0 = 0, tau = 1, gamma = 0, s = rep(0, 4))

Rmodel <- nimbleModel(code, constants, data, inits)


## internet use dataset for STAT101

df <- read.csv('~/Downloads/InternetUse.csv')
head(df)
names(df)
df$GDP.in.billions.of..US

## testing new functionality with zero neighbor components
## for dcar_normal and dcar_proper

library(nimble)

code <- nimbleCode({
    ##x[1:N] ~ dcar_proper(mu[1:N], C[1:L], adj[1:L], num[1:N], M[1:N], tau, gamma)
    for(i in 1:N) {
        y[1] ~ dnorm(x[1], 1)
        y[2] ~ dexp(x[2])
        y[3] ~ dnorm(x[4], 1)
        y[4] ~ dnorm(x[5], 1)
    }
})

mu <- 1:5
adj <- c(5,4)
num <- c(0,0,0,1,1)
M <- rep(1,5)
C <- CAR_calcC(adj, num, M)
bounds <- CAR_calcBounds(C, adj, num, M)
tau <- 1
gamma <- as.numeric(quantile(bounds, 0.8))
constants <- list(mu = mu, C = C, adj = adj, num = num, M = M, N = 5, L = 2, tau = tau, gamma = gamma)
data <- list(y = c(3, 6, 8, 10))
inits <- list(x = rep(1, 5))

Rmodel <- nimbleModel(code, constants, data, inits)
##Cmodel <- compileNimble(Rmodel)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$addMonitors('x')
Rmcmc <- buildMCMC(conf)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

niter <- 3
set.seed(0); Rmcmc$run(niter)


library(nimble)

code <- nimbleCode({
    x[1:N] ~ dcar_normal(adj[1:L], weights[1:L], num[1:N], tau)
    for(i in 1:N) {
        y[1] ~ dnorm(x[1], 1)
        y[2] ~ dexp(x[2])
        y[3] ~ dnorm(x[4], 1)
        y[4] ~ dnorm(x[5], 1)
    }
})

adj <- c(5,4)
weights <- c(1,1)
num <- c(0,0,0,1,1)
CAR_calcNumIslands(adj, num)
constants <- list(adj = adj, weights = weights, num = num, tau = 1, N = 5, L = 2)
data <- list(y = c(3, 6, 8, 10))
inits <- list(x = rep(1, 5))

Rmodel <- nimbleModel(code, constants, data, inits, calculate=FALSE)
Rmodel$calculate()
##Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$addMonitors('x')
Rmcmc <- buildMCMC(conf)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

niter <- 3
set.seed(0); Rmcmc$run(niter)





## STAT101 class demo on historgrams and barplots

setwd('~/Downloads')
survey <- read.csv('ClassSurvey.csv')
head(survey)
survey$Phone <- as.character(survey$Phone)
survey$Phone <- factor(survey$Phone, levels = c('iPhone', 'Android', 'other smartphone'))
levels(survey$Phone) <- factor(c('iPhone', 'Android', 'other smartphone'))

head(survey)
gender <- survey$Gender
chocolate <- survey$Chocolate
table(gender, chocolate)

x <- survey$Friends
x
mean(x)
sd(x)
length(x)

sd(x) / sqrt(96)



p <- 82/96
p
se <- sqrt(p*(1-p)/96)
se
p + c(-1,1)*3*se

survey$Phone

table(survey$Phone)


barplot(table(survey$Phone), col=3:5)

pie(table(survey$Phone), init.angle = 50, col=3:5)

pie(table(survey$Chapter1), init.angle = 50, col=2:3)

hist(survey$Minutes, xlim = c(0,20))

br <- c(4,8,20)
N <- length(br)
par(mfrow = c(N, 1))
for(i in 1:N) hist(survey$Minutes, breaks = br[i], main = paste0('breaks = ', br[i]), xlab='', ylab='', xlim=c(0,20))
par(mfrow = c(1,1))


hist(data, breaks = xxxxxx)

## bug in conjugacy??  github issue #590

library(nimble)

code <- nimbleCode({
    intercept ~ dnorm(0,1)
    a <- intercept + (X[1,1:K] %*% params[1:K])[1,1]
    y ~ dnorm(a, 1)
})
constants <- list(K = 2)

Rmodel <- nimbleModel(code, constants)

##Rmodel$checkConjugacy('intercept')
##nimble:::cc_expandDetermNodesInExpr(Rmodel, quote(a), targetNode = 'intercept')
##nimble:::checkConjugacyOneDep
##= function(model, targetNode, depNode) {
##conf <- configureMCMC(Rmodel)

library(nimble)

expr <- quote(structureExpr(w[1:3, 1], w[1:3, 2]))
targetNode <- "w[1:3, 1]"

nimble:::cc_checkLinearity(expr, t


nimble:::cc_checkLinearity(expr, targetNode)



conf <- configureMCMC(Rmodel)
conf <- configureMCMC(Rmodel, useConjugacy = FALSE)

conf$printSamplers()



## testing of nimbleMCMC()

library(nimble)

code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:10)
        x[i] ~ dnorm(mu, sd = sigma)
})
data <- list(x = c(2, 5, 3, 4, 1, 0, 1, 3, 5, 3))
inits <- function() list(mu = rnorm(1,0,1), sigma = runif(1,0,10))

nchains <- 1
nchains <- 2

## execute one MCMC chain, monitoring the "mu" 
## and "sigma" variables with thinning interval 10.
## also fix the random number seed, for reproducible results.
## by default, only returns posterior samples.
mcmc.out <- nimbleMCMC(code = code, data = data, inits = inits,
                       monitors = c("mu", "sigma"), thin = 10,
                       niter = 20000, nchains = nchains, setSeed = TRUE)

str(mcmc.out)

## note that the inits argument to nimbleModel must be a
## list of initial values, whereas nimbleMCMC can accept
## inits as a function for generating initial values
Rmodel <- nimbleModel(code, data = data, inits = inits())

## this time using the existing Rmodel object,
## execute 3 MCMC chains with specified burn-in.
## return posterior summary statistics and WAIC values,
## in addition to the posterior samples
mcmc.out <- nimbleMCMC(model = Rmodel,
                       niter = 20000, nchains = nchains, nburnin = 2000,
                       returnSummary = TRUE, returnWAIC = TRUE)

str(mcmc.out)
mcmc.out$summary
mcmc.out$waic




## working on implementing nimbleMCMC() function

cbind(
    `Mean`     = apply(samples, 2, mean),
    `Median`   = apply(samples, 2, median),
    `St.Dev.`  = apply(samples, 2, sd),
    `95% CI Lower` = apply(samples, 2, function(x) quantile(x, 0.025)),
    `95% CI Upper` = apply(samples, 2, function(x) quantile(x, 0.975))
)

library(nimble)

code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:10) {
        x[i] ~ dnorm(mu, sd = sigma)
    }
})
Rmodel <- nimbleModel(code)
Rmodel$setData(list(x = c(2, 5, 3, 4, 1, 0, 1, 3, 5, 3)))
Rmcmc <- buildMCMC(Rmodel)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
inits <- function() list(mu = rnorm(1,0,1), sigma = runif(1,0,10))
samplesList <- runMCMC(Cmcmc, niter = 10000, nchains = 3, inits = inits, returnWAIC = TRUE)
str(samplesList)

library(nimble)

code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:10) {
        x[i] ~ dnorm(mu, sd = sigma)
    }
})
data <- list(x = c(2, 5, 3, 4, 1, 0, 1, 3, 5, 3))
inits <- function() list(mu = rnorm(1,0,1), sigma = runif(1,0,10))

mcmc.output <- nimbleMCMC(code, data = data, inits = inits,
                          monitors = c("mu", "sigma"), thin = 10,
                          niter = 20000, nburnin = 1000, nchains = 3,
                          returnSummary = TRUE, returnWAIC = TRUE)

str(mcmc.output)
mcmc.output$summary
mcmc.output$waic



## experimenting with RW2 sampler for Frederick (by email)

library(nimble)

RW2 <- nimbleFunction(
    name = 'RW2',
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        ## control list extraction
        logScale      <- if(!is.null(control$log))           control$log           else FALSE
        reflective    <- if(!is.null(control$reflective))    control$reflective    else FALSE
        adaptive      <- if(!is.null(control$adaptive))      control$adaptive      else TRUE
        adaptInterval <- if(!is.null(control$adaptInterval)) control$adaptInterval else 200
        scale         <- if(!is.null(control$scale))         control$scale         else 1
        niterNoUpdateTarget  <- if(!is.null(control$niterNoUpdateTarget))  control$niterNoUpdateTarget  else 0
        niterNoAdaptation    <- if(!is.null(control$niterNoAdaptation))    control$niterNoAdaptation    else 0
        niterNoAdaptiveDecay <- if(!is.null(control$niterNoAdaptiveDecay)) control$niterNoAdaptiveDecay else 0
        ## node list generation
        targetAsScalar <- model$expandNodeNames(target, returnScalarComponents = TRUE)
        calcNodes <- model$getDependencies(target)
        ## numeric value generation
        scaleOriginal <- scale
        timesRan      <- 0
        timesAccepted <- 0
        timesAdapted  <- 0
        optimalAR     <- 0.44
        gamma1        <- 0
        ## checks
        if(length(targetAsScalar) > 1)   stop('cannot use RW sampler on more than one target; try RW_block sampler')
        if(model$isDiscrete(target))     stop('cannot use RW sampler on discrete-valued target; try slice sampler')
        if(logScale & reflective)        stop('cannot use reflective RW sampler on a log scale (i.e. with options log=TRUE and reflective=TRUE')
    },
    run = function() {
        timesRan <<- timesRan + 1
        if(timesRan > niterNoUpdateTarget) {
            currentValue <- model[[target]]
            propLogScale <- 0
            if(logScale) { propLogScale <- rnorm(1, mean = 0, sd = scale)
                           propValue <- currentValue * exp(propLogScale)
                       } else         propValue <- rnorm(1, mean = currentValue,  sd = scale)
            if(reflective) {
                lower <- model$getBound(target, 'lower')
                upper <- model$getBound(target, 'upper')
                while(propValue < lower | propValue > upper) {
                    if(propValue < lower) propValue <- 2*lower - propValue
                    if(propValue > upper) propValue <- 2*upper - propValue
                }
            }
            model[[target]] <<- propValue
            logMHR <- calculateDiff(model, calcNodes) + propLogScale
            jump <- decide(logMHR)
            if(jump) nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
            else     nimCopy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE)
            if(adaptive & timesRan > niterNoAdaptation)     adaptiveProcedure(jump)
        }
    },
    methods = list(
        adaptiveProcedure = function(jump = logical()) {
            if(jump)     timesAccepted <<- timesAccepted + 1
            if(timesRan %% adaptInterval == 0) {
                acceptanceRate <- timesAccepted / adaptInterval
                if(timesRan > niterNoAdaptiveDecay)   timesAdapted <<- timesAdapted + 1
                gamma1 <<- 1/((timesAdapted + 3)^0.8)
                gamma2 <- 10 * gamma1
                adaptFactor <- exp(gamma2 * (acceptanceRate - optimalAR))
                scale <<- scale * adaptFactor
                timesAccepted <<- 0
            }
        },
        reset = function() {
            scale <<- scaleOriginal
            timesRan      <<- 0
            timesAccepted <<- 0
            timesAdapted  <<- 0
            gamma1 <<- 0
        }
    )
)


code <- nimbleCode({
    a ~ dexp(5)
    b ~ dnorm(a, 1)
})
constants <- list()
data <- list(b = 4)
inits <- list(a = 5)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$printSamplers()
conf$addSampler(target = 'a', type = 'RW2', control = list(niterNoAdaptation = 1000))
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)






## testing how to implement ddirch-dcat conjugacy, in light of complication
## with dcat distribution being univariate, but with multivariate 'prob' parameter
## addressing github issue #559

library(nimble)

set.seed(0)
n <- 100
alpha <- c(10, 30, 15, 60, 1)
K <- length(alpha)
p <- c(.12, .24, .09, .54, .01)
y <- rmulti(1, n, p)
y <- rep(seq_along(y), times = y)
code <- nimbleCode( {
    for(i in 1:n) {
        y[i] ~ dcat(p[1:K])
    }
    p[1:K] ~ ddirch(alpha[1:K])
    for(i in 1:K) {
        alpha[i] ~ dgamma(.001, .001)
    }
    ##yy ~ dnorm(p[1], 1)
})
constants <- list(n = n, K = K)
data <- list(y = y)##, yy = 1)
inits <- list(alpha = 1:5, p = rep(0.2, 5))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()
##Rmodel$calculate('alpha')
##Rmodel$calculate('p')
##Rmodel$calculate('y')
##Rmodel$calculate('yy')

conf <- configureMCMC(Rmodel)
conf$printSamplers()

##conf$getSamplerDefinition(6)
## 
##[6] conjugate_ddirch_dcat sampler: p[1:5]
##$setup
##function (model, mvSaved, target, control) 
##{
##    calcNodes <- model$getDependencies(target)
##    calcNodesDeterm <- model$getDependencies(target, determOnly = TRUE)
##    d <- max(determineNodeIndexSizes(target))
##    dep_dcat_nodeNames <- control$dep_dcat
##    N_dep_dcat <- length(control$dep_dcat)
##    dep_dcat_nodeSizes <- sapply(dep_dcat_nodeNames, function(node) max(nimDim(model$getParam(node, 
##        "prob"))), USE.NAMES = FALSE)
##    if (length(dep_dcat_nodeSizes) == 1) 
##        dep_dcat_nodeSizes <- c(dep_dcat_nodeSizes, -1)
##    dep_dcat_nodeSizeMax <- max(dep_dcat_nodeSizes)
##    dep_dcat_values <- array(0, dim = N_dep_dcat)
##    dep_dcat_prob <- array(0, dim = c(N_dep_dcat, dep_dcat_nodeSizeMax))
##    contribution_alpha <- rep(0, length = d)
##}
##<environment: 0x7fad2133ca00>
## 
##$run
##function () 
##{
##    prior_alpha <- model$getParam(target[1], "alpha")
##    for (iDep in 1:N_dep_dcat) {
##        thisNodeSize <- dep_dcat_nodeSizes[iDep]
##        dep_dcat_values[iDep] <<- model$getParam(dep_dcat_nodeNames[iDep], 
##            "value")
##        dep_dcat_prob[iDep, 1:thisNodeSize] <<- model$getParam(dep_dcat_nodeNames[iDep], 
##            "prob")
##    }
##    contribution_alpha <<- rep(0, length = d)
##    for (iDep in 1:N_dep_dcat) {
##        thisNodeSize <- dep_dcat_nodeSizes[iDep]
##        contribution_alpha <<- contribution_alpha + calc_dcatConjugacyContributions(dep_dcat_prob[iDep, 
##            1:thisNodeSize], dep_dcat_values[iDep])
##    }
##    newValue <- rdirch(1, alpha = prior_alpha + contribution_alpha)
##    model[[target]] <<- newValue
##    calculate(model, calcNodes)
##    nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, 
##        logProb = TRUE)
##}
##<environment: 0x7fad2133ca00>

Rmcmc <- buildMCMC(conf)

##lst <- compileNimble(Rmodel, Rmcmc)
##lst

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

set.seed(0)
samplesList <- runMCMC(Cmcmc, 10000, nchains = 3, returnCodaMCMC = TRUE, returnWAIC = TRUE)
str(samplesList)
samplesList$waic

Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
Cmcmc$calculateWAIC()


colnames(samples)
apply(samples, 2, mean)

samplesPlot(samples)

library(coda)
apply(samples, 2, effectiveSize)

class(samplesList)
traceplot(samplesList, ask = TRUE, param = c('alpha[2]'))
traceplot(samplesList[[1]])


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()


    

## testing unneeded variables created in conjugate samplers?
## github issue #557

library(nimble)

nimbleOptions('verifyConjugatePosteriors')
nimbleOptions(verifyConjugatePosteriors = TRUE)
nimbleOptions('verifyConjugatePosteriors')

code <- nimbleCode({
    x ~ dbeta(3, 13)
    y[1] ~ dbin(x, 10)
    y[2] ~ dbin(x, 20)
})

constants <- list()
data <- list(y = c(3,4))
inits <- list(x = 0.5)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()

conf$getSamplerDefinition(1)

[1] conjugate_dbeta_dbin sampler: x
$setup
function (model, mvSaved, target, control) 
{
    calcNodes <- model$getDependencies(target)
    calcNodesDeterm <- model$getDependencies(target, determOnly = TRUE)
    dep_dbin_nodeNames <- control$dep_dbin
    N_dep_dbin <- length(control$dep_dbin)
    dep_dbin_values <- array(0, dim = N_dep_dbin)
    dep_dbin_prob <- array(0, dim = N_dep_dbin)
    dep_dbin_size <- array(0, dim = N_dep_dbin)
    contribution_shape1 <- 0
    contribution_shape2 <- 0
}
<environment: 0x7f84b7cf04a8>

$run
function () 
{
    prior_shape1 <- model$getParam(target[1], "shape1")
    prior_shape2 <- model$getParam(target[1], "shape2")
    for (iDep in 1:N_dep_dbin) {
        dep_dbin_values[iDep] <<- model$getParam(dep_dbin_nodeNames[iDep], 
            "value")
        dep_dbin_prob[iDep] <<- model$getParam(dep_dbin_nodeNames[iDep], 
            "prob")
        dep_dbin_size[iDep] <<- model$getParam(dep_dbin_nodeNames[iDep], 
            "size")
    }
    contribution_shape1 <<- 0
    contribution_shape2 <<- 0
    for (iDep in 1:N_dep_dbin) {
        contribution_shape1 <<- contribution_shape1 + dep_dbin_values[iDep]
        contribution_shape2 <<- contribution_shape2 + (dep_dbin_size[iDep] - 
            dep_dbin_values[iDep])
    }
    newValue <- rbeta(1, shape1 = prior_shape1 + contribution_shape1, 
        shape2 = prior_shape2 + contribution_shape2)
    model[[target]] <<- newValue
    calculate(model, calcNodes)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, 
        logProb = TRUE)
}
<environment: 0x7f84b7cf04a8>

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)
    

## testing sd() and var() functions in DSL

library(nimble)

nfDef <- nimbleFunction(
    setup = function() {
        a <- 1:10
    },
    run = function() {
        s <- sd(a)
        print(s)
        v <- var(a)
        print(v)
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf, dirName = '~/temp/tempcpp')#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()





# here is the script:
                                        # Imports
library(dplyr)
library(tibble)
library(coda)
library(nimble)

##library(gProfileR)

##library(devtools)
##devtools::install_github('nimble-dev/gprofiler')
##library(gprofiler)

# Subsample data during profiling.
subsample <- TRUE

# Set user-specific file paths.
data_dir <- '.'
out_dir <- '.'
cpp_dir <- out_dir
setwd('~/temp')
nimbleOptions(showCompilerOutput = TRUE)

# getting data ready ------------------------------------------------------------------------------
# Observed tone data
tone_data <- read.csv(file.path(data_dir, 'tone-data.csv')) 
if (subsample) {
    tone_data <- tone_data[sample(1:nrow(tone_data), 5000, replace=FALSE),]
}
bad_eas <- tone_data %>%
    group_by(ea) %>%
    filter(length(unique(neg_count)) == 1) %>%
    filter(neg_count == 0) %>%
    ungroup(ea) %>%
    distinct(ea) %>%
    mutate(ea = as.character(ea))
bad_eas <- bad_eas$ea
# taking out bad eas and recomputing ea and src ids.
tone_data <- tone_data %>%
    filter(!is.element(ea, bad_eas)) %>%
    mutate(ea=as.factor(as.character(ea)),
           src=as.factor(as.character(src))) %>%
    mutate(src_id=as.numeric(src),
           ea_id=as.numeric(ea))
glimpse(tone_data)
# Number non-missing values:
colSums(!is.na(tone_data))

# Selection model data
selection_vars <- c("BtoM", "logMV", "RetVol", "ESurp", "NrAnalyst",
                    "PriRet", "InstHold", "SToneEA", "NrEAs")

selection_data <- read.csv(file.path(data_dir, 'selection-model-data.csv')) %>%
    filter(!is.element(ea, bad_eas)) %>%
    filter(is.element(as.character(ea), as.character(tone_data$ea))) %>% # eas that are also in tone
    filter(is.element(as.character(src), as.character(tone_data$src))) %>%
    mutate(ea=as.factor(as.character(ea)),
           src=as.factor(as.character(src))) %>%
    mutate(src_id=as.numeric(src),
           ea_id=as.numeric(ea))
# center all continuous variables to help the mcmc a algorithm fight serial dependence between variables
selection_data[, selection_vars] <- scale(selection_data[, selection_vars])
selection_data <- as.data.frame(selection_data)
if (subsample) {
    selection_data <- selection_data[sample(1:nrow(selection_data), 10000, replace=FALSE),]
}
glimpse(selection_data)
# Number non-missing values:
colSums(!is.na(selection_data))
#' **IMPORTANT: Check this every time. Both datasets should not have missing values**
#' This is important so that the news source ids in both datasets match
#' Always check whether src ids align

# The Model ----------------------------------------------------------------------------------
code <- nimbleCode({
    # sampling distribution
    for (n in 1:SELMOD_N) {
        # coverage decision model
        # Icov is 1 if news source i wrote an article about an earnings announcement j
        # and 0 if not
        Icov[n] ~  dbern(prob=prob_cov[n]);
        probit(prob_cov[n])  <- b_0 +
            b_1 * scaled_theta_n[ sel_src[n] ] + b_2 * scaled_theta_c[ sel_src[n] ] +
            b_3 * RetVol[n] +
            b_4 * BtoM[n] + b_5 * BtoM[n] * scaled_theta_n[ sel_src[n] ] +
            b_6 * logMV[n] + b_7 * logMV[n] * scaled_theta_c[ sel_src[n] ] +
            b_8 * InstHold[n] + b_9 * InstHold[n] * scaled_theta_c[ sel_src[n] ] +
            b_10 * PriRet[n] +
            b_11 * ESurp[n] + b_12 * ESurp[n] * scaled_theta_n[ sel_src[n] ] +
            b_13 * SToneEA[n] + b_14 * SToneEA[n] * scaled_theta_n[ sel_src[n] ] +
            b_15 * NrAnalyst[n] + b_16 * NrAnalyst[n] * scaled_theta_c[ sel_src[n] ] +
            b_17 * NrEAs[n];
        }
    for (s in 1:TONE_N) {
        # writing dimensions for actually written posts
        # This is explaining the amount of negatively connoted words in a news post
        neg[s] ~ dbin(prob=prob_neg[s], size=wrd_cnt[s]);
        probit(prob_neg[s]) <- a_n[ ea[s] ] + theta_n[ tone_src[s] ];
        # This is explaining the amount of complex financial words in a news post
        cplx[s] ~ dbin(prob=prob_cplx[s], size=wrd_cnt[s]);
        probit(prob_cplx[s]) <- a_c[ ea[s] ] + theta_c[ tone_src[s] ];
        }
    for (i in 1:SOURCES){
        # These are the latent random effects of prime interest:
        #  (also creating demeaned, scaled version for the regression to make coefficient
        #   better interpretable)
        # _n is for the negativity/optimism dimension
        scaled_theta_n[i] <- (theta_n[i] - u_n_i) / sigma_n_i;
        theta_n[i] ~ dnorm(mean=u_n_i, sd=sigma_n_i);
        # _c is for the complexity/financial sophistication dimension
        scaled_theta_c[i] <- (theta_c[i] - u_c_i) / sigma_c_i;
        theta_c[i] ~ dnorm(mean=u_c_i, sd=sigma_c_i);
        }
    for (j in 1:EAS) {
        # these are earnings announcement specific effects
        # fixed effects specification
        a_n[j] ~ dnorm(0, sd=100);
        a_c[j] ~ dnorm(0, sd=100);
        }
    # priors
    # coverage coefficients
    b_0  ~ dnorm(mean=0, sd=100);
    b_1  ~ dnorm(mean=0, sd=100);
    b_2  ~ dnorm(mean=0, sd=100);
    b_3  ~ dnorm(mean=0, sd=100);
    b_4  ~ dnorm(mean=0, sd=100);
    b_5  ~ dnorm(mean=0, sd=100);
    b_6  ~ dnorm(mean=0, sd=100);
    b_7  ~ dnorm(mean=0, sd=100);
    b_8  ~ dnorm(mean=0, sd=100);
    b_9  ~ dnorm(mean=0, sd=100);
    b_10 ~ dnorm(mean=0, sd=100);
    b_11 ~ dnorm(mean=0, sd=100);
    b_12 ~ dnorm(mean=0, sd=100);
    b_13 ~ dnorm(mean=0, sd=100);
    b_14 ~ dnorm(mean=0, sd=100);
    b_15 ~ dnorm(mean=0, sd=100);
    b_16 ~ dnorm(mean=0, sd=100);
    b_17 ~ dnorm(mean=0, sd=100);
    # target audience random effects
    u_n_i     <- 0;
    sigma_n_i ~ dunif(0, 2);
    u_c_i     <- 0;
    sigma_c_i ~ dunif(0, 2);
    # monitoring variables
    sd_an <- sd(a_n[1:EAS]);
    sd_ac <- sd(a_c[1:EAS]);
    })

input_data <- list(neg     = tone_data$neg_count,
                   cplx    = tone_data$cmplx_count,
                   Icov    = selection_data$covered)
str(input_data)

constants <- list(SELMOD_N= length(selection_data$covered),
                  TONE_N  = length(tone_data$neg_count),
                  SOURCES = max(tone_data$src_id),
                  EAS     = max(tone_data$ea_id),
                  ea      = tone_data$ea_id,
                  sel_src = selection_data$src_id,
                  tone_src= tone_data$src_id,
                  SToneEA = selection_data$SToneEA,
                  BtoM    = selection_data$BtoM,
                  logMV   = selection_data$logMV,
                  RetVol  = selection_data$RetVol,
                  ESurp   = selection_data$ESurp,
                  NrAnalyst= selection_data$NrAnalyst,
                  NrEAs   = selection_data$NrEAs,
                  PriRet  = selection_data$PriRet,
                  InstHold=selection_data$InstHold,
                  wrd_cnt = tone_data$total_words)
str(constants)

initsFunction <- function(){
    list(
          b_0=0, b_1=0, b_2=0, b_3=0, b_4=0, b_5=0,
          b_6=0, b_7=0, b_8=0, b_9=0, b_10=0, b_11=0,
          b_12=0, b_13=0, b_14=0, b_15=0, b_16=0, b_17=0,

          sigma_n_i=0.1,
          theta_n  =rnorm(constants$SOURCES, 0, 1),
          a_n      =rnorm(constants$EAS, 0, 1),
          sigma_c_i=0.01,
          theta_c  =rnorm(constants$SOURCES, 0, 1),
          a_c      =rnorm(constants$EAS, 0, 1)
          )
}
str(initsFunction())
str(initsFunction())

Model <- nimbleModel(
    name = 'gmbh',
    code = code,
    data = input_data,
    constants = constants,
    inits = initsFunction(),
    calculate = TRUE
)

Model$calculate()

# Running Model -----------------------------------------------------------------------------------

#' ## Compile and Run the Model
# MCMC configuration (choice of samplers)
model_parameters <- c('sigma_n_i', "sd_an",
                      'sigma_c_i', "sd_ac",
                      "b_0", "b_1", "b_2", 'b_3', 'b_4',
                      'b_5', 'b_6', 'b_7', 'b_8',
                      'b_9', 'b_10', 'b_11', 'b_12',
                      'b_13', 'b_14', 'b_15', 'b_16', 'b_17')

monitors <- c(model_parameters, c('theta_n', 'theta_c'))
monitors

conf <- configureMCMC(Model, monitors  = monitors)
conf$printMonitors()
conf$printSamplers()

mcmc_built <- buildMCMC(conf) # MCMC algorithm in R

# Compile model and MCMC.
cm <- compileNimble(Model, dirName = cpp_dir, resetFunctions = TRUE)
cm$calculate()

cmcmc <- compileNimble(mcmc_built, dirName = cpp_dir, project = Model, resetFunctions = TRUE)

if (subsample) {
    nIts <- 150
    burn <- 50
} else {
    nIts <- 15000
    burn <- 5000
}


# FIXME This freezes on second call.
samplesList <- runMCMC(cmcmc, niter = nIts, nburnin = burn, nchains = 2, inits = initsFunction, returnCodaMCMC = TRUE)

str(samplesList)
i <- 2
dim(samplesList[[i]])


                                        # FIXME This freezes on second call.
samplesList <- profile(
    runMCMC(
        cmcmc,
        niter = nIts,
        nburnin = burn,
        nchains = 2,
        inits = initsFunction,
        returnCodaMCMC = TRUE
    ),
    filename = file.path('temp', 'runMCMC.prof')
)




## testing autoBlock:
library(nimble)

code <- nimbleCode({
    a[1] ~ dnorm(0, 1)
    a[2] ~ dnorm(a[1]^2, 1)
    a[3] ~ dnorm(a[1]^3 + a[2]^2, 1)
    y[1] ~ dnorm(a[1], 1)
    y[2] ~ dnorm(a[2], 1)
    y[3] ~ dnorm(a[3], 1)
})
constants <- list()
data <- list(y = 1:3)
inits <- list(a = (1:3)/10)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)

conf <- configureMCMC(Rmodel, autoBlock = TRUE)

conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)
##Cmcmc$run(10000)
##samples <- as.matrix(Cmcmc$mvSamples)

colnames(samples)
apply(samples, 2, mean)

samplesPlot(samples)

library(coda)
apply(samples, 2, effectiveSize)


nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)#, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()






## testing compilation of HMC sampler:
## also can do testing of langevin sampler
library(nimble)
nimbleOptions('experimentalEnableDerivs')

nimbleOptions(experimentalEnableDerivs = TRUE)
nimbleOptions('experimentalEnableDerivs')

code <- nimbleCode({
    a[1] ~ dnorm(0, 1)
    a[2] ~ dnorm(a[1]^2, 1)
    a[3] ~ dnorm(a[1]^3 + a[2]^2, 1)
    y[1] ~ dnorm(a[1], 1)
    y[2] ~ dnorm(a[2], 1)
    y[3] ~ dnorm(a[3], 1)
})
constants <- list()
data <- list(y = 1:3)
inits <- list(a = (1:3)/10)
##
Rmodel <- nimbleModel(code, constants, data, inits)
##
conf <- configureMCMC(Rmodel, nodes = NULL)
conf$printSamplers()

conf$addSampler(c('a'), 'langevin')
conf$addSampler(c('a'), 'HMC')

conf$printSamplers()
##
Rmcmc <- buildMCMC(conf)

debug(Rmcmc$samplerFunctions$contentsList[[1]]$run)
set.seed(0); Rmcmc$run(3)

##
Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)
##
Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

niter <- 20
set.seed(0); Rmcmc$run(niter)
set.seed(0); Cmcmc$run(niter)

Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)

sampleNames <- colnames(Rsamples)

Rsamples[, sampleNames] - Csamples[, sampleNames]

colnames(samples)
apply(samples, 2, mean)



## making github issue about step(inprod(..., ...))
## failing at compilation

library(nimble)

nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        a <- rep(1, 5)
        b <- rep(-12, 5)
        ##c <- step(inprod(a, b))
        c1 <- inprod(a, b)
        c <- step(c1)
        returnType(double())
        return(c)
    }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf, showCompilerOutput = TRUE)

Rnf$run()
Cnf$run()




## looks like recursion like this is ok:

library(nimble)

nfDef <- nimbleFunction(
    setup = function() {},
    run = function(a = double()) {
        print('in run, a = ', a)
        m(a)
        print('back in run, a = ', a)
        ##if(a > 5) c <- 1 else c <- 0
        ##print('in run, c = ', c)
        ##d <- step(a-10)
        ##print('in run, d = ', d)
    },
    methods = list(
        m = function(a = double()) {
            print('in m, a = ', a)
            a <- a + 10
            print('in m, now a = ', a)
        }
    )
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf, showCompilerOutput = TRUE)

a <- 1
Rnf$run(a)
Cnf$run(a)


## test of WAIC:
{
    library(nimble)
    code <- nimbleCode({
        x ~ dgamma(1, 1)       # should satisfy 'gamma' conjugacy class
        a  ~ dnorm(0, x)     # should satisfy 'norm' conjugacy class
        a2 ~ dnorm(0, tau = 3*x+0)
        b  ~ dpois(0+5*x)
        b2 ~ dpois(1*x*1)
        c ~ dgamma(1, 7*x*5)
        ##for(i in 1:3) {
        ##    xa[i] ~ dnorm(0, 1)
        ##    ya[i] ~ dnorm(xa[i], 1)
        ##}
        for(i in 2:3) {
            jTau[i] <- 1
            jNorm[i] ~ dnorm(c * (a+3) - i, var = jTau[i])
            kTauSd[i] <- 2
            kLogNorm[i] ~ dlnorm(0 - a - 6*i, kTauSd[i])
        }
        z[1:3] ~ dmnorm(mu0[1:3], prec = ident[1:3,1:3])
        mu_y2[1:2] <- asCol(A[1:2]) + B[1:2,1:3] %*% asCol(z[1:3])
        mu_y3[1:3] <- asCol(A[1:3]) + B[1:3,1:3] %*% asCol(z[1:3])
        mu_y5[1:5] <- asCol(A[1:5]) + B[1:5,1:3] %*% asCol(z[1:3])
        y2[1:2] ~ dmnorm(mu_y2[1:2], prec = prec_y[1:2,1:2])
        y3[1:3] ~ dmnorm(mu_y3[1:3], prec = prec_y[1:3,1:3])
        y5[1:5] ~ dmnorm(mu_y5[1:5], prec = prec_y[1:5,1:5])
    })
    mu0 <- rep(0,3)
    ident <- diag(3)
    A <- 11:15
    B <- matrix(1:15, nrow=5, ncol=3, byrow=TRUE)
    prec_y <- diag(1:5)
    constants <- list(mu0=mu0, ident=ident, A=A, B=B, prec_y=prec_y)
    data <- list(y2=1:2, y3=1:3, y5=1:5)##, ya = 1:3)
    inits <- list(a = 0, z=rep(0,3))
    Rmodel <- nimbleModel(code, constants, data, inits)
    conf <- configureMCMC(Rmodel)
    Rmcmc <- buildMCMC(conf)
    Cmodel <- compileNimble(Rmodel)
    Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
    ##niter <- 20
    ##Rmcmc$run(niter)
    ##as.matrix(Rmcmc$mvSamples)
    ##Rmcmc$calculateWAIC()
    niter <- 10000
    set.seed(0)
    Cmcmc$run(niter)
    waic <- Cmcmc$calculateWAIC(burnIn = 2000)
    print(waic)
    ## 20.46023
}



## making a nimble package "test" for dcar_proper:

library(nimble)
source(system.file(file.path('tests', 'test_utils.R'), package = 'nimble'))
context("Testing of default MCMC")
pRwarnLevel <- options('warn')$warn
options(warn = -1)
nimbleVerboseSetting <- nimbleOptions('verbose')

code <- nimbleCode({
    tau ~ dgamma(0.001, 0.001)
    gamma ~ dunif(gMin, gMax)
    x[1:N] ~ dcar_proper(mu[1:N], C[1:L], adj[1:L], num[1:N], M[1:N], tau, gamma)
    for(i in 1:N) {
        y[1] ~ dnorm(x[1], 1)
        y[2] ~ dnorm(3*x[2] + 5, 10)
        y[3] ~ dnorm(x[3]^2, 1)
        y[4] ~ dnorm(x[4]^2, 10)
    }
})

mu <- 1:4
adj <- c(2, 1, 3, 2, 4, 3)
num <- c(1, 2, 2, 1)
M <- 1:4
C <- CAR_calcC(adj, num, M)
bounds <- CAR_calcBounds(C, adj, num, M)
tau <- 1
gamma <- as.numeric(quantile(bounds, 0.8))
constants <- list(mu = mu, C = C, adj = adj, num = num, M = M, N = 4, L = 6, gMin = bounds[1], gMax = bounds[2])
data <- list(y = c(3, 6, 8, 10))
inits <- list(tau = tau, gamma = gamma, x = rep(0, 4))

Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)

expect_equal(calculate(Rmodel), -557.7978,    ## CHANGE
             tol = 1E-5,
             info = 'calculate for dcar_proper()')

expect_equal(calculate(Cmodel), -557.7978,    ## CHANGE
             tol = 1E-5,
             info = 'calculate for dcar_proper(), compiled')

conf <- configureMCMC(Rmodel)
conf$addMonitors('x')
Rmcmc <- buildMCMC(conf)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

niter <- 20
set.seed(0); Rmcmc$run(niter)
set.seed(0); Cmcmc$run(niter)

Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)

sampleNames <- colnames(Rsamples)

expect_true(all(Rsamples[, sampleNames] - Csamples[, sampleNames] == 0),
            info = 'agreement between R and C sampling of dcar_proper')

expect_equal(as.numeric(Csamples[20, sampleNames]),   ## CHANGE
             c(0.3957632, 0.4388424, 1.5625223, 0.2071691, 2.6513380, -3.1753125),
             tolerance = 1e-6,
             info = 'exact sample values for dcar_proper')


## now with the correct eigen values!!!!

Rmodel$calculate()
Cmodel$calculate()

Rmodel$lifted_CAR_calcEVs_oPC_oB1to6_cB_comma_adj_oB1to6_cB_comma_num_oB1to4_cB_cP
Cmodel$lifted_CAR_calcEVs_oPC_oB1to6_cB_comma_adj_oB1to6_cB_comma_num_oB1to4_cB_cP

Cmatrix <- CAR_calcCmatrix(C, adj, num)
Cmatrix
##evs <- Rmodel$lifted_CAR_calcEVs_oPC_oB1to6_cB_comma_adj_oB1to6_cB_comma_num_oB1to4_cB_cP
evs <- eigen(Cmatrix)$values
evs

Rmodel$lifted_CAR_calcEVs_oPC_oB1to6_cB_comma_adj_oB1to6_cB_comma_num_oB1to4_cB_cP <- evs
Cmodel$lifted_CAR_calcEVs_oPC_oB1to6_cB_comma_adj_oB1to6_cB_comma_num_oB1to4_cB_cP <- evs
Rmodel$lifted_CAR_calcEVs_oPC_oB1to6_cB_comma_adj_oB1to6_cB_comma_num_oB1to4_cB_cP
Cmodel$lifted_CAR_calcEVs_oPC_oB1to6_cB_comma_adj_oB1to6_cB_comma_num_oB1to4_cB_cP

Rmodel$calculate()
Cmodel$calculate()
Rmodel$calculate()
Cmodel$calculate()

evs
gamma
gamma*evs
1 - gamma*evs
log(1 - gamma*evs)
sum(log(1 - gamma*evs))


## making the "Example" for dcar_proper for the User Manual:


library(nimble)

code <- nimbleCode({
    mu0 ~ dnorm(0, 0.0001)
    tau ~ dgamma(0.001, 0.001)
    gamma ~ dunif(gMin, gMax)
    s[1:N] ~ dcar_proper(mu[1:N], C[1:L], adj[1:L], num[1:N], M[1:N], tau, gamma)
    for(i in 1:N) {
        mu[i] <- mu0
        logit(p[i]) <- s[i]
        y[i] ~ dbern(p[i])
    }
})

adj <- c(2, 1, 3, 2, 4, 3)
num <- c(1, 2, 2, 1)
M <- rep(1, 4)
C <- CAR_calcC(adj, num, M)
bounds <- CAR_calcBounds(C, adj, num, M)

constants <- list(C = C, adj = adj, num = num, M = M, N = 4, L = 6,
                  gMin = bounds[1], gMax = bounds[2])

data <- list(y = c(1, 0, 1, 1))

inits <- list(mu0 = 0, tau = 1, gamma = mean(bounds), s = rep(0, 4))

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$addMonitors('s')
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0); samples <- runMCMC(Cmcmc, 10000)
##Cmcmc$run(10000)
##samples <- as.matrix(Cmcmc$mvSamples)

colnames(samples)
apply(samples, 2, mean)

b <- 5000
samplesPlot(samples, 1, burnin=b)
samplesPlot(samples, 2, burnin=b)
samplesPlot(samples, 3:6, burnin=b)
samplesPlot(samples, 7, burnin=b)

library(coda)
apply(samples, 2, effectiveSize)





## my first example using dcar_proper:

mu <- 1:4
C <- c(1, .5,.5, .5,.5, 1)
adj <- c(2, 1,3, 2,4, 3)
num <- c(1, 2, 2, 1)
M <- c(2, 1, 1, 2)  ## satifies symmetry constraint on C and M
N <- length(num)
L <- length(adj)
x <- c(1, 2, 3, 4)
tau <- 0.3
gamma <- quantile(CAR_calcBounds(C, adj, num, M), 0.75)
CAR_calcBounds(C, adj, num, M)
gamma
##constants <- list(mu=mu, C=C, adj=adj, num=num, M=M, tau=tau, gamma=gamma, N=N, L=L)
constants <- list(mu=mu, adj=adj, num=num, M=M, tau=tau, gamma=gamma, N=N, L=L)
data <- list(y = 1:4)
inits <- list(x = x)
Rmodel <- nimbleModel(code, constants, data, inits)##, calculate=FALSE)



## testing how inprod() works (or doesn't work?) in a model

library(nimble)

code <- nimbleCode({   a <- abs(inprod(x[1:3], y[1:3]))   })

constants <- list(x = rep(0,3), y = rep(0,3))

Rmodel <- nimbleModel(code, constants)

Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)

1

## testing haveing a . in a nimbleFunction name

library(nimble)

my.RCfun <- nimbleFunction(
    name = 'my.RCfun',
    run = function() { a <- 1 }
)

Cfunction <- compileNimble(my.RCfun, showCompilerOutput = TRUE)

my.NF <- nimbleFunction(
    name = 'my.NF',
    setup = function() { a <- 0 },
    run = function() { b <- a }
)

Rfun <- my.NF()

Cfunction <- compileNimble(Rfun, showCompilerOutput = TRUE)


## testing hose arra() and numeric() work in a nimbleFunction

library(nimble)
Rnf <- nimbleFunction(
    run = function() {
        a <- array(dim = c(3,3))
        a[1,1] <- 4
        a[1,2] <- 2
        a[1,3] <- 3
        a[2,1] <- 2
        a[2,2] <- 7
        a[2,3] <- 1
        a[3,1] <- 3
        a[3,2] <- 1
        a[3,3] <- 6
        ##print(a)
        b <- chol(a)
        ##print(b)
        d <- numeric(3)
        d[1] <- b[1,1]
        d[2] <- b[1,2]
        d[3] <- b[1,3]
        print(d)
    }
)
system('rm ~/temp/aaa/*')
Cnf <- compileNimble(Rnf, dirName = '~/temp/aaa')

Rnf()
Cnf()

a <- array(c(4,2,3,2,7,1,3,1,6), c(3,3))
a
a - t(a)
cha <- chol(a)
cha

n <- 10000
out <- array(0, c(n,3))
for(i in 1:n) {
    r <- t(cha) %*% rnorm(3)
    out[i,] <- r
}
cov(out)
a

list.files('~/temp/aaa/', pattern = '*.cpp', full.names = TRUE)
for(f in list.files('~/temp/aaa/', pattern = '*.cpp', full.names = TRUE)) {
    cat('============================================\n')
    print(f)
    system(paste0('cat ', f))
    cat('============================================\n')
}





## testing min.bound, max.bound, and CAR_calcBounds
## PROBLEMS:
## (1) min.bound and max.bound don't compile


C <- c(1, .5,.5, .5,.5, 1)
adj <- c(2, 1,3, 2,4, 3)
num <- c(1, 2, 2, 1)
M <- c(2, 1, 1, 2)  ## satifies symmetry constraint on C and M

M1 <- diag(M^-0.5)
M2 <- diag(M^ 0.5)
Cmatrix <- t(array(c(0,1,0,0,  .5,0,.5,0,  0,.5,0,.5,   0,0,1,0), c(4,4)))
x <- M1 %*% Cmatrix %*% M2

eigen(x)
library(nimble)
nimEigen(x)

values <- eigen(x)$values
values

bounds <- c(1/max(values), 1/min(values))
orderedBounds <- c(min(bounds), max(bounds))
orderedBounds

CAR_calcBounds(C, adj, num, M)
min.bound(C, adj, num, M)
max.bound(C, adj, num, M)

##gamma <- seq(-1.1, 1.1, by = 0.1)
##names(gamma) <- gamma
##V <- (diag(4) - gamma*Cmatrix) %*% diag(M)
##lapply(gamma, function(g) try(inverse((diag(4) - g*Cmatrix) %*% diag(M))))

CcalcBounds <- compileNimble(CAR_calcBounds)
## PROBLEM: min.bound and max.bound don't compile
Cmin <- compileNimble(min.bound, showCompilerOutput = TRUE)
Cmax <- compileNimble(max.bound, showCompilerOutput = TRUE)

CcalcBounds(C, adj, num, M)



## test of compilation of dcar_proper()
## now compiles fine, after adding handlers to genCpp_processSpecificCalls.R
##

library(nimble)
code <- nimbleCode({
    ##x[1:N] ~ dcar_proper(mu[1:N], C[1:L], adj[1:L], num[1:N], M[1:N], tau, gamma)
    x[1:N] ~ dcar_proper(mu[1:N], adj=adj[1:L], num=num[1:N], M=M[1:N], tau=tau, gamma=gamma)
    y[1] ~ dnorm(x[1], 1)
    y[2] ~ dexp(x[2])
    y[3] ~ dexp(x[3])
    y[4] ~ dexp(x[4])
})
mu <- 1:4
C <- c(1, .5,.5, .5,.5, 1)
adj <- c(2, 1,3, 2,4, 3)
num <- c(1, 2, 2, 1)
M <- c(2, 1, 1, 2)  ## satifies symmetry constraint on C and M
N <- length(num)
L <- length(adj)
x <- c(1, 2, 3, 4)
tau <- 0.3
gamma <- quantile(CAR_calcBounds(C, adj, num, M), 0.75)
CAR_calcBounds(C, adj, num, M)
gamma
##constants <- list(mu=mu, C=C, adj=adj, num=num, M=M, tau=tau, gamma=gamma, N=N, L=L)
constants <- list(mu=mu, adj=adj, num=num, M=M, tau=tau, gamma=gamma, N=N, L=L)
data <- list(y = 1:4)
inits <- list(x = x)
Rmodel <- nimbleModel(code, constants, data, inits)##, calculate=FALSE)
c(1.144123, 0.437016, -0.437016, -1.144123)

Rmodel$getVarNames()
Rmodel$lifted_CAR_calcC_oPadj_oB1to6_cB_comma_num_oB1to4_cB_comma_M_oB1to4_cB_cP / sqrt(2)
CAR_calcCmatrix(Rmodel$lifted_CAR_calcC_oPadj_oB1to6_cB_comma_num_oB1to4_cB_comma_M_oB1to4_cB_cP / sqrt(2), adj, num)
Rmodel$nodes[['x_L1_UID_16']]$simulate
CAR_calcC(adj, num, M)

Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0); samples <- runMCMC(Cmcmc, 10000)

samples[10, ]
##[10,]  1.6416400 1.4296775 1.168077 1.227622
apply(samples, 2, mean)
##     x[1]      x[2]      x[3]      x[4] 
##1.0083198 0.9503674 0.6749227 0.4959068 
library(coda); apply(samples, 2, effectiveSize)
##     x[1]      x[2]      x[3]      x[4] 
##10000.000  1721.909  1491.630  1446.166 

11


##Rmodel$x
##print('========================================='); set.seed(0); Rmodel$simulate('x')
##Rmodel$x
## 
##Cmatrix <- t(array(c(0,1,0,0,  .5,0,.5,0,  0,.5,0,.5,   0,0,1,0), c(4,4)))
##Sigma <- solve(diag(4) - gamma*Cmatrix) %*% diag(M) / tau
##Q <- round(solve(Sigma), 10)
##t(t(as.numeric(Q)))
##cholQ <- chol(Q)
##Q
##cholQ
##round(t(cholQ) %*% cholQ - Q, 10)
##t(t(as.numeric(cholQ)))
## 
##ans <- c(1.262954,-0.326233,1.329799,1.272429)
##ans
##solve(cholQ, ans)
##solve(cholQ, ans) + mu
## 
##n <- 50000
##out <- array(0, c(n, 4))
##for(i in 1:n) {
##    Cmodel$simulate('x')
##    out[i,] <- Cmodel$x #solve(cholQ, rnorm(4))##as.numeric(rnorm(4) %*% t(cholQ))# %*% rnorm(4))
##}
####out
##cov(out) - Sigma

## C++ output of cholQ:
## 0.387298
## -0.105000
## 0.000000
## 0.000000
## -0.271109
## 0.475920
## -0.105000
## 0.000000
## 0.000000
## -0.220625
## 0.501323
## -0.105000
## 0.000000
## 0.000000
## -0.209446
## 0.325780
##array(c(0.387298,-0.105000,0.000000,0.000000,-0.271109,0.475920,-0.105000,0.000000,0.000000,-0.220625,0.501323,-0.105000,0.000000,0.000000,-0.209446,0.325780), c(4,4))



Q <- solve(Sigma)
cholQ <- chol(Q)
##cholQ <- chol(Sigma)

n <- 50000
out <- array(0, c(n, 4))
for(i in 1:n) {
    out[i,] <- solve(cholQ, rnorm(4))##as.numeric(rnorm(4) %*% t(cholQ))# %*% rnorm(4))
}
##out
cov(out) - Sigma

cholQ

Cmatrix <- t(array(c(0,1,0,0,  .5,0,.5,0,  0,.5,0,.5,   0,0,1,0), c(4,4)))
e <- eigen(Cmatrix)$values
e
Rmodel$lifted_CAR_calcEVs_oPC_oB1to6_cB_comma_adj_oB1to6_cB_comma_num_oB1to4_cB_cP
Rmodel$lifted_CAR_calcEVs_oPC_oB1to6_cB_comma_adj_oB1to6_cB_comma_num_oB1to4_cB_cP <- e
Rmodel$lifted_CAR_calcEVs_oPC_oB1to6_cB_comma_adj_oB1to6_cB_comma_num_oB1to4_cB_cP

Cmatrix <- t(array(c(0,1,0,0,  .5,0,.5,0,  0,.5,0,.5,   0,0,1,0), c(4,4)))
e <- eigen(Cmatrix)$values
e
model$lifted_CAR_calcEVs_oPC_oB1to6_cB_comma_adj_oB1to6_cB_comma_num_oB1to4_cB_cP
model$lifted_CAR_calcEVs_oPC_oB1to6_cB_comma_adj_oB1to6_cB_comma_num_oB1to4_cB_cP <- e
model$lifted_CAR_calcEVs_oPC_oB1to6_cB_comma_adj_oB1to6_cB_comma_num_oB1to4_cB_cP


##options(error = recover)
##Rmodel <- nimbleModel(code, constants, data, inits, calculate=FALSE)

##model <- Rmodel
model <- Cmodel
model$x
model$calculate('x')
model$x
set.seed(0); model$simulate('x')
model$simulate('x')
##x <- rep(2.1, 4)
model$x
model$calculate('x')

Sigma <- solve(diag(4) - gamma*Cmatrix) %*% diag(M) / tau
x <- Rmodel$x
x
log(det(2*pi*Sigma)^(-1/2) * exp((-1/2)*((x-mu) %*% solve(Sigma) %*% (x-mu))[1,1]))
dmnorm_chol(x, mu, chol(Sigma), prec_param = FALSE, log = TRUE)

1


Cmatrix <- t(array(c(0,1,0,0,  .5,0,.5,0,  0,.5,0,.5,   0,0,1,0), c(4,4)))
Sigma <- solve(diag(4) - gamma*Cmatrix) %*% diag(M) / tau
round(Sigma - t(Sigma), 10)
e <- eigen(Cmatrix)$values
e
v <- eigen(Cmatrix)$vectors
v
prod(M) / prod(1 - gamma*e) / tau^N   ## correct calulation here
det(Sigma)



x <- rep(2.1, 4)
(x - mu) %*% solve(Sigma) %*% (x - mu) * (-1/2)



Sigma <- solve(diag(4) - gamma*Cmatrix) %*% diag(M) / tau
Sigma - t(Sigma)
det(Sigma)
e <- eigen(Cmatrix)$values
##v <- eigen(Cmatrix)$vectors
##v
##det(v)
##det(t(v))
prod(M) / prod(1 - gamma*e) / tau^N


Cmatrix <- t(array(c(0,1,0,.5,0,.5,0,1,0), c(3,3)))
Cmatrix
Cmatrix <- t(array(c(0,1,0,0,  .5,0,.5,0,  0,.5,0,.5,   0,0,1,0), c(4,4)))
e <- eigen(Cmatrix)$values
v <- eigen(Cmatrix)$vectors
v
det(v)
Cmatrix
round(v %*% t(v), 10)
round(t(v) %*% v, 10)
cc <- v %*% diag(e) %*% solve(v)   ## (non-symmetric) C in terms of eigen decomp.
round(cc, 10)
round(cc - Cmatrix, 10)
det(v)
1/det(v)
det(solve(v))





## check compilation of dcar_normal():
library(nimble)
code <- nimbleCode({
    x[1:3] ~ dcar_normal(adj[1:4], weights[1:4], num[1:3], t, c, zm)
})
constants <- list(adj = c(2,3,1,1), weights = c(1,1,1,1), num = c(2,1,1), t = 1, c = 1, zm = 0)
data <- list()
inits <- list(x = c(1, 2, NA))
Rmodel <- nimbleModel(code, constants, data, inits, calculate=FALSE)

Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)






## what's wrong with biops test in efficient-conj ????
source(system.file(file.path('tests', 'test_utils.R'), package = 'nimble'))
context("Testing of dynamic indexing")
RwarnLevel <- options('warn')$warn
options(warn = -1)
source(system.file(file.path('tests', 'dynamicIndexingTestLists.R'), package = 'nimble'))
nimbleOptions(allowDynamicIndexing = TRUE)
library(nimble)

code <- nimbleCode({
    for(i in 1:ns){
        truex[i] ~ dcat(p[])
        biopsies[i,] ~ dmulti(error[truex[i],],nbiops[i])
    }
    error[1, 1:4] <- c(1, 0, 0, 0)
    error[2, 1:2] ~ ddirch(prior[2, 1:2])
    error[3, 1:3] ~ ddirch(prior[3, 1:3])
    error[4, 1:4] ~ ddirch(prior[4, 1:4])
    p[] ~ ddirch(prior[4,])
})
##system.in.dir(paste("sed 's/true/truex/g' biops-inits.R > ", file.path(tempdir(), "biops-inits.R")), dir = system.file('classic-bugs','vol2','biops', package = 'nimble'))
##system.in.dir(paste("echo 'error <- matrix(c(1,0,0,0, .5, .5, 0, 0, 1/3,1/3,1/3,0,1/4,1/4,1/4,1/4), 4,4, byrow=T)'  >> ", file.path(tempdir(), "biops-inits.R")), dir = system.file('classic-bugs','vol2','cervix', package = 'nimble'))
inits <- list(p = c(0.25, 0.25, 0.25, 0.25),
              truex = c(4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4),
              error = matrix(c(1,0,0,0, .5, .5, 0, 0, 1/3,1/3,1/3,0,1/4,1/4,1/4,1/4), 4,4, byrow=TRUE))
##testBUGSmodel('biops', dir = "", model = file.path(tempdir(), "biops.bug"), data = system.file('classic-bugs','vol2','biops','biops-data.R', package = 'nimble'),  inits = file.path(tempdir(), "biops-inits.R"),  useInits = TRUE)
ns <- 157
constants <- list(
    ns = ns,
    prior = structure(c(1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1), .Dim = as.integer(c(4,4))),
    nbiops = c(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3))
data <- list(
    biopsies = structure(c(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
        3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 
        1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 1, 1, 
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 
        1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 
        2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 
        3, 3, 0, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 
        1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 
        3, 3), .Dim = as.integer(c(157, 4))))
##    biopsies[ns,4], #  grades observed in ith session (multinomial)\n
##    nbiops[ns],     # total number of biopsies in ith session\n
##    truex[ns],       # true state in ith session\n
##    error[4,4],     # error matrix in taking biopsies\n
##    prior[4,4],     # prior parameters for rows of error[,]\n
##    p[4];           # underlying   incidence of true  states\n
##"))
dimensions <- list(
    biopsies = c(ns,4),
    nbiops = ns,
    truex = ns,
    error = c(4,4),
    prior = c(4,4),
    p = 4
)

##test_mcmc(model = file.path(tempdir(), "biops.bug"), name = 'biops', inits = file.path(tempdir(), "biops-inits.R"), data = system.file('classic-bugs', 'vol2', 'biops','biops-data.R', package = 'nimble'), numItsC = 1000)

Rmodel <- nimbleModel(code, constants, data, inits, dimensions = dimensions)

conf <- configureMCMC(Rmodel)
conf$printSamplers()

##conf$printSamplers()
##conf$removeSamplers(3)
##conf$printSamplers()

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

set.seed(0); samples <- runMCMC(Cmcmc, 1000)

conf$printSamplers(3)
conf$getSamplerDefinition(3)


## trying to figure out what's wrong with
## moving conjugate contribution CONTRIB_NAME variables to setup outputs
library(nimble)
code <- nimbleCode({
    x ~ dbeta(3, 13)
    y[1] ~ dbin(x, 10)
    y[2] ~ dbin(x, 20)
})
constants <- list()
inits <- list()
data = list(y = c(3,4))
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0); samples <- runMCMC(Cmcmc, 10)
ex <- c(0.195510839527966, 0.332847482503424, 0.247768152764931, 0.121748195439553, 0.157842271774841, 0.197566496350904, 0.216991517500577, 0.276609942874852, 0.165733872345582, 0.144695512780252)
round(samples[,1] - ex, 12)

conf$getSamplerDefinition(1)


## something wrong with conjugate sampler test:
## test_that('MVN conjugate setup', {

set.seed(0)
mu0 = 1:3
Q0 = matrix(c(1, .2, .8, .2, 2, 1, .8, 1, 2), nrow = 3)
Q = solve(matrix(c(3, 1.7, .9, 1.7, 2, .6, .9, .6, 1), nrow = 3))
a = c(-2, .5, 1)
B = matrix(rnorm(9), 3)
code <- nimbleCode({
    mu[1:3] ~ dmnorm(mu0[1:3], Q0[1:3, 1:3])
    y_mean[1:3] <- asCol(a[1:3]) + B[1:3, 1:3] %*% asCol(mu[1:3])
    y[1:3] ~ dmnorm(y_mean[1:3], Q[1:3, 1:3])
})
mu <- mu0 + chol(solve(Q0)) %*% rnorm(3)
y <- c(a + B%*%mu + chol(solve(Q)) %*% rnorm(3))
constants <- list(mu0 = mu0, Q0 = Q0, Q = Q, a = a, B = B)
data <- list(y = y)
inits <- list()

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

conf$getSamplerDefinition(1)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

muQtrue = t(B) %*% Q%*%B + Q0
muMeanTrue = c(solve(muQtrue, crossprod(B, Q%*%(y-a)) + Q0%*%mu0))

test_mcmc(model = code, name = 'two-level multivariate normal', data = data, seed = 0, numItsC = 10000,
          results = list(mean = list(mu = muMeanTrue),
              cov = list(mu = solve(muQtrue))),
          resultsTolerance = list(mean = list(mu = rep(.02,3)),
              cov = list(mu = matrix(.01, 3, 3))))


### scalar RW updates in place of conjugate mv update

test_mcmc(model = code, name = 'two-level multivariate normal with scalar updaters', data = data, seed = 0, numItsC = 100000,
          results = list(mean = list(mu = muMeanTrue),
              cov = list(mu = solve(muQtrue))),
          resultsTolerance = list(mean = list(mu = rep(.03,3)),
              cov = list(mu = matrix(.03, 3, 3))),
          samplers = list(list(type = 'RW', target = 'mu[1]'),
              list(type = 'RW', target = 'mu[2]'),
              list(type = 'RW', target = 'mu[3]')),
          removeAllDefaultSamplers = TRUE)






    
## testing usage of array() or nimArray() in NIMBLE DSL

library(nimble)

Rnf <- nimbleFunction(
    run = function() {
        a <- array(1, 1)
        returnType(double(1))
        return(a)
    }
)

Cnf <- compileNimble(Rnf)

Rnf()
Cnf()

## trying to move declare() statements in conjugate samplers
## into setup declarations, branch efficient-conj
library(nimble)
code <- nimbleCode({
    x ~ dgamma(1, 1)       # should satisfy 'gamma' conjugacy class
    a  ~ dnorm(0, x)     # should satisfy 'norm' conjugacy class
    a2 ~ dnorm(0, tau = 3*x+0)
    b  ~ dpois(0+5*x)
    b2 ~ dpois(1*x*1)
    c ~ dgamma(1, 7*x*5)
    for(i in 2:3) {
        jTau[i] <- 1
        jNorm[i] ~ dnorm(c * (a+3) - i, var = jTau[i])
        kTauSd[i] <- 2
        kLogNorm[i] ~ dlnorm(0 - a - 6*i, kTauSd[i])
    }
    z[1:3] ~ dmnorm(mu0[1:3], prec = ident[1:3,1:3])
    mu_y2[1:2] <- asCol(A[1:2]) + B[1:2,1:3] %*% asCol(z[1:3])
    mu_y3[1:3] <- asCol(A[1:3]) + B[1:3,1:3] %*% asCol(z[1:3])
    mu_y5[1:5] <- asCol(A[1:5]) + B[1:5,1:3] %*% asCol(z[1:3])
    y2[1:2] ~ dmnorm(mu_y2[1:2], prec = prec_y[1:2,1:2])
    y3[1:3] ~ dmnorm(mu_y3[1:3], prec = prec_y[1:3,1:3])
    y5[1:5] ~ dmnorm(mu_y5[1:5], prec = prec_y[1:5,1:5])
})
mu0 <- rep(0,3)
ident <- diag(3)
A <- 11:15
B <- matrix(1:15, nrow=5, ncol=3, byrow=TRUE)
prec_y <- diag(1:5)
constants <- list(mu0=mu0, ident=ident, A=A, B=B, prec_y=prec_y)
data <- list(y2=1:2, y3=1:3, y5=1:5)
inits <- list(a = 0, z=rep(0,3))
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printMonitors()
conf$addMonitors('a')
conf$printMonitors()
conf$addMonitors('jTau')
conf$printMonitors()
Rmcmc <- buildMCMC(conf)
system.time(Cmodel <- compileNimble(Rmodel))
system.time(Cmcmc <- compileNimble(Rmcmc, project = Rmodel))
niter <- 20
set.seed(0); Rsamples <- runMCMC(Rmcmc, niter)
set.seed(0); Csamples <- runMCMC(Cmcmc, niter)
dim(Rsamples)
dim(Csamples)
colnames(Rsamples)
colnames(Csamples)
nms <- colnames(Rsamples)
R <- Rsamples[niter,][nms]
C <- Csamples[niter,][nms]
all(round(R-C, 10) == 0) & all(round(C - c(1.247394961505, 4.887724543392, -0.883081353521, -4.096508841617, 2.287670748796), 10) == 0)

conf$getSamplerDefinition(1)
conf$getSamplerDefinition(2)
conf$getSamplerDefinition(7)







## looking a zeilinger grapevine xf analysis, and probabilities
## p_obs_plant and p_obs_vector

length(data$xf_source_plant)

indkeep <- !is.na(data$xf_source_plant)
length(indkeep)
sum(indkeep)

data2 <- lapply(data, function(x) x[indkeep])

data2

obs <- data2$xf_source_plant == 0
obs
sum(obs)

mean(data2$xf_source_plant == 0)

data2

!is.na(data2$infected)

indkeep2 <- !is.na(data2$infected)

indkeep2

data3 <- lapply(data2, function(x) x[indkeep2])

data3

##indkeep3 <- data3$xf_source_plant > 0 | data3$infected == 1
indkeep3 <- data3$infected == 1

indkeep3

data4 <- lapply(data3, function(x) x[indkeep3])

data4

vec <- data4$xf_vector[!is.na(data4$xf_vector)]

w <- vec > 0
w
sum(w)
mean(w)
length(w)

vec <- data$xf_vector[which(data$infected == 1)]

library(dplyr)
setwd('~/github/zeilinger_grapevine/')
load('data/temp1.RData')
df <- as.data.frame(data)
head(df)

reduced <- df %>% filter(!is.na(xf_source_plant) & (infected == 1 | xf_vector > 0))

reduced

## a total of 23 observations with a measurement for xf_source_plant,
## and xf_vector > 0, OR infected == 1
## meaning we *know* the the source plant was infected (where the vector was)

sum(reduced$xf_source_plant > 0)

## 17 of which have xf_source_plant > 0

mean(reduced$xf_source_plant > 0)

## 17 / 23 = 74% detection in those cases

reduced2 <- df %>% filter(!is.na(xf_source_plant) & !(infected == 1 | xf_vector > 0))

reduced2

## a total of 29 observations with a measurement for xf_source_plant,
## and xf_vector == 0, AND infected == 0
## meaning we're *not sure* if the source plant was infected (where the vector was)

sum(reduced2$xf_source_plant > 0)

## 9 of which have xf_source_plant > 0

mean(reduced2$xf_source_plant > 0)

## 31% detection rate, when we're *not sure* if any xf was present
            

## making plots for JSM 2017 Baltimore presentation
## on AFSS_to_RW_block hybrid block samplers

setwd('~/github/nimble/nimble-samplerCompare')

load('data/compareDF_litters_fixed2.Rdata') 
library(ggplot2)
library(dplyr)
compareDF %>%
    filter(sampler == 'AFSS_to_RW_block_v2')  %>%
    mutate(n = as.factor(numESSAdaptations), r = as.factor(r)) %>%  
    group_by(r, n, nIter) %>%
    summarise(avgEff = mean(minEfficiency, na.rm = TRUE),
              seEff = sd(minEfficiency, na.rm = TRUE)/sqrt(length(minEfficiency)),
              l95  = max(0,avgEff - 1.96*seEff),
              u95 = max(0,avgEff + 1.96*seEff)) -> df
lineChart <- ggplot(df, aes(x = nIter, y = avgEff, color = r, lty = n)) +
    geom_line(lwd = 1) +
        labs(title = 'AFSS_to_RW sampler parameters: SSM',
             y = 'Avg. Min. Eff.',
             x = 'Iterations')
lineChart



## testing using rcat()
## do probabilities argument probs need to be normalized?
## answer: nope!
library(nimble)

Rnf <- nimbleFunction(
    run = function(probs = double(1)) {
        newValue <- rcat(1, probs)
        returnType(double())
        return(newValue)
    }
)
Cnf <- compileNimble(Rnf)

probs <- c(.1, .1, .8) * .003
Rnf(probs)
Cnf(probs)

n <- 1000
Rout <- numeric(n)
Cout <- numeric(n)
set.seed(0); for(i in 1:n) Rout[i] <- Rnf(probs)
set.seed(0); for(i in 1:n) Cout[i] <- Cnf(probs)
table(Rout) / n
table(Cout) / n
##    1     2     3 
##0.096 0.104 0.800 




## looking at some of the sampler-compare results
modelName <- 'pump'
modelName <- 'litters'
modelName <- 'ice'
modelName <- 'bliss'
modelName <- 'ssm'

setwd('~/github/nimble/nimble-samplerCompare')
file <- paste0('data/compareDF_', modelName, '_fixed2.Rdata')
load(file)

head(compareDF)
dim(compareDF)
compareDF

head(allEffDF)
allEffDF[,11:12]


## testing Chris's new sampler_categorical for dcat nodes:
library(nimble)

code <- nimbleCode({
    x ~ dcat(p[1:5])
    ##y ~ dnorm(x, 1)
})
constants <- list(p = c(.1, .15, .2, .25, .3))
data <- list()##y=1)
inits <- list(x = 1)
Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel, nodes = NULL)
##conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$addSampler('x', 'categorical')
conf$printSamplers()

Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

niter <- 100000
set.seed(0); samples <- runMCMC(Cmcmc, niter)
##Cmcmc$run(10000)
##samples <- as.matrix(Cmcmc$mvSamples)

table(samples[,1])/niter
##      1       2       3       4       5 
##0.10152 0.15017 0.19835 0.24901 0.30095 







## Nick's test of fakeDist() and mixedSizes

library(nimble)

dFakeDist <- nimbleFunction(
    run = function(x = double(0), y = double(2), log = integer(0, default = 0)) {
        returnType(double(0))
        return(0)
    })

rFakeDist <- nimbleFunction(
    run = function(n = integer(0),  y = double(2)) {
        returnType(double(0))
        if(n != 1) nimPrint('rmyexp only allows n = 1; using n = 1')
        return(0)
    }
)

registerDistributions(list(
    dFakeDist = list(
        BUGSdist = 'dFakeDist(y)',
        types = c('value = double(0)', 'y = double(2)'),
        mixedSizes = TRUE)
))

fakeCode <- nimbleCode({
    x ~ dFakeDist(y[1:3, 1:2])
})

fakeModel <- nimbleModel(code = fakeCode, data = list(x = 0), constants = list(y = matrix(nrow = 3, ncol = 2)))



## old MCMC control list defaults
##log = FALSE,
##reflective = FALSE,
##adaptive = TRUE,
##adaptScaleOnly = FALSE,
##adaptInterval = 200,
##scale = 1,
##propCov = 'identity',
##sliceWidth = 1,
##sliceMaxSteps = 100,
##sliceAdaptFactorMaxIter = 15000,  ##factorBurnIn = 15000,
##sliceAdaptFactorInterval = 1000,  ##factorAdaptInterval = 1000,
##sliceAdaptWidthMaxIter = 512,     ##sliceBurnIn = 512,
##sliceAdaptWidthTolerance = 0.1,
##scaleAdaptInterval = 200,
##sliceWidths = 'oneVec',
##pfNparticles = 1000,
##pfResample = FALSE,
##pfOptimizeNparticles = FALSE,
##pfType = 'bootstrap',
##pfLookahead = 'simulate',
##carUseConjugacy = TRUE




## test of current samples from AF_slice sampler
library(nimble)
load('~/github/hybridBlockSamplers/data/model_litters.RData')
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$addSampler(c('a','b'), 'AF_slice')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0); system.time(samples <- runMCMC(Cmcmc, 20000))
if(!all(c(round(samples[1,],7) == c(14.7334414, 1.2708328, 2.8708787, 0.9404118),
          round(samples[1000,],7) == c(321.6325351, 1.5995729, 34.4197040, 0.5525827),
          round(samples[10000,2:4],6) == c(4.443445, 329.645757, 1.776704),
          round(samples[20000,],6) == c(2803.235783, 4.217422, 390.150481, 1.218532))))
    stop('something broke') else message('OK')




## test of conf$removeSamplers('a', 'b', 'c')
library(nimble)
load('~/github/hybridBlockSamplers/data/model_litters.RData')
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$removeSamplers('a', 'b')
conf$printSamplers()




## dcar_proper example from WinBUGS geoBUGS user manual:
## for running in NIMBLE
## CURRENT PROBLEM: waiting for a fix about sqrt(inprod(..., ...))
library(nimble)

######## CHANGE BACK BUGS CODE, to original line: sqrt(inprod(...))

code <- nimbleCode({
    ## Set up 'data' to define spatial dependence structure
    ## =====================================
    ## The vector C[] required as input into the car.proper distribution is a vector respresention
    ## of the weight matrix with elements Cij. The first J1 elements of the C[] vector contain the
    ## weights for the J1 neighbours of area i=1; the (J1+1) to J2 elements of the C[] vector contain
    ## the weights for the J2 neighbours of area i=2; etc.
    ## To set up this vector, we need to define a variable cumsum, which gives the values of J1,
    ## J2, etc.; we then set up an index matrix pick[,] with N columns corresponding to the
    ## i=1,...,N areas, and with the same number of rows as there are elements in the C[] vector 
    ## (i.e. L). The elements C[ (cumsum[i]+1):cumsum[i+1] ] correspond to 
    ## the set of weights Cij associated with area i, and so we set up ith column of the matrix pick[,]
    ## to have a 1 in all the rows k for which cumsum[i] < k <= cumsum[i+1], and 0's elsewhere. 
    ## For example, let N=4 and cumsum=c(0,3,5,6,8), so area i=1 has 3 neighbours, area i=2 has 2 
    ## neighbours, area i=3 has 1 neighbour and area i=4 has 2 neighbours. The the matrix pick[,] is:
    ##                pick                  
    ##             1, 0, 0, 0,                   
    ##             1, 0, 0, 0,                   
    ##             1, 0, 0, 0,                    
    ##             0, 1, 0, 0,                   
    ##             0, 1, 0, 0,                   
    ##             0, 0, 1, 0,                   
    ##             0, 0, 0, 1,                   
    ##             0, 0, 0, 1,                   
    ##
    ## We can then use the inner product (inprod(,)) function in WinBUGS and the kth row of pick to 
    ## select which area corresponds to the kth element in the vector C[]; likewise, we can use inprod(,) 
    ## and the ith column of pick to select the elements of C[] which correspond to area i.
    ##
    ## Note: this way of setting up the C vector is somewhat convoluted!!!! In future versions, we hope the 
    ## GeoBUGS adjacency matrix tool will be able to dump out the relevant vectors required. Alternatively, 
    ## the C vector could be created using another package (e.g. Splus) and read into WinBUGS as data.
    ##
    cumsum[1] <- 0
    for(i in 2:(N+1)) {
        cumsum[i] <- sum(num[1:(i-1)])
    }
    for(k in 1:L) {
        for(i in 1:N) {
            pick[k,i] <- step(k - cumsum[i] - epsilon)  * step(cumsum[i+1] - k)
            ##  pick[k,i] = 1    if     cumsum[i] < k <= cumsum[i=1];  otherwise, pick[k,i] = 0
        }
        ##C[k] <- sqrt(E[adj[k]] / inprod(E[1:N], pick[k,1:N]))    # weight for each pair of neighbours
        CTEMP[k] <- E[adj[k]] / inprod(E[1:N], pick[k,1:N])   ## TEMPORARY
        C[k] <- sqrt(CTEMP[k])                                ## TEMPORARY
    }
    epsilon <- 0.0001
    ## Model
    ## =====
    ## Priors:
    alpha  ~ dnorm(0, 0.0001)
    prec  ~ dgamma(0.5, 0.0005)     ## prior on precision
    v <- 1/prec                     ## variance
    sigma <- sqrt(1 / prec)         ## standard deviation
    gamma.min <- min.bound(C[1:L], adj[1:L], num[1:N], M[1:N])
    gamma.max <- max.bound(C[1:L], adj[1:L], num[1:N], M[1:N])
    gamma ~ dunif(gamma.min, gamma.max)
    S[1:N] ~ car.proper(theta[1:N], C[1:L], adj[1:L], num[1:N], M[1:N], prec, gamma)
    ## Likelihood:
    for(i in 1:N) {
        log(mu[i]) <- log(E[i]) + S[i]
        Y[i] ~ dpois(mu[i])
        RR[i] <- exp(S[i])      ## Area-specific relative risk
        theta[i] <- alpha
    }
})
N <- 56
E <- c(1.4, 8.7, 3.0, 2.5, 4.3, 2.4, 8.1, 2.3, 2.0, 6.6,
       4.4, 1.8, 1.1, 3.3, 7.8, 4.6, 1.1, 4.2, 5.5, 4.4,
       10.5,22.7, 8.8, 5.6,15.5,12.5, 6.0, 9.0,14.4,10.2,
       4.8, 2.9, 7.0, 8.5,12.3,10.1,12.7, 9.4, 7.2, 5.3,
       18.8,15.8, 4.3,14.6,50.7, 8.2, 5.6, 9.3,88.7,19.6,
       3.4, 3.6, 5.7, 7.0, 4.2, 1.8)
M <- 1/E
##X <- c(16,16,10,24,10,24,10, 7, 7,16, 7,16,10,24, 7,16,10,
##       7, 7,10, 7,16,10, 7, 1, 1, 7, 7,10,10,7,24,10, 7, 7,
##       0,10, 1,16, 0, 1,16,16, 0, 1, 7, 1, 1, 0, 1,1, 0, 1, 1,16,10)
num <- c(3, 2, 1, 3, 3, 0, 5, 0, 5, 4, 0, 2, 3, 3, 2, 6, 6, 6, 5, 3,
         3, 2, 4, 8, 3, 3, 4, 4, 11, 6, 7, 3, 4, 9, 4, 2, 4, 6, 3, 4, 
         5, 5, 4, 5, 4, 6, 6, 4, 9, 2, 4, 4, 4, 5, 6, 5)
adj <- c(19, 9, 5, 10, 7, 12, 28, 20, 18, 19, 12, 1, 
         17, 16, 13, 10, 2, 29, 23, 19, 17, 1, 22, 16, 7, 2, 
         5, 3, 19, 17, 7, 35, 32, 31, 29, 25, 29, 22, 21, 17,
         10, 7, 29, 19, 16, 13, 9, 7, 56, 55, 33, 28, 20, 4,
         17, 13, 9, 5, 1, 56, 18, 4, 50, 29, 16, 16, 10, 39, 34, 29, 9,
         56, 55, 48, 47, 44, 31, 30, 27, 29, 26, 15, 43, 29, 25,
         56, 32, 31, 24, 45, 33, 18, 4, 50, 43, 34, 26, 25, 23, 21,
         17, 16, 15, 9, 55, 45, 44, 42, 38, 24, 47, 46, 35, 32, 27, 24, 14, 
         31, 27, 14, 55, 45, 28, 18, 54, 52, 51, 43, 42, 40, 39, 29, 23,
         46, 37, 31, 14, 41, 37, 46, 41, 36, 35, 54, 51, 49, 44, 42, 30,
         40, 34, 23, 52, 49, 39, 34, 53, 49, 46, 37, 36, 51, 43, 38, 34, 30,
         42, 34, 29, 26, 49, 48, 38, 30, 24, 55, 33, 30, 28, 53, 47, 41,
         37, 35, 31, 53, 49, 48, 46, 31, 24, 49, 47, 44, 24, 54, 53, 52,
         48, 47, 44, 41, 40, 38, 29, 21, 54, 42, 38, 34, 54, 49, 40, 34,
         49, 47, 46, 41, 52, 51, 49, 38, 34, 56, 45, 33, 30, 24, 18,
         55, 27, 24, 20, 18)
L <- length(adj)
constants <- list(N=N, L=L, E=E, num=num, adj=adj, M=M)
Y <- c(9, 39, 11, 9, 15, 8, 26, 7, 6, 20, 13, 5, 3, 8, 17, 9, 2, 7, 9,
       7, 16, 31, 11, 7, 19, 15, 7, 10, 16, 11, 5, 3, 7, 8, 11, 9, 11,
       8, 6, 4, 10, 8, 2, 6, 19, 3, 2, 3, 28, 6, 1, 1, 1, 1, 0, 0)
data <- list(Y=Y)
inits <- list(alpha = 3, prec = 1, gamma = 0.1,
              S = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
                  0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0))

Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)

## THIS NEXT SECTION,
## STORES THE **CORRECT** eigen values of C into Rmodel object
##all(round(CAR_calcC(adj, num, M) - Rmodel$C, 15) == 0)
##C <- CAR_calcC(adj, num, M)
##Cmatrix <- CAR_calcCmatrix(C, adj, num)
##Cmatrix - t(Cmatrix)
##x <- diag(1/M) %*% Cmatrix;   all(round(x - t(x), 10) == 0)
##evs <- eigen(Cmatrix)$values
##Rmodel$lifted_CAR_calcEVs_oPC_oB1to234_cB_comma_adj_oB1to234_cB_comma_num_oB1to56_cB_cP <- evs
##round(evs - Rmodel$lifted_CAR_calcEVs_oPC_oB1to234_cB_comma_adj_oB1to234_cB_comma_num_oB1to56_cB_cP, 10)
## CHECK THAT Cmodel now has the **CORRECT** eigen values:
##round(evs - Cmodel$lifted_CAR_calcEVs_oPC_oB1to234_cB_comma_adj_oB1to234_cB_comma_num_oB1to56_cB_cP, 10)

conf <- configureMCMC(Rmodel)
conf$printSamplers()

conf$addMonitors('S')

Rmcmc <- buildMCMC(conf)

Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0); samples <- runMCMC(Cmcmc, 20000)

## the following is all a workaround, to get the right eigen values in:
##set.seed(0); samples <- runMCMC(Cmcmc, 1)
##Cmodel$lifted_CAR_calcEVs_oPC_oB1to234_cB_comma_adj_oB1to234_cB_comma_num_oB1to56_cB_cP <- evs
##evs - Cmodel$lifted_CAR_calcEVs_oPC_oB1to234_cB_comma_adj_oB1to234_cB_comma_num_oB1to56_cB_cP
##set.seed(0)
##Cmcmc$run(10000, reset = FALSE)
##samples <- as.matrix(Cmcmc$mvSamples)

CAR_calcBounds(Rmodel$C, adj, num, 1/E)
samplesPlot(samples, 'gamma')

##dimnames(samples)
##ind <- 51:55; var <- dimnames(samples)[[2]][ind]
##ind <- 57:59; var <- dimnames(samples)[[2]][ind]
##samplesPlot(samples, var)
##library(coda)
##apply(samples, 2, effectiveSize)

means <- apply(samples, 2, mean)
sds <- apply(samples, 2, sd)
medians <- apply(samples, 2, median)
q025 <- apply(samples, 2, function(x) quantile(x, 0.025, na.rm=TRUE))
q975 <- apply(samples, 2, function(x) quantile(x, 0.975, na.rm=TRUE))
res <- cbind(means, sds, q025, medians, q975)
res

##             means        sds        q025     medians        q975
## S[1]   1.75493676 0.34342751  1.024601288  1.77039978  2.36615014
## S[2]   1.39890145 0.16632175  1.061053578  1.40343367  1.71338038
## S[3]   1.14701201 0.31250172  0.487667361  1.16445300  1.71113958
## S[4]   1.11700013 0.34162724  0.411839323  1.12458625  1.74588824
## S[5]   1.13622797 0.25984895  0.591375949  1.14512671  1.60404030
## S[6]   1.13415135 0.37126434  0.348697964  1.15056410  1.79173367
## S[7]   1.08705787 0.19449271  0.694209794  1.09410516  1.45074529
## S[8]   1.03942043 0.39966486  0.193168929  1.06751229  1.74383512
## S[9]   0.97350948 0.41881332  0.067542394  1.01200711  1.71828129
## S[10]  1.03849485 0.22194162  0.579655270  1.04721199  1.45334232
## S[11]  1.04054729 0.28259977  0.443637875  1.05002508  1.56132010
## S[12]  0.87149658 0.45151410 -0.115975034  0.89438911  1.66650008
## S[13]  0.83817061 0.59274100 -0.478019939  0.89025567  1.86208606
## S[14]  0.69181412 0.36470352 -0.082867808  0.71318563  1.35351318
## S[15]  0.64522473 0.23819097  0.144534740  0.65532540  1.08228495
## S[16]  0.63586275 0.31114771 -0.001912805  0.64825055  1.20756523
## S[17]  0.54608699 0.65214752 -0.884088682  0.60890176  1.63704788
## S[18]  0.33272515 0.36971298 -0.439850934  0.34992452  1.01214873
## S[19]  0.45322252 0.30697060 -0.182971174  0.46926152  1.01415474
## S[20]  0.32405555 0.36615845 -0.429347639  0.33732852  0.98984847
## S[21]  0.30111451 0.23714435 -0.167479242  0.30145966  0.76593078
## S[22]  0.24277321 0.16818570 -0.085405975  0.24240486  0.56613004
## S[23]  0.14499322 0.28119584 -0.417706382  0.14822224  0.66563561
## S[24] -0.08733732 0.39309596 -0.918089425 -0.07073871  0.62959535
## S[25]  0.15620228 0.20897600 -0.267339669  0.15862280  0.55292078
## S[26]  0.09943259 0.23608084 -0.386309627  0.10693610  0.53861629
## S[27]  0.01967325 0.35177955 -0.720140962  0.03523577  0.66066625
## S[28]  0.01315140 0.28998260 -0.588888010  0.02492488  0.55274685
## S[29]  0.09036654 0.22388423 -0.370480241  0.09443472  0.50672752
## S[30] -0.14997938 0.29735255 -0.743832634 -0.14638852  0.40300592
## S[31] -0.11527532 0.41243309 -1.001631219 -0.09981057  0.63417016
## S[32] -0.06240479 0.52051960 -1.162524873 -0.02836957  0.86499059
## S[33] -0.14952682 0.34380647 -0.853967102 -0.13109259  0.48357134
## S[34] -0.26377317 0.33419603 -0.964054323 -0.24786752  0.35906533
## S[35] -0.15485900 0.26887185 -0.699412734 -0.14824859  0.35847774
## S[36] -0.18737771 0.28953971 -0.778187460 -0.17642867  0.35783083
## S[37] -0.22626069 0.26999705 -0.773456797 -0.21913017  0.27740911
## S[38] -0.44545140 0.33877285 -1.138708984 -0.43082068  0.18127785
## S[39] -0.21184008 0.34854906 -0.932598988 -0.19178301  0.41993799
## S[40] -0.51806086 0.44361464 -1.439029392 -0.50117301  0.31513366
## S[41] -0.63534429 0.25153127 -1.164381427 -0.62772535 -0.17646564
## S[42] -0.56897303 0.26956953 -1.120537638 -0.55861123 -0.06627916
## S[43] -0.62155023 0.52414360 -1.743611023 -0.58213480  0.31525455
## S[44] -0.78345332 0.30245086 -1.417641888 -0.77364889 -0.22031700
## S[45] -0.65623965 0.17260491 -1.015151093 -0.65238620 -0.33498160
## S[46] -0.86212917 0.41445118 -1.745558112 -0.83476189 -0.12252798
## S[47] -1.14548549 0.53517934 -2.272348122 -1.12683895 -0.14877358
## S[48] -0.99531628 0.39933620 -1.822129459 -0.97827311 -0.27811643
## S[49] -0.84492481 0.14089094 -1.128293770 -0.84155455 -0.57919194
## S[50] -0.69660971 0.27065294 -1.266225375 -0.67769308 -0.20952026
## S[51] -1.08638527 0.66308099 -2.483981599 -1.04288791  0.12372150
## S[52] -1.32127519 0.69025495 -2.790522712 -1.26893593 -0.10808565
## S[53] -1.39970611 0.57084948 -2.599841791 -1.36696201 -0.37133067
## S[54] -1.35986177 0.51624768 -2.471914269 -1.32893035 -0.42441920
## S[55] -1.40015562 0.66860904 -2.831489740 -1.36431399 -0.22494411
## S[56] -1.34306587 0.98092867 -3.549954286 -1.25738287  0.29745151
## alpha -0.12882011 0.16293022 -0.447856360 -0.12834769  0.18486099
## gamma  0.16288608 0.01978254  0.108110129  0.16861104  0.18233285
## prec   0.33202411 0.09350970  0.184821714  0.31975385  0.55034834




##getNeighbors <- function(adj, num, i) {
##    if(num[i] == 0)     return(numeric(0))
##    if(i == 1)          return(adj[1:num[1]])
##    cs <- cumsum(num)
##    neighbors <- adj[(cs[i-1] + 1) : (cs[i-1] + num[i])]
##    return(neighbors)
##}
## 
##Rmodel$C[1]*M[19]
##getNeighbors(Rmodel$C, num, 19)[5] * M[1]
## 
##Rmodel$C[2]*M[9]
##getNeighbors(Rmodel$C, num, 9)[5] * M[1]
## 
## 
## 
##getNeighbors(adj, num, 1)
##getNeighbors(Rmodel$C, num, 1)
##getNeighbors(adj, num, 19)
## 
##getNeighbors(adj, num, 9)
##getNeighbors(adj, num, 5)
## 
##E[c(1,19,9,5)]
##sqrt(E[c(19,9,5)] / E[1])
##Rmodel$C[1:5]
##E[1:5]
##Rmodel$M[1:5]
##1/E[1:5]
## 
## 
## 
## 
##Rmodel$getNodeNames()
##Rmodel$getVarNames()
##Rmodel$C
##Rmodel$M
##Cmatrix <- CAR_calcCmatrix(Rmodel$C, Rmodel$adj, Rmodel$num)
##Mmatrix <- diag(M)
##MmatrixInv <- diag(1/M)
##MinvC <- MmatrixInv %*% Cmatrix
##out <- MinvC - t(MinvC)
##all(round(out, 10) == 0)
##Rmodel$C






## dcar_proper example from WinBUGS geoBUGS user manual:
## this is the model as I ran it in WinBUGS,
## and the RESULTS FROM WINBUGS for comparison.

model
{
  for(i in 1 : N) { 
     m[i] <- 1/E[i]       # scaling factor for variance in each cell
  } 
  cumsum[1] <- 0
  for(i in 2:(N+1)) {
     cumsum[i] <- sum(num[1:(i-1)])
  }	
  for(k in 1 : sumNumNeigh) { 
 	 for(i in 1:N) {
          pick[k,i] <- step(k - cumsum[i] - epsilon)  * step(cumsum[i+1] - k)   
      }                                                       
      C[k] <- sqrt(E[adj[k]] / inprod(E[], pick[k,]))    # weight for each pair of neighbours
  }
  epsilon <- 0.0001
  for (i in 1 : N) {
      Y[i]  ~ dpois(mu[i])
      log(mu[i]) <- log(E[i]) + S[i]
      RR[i] <- exp(S[i])      # Area-specific relative risk 
      theta[i] <- alpha
  }
  S[1:N] ~ car.proper(theta[], C[], adj[], num[], m[], prec, gamma)
  alpha  ~ dnorm(0, 0.0001)  
  prec  ~ dgamma(0.5, 0.0005)     # prior on precision
   v <- 1/prec                               # variance
  sigma <- sqrt(1 / prec)               # standard deviation
  gamma.min <- min.bound(C[], adj[], num[], m[])
  gamma.max <- max.bound(C[], adj[], num[], m[])
  gamma ~ dunif(gamma.min, gamma.max)
}

## data
list(N = 56,  
     Y   = c(    9,   39,   11,    9,   15,    8,   26,    7,    6,   20,
         13,    5,    3,    8,   17,    9,    2,    7,    9,    7,
         16,   31,   11,    7,   19,   15,    7,   10,   16,   11,
         5,    3,    7,    8,   11,    9,   11,    8,    6,    4,
         10,    8,    2,    6,   19,    3,    2,    3,   28,    6,
         1,    1,    1,    1,    0,    0),
     E = c(1.4, 8.7, 3.0, 2.5, 4.3, 2.4, 8.1, 2.3, 2.0, 6.6, 4.4, 1.8, 1.1, 3.3, 7.8, 4.6, 1.1, 4.2, 5.5, 4.4,
         10.5,22.7, 8.8, 5.6,15.5,12.5, 6.0, 9.0,14.4,10.2, 4.8, 2.9, 7.0, 8.5,12.3,10.1,12.7, 9.4, 7.2, 5.3,
         18.8,15.8, 4.3,14.6,50.7, 8.2, 5.6, 9.3,88.7,19.6, 3.4, 3.6, 5.7, 7.0, 4.2, 1.8),
     num = c(3, 2, 1, 3, 3, 0, 5, 0, 5, 4, 0, 2, 3, 3, 2, 6, 6, 6, 5, 3, 3, 2, 4, 8, 3, 3, 4, 4, 11, 6, 7, 3,
         4, 9, 4, 2, 4, 6, 3, 4, 5, 5, 4, 5, 4, 6, 6, 4, 9, 2, 4, 4, 4, 5, 6, 5),
     adj = c(19, 9, 5, 10, 7, 12, 28, 20, 18, 19, 12, 1, 17, 16, 13, 10, 2, 
         29, 23, 19, 17, 1, 22, 16, 7, 2, 5, 3, 19, 17, 7, 35, 32, 31, 
         29, 25, 29, 22, 21, 17, 10, 7, 29, 19, 16, 13, 9, 7, 56, 55, 33, 28, 20, 4, 
         17, 13, 9, 5, 1, 56, 18, 4, 50, 29, 16, 16, 10, 39, 34, 29, 9, 56, 55, 48, 47, 44, 31, 30, 27, 
         29, 26, 15, 43, 29, 25, 56, 32, 31, 24, 45, 33, 18, 4, 50, 43, 34, 26, 25, 23, 21, 17, 16, 15, 9, 
         55, 45, 44, 42, 38, 24, 47, 46, 35, 32, 27, 24, 14, 31, 27, 14, 55, 45, 28, 18, 
         54, 52, 51, 43, 42, 40, 39, 29, 23, 46, 37, 31, 14, 41, 37, 46, 41, 36, 35, 54, 51, 49, 44, 42, 30, 
         40, 34, 23, 52, 49, 39, 34, 53, 49, 46, 37, 36, 51, 43, 38, 34, 30, 42, 34, 29, 26, 49, 48, 38, 30, 24, 
         55, 33, 30, 28, 53, 47, 41, 37, 35, 31, 53, 49, 48, 46, 31, 24, 49, 47, 44, 24, 54, 53, 52, 48, 47, 44, 41, 40, 38, 
         29, 21, 54, 42, 38, 34, 54, 49, 40, 34, 49, 47, 46, 41, 52, 51, 49, 38, 34, 56, 45, 33, 30, 24, 18, 55, 27, 24, 20, 18),
     sumNumNeigh = 234)

## inits
list(alpha=3, prec=1, S=c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
                          0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0), gamma=0.1)

## results from WinBUGS for dcar_proper:
## dcar_proper example from WinBUGS geoBUGS user manual:
## for running in NIMBLE
##  node	 mean	 sd	2.5%	median	97.5%
##  alpha	-0.1391	0.1592	-0.4531	-0.1396	0.1732
##  gamma	0.1634	0.01875	0.1129	0.1689	0.1823
##  prec	0.3342	0.09314	0.1859	0.3226	0.5491

winbugsResults <- matrix(c(-0.1391,0.1592,-0.4531,-0.1396,0.1732,0.1634,0.01875,0.1129,0.1689,0.1823,0.3342,0.09314,0.1859,0.3226,0.5491), nrow=3, ncol=5, byrow=TRUE)
rownames(winbugsResults) <- c('alpha', 'gamma', 'prec')
colnames(winbugsResults) <- c('mean', 'sd', '2.5%', 'median', '97.5%')
winbugsResults
##          mean      sd    2.5%  median  97.5%
## alpha -0.1391 0.15920 -0.4531 -0.1396 0.1732
## gamma  0.1634 0.01875  0.1129  0.1689 0.1823
## prec   0.3342 0.09314  0.1859  0.3226 0.5491

##S[1]	1.743	0.3439	0.001097	1.016	1.762	2.368	501	99500
##S[2]	1.397	0.1632	5.225E-4	1.067	1.402	1.707	501	99500
##S[3]	1.143	0.3117	9.69E-4	0.4913	1.157	1.714	501	99500
##S[4]	1.126	0.3423	0.001084	0.4093	1.143	1.752	501	99500
##S[5]	1.129	0.2605	7.779E-4	0.59	1.14	1.611	501	99500
##S[6]	1.018	0.3665	0.001187	0.2505	1.037	1.681	501	99500
##S[7]	1.084	0.1942	5.992E-4	0.6894	1.09	1.449	501	99500
##S[8]	0.9187	0.3893	0.001243	0.09141	0.9405	1.618	501	99500
##S[9]	0.9715	0.4085	0.001309	0.1061	0.9946	1.7	501	99500
##S[10]	1.036	0.2199	6.416E-4	0.5859	1.043	1.446	501	99500
##S[11]	0.9193	0.2842	9.177E-4	0.3267	0.9312	1.442	501	99500
##S[12]	0.8776	0.4498	0.001464	-0.07956	0.9062	1.679	501	99500
##S[13]	0.8362	0.5806	0.001909	-0.4341	0.8821	1.84	501	99500
##S[14]	0.7018	0.3602	0.001163	-0.05503	0.7194	1.357	501	99500
##S[15]	0.6455	0.2399	7.549E-4	0.1524	0.6531	1.092	501	99500
##S[16]	0.6387	0.3135	9.77E-4	-0.01128	0.6519	1.214	501	99500
##S[17]	0.5477	0.6598	0.002289	-0.8985	0.606	1.674	501	99500
##S[18]	0.3313	0.3711	0.001216	-0.4439	0.3496	1.005	501	99500
##S[19]	0.4437	0.3107	9.139E-4	-0.2032	0.4586	1.013	501	99500
##S[20]	0.3253	0.364	0.001044	-0.4397	0.3424	0.9856	501	99500
##S[21]	0.2978	0.2382	7.383E-4	-0.1894	0.3042	0.7465	501	99500
##S[22]	0.245	0.1679	5.286E-4	-0.09421	0.2487	0.5618	501	99500
##S[23]	0.1375	0.2776	8.674E-4	-0.4362	0.1469	0.6536	501	99500
##S[24]	-0.09633	0.3869	0.001406	-0.9039	-0.07928	0.6078	501	99500
##S[25]	0.1488	0.2098	7.077E-4	-0.278	0.1544	0.5439	501	99500
##S[26]	0.1014	0.2378	7.583E-4	-0.3835	0.1082	0.5479	501	99500
##S[27]	0.02065	0.3505	0.001112	-0.7117	0.03649	0.6625	501	99500
##S[28]	0.007397	0.2895	9.733E-4	-0.589	0.01767	0.5463	501	99500
##S[29]	0.0935	0.2217	6.994E-4	-0.3598	0.09998	0.5113	501	99500
##S[30]	-0.1558	0.2916	9.563E-4	-0.7564	-0.1461	0.3874	501	99500
##S[31]	-0.111	0.4157	0.001364	-0.9859	-0.08949	0.6387	501	99500
##S[32]	-0.05884	0.5173	0.001618	-1.162	-0.02871	0.8635	501	99500
##S[33]	-0.155	0.3465	0.001048	-0.8779	-0.1405	0.4833	501	99500
##S[34]	-0.266	0.3345	0.001164	-0.9582	-0.2517	0.3513	501	99500
##S[35]	-0.1531	0.2626	8.8E-4	-0.6914	-0.1448	0.3414	501	99500
##S[36]	-0.1871	0.2935	9.404E-4	-0.7946	-0.1764	0.3574	501	99500
##S[37]	-0.2241	0.2653	8.216E-4	-0.7694	-0.2154	0.2753	501	99500
##S[38]	-0.4511	0.3403	0.001183	-1.152	-0.4382	0.1817	501	99500
##S[39]	-0.2213	0.3515	0.001097	-0.9473	-0.2063	0.428	501	99500
##S[40]	-0.517	0.4516	0.001434	-1.464	-0.4969	0.3053	501	99500
##S[41]	-0.641	0.2547	8.162E-4	-1.164	-0.6343	-0.1627	501	99500
##S[42]	-0.5665	0.2712	9.55E-4	-1.124	-0.557	-0.06094	501	99500
##S[43]	-0.619	0.5202	0.001687	-1.717	-0.5893	0.3157	501	99500
##S[44]	-0.7859	0.3004	9.681E-4	-1.404	-0.7759	-0.2268	501	99500
##S[45]	-0.6555	0.172	7.411E-4	-1.009	-0.6506	-0.3324	501	99500
##S[46]	-0.8712	0.4106	0.001378	-1.728	-0.8558	-0.1191	501	99500
##S[47]	-1.154	0.5385	0.001751	-2.284	-1.128	-0.1731	501	99500
##S[48]	-0.997	0.3996	0.001411	-1.83	-0.9819	-0.2598	501	99500
##S[49]	-0.8482	0.1408	6.438E-4	-1.136	-0.8441	-0.5841	501	99500
##S[50]	-0.6966	0.2663	0.001019	-1.253	-0.6847	-0.2069	501	99500
##S[51]	-1.083	0.6658	0.002157	-2.505	-1.042	0.1048	501	99500
##S[52]	-1.299	0.68	0.002363	-2.738	-1.26	-0.0797	501	99500
##S[53]	-1.406	0.566	0.002052	-2.597	-1.376	-0.3815	501	99500
##S[54]	-1.354	0.5102	0.001772	-2.43	-1.328	-0.4254	501	99500
##S[55]	-1.422	0.6701	0.002154	-2.848	-1.378	-0.2378	501	99500
##S[56]	-1.364	0.9762	0.003289	-3.505	-1.283	0.2978	501	99500






## problem from TR J/A 2
## deck of 52, A=1, face = 10
## drawing cards w/ replacement
## draw 2, prob(sum is even) ?
## draw 4, or draw 100?
pE <- 8/13
pO <- 5/13

N <- 100
pevensum <- numeric(N)
pevensum[1] <- pE

if(N > 1) for(i in 2:N) {
    pevensum[i] <- (1-pevensum[i-1])*pO + pevensum[i-1]*pE
    }
pevensum

pevensum[99] - pevensum[100]
plot(1:N, pevensum, type = 'l', xlim=c(1,10))

## testing as.carAdjacency() conversions
library(nimble)
as.carAdjacency
as.carAdjacency(matrix(c(0,0,0,0,0,2,0,2,0), 3, 3))

as.carAdjacency(list(numeric(0), 3, 2), list(numeric(0), 2, 2))

## testing about providing numIslands argument to dcar_normal()

library(nimble)

code <- nimbleCode({
    for(i in 1:L) {   weights[i] <- 1   }
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], 1)
    for(i in 1:N) { y[i] ~ dnorm(S[i], 1) }
})
constants <- list(
    N = 5,
    num = c(1,1,2,2,2),
    adj = c(2,1,   4,5,   3,5,   3,4),
    ##weights = c(1,1,1,1,1,1,1,1),
    L = 8)
data <- list( y = c(1,2,3,4,5) )
inits <- list( S = c(1,2,3,4,5) )

code <- nimbleCode({
    for(i in 1:L) {   weights[i] <- 1   }
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], 1, 3)
    for(i in 1:N) { y[i] ~ dnorm(S[i], 1) }
})
constants <- list(
    N = 5,
    num = c(1,1,2,2,2),
    adj = c(2,1,   4,5,   3,5,   3,4),
    ##weights = c(1,1,1,1,1,1,1,1),
    L = 8)
data <- list( y = c(1,2,3,4,5) )
inits <- list( S = c(1,2,3,4,5) )

code <- nimbleCode({
    for(i in 1:L) {   weights[i] <- 1   }
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], 1, numI)
    for(i in 1:N) { y[i] ~ dnorm(S[i], 1) }
})
constants <- list(
    N = 5,
    num = c(1,1,2,2,2),
    adj = c(2,1,   4,5,   3,5,   3,4),
    ##weights = c(1,1,1,1,1,1,1,1),
    L = 8)
data <- list( y = c(1,2,3,4,5) )
inits <- list( S = c(1,2,3,4,5), numI = 4)

code <- nimbleCode({
    for(i in 1:L) {   weights[i] <- 1   }
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], 1, numI)
    for(i in 1:N) { y[i] ~ dnorm(S[i], 1) }
})
constants <- list(
    N = 5,
    num = c(1,1,2,2,2),
    adj = c(2,1,   4,5,   3,5,   3,4),
    ##weights = c(1,1,1,1,1,1,1,1),
    L = 8,
    numI = 1)
data <- list( y = c(1,2,3,4,5) )
inits <- list( S = c(1,2,3,4,5) )

## 5-8

code <- nimbleCode({
    S[1:N] ~ car.normal(adj[1:L], num = num[1:N], tau = 1)
    for(i in 1:N) { y[i] ~ dnorm(S[i], 1) }
})
constants <- list(
    N = 5,
    num = c(1,1,2,2,2),
    adj = c(2,1,   4,5,   3,5,   3,4),
    L = 8)
data <- list( y = c(1,2,3,4,5) )
inits <- list( S = c(1,2,3,4,5) )

code <- nimbleCode({
    S[1:N] ~ car.normal(adj[1:L], num = num[1:N], tau = 1, numIslands = 3)
    for(i in 1:N) { y[i] ~ dnorm(S[i], 1) }
})
constants <- list(
    N = 5,
    num = c(1,1,2,2,2),
    adj = c(2,1,   4,5,   3,5,   3,4),
    L = 8)
data <- list( y = c(1,2,3,4,5) )
inits <- list( S = c(1,2,3,4,5) )

code <- nimbleCode({
    S[1:N] ~ car.normal(adj[1:L], num = num[1:N], tau = 1, numIslands = numI)
    for(i in 1:N) { y[i] ~ dnorm(S[i], 1) }
})
constants <- list(
    N = 5,
    num = c(1,1,2,2,2),
    adj = c(2,1,   4,5,   3,5,   3,4),
    L = 8)
data <- list( y = c(1,2,3,4,5) )
inits <- list( S = c(1,2,3,4,5), numI = 4)

library(nimble)
code <- nimbleCode({
    ##S[1:N] ~ car.normal(adj[1:L], num = num[1:N], tau = 1, numIslands = numI)
    S[1:N] ~ car.normal(adj[1:L], num = num[1:N], tau = 1, c = numI, zero_mean = 1)
    for(i in 1:N) { y[i] ~ dnorm(S[i], 1) }
})
constants <- list(
    N = 5,
    num = c(1,1,2,2,2),
    adj = c(2,1,   4,5,   3,5,   3,4),
    L = 8,
    numI = 1)
data <- list( y = c(1,2,3,4,5) )
inits <- list( S = c(1,2,3,4,5) )
inits <- list( S = rep(NA,5) )


Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Rmodel$S
Cmodel$S

Rmodel$calculate()
Cmodel$calculate()
niter <- 20
set.seed(0); Rmcmc$run(niter)
set.seed(0); Cmcmc$run(niter)

Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)
round(Rsamples - Csamples, 3)
Rsamples
Csamples
round(apply(Rsamples, 1, sum), 5)
round(apply(Csamples, 1, sum), 5)


debug(Rmcmc$run)

##[20,] -1.81243476 -1.10311996  0.31018093  0.69365670 1.9117171


niter <- 10000
set.seed(0); Cmcmc$run(niter)
Csamples <- as.matrix(Cmcmc$mvSamples)
samplesPlot(Csamples)
samplesPlot(Csamples, var = 1:2)



##options(warn=2)
##options(error = recover)



## testing how WinBUGS imposes the sum-to-zero constraint...
## on a per-island-basis?

library(nimble)

code <- nimbleCode({
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], 1)
    for(i in 1:N) {
        y[i] ~ dnorm(S[i], 1)
    }
    sum1 <- S[1] + S[2]
    sum2 <- S[3] + S[4] + S[5]
    sum3 <- S[1] + S[2] + S[3] + S[4] + S[5]
})
##
data <- list(
    N = 5,
    num = c(1,1,2,2,2),
    adj = c(2,1,   4,5,   3,5,   3,4),
    weights = c(1,1,1,1,1,1,1,1),
    L = 8,
    y = c(1,2,3,4,5)
)
##
inits <- list(
    S = c(1,2,3,4,5)
)

catCode(code, data, inits, file='~/temp/BUGS.txt')

Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','weights','L')], data=data['y'], inits)


conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)




## learning hose to use tapply()

require(stats)
rbinom(32, n = 5, prob = 0.4)
as.factor(rbinom(32, n = 5, prob = 0.4))
groups <- as.factor(rbinom(32, n = 5, prob = 0.4))
groups
tapply(groups, groups, length) #- is almost the same as
table(groups)

## contingency table from data.frame : array with named dimnames
str(warpbreaks)
head(warpbreaks)
warpbreaks$breaks
identical(warpbreaks[,-1], warpbreaks[-1])
str(warpbreaks[-1])

tapply(warpbreaks$breaks, warpbreaks[,-1], sum)
tapply(warpbreaks$breaks, warpbreaks[,-1], sum, simplify=FALSE)
tapply(warpbreaks$breaks, warpbreaks[,-1], function(x) x+1000)

class(tapply(warpbreaks$breaks, warpbreaks[,-1], function(x) x+1000))
typeof(tapply(warpbreaks$breaks, warpbreaks[,-1], function(x) x+1000))
tapply(warpbreaks$breaks, warpbreaks[,-1], function(x) x+1000)[1,1]

warpbreaks[, 3, drop = FALSE]
warpbreaks[, 3]
tapply(warpbreaks$breaks, warpbreaks[, 3, drop = FALSE], sum)
tapply(warpbreaks$breaks, warpbreaks[, 3], sum)

aggregate(warpbreaks$breaks, warpbreaks[, 3, drop=FALSE], sum)
aggregate(warpbreaks$breaks, warpbreaks[, -1, drop=FALSE], sum)




n <- 17
rep_len(1:3, n)
factor(rep_len(1:3, n), levels = 1:5)
fac <- factor(rep_len(1:3, n), levels = 1:5)
fac
table(fac)
tapply(1:n, fac, sum)
tapply(1:n, fac, sum, default = 0) # maybe more desirable
tapply(1:n, fac, sum, simplify = FALSE)
##tapply(1:n, fac, range, default=c(1,3))
tapply(1:n, fac, quantile)
tapply(1:n, fac, length) ## NA's
tapply(1:n, fac, length, default = 0) # == table(fac)



## example of ... argument: find quarterly means
tapply(presidents, cycle(presidents), mean, na.rm = TRUE)

ind <- list(c(1, 2, 2), c("A", "A", "B"))
ind
table(ind)
tapply(1:3, ind) #-> the split vector
tapply(1:3, ind, sum)




## checking car car_calcNumIslands()

myfun <- function() CAR_calcNumIslands(adj, num)

c_CAR_calcNumIslands <- compileNimble(CAR_calcNumIslands)
myfun <- function() c_CAR_calcNumIslands(adj, num)

num <- c(1, 1)
adj <- c(2, 1)
myfun()
## c = 1

library(nimble)
myfun <- function() {
    N <- length(num)
    L <- sum(num)
    weights <- rep(1, L)
    code <- nimbleCode({
        S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
    })
    constants <- list(adj=adj, weights=weights, num=num, N=N, L=L)
    data <- list(S = rep(0,N))
    inits <- list(tau = 1)
    Rmodel <- nimbleModel(code, constants, data, inits)
    Rmodel$calculate()
}

num <- c(1, 1)
adj <- c(2, 1)
myfun()
## c = 1

num <- c(2, 1, 1)
adj <- c(2,3,   1,  1)
myfun()
## c = 1

num <- c(2, 2, 2)
adj <- c(2,3,   1,3,   1,2)
myfun()
## c = 1

num <- c(1, 0, 1)
adj <- c(3,    1)
myfun()
## c = 2

num <- c(2, 2, 2, 0)
adj <- c(2,3,   1,3,   1,2)
myfun()
## c = 2

num <- c(2, 2, 2, 0, 0)
adj <- c(2,3,   1,3,   1,2)
myfun()
## c = 3

num <- c(2, 2, 2, 0, 1, 1)
adj <- c(2,3,   1,3,   1,2,     6,    5)
myfun()
## c = 3

num <- c(2, 2, 2, 0, 1, 1, 1, 1)
adj <- c(2,3,   1,3,   1,2,     6,    5,    8,     7)
myfun()
## c = 4

num <- c(2, 2, 2, 0, 1, 1, 2, 2, 2)
adj <- c(2,3,   1,3,   1,2,     6,    5,    8,9,     7,9,   7,8)
myfun()
## c = 4

num <- c(2, 2, 2, 0, 1, 1, 2, 2, 2, 0, 0)
adj <- c(2,3,   1,3,   1,2,     6,    5,    8,9,     7,9,   7,8)
myfun()
## c = 6




## equivalance of geoBUGS / winBUGS and nimble
## CAR dcar_normal density evaluation for tau?

N <- 4
x <- 1:4
num <- c(3,2,2,1)
adj <- c(2,3,4,   1,3,    1,2,   1)
weights <- c(2,2,2,   2,3,   2,3,  2)

k <- length(x)
c <- 1
lp <- 0
count <- 1
for(i in 1:k) {
    if(num[i] == 0)   c <- c + 1
    xi <- x[i]
    for(j in 1:num[i]) {
        xj <- x[adj[count]]
        lp <- lp + weights[count] * (xi-xj)^2
        count <- count + 1
    }
}
lp

k <- length(x)
lp <- 0
count <- 1
qf <- 0
for(i in 1:k) {
    xi <- x[i]
    mu <- 0
    wPlus <- 0
    for(j in 1:num[i]) {
        mu <- mu + x[adj[count]] * weights[count]
        wPlus <- wPlus + weights[count]
        count <- count + 1
    }
    mu <- mu / wPlus
    qf <- qf + 1/2 * xi * (xi - mu) * wPlus
}
qf
qf*4


if(count != (length(adj)+1)) stop('something wrong')
lp <- lp * (-1/2) * tau / 2
lp <- lp + (k-c)/2 * log(tau/2/pi)
lp




## testing island nodes = NA values
library(nimble)
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
##
code <- nimbleCode({
    alpha0 ~ dflat()
    tau ~ dgamma(0.001, 0.001)
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
    for(i in 1:3) {
        mu[i] <- S[i] + alpha0
        Y[i] ~ dnorm(mu[i], 2)
    }
    z[1] ~ dnorm(S[6], 1)
    z[2] ~ dnorm(S[7], 1)
    z[3] ~ dnorm(S[8] + 0*S[8]^2, 1)
    z[4] ~ dnorm(S[9] + 0*S[9]^2, 1)
    z[5] ~ dnorm(S[10] + 0*S[10]^2, 1)
})
##
data <- list(
    N = 10,
    num = c(1, 2, 1, 0, 0, 0, 0, 0, 0, 0),
    adj = c(2,   1,3,    2),
    weights = c(1,1,1,1),
    L = 4,
    Y = c(1,2,3),
    z = c(0, 0, 0, 0, 0)
)
##
inits <- list(
    alpha0 = 0,
    tau = 1,
    S = c(0, 0, 0, 3, NA, 0, NA, 0, NA, NaN)
)
##
Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','weights','L')], data=data[c('Y','z')], inits)

conf <- configureMCMC(Rmodel, control = list(log=TRUE))
conf$printSamplers()
conf$addMonitors('S')
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

niter <- 200000#500000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)
dim(samples)
length(samples)
##
means <- apply(samples, 2, mean)
sds <- apply(samples, 2, sd)
medians <- apply(samples, 2, median)
q025 <- apply(samples, 2, function(x) quantile(x, 0.025, na.rm=TRUE))
q975 <- apply(samples, 2, function(x) quantile(x, 0.975, na.rm=TRUE))
res <- cbind(means, sds, medians, q025, q975)
res

               means         sds      medians        q025        q975
S[1]   -3.941210e-01   0.4893330 -0.225937468 -1.59038344   0.1910843
S[2]    1.565404e-05   0.2975876  0.000138869 -0.67216986   0.6661552
S[3]    3.941054e-01   0.4887008  0.227770323 -0.19113529   1.5807439
alpha0  2.003152e+00   0.4080147  2.003949375  1.19972110   2.8056662
tau     9.120936e+01 280.8845605  4.164618835  0.04471161 861.5163085

## winBUGS output


conf$printSamplers()
Rmodel$getLogProb('alpha0')
Rmodel$getLogProb('tau')
Rmodel$getLogProb('S')
Rmodel$getLogProb('Y')

debug(Rmcmc$samplerFunctions$contentsList[[1]]$run)

niter <- 30
set.seed(0); Rmcmc$run(niter)
set.seed(0); Cmcmc$run(niter)
Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)
sampnames <- dimnames(Rsamples)[[2]]
Rsamples[, sampnames]

Csamples[, sampnames]

Rsamples[, sampnames] - Csamples[, sampnames]


catListContents <- function(lst) {
    cat('list(')
    for(i in seq_along(lst)) {
        cat(names(lst)[i])
        cat(' = ')
        val <- lst[[i]]
        if(length(val) == 1) {
            cat(val)
        } else {
            cat('c(')
            cat(paste(val, collapse=', '))
            cat(')')
        }
        if(i < length(lst)) cat(', ')
    }
    cat(')\n\n')
}
catCode <- function(code, data, inits, file) {
    if(!missing(file)) sink(file)
    cat('\n\nmodel\n')
    print(code)
    cat('\n\n## data\n')
    catListContents(data)
    cat('\n## inits\n')
    catListContents(inits)
    if(!missing(file)) sink()
}

catCode(code, data, inits)
catCode(code, data, inits, file='~/temp/BUGS.txt')


x <- runif(100000, 0, 10000000)
hist(x)
t <- 1/x
hist(t, breaks=1000, xlim=c(0, 0.0001))



## finding BUGS tau sampling (11)
## with a gamma(0.001, 0.001) prior on tau
library(nimble)
##
code <- nimbleCode({
    tau ~ dgamma(0.001, 0.001)
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
})
##
data <- list(
    N = 4,
    num = c(1,2,1,0),
    adj = c(2,   1,3,    2),
    weights = c(1,1,1,1),
    L = 4,
    S = c(1,0,0,0)
)
##
inits <- list(
    tau = 1
)
##
catCode(code, data, inits, file='~/temp/BUGS.txt')
Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','weights','L')], data=data['S'], inits)
##
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##
niter <- 1000000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)[100001:1000000,]
dim(samples)
length(samples)
##

##means <- apply(samples, 2, mean)
##sds <- apply(samples, 2, sd)
##medians <- apply(samples, 2, median)
##q025 <- apply(samples, 2, function(x) quantile(x, 0.025))
##q975 <- apply(samples, 2, function(x) quantile(x, 0.975))
mean(samples)
var(samples)
means <- mean(samples)
sds <- sd(samples)
medians <- median(samples)
q025 <- quantile(samples, 0.025)
q975 <- quantile(samples, 0.975)
res <- cbind(means, sds, q025, medians, q975)
res
##        means     sds       q025 medians     q975
##2.5% 2.006003 1.99855 0.05131201  1.3909 7.396893



## true posterior (for tau):
WSSD <- 1
N <- 4
c <- 2
a <- (N-c)/2 
b <- WSSD/2
a
b

## NIMBLE
## a
a <- mean(samples)^2 / var(samples)
## b
b <- mean(samples) / var(samples)
a
b


## winBUGS output
##node  mean   sd     2.5%     median  97.5%
##tau   2.008  2.006  0.04963  1.389   7.42

m <- 2.008
s <- 2.006
a <- m^2 / s^2
b <- m / s^2
a
b







## finding BUGS tau sampling (10)
## now with a transformation on tau
## prior is uniform on variance
## this time, with two more islands!
library(nimble)
##
code <- nimbleCode({
    sigma2 ~ dunif(0, 10000)
    tau <- 1/sigma2
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
})
##
data <- list(             
    N = 9,
    num = c(1,2,2,2,2,2,1,0,0),
    adj = c(2,   1,3,    2,4,   3,5,   4,6,   5,7,   6),
    weights = c(1,1,1,1,1,1,1,1,1,1,1,1),
    L = 12,
    S = c(1,0,0,0,0,0,0,0,0)
)
##
inits <- list(
    sigma2 = 1
)
##
catCode(code, data, inits, file='~/temp/BUGS.txt')
Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','weights','L')], data=data['S'], inits)
##
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$addMonitors('tau')
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##
niter <- 1000000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)[100001:1000000,]
dim(samples)
length(samples)
##

means <- apply(samples, 2, mean)
sds <- apply(samples, 2, sd)
medians <- apply(samples, 2, median)
q025 <- apply(samples, 2, function(x) quantile(x, 0.025))
q975 <- apply(samples, 2, function(x) quantile(x, 0.975))
##mean(samples)
##var(samples)
##means <- mean(samples)
##q025 <- quantile(samples, 0.025)
##q975 <- quantile(samples, 0.975)
res <- cbind(means, sds, q025, medians, q975)
res
##           means      sds       q025   medians      q975
##sigma2 0.5124162 1.035113 0.08940902 0.2994832  2.081309
##tau    3.9918965 2.838581 0.48046693 3.3390858 11.184554


## true posterior (for tau):
WSSD <- 1
N <- 9
c <- 3
a <- (N-c)/2 - 1
b <- WSSD/2
a
b

## NIMBLE
tau.samp <- samples[, 'tau']
mean(tau.samp)
var(tau.samp)

## theoretical gamma, for tau:
## mean = a/b
a / b
## var = a / b^2
a / b^2

sigma2.samp <- samples[, 'sigma2']
mean(sigma2.samp)
var(sigma2.samp)

## theoretical inverse-gamma, for sigma2:
## mean = b/(a-1), for a>1   (from wikipedia)
b / (a-1)
## var = b^2 / ((a-1)^2 * (a-2)),   for a>2   (from wikipedia)
b^2 / ((a-1)^2 * (a-2))



## winBUGS output
##node     mean   sd      2.5%     median  97.5%
##sigma2   0.487  0.8645  0.09028  0.2972  2.009
##tau      4.004  2.81    0.4977   3.365   11.08


    
## finding BUGS tau sampling (9)
## now with a transformation on tau
## prior is uniform on variance
library(nimble)
##
code <- nimbleCode({
    sigma2 ~ dunif(0, 10000)
    tau <- 1/sigma2
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
})
##
data <- list(             
    N = 9,
    num = c(1,2,2,2,2,2,2,2,1),
    adj = c(2,   1,3,    2,4,   3,5,   4,6,   5,7,   6,8,   7,9,   8),
    weights = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1),
    L = 16,
    S = c(1,0,0,0,0,0,0,0,0)
)
##
inits <- list(
    sigma2 = 1
)
##
catCode(code, data, inits, file='~/temp/BUGS.txt')
Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','weights','L')], data=data['S'], inits)
##
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$addMonitors('tau')
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##
niter <- 1000000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)[100001:1000000,]
dim(samples)
length(samples)
##

means <- apply(samples, 2, mean)
sds <- apply(samples, 2, sd)
medians <- apply(samples, 2, median)
q025 <- apply(samples, 2, function(x) quantile(x, 0.025))
q975 <- apply(samples, 2, function(x) quantile(x, 0.975))
##mean(samples)
##var(samples)
##means <- mean(samples)
##q025 <- quantile(samples, 0.025)
##q975 <- quantile(samples, 0.975)
res <- cbind(means, sds, q025, medians, q975)
res
##           means      sds       q025  medians       q975
##sigma2 0.2515433 0.237622 0.06912374 0.188005  0.8239204
##tau    5.9778155 3.469859 1.21370951 5.319008 14.4668096

## true posterior (for tau):
WSSD <- 1
N <- 9
c <- 1
a <- (N-c)/2 - 1
b <- WSSD/2
a
b

## NIMBLE
tau.samp <- samples[, 'tau']
mean(tau.samp)
var(tau.samp)

## theoretical gamma, for tau:
## mean = a/b
a / b
## var = a / b^2
a / b^2

sigma2.samp <- samples[, 'sigma2']
mean(sigma2.samp)
var(sigma2.samp)

## theoretical inverse-gamma, for sigma2:
## mean = b/(a-1), for a>1   (from wikipedia)
b / (a-1)
## var = b^2 / ((a-1)^2 * (a-2)),   for a>2   (from wikipedia)
b^2 / ((a-1)^2 * (a-2))



## winBUGS output
##node     mean    sd      2.5%      median   97.5%
##sigma2   0.2467  0.2254  0.06937   0.186    0.7953
##tau      6.014   3.438   1.257     5.375    14.41





## finding BUGS tau sampling (8)
library(nimble)
##
code <- nimbleCode({
    tau ~ dunif(0, 10000)
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
})
##
data <- list(
    N = 4,
    num = c(1,2,1,0),
    adj = c(2,   1,3,    2),
    weights = c(1,1,1,1),
    L = 4,
    S = c(1,0,0,0)
)
##
inits <- list(
    tau = 1
)
##
catCode(code, data, inits, file='~/temp/BUGS.txt')
Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','weights','L')], data=data['S'], inits)
##
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##
niter <- 500000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)[100001:500000,]
dim(samples)
length(samples)
##

##means <- apply(samples, 2, mean)
##sds <- apply(samples, 2, sd)
##medians <- apply(samples, 2, median)
mean(samples)
var(samples)
means <- mean(samples)
sds <- sd(samples)
medians <- median(samples)
q025 <- quantile(samples, 0.025)
q975 <- quantile(samples, 0.975)
res <- cbind(means, sds, medians, q025, q975)
res

## true posterior:
WSSD <- 1
N <- 4
c <- 2
a <- (N-c)/2 + 1
b <- WSSD/2
a
b

## NIMBLE
## a
a <- mean(samples)^2 / var(samples)
## b
b <- mean(samples) / var(samples)
a
b
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))
a
b

## winBUGS output
 node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
tau	2.002	2.01	0.009006	0.04942	1.389	7.469	1	50000

##a = mean^2  / sd^2
##b = mean    / sd^2

m <- 2.001
s <- 2.01
a <- m^2 / s^2
b <- m / s^2
a
b

a <- 1
b <- 0.5
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))

WSSD <- 1
N <- 4
c <- 4 ## BUGS is using 4 islands here
a <- (N-c)/2 + 1
b <- WSSD/2
a
b





## finding BUGS tau sampling (7)
library(nimble)
##
code <- nimbleCode({
    tau ~ dunif(0, 10000)
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
})
##
data <- list(
    N = 4,
    num = c(1,2,2,1),
    adj = c(2,   1,3,    2,4,   3),
    weights = c(1,1,1,1,1,1),
    L = 6,
    S = c(1,0,0,0)
)
##
inits <- list(
    tau = 1
)
##
catCode(code, data, inits, file='~/temp/BUGS.txt')
Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','weights','L')], data=data['S'], inits)
##
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##
niter <- 500000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)[100001:500000,]
dim(samples)
length(samples)
##

##means <- apply(samples, 2, mean)
##sds <- apply(samples, 2, sd)
##medians <- apply(samples, 2, median)
mean(samples)
var(samples)
means <- mean(samples)
sds <- sd(samples)
medians <- median(samples)
q025 <- quantile(samples, 0.025)
q975 <- quantile(samples, 0.975)
res <- cbind(means, sds, medians, q025, q975)
res

## true posterior:
WSSD <- 1
N <- 4
c <- 1
a <- (N-c)/2 + 1
b <- WSSD/2
a
b

## NIMBLE
## a
a <- mean(samples)^2 / var(samples)
## b
b <- mean(samples) / var(samples)
a
b
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))
a
b

## winBUGS output
node	 mean	sd	MC error  2.5%	 median	97.5%	start	sample
tau	3.012  	2.463	0.01127	  0.2203 2.373	9.356	1	50000

##a = mean^2  / sd^2
##b = mean    / sd^2

m <- 3.012
s <- 2.463
a <- m^2 / s^2
b <- m / s^2
a
b

a <- 1.5
b <- 0.5
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))

WSSD <- 1
N <- 4
c <- 3 ## BUGS is using 3 islands here
a <- (N-c)/2 + 1
b <- WSSD/2
a
b



## finding BUGS tau sampling (6)
## transformation of tau (to sd)
library(nimble)
##
code <- nimbleCode({
    ##sd ~ dgamma(0.001, 0.001)
    ##tau ~ dgamma(0.001, 0.001)
    b ~ dbern(0.5)
    tau <- b+1
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
    ##for(i in 1:N) {
    ##    mu[i] <- S[i] + alpha0
    ##    Y[i] ~ dnorm(mu[i], 1)
    ##}
})
##
data <- list(
    N = 3,
    num = c(1,2,1),
    adj = c(2,   1,3,    2),
    weights = c(1,1,1,1),
    L = 4,
    ##Y = c(1,2,3)
    S = c(0, 0, 0)
)
##
inits <- list(
    ##tau = 1
    b=0
)
##
catCode(code, data, inits, file='~/temp/BUGS.txt')
Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','weights','L')], data=data['S'], inits)
##
conf <- configureMCMC(Rmodel, control = list(log=TRUE))
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##
niter <- 100000#500000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)##[100001:500000,]
dim(samples)
length(samples)
##
##means <- apply(samples, 2, mean)
##sds <- apply(samples, 2, sd)
##medians <- apply(samples, 2, median)
means <- mean(samples)
sds <- sd(samples)
medians <- median(samples)
q025 <- quantile(samples, 0.025)
q975 <- quantile(samples, 0.975)
res <- cbind(means, sds, medians, q025, q975)
res

##      means      sds  medians     q025     q975
##sd 500.2603 287.7832 500.1425 26.28444 974.5512


## winBUGS output
node   mean	sd	2.5%	  median	97.5%	
sd     7.2166	31.5099	0.276503  0.630709	2.09074	


x0 <- sum(samples==0)
x1 <- sum(samples==1)

x1/(x0+x1)


Rmodel$sd <- Cmodel$sd
Rmodel$sd
Rmodel$tau
Rmodel$calculate()
exp(Rmodel$calculate('sd'))
Rmodel$calculate('S')


Cmodel$sd
Cmodel$tau
exp(Cmodel$calculate('sd'))
Cmodel$calculate('S')

debug(Rmcmc$samplerFunctions[[1]]$run)
Rmcmc$run(10)

samples <- as.matrix(Cmcmc$mvSamples)
length(samples)

samplesPlot(samples, ind=1:1000)

## finding BUGS tau sampling (5)
library(nimble)
##
code <- nimbleCode({
    tau ~ dunif(0, 1000)
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
})
##
data <- list(
    N = 8,
    num = c(2,2,2,2,2,2,2,2),
    adj = c(2,8,    1,3,    2,4,     3,5,     4,6,     5,7,      6,8,     1,7),
    weights = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1),
    L = 16,
    S = c(0,1,0,1,0,1,0,1)
)
##
inits <- list(
    tau = 1
)
##
catCode(code, data, inits, file='~/temp/BUGS.txt')
Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','weights','L')], data=data['S'], inits)
##
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

##
niter <- 500000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)[100001:500000,]
dim(samples)
length(samples)
##

##means <- apply(samples, 2, mean)
##sds <- apply(samples, 2, sd)
##medians <- apply(samples, 2, median)
means <- mean(samples)
sds <- sd(samples)
medians <- median(samples)
q025 <- quantile(samples, 0.025)
q975 <- quantile(samples, 0.975)
res <- cbind(means, sds, medians, q025, q975)
res
## a
a <- mean(samples)^2 / var(samples)
## b
b <- mean(samples) / var(samples)
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))
a
b

## winBUGS output
 node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
tau	0.874155	0.467162	0.00145191	0.214319	0.792857	1.99685	1	100000

##a = mean^2  / sd^2
##b = mean    / sd^2
## ===> WinBUGS is getting a gamma(1.5,3.5) posterior

a <- 1.5
b <- 3.5
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))



## finding BUGS tau sampling (4)
library(nimble)
##
code <- nimbleCode({
    tau ~ dunif(0, 1000)
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
})
##
data <- list(
    N = 5,
    num = c(1,3,3,3,2),
    adj = c(2,   1,3,4,    2,4,5,    2,3,5,    3,4),
    weights = rep(1,12),
    L = 12,
    S = c(-1, 0, 1, 2, 2)
)
##
inits <- list(
    tau = 1
)
##
catCode(code, data, inits, file='~/temp/BUGS.txt')
Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','weights','L')], data=data['S'], inits)
##
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

##
niter <- 500000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)[100001:500000,]
dim(samples)
length(samples)
##

##means <- apply(samples, 2, mean)
##sds <- apply(samples, 2, sd)
##medians <- apply(samples, 2, median)
means <- mean(samples)
sds <- sd(samples)
medians <- median(samples)
q025 <- quantile(samples, 0.025)
q975 <- quantile(samples, 0.975)
res <- cbind(means, sds, medians, q025, q975)
res
## a
a <- mean(samples)^2 / var(samples)
## b
b <- mean(samples) / var(samples)
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))
a
b

## winBUGS output
node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
tau	0.499872	0.354029	0.00117622	0.0613616	0.419798	1.39341	1	100000

##a = mean^2  / sd^2
##b = mean    / sd^2
## ===> WinBUGS is getting a gamma(1.5,3.5) posterior

a <- 1.5
b <- 3.5
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))



## finding BUGS tau sampling (3)
library(nimble)
##
code <- nimbleCode({
    tau ~ dunif(0, 1000)
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
})
##
data <- list(
    N = 4,
    num = c(1,3,2,2),
    adj = c(2,   1,3,4,    2,4,   2,3),
    weights = c(1,1,1,1,1,1,1,1),
    L = 8,
    S = c(-1, 0, 1, 2)
)
##
inits <- list(
    tau = 1
)
##
catCode(code, data, inits, file='~/temp/BUGS.txt')
Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','weights','L')], data=data['S'], inits)
##
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

##
niter <- 500000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)[100001:500000,]
dim(samples)
length(samples)
##

##means <- apply(samples, 2, mean)
##sds <- apply(samples, 2, sd)
##medians <- apply(samples, 2, median)
means <- mean(samples)
sds <- sd(samples)
medians <- median(samples)
q025 <- quantile(samples, 0.025)
q975 <- quantile(samples, 0.975)
res <- cbind(means, sds, medians, q025, q975)
res
## a
a <- mean(samples)^2 / var(samples)
## b
b <- mean(samples) / var(samples)
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))
a
b

## winBUGS output
node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
tau	0.429886	0.35268	0.00114715	0.031528	0.338512	1.33672	1	100000

##a = mean^2  / sd^2
##b = mean    / sd^2
## ===> WinBUGS is getting a gamma(1.5,3.5) posterior

a <- 1.5
b <- 3.5
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))


## finding BUGS tau sampling (2)
library(nimble)
##
code <- nimbleCode({
    tau ~ dunif(0, 1000)
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
})
##
data <- list(
    N = 4,
    num = c(1,2,2,1),
    adj = c(2,   1,3,    2,4,   3),
    weights = c(1,1,1,1,1,1),
    L = 6,
    S = c(-1, 0, 1, 2)
)
##
inits <- list(
    tau = 1
)
##
catCode(code, data, inits, file='~/temp/BUGS.txt')
Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','weights','L')], data=data['S'], inits)
##
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

##
niter <- 500000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)[100001:500000,]
dim(samples)
length(samples)
##

##means <- apply(samples, 2, mean)
##sds <- apply(samples, 2, sd)
##medians <- apply(samples, 2, median)
means <- mean(samples)
sds <- sd(samples)
medians <- median(samples)
q025 <- quantile(samples, 0.025)
q975 <- quantile(samples, 0.975)
res <- cbind(means, sds, medians, q025, q975)
res
## a
a <- mean(samples)^2 / var(samples)
## b
b <- mean(samples) / var(samples)
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))
a
b

## winBUGS output
node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
tau	1.003	0.8229	0.002677	0.07357	0.7899	3.119	1	100000

##a = mean^2  / sd^2
##b = mean    / sd^2
## ===> WinBUGS is getting a gamma(1.5,1.5) posterior

a <- 1.5
b <- 1.5
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))


## finding BUGS tau sampling (1)
library(nimble)
##
code <- nimbleCode({
    ##alpha0 ~ dflat()
    ##sd ~ dunif(0, 1000)
    ##tau <- 1/(sd*sd)
    tau ~ dunif(0, 1000)
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
    ##for(i in 1:N) {
    ##    mu[i] <- S[i] + alpha0
    ##    Y[i] ~ dnorm(mu[i], 1)
    ##}
})
##
data <- list(
    N = 3,
    num = c(1,2,1),
    adj = c(2,   1,3,    2),
    weights = c(1,1,1,1),
    L = 4,
    ##Y = c(1,2,3)
    S = c(-1, 0, 1)
)
##
inits <- list(
    tau = 1
    ##alpha0 = 0,
    ##S = c(0,0,0)
)
##
catCode(code, data, inits, file='~/temp/BUGS.txt')
Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','weights','L')], data=data['S'], inits)
##
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

##
niter <- 500000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)[100001:500000,]
dim(samples)
length(samples)
##

##means <- apply(samples, 2, mean)
##sds <- apply(samples, 2, sd)
##medians <- apply(samples, 2, median)
means <- mean(samples)
sds <- sd(samples)
medians <- median(samples)
res <- cbind(means, sds, medians)
res
        means      sds  medians
[1,] 2.517552 1.586795 2.194357

## a
mean(samples)^2 / var(samples)
## b
mean(samples) / var(samples)


a <- 5
b <- 4
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))


## winBUGS output
node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
tau	1.002	1.008	0.004465	0.02487	0.6956	3.749	1	47200
## ===> WinBUGS is getting a gamma(1,1) posterior
a <- 1
b <- 1
n <- 1e5
x <- rgamma(n,a,b)
mean(x)
sd(x)
median(x)
quantile(x, probs=c(0.025, 0.975))


## preproducible example of problem with weights[k] <- 1
## for Perry
## FILED ON GITHUB 5/31/17
library(nimble)
##
code <- nimbleCode({
    ##for(i in 1:L) {   weights[i] <- 1   }
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], 3)
    for(i in 1:N) {
	Y[i] ~ dnorm(S[i], 1)
    }
})
##
constants <- list(
    N = 3,
    num = c(1,2,1),
    adj = c(2,   1,3,    2),
    weights = rep(1,4),
    L = 4
)
##
data <- list(
    Y = c(1,2,3)
)
##
inits <- list(
    S = c(0,0,0)
)
##
Rmodel <- nimbleModel(code, constants, data, inits)
##
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Rmcmc$samplerFunctions[[1]]$componentSamplerFunctions[[1]]$dcar$neighborWeights
Rmcmc$samplerFunctions[[1]]$componentSamplerFunctions[[2]]$dcar$neighborWeights
Rmcmc$samplerFunctions[[1]]$componentSamplerFunctions[[3]]$dcar$neighborWeights

niter <- 100
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)

samples[98:100, ]
## [98,]  0.190857546 -3.875851e-01  0.19672751
## [99,] -0.473204489  1.077377e-01  0.36546680
##[100,] -0.370861918 -8.497980e-02  0.45584172




## NOT AGREEING WITH SAMPLING FOR TAU
## CAR scalar_RW sampler
## no islands
## also sampling tau
library(nimble)
##
code <- nimbleCode({
    alpha0 ~ dflat()
    for(k in 1:L) {
        weights[k] <- 1
    }
    sd ~ dunif(0, 1000)
    tau <- 1/(sd*sd)
    ##tau ~ dunif(0, 1000)
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau)
    for(i in 1:N) {
        log(mu[i]) <- alpha0 + S[i]
        Y[i] ~ dpois(mu[i])
    }
})
##
data <- list(
    N = 6,
    num = c(3,4,4,3,2,2),
    adj =     c(2,3,4,   1,3,5,6,   1,2,4,5,   1,3,6,  2,3,   2,4),
    L = 18,
    Y = c(10,12,12,10,14,10)
)
##
inits <- list(
    alpha0 = 0,
    sd = 1,
    S = c(0,0,0,0,0,0)
)
##
Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','L')], data=data['Y'], inits)
##
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$addMonitors('S','sd','tau')
Rmcmc <- buildMCMC(conf)
##
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##
niter <- 200000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)[50000:200000,]
##
##samplesPlot(samples, 'alpha0')
##samplesPlot(samples, 'tau')
##
means <- apply(samples, 2, mean)
sds <- apply(samples, 2, sd)
medians <- apply(samples, 2, median)
res <- cbind(means, sds, medians)
res


## winBUGS output
node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
sd	0.3021	0.2928	0.004865	0.01032	0.2264	1.047	501	49500





## CAR scalar_RW, scalar_conjugate, and scalar_postPred samplers
## no islands
## fixed tau = 3
library(nimble)

code <- nimbleCode({
    alpha0 ~ dflat()
    for(k in 1:L) {
        weights[k] <- 1
    }
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], 3)
    for(i in 1:N) {
        mu[i] <- alpha0 + S[i]
    }
    for(i in 1:2) {
        log(lambda[i]) <- mu[i]
        Y[i] ~ dpois(lambda[i])
    }
    Y[3] ~ dnorm(mu[3], 3)
    ymean4 <- 5*mu[4]
    Y[4] ~ dnorm(ymean4, 7)
    ymean5 <- 2*mu[5]
    Y[5] ~ dnorm(ymean5, 1)
})

data <- list(
    N = 6,
    num = c(3,4,4,3,2,2),
    adj =     c(2,3,4,   1,3,5,6,   1,2,4,5,   1,3,6,  2,3,   2,4),
    L = 18,
    Y = c(10,12,15,20,24)
)

inits <- list(
    alpha0 = 0,
    S = c(0,0,0,0,0,0)
)

Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','L')], data=data['Y'], inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

niter <- 100000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)

means <- apply(samples, 2, mean)
sds <- apply(samples, 2, sd)
res <- cbind(means, sds)
res
##            means       sds
##S[1]   -1.6762604 0.1988165
##S[2]   -1.3264934 0.1621055
##S[3]    1.8720905 0.2261232
##S[4]   -0.8194848 0.1493609
##S[5]    3.0231445 0.2757768
##S[6]   -1.0729963 0.3532078
##alpha0  4.8462442 0.1313957

## winBUGS output
node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
S[1]	-1.675	0.1776	0.001971	-2.027	-1.674	-1.328	1	10000
S[2]	-1.323	0.1479	0.0013	-1.616	-1.32	-1.036	1	10000
S[3]	1.867	0.2275	0.001976	1.417	1.867	2.316	1	10000
S[4]	-0.8196	0.1365	0.001492	-1.088	-0.819	-0.5559	1	10000
S[5]	3.024	0.276	0.002745	2.479	3.025	3.564	1	10000
S[6]	-1.073	0.3552	0.003718	-1.773	-1.072	-0.3791	1	10000
alpha0	4.846	0.1308	0.001459	4.589	4.847	5.098	1	10000

## agrees!!





## CAR scalar_RW sampler
## no islands
## fixed tau = 3
library(nimble)

code <- nimbleCode({
    alpha0 ~ dflat()
    for(k in 1:L) {
        weights[k] <- 1
    }
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], 3)
    for(i in 1:N) {
        log(mu[i]) <- alpha0 + S[i]
        Y[i] ~ dpois(mu[i])
    }
})

data <- list(
    N = 6,
    num = c(3,4,4,3,2,2),
    adj =     c(2,3,4,   1,3,5,6,   1,2,4,5,   1,3,6,  2,3,   2,4),
    L = 18,
    Y = c(10,12,15,20,24,16)
)

inits <- list(
    alpha0 = 0,
    S = c(0,0,0,0,0,0)
)

Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','L')], data=data['Y'], inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

niter <- 100000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)

means <- apply(samples, 2, mean)
sds <- apply(samples, 2, sd)
res <- cbind(means, sds)
res
##              means       sds
##S[1]   -0.265103019 0.1899968
##S[2]   -0.144243381 0.1713349
##S[3]   -0.028341197 0.1699618
##S[4]    0.125589974 0.1722319
##S[5]    0.310280206 0.1774359
##S[6]    0.001817417 0.1920584
##alpha0  2.744200920 0.1039359

## winBUGS output
 
##node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
##S[1]	-0.2619	0.1891	0.001877	-0.6427	-0.2585	0.1017	1	10000
##S[2]	-0.1415	0.1718	0.001695	-0.4899	-0.1373	0.1827	1	10000
##S[3]	-0.0275	0.1687	0.001798	-0.3593	-0.02419	0.2992	1	10000
##S[4]	0.1236	0.1738	0.001715	-0.2203	0.1239	0.4595	1	10000
##S[5]	0.3083	0.1755	0.001711	-0.04451	0.3102	0.6477	1	10000
##S[6]  -0.001004	0.1907	0.001934	-0.3914	0.004113	0.3587	1	10000
##alpha0	2.744	0.103	0.001042	2.535	2.746	2.939	1	10000

## agrees!!





## CAR scalar_RW sampler
## no islands
## fixed tau = 1
library(nimble)

code <- nimbleCode({
    alpha0 ~ dflat()
    for(k in 1:L) {
	weights[k] <- 1
    }
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], 1)
    for(i in 1:N) {
        log(mu[i]) <- alpha0 + S[i]
        Y[i] ~ dpois(mu[i])
    }
})

data <- list(
    N = 6,
    num = c(3,4,4,3,2,2),
    adj =     c(2,3,4,   1,3,5,6,   1,2,4,5,   1,3,6,  2,3,   2,4),
    ##weights = c(1,1,1,   1,1,1,1,   1,1,1,1,   1,1,1,  1,1,   1,1),
    L = 18,
    Y = c(10,12,15,20,24,16)
)

inits <- list(
    alpha0 = 0,
    S = c(0,0,0,0,0,0)
)

Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','L')], data=data['Y'], inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

niter <- 100000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)

library(coda)
apply(samples, 2, effectiveSize)

means <- apply(samples, 2, mean)
sds <- apply(samples, 2, sd)
res <- cbind(means, sds)
res
##             means       sds
##S[1]   -0.36240674 0.2350176
##S[2]   -0.20235429 0.2146017
##S[3]   -0.03327436 0.2060798
##S[4]    0.19690032 0.1976192
##S[5]    0.38665926 0.1936092
##S[6]    0.01447581 0.2166168
##alpha0  2.72605237 0.1060945

## winBUGS output
##node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
##alp0	2.726	0.1055	0.001117	2.514	2.728	2.929	1	10000
##S[1]	-0.3636	0.2341	0.002337	-0.8376	-0.3549	0.08407	1	10000
##S[2]	-0.2063	0.216	0.002338	-0.6496	-0.2007	0.1983	1	10000
##S[3]	-0.0335	0.203	0.001985	-0.4438	-0.0292	0.3531	1	10000
##S[4]	0.1971	0.1987	0.001868	-0.209	0.2003	0.5715	1	10000
##S[5]	0.3891	0.1914	0.00197	0.009187	0.3927	0.7583	1	10000
##S[6]	0.01712	0.2166	0.002037	-0.4136	0.02405	0.4305	1	10000

## agrees!!




## testing default weights in dcar_normal() distribution
library(nimble)

code <- nimbleCode({
    alpha0 ~ dflat()
    ##for(i in 1:L) {
    ##    weights[i] <- 1
    ##}
    S[1:N] ~ car.normal(adj = adj[1:L], num = num[1:N], tau = 1)
    for(i in 1:N) {
        log(mu[i]) <- alpha0 + S[i]
        Y[i] ~ dpois(mu[i])
    }
})
data <- list(
    N = 6,
    num = c(3,4,4,3,2,2),
    adj =     c(2,3,4,   1,3,5,6,   1,2,4,5,   1,3,6,  2,3,   2,4),
    ##weights = rep(1, 18),
    L = 18,
    Y = c(10,12,15,20,24,16)
)
inits <- list(
    alpha0 = 0,
    ##weights = rep(1, 18),
    S = c(0,0,0,0,0,0)
)

Rmodel <- nimbleModel(code, constants=data[c('N','num','adj','L')], data=data['Y'], inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Rmodel$calculate('S')
Cmodel$calculate('S')

niter <- 100000
set.seed(0); Cmcmc$run(niter)
samples <- as.matrix(Cmcmc$mvSamples)
means <- apply(samples, 2, mean)
sds <- apply(samples, 2, sd)
res <- cbind(means, sds)
res
##             means       sds
##S[1]   -0.36240674 0.2350176
##S[2]   -0.20235429 0.2146017
##S[3]   -0.03327436 0.2060798
##S[4]    0.19690032 0.1976192
##S[5]    0.38665926 0.1936092
##S[6]    0.01447581 0.2166168
##alpha0  2.72605237 0.1060945





library(nimble)
##nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
##
## Scottish Lip Cancer example from WinBUGS/geoBUGS of dcar_normal() distribuion
##
## data copied from WinBUGS
N <- 56
E <- c(1.4, 8.7, 3.0, 2.5, 4.3, 2.4, 8.1, 2.3, 2.0, 6.6,
       4.4, 1.8, 1.1, 3.3, 7.8, 4.6, 1.1, 4.2, 5.5, 4.4,
       10.5,22.7, 8.8, 5.6,15.5,12.5, 6.0, 9.0,14.4,10.2,
       4.8, 2.9, 7.0, 8.5,12.3,10.1,12.7, 9.4, 7.2, 5.3,
       18.8,15.8, 4.3,14.6,50.7, 8.2, 5.6, 9.3,88.7,19.6,
       3.4, 3.6, 5.7, 7.0, 4.2, 1.8)
X <- c(16,16,10,24,10,24,10, 7, 7,16, 7,16,10,24, 7,16,10,
       7, 7,10, 7,16,10, 7, 1, 1, 7, 7,10,10,7,24,10, 7, 7,
       0,10, 1,16, 0, 1,16,16, 0, 1, 7, 1, 1, 0, 1,1, 0, 1, 1,16,10)
num <- c(3, 2, 1, 3, 3, 0, 5, 0, 5, 4, 0, 2, 3, 3, 2, 6, 6, 6, 5, 3,
         3, 2, 4, 8, 3, 3, 4, 4, 11, 6, 7, 3, 4, 9, 4, 2, 4, 6, 3, 4, 
         5, 5, 4, 5, 4, 6, 6, 4, 9, 2, 4, 4, 4, 5, 6, 5)
adj <- c(19, 9, 5, 10, 7, 12, 28, 20, 18, 19, 12, 1, 
         17, 16, 13, 10, 2, 29, 23, 19, 17, 1, 22, 16, 7, 2, 
         5, 3, 19, 17, 7, 35, 32, 31, 29, 25, 29, 22, 21, 17,
         10, 7, 29, 19, 16, 13, 9, 7, 56, 55, 33, 28, 20, 4,
         17, 13, 9, 5, 1, 56, 18, 4, 50, 29, 16, 16, 10, 39, 34, 29, 9,
         56, 55, 48, 47, 44, 31, 30, 27, 29, 26, 15, 43, 29, 25,
         56, 32, 31, 24, 45, 33, 18, 4, 50, 43, 34, 26, 25, 23, 21,
         17, 16, 15, 9, 55, 45, 44, 42, 38, 24, 47, 46, 35, 32, 27, 24, 14, 
         31, 27, 14, 55, 45, 28, 18, 54, 52, 51, 43, 42, 40, 39, 29, 23,
         46, 37, 31, 14, 41, 37, 46, 41, 36, 35, 54, 51, 49, 44, 42, 30,
         40, 34, 23, 52, 49, 39, 34, 53, 49, 46, 37, 36, 51, 43, 38, 34, 30,
         42, 34, 29, 26, 49, 48, 38, 30, 24, 55, 33, 30, 28, 53, 47, 41,
         37, 35, 31, 53, 49, 48, 46, 31, 24, 49, 47, 44, 24, 54, 53, 52,
         48, 47, 44, 41, 40, 38, 29, 21, 54, 42, 38, 34, 54, 49, 40, 34,
         49, 47, 46, 41, 52, 51, 49, 38, 34, 56, 45, 33, 30, 24, 18,
         55, 27, 24, 20, 18)
L <- length(adj)
weights <- rep(1, L)
constantsIslands <- list(N=N, L=L, E=E, X=X, num=num, adj=adj, weights=weights)
##
## alternate data with no islands, copied from Breslow and Clatyon (1993)
## no islands in this data
num <- c(4, 2, 2, 3, 5, 2, 5, 1, 6, 4, 4, 3, 4, 3, 3, 6, 6, 6, 5,  ##different!
         3, 3, 2, 6, 8, 3, 4, 4, 4, 11, 6, 7, 4, 4, 9, 5, 4, 5, 6,
         5, 5, 7, 6, 4, 5, 4, 6, 6, 4, 9, 3, 4, 4, 4, 5, 5, 6)
adj <- c(5,9,11, 19, 7,10, 6,12, 18, 20, 28, 1, 11, 12,13,19, 3,  ## different!!
         8, 2,10,13,16,17, 6, 1, 11, 17,19,23,29, 2, 7, 16, 22,
         1,5,9,12, 3, 5, 11, 5, 7,17,19, 31, 32, 35, 25, 29, 50,
         7,10,17,21, 22,29, 7,9,13,16,19,29, 4,20,28,33,55,56, 1,
         5,9,13,17, 4,18,55, 16,29,50, 10,16, 9,29,34,36,37,39, 27,
         30,31,44,47,48,55,56, 15,26,29, 25,29,42,43, 24,31,32,55,
         4,18,33,45, 9, 15, 16, 17, 21, 23, 25, 26, 34, 43, 50, 24,
         38,42,44,45,56, 14,24,27,32,35,46,47, 14,27,31,35, 18,28,
         45,56, 23, 29, 39, 40, 42, 43, 51, 52, 54, 14, 31, 32, 37,
         46, 23, 37, 39, 41, 23, 35, 36, 41, 46, 30,42,44,49,51,54,
         23,34,36,40,41, 34,39,41,49,52, 36,37,39,40,46,49,53, 26,
         30, 34, 38, 43, 51, 26, 29, 34, 42, 24, 30, 38, 48, 49, 28,
         30, 33, 56, 31, 35, 37, 41, 47, 53, 24, 31, 46, 48, 49, 53,
         24,44,47,49, 38, 40, 41, 44, 47, 48, 52, 53, 54, 15,21, 29,
         34, 38, 42, 54, 34,40,49,54, 41, 46, 47, 49, 34, 38, 49,
         51, 52, 18, 20, 24, 27, 56, 18, 24, 30, 33, 45, 55)
L <- length(adj)
weights <- rep(1, L)
constantsNoIslands <- list(N=N, L=L, E=E, X=X, num=num, adj=adj, weights=weights)
##
## observations:
Y <- c(9, 39, 11, 9, 15, 8, 26, 7, 6, 20, 13, 5, 3, 8, 17, 9, 2, 7, 9,
       7, 16, 31, 11, 7, 19, 15, 7, 10, 16, 11, 5, 3, 7, 8, 11, 9, 11,
       8, 6, 4, 10, 8, 2, 6, 19, 3, 2, 3, 28, 6, 1, 1, 1, 1, 0, 0)
data <- list(Y=Y)
##
## model code (simple)
## from geoBUGS manual
## and used in Breslow and Clatyon (1993)
codeSimple <- nimbleCode({
    alpha0  ~ dflat()  
    alpha1 ~ dnorm(0, 0.00001)
    tau ~ dgamma(0.5, 0.0005)
    sigma <- sqrt(1 / tau)
    ##for(k in 1:L)    ## need to make this work!
    ##    weights[k] <- 1
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau, zero_mean = 1)
    for(i in 1:N) {
        log(mu[i]) <- log(E[i]) + alpha0 + alpha1 * X[i]/10 + S[i]
        Y[i] ~ dpois(mu[i])
        RR[i] <- exp(alpha0 + alpha1 * X[i]/10 + S[i])
    }
})
##initsSimple <- list(tau = 1, alpha0 = 0, alpha1 = 0,    ## need to make this work!!
##                    S=c(0,0,0,0,0,NA,0,NA,0,0,
##                        NA,0,0,0,0,0,0,0,0,0,
##                        0,0,0,0,0,0,0,0,0,0,
##                        0,0,0,0,0,0,0,0,0,0,
##                        0,0,0,0,0,0,0,0,0,0,
##                        0,0,0,0,0,0))
initsSimple <- list(tau = 1, alpha0 = 0, alpha1 = 0, 
                    S=c(0,0,0,0,0,0,0,0,0,0,
                        0,0,0,0,0,0,0,0,0,0,
                        0,0,0,0,0,0,0,0,0,0,
                        0,0,0,0,0,0,0,0,0,0,
                        0,0,0,0,0,0,0,0,0,0,
                        0,0,0,0,0,0))
##
## more complex model code
## also used in geoBUGS manual, includes results for this:
## code from geoBUGS example model
codeComplex <- nimbleCode({
    alpha0 ~ dflat()
    alpha1 ~ dnorm(0, 0.00001)
    tau ~ dgamma(0.5, 0.0005)
    tau.h ~ dgamma(0.5, 0.0005)
    S[1:N] ~ car.normal(adj[1:L], weights[1:L], num[1:N], tau, zero_mean = 1)
    for(i in 1:N) {
        H[i] ~ dnorm(0, tau.h)
        log(mu[i]) <- log(E[i]) + alpha0 + alpha1 * X[i]/10 + S[i] + H[i]
        Y[i] ~ dpois(mu[i])
        RR[i]  <- exp(alpha0 + alpha1 * X[i]/10 + S[i] + H[i])
        residRR[i] <- exp(S[i] + H[i])
        X.pred[i] <- alpha1 * X[i]/10
        lE[i] <- log(E[i])
    }
    rr.x <- exp(alpha1)
    sdS <- sd(S[1:N])
    sdH <- sd(H[1:N])
    sdX <- sd(X.pred[1:N])
    sdE <- sd(lE[1:N])
    sumvar <- sdS^2 + sdH^2 + sdX^2 + sdE^2
    pS <- sdS^2 / sumvar
    pH <- sdH^2 / sumvar
    pX <- sdX^2 / sumvar
    pE <- sdE^2 / sumvar
})
initsComplex <- list(alpha0=0, alpha1=0, tau=1, tau.h=1, S=rep(0,N), H=rep(0,N))

## this mimics Breslow and Clatyon (1993)
Rmodel <- nimbleModel(codeSimple, constantsNoIslands, data, initsSimple)

## this mimics geoBUGS manual
Rmodel <- nimbleModel(codeComplex, constantsIslands, data, initsComplex)

Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()

conf$printMonitors()
conf$addMonitors('sigma')      ## for (B&C 1993) model
conf$addMonitors('sigma', 'S')
conf$addMonitors('sigma', 'S', 'RR')
conf$addMonitors('rr.x', 'pS', 'pH', 'pX', 'pE')        ## for geoBUGS manual model
conf$addMonitors('rr.x', 'pS', 'pH', 'pX', 'pE', 'RR')

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##
niter <- 300000
nburnin <- 50000
set.seed(0); system.time(Cmcmc$run(niter))   ## ~30 sec (B&C 1993 model), ~45 sec (geoBUGS manual model)
samples <- as.matrix(Cmcmc$mvSamples)[(nburnin+1):niter, ]
dim(samples)
colnames(samples)

library(coda)
apply(samples, 2, effectiveSize)

means <- apply(samples, 2, mean)
sds <- apply(samples, 2, sd)
res <- cbind(means, sds)

res
res[c('alpha0','alpha1','sigma'), ]
res[c('alpha0','alpha1','sigma','tau'), ]
res[c('pE','pH','pS','pX','rr.x'), ]

## this mimics Breslow and Clatyon (1993)
## their results:
## alpha0: -0.18 (sd = 0.12)
## alpha1: -0.35 (sd = 0.12)
## sigma:   0.73 (sd = 0.13)
## NIMBLE:
##            means       sds
##alpha0 -0.2099181 0.1201011
##alpha1  0.3582498 0.1279591
##sigma   0.7176594 0.1239661

## this mimics geoBUGS manual
## their results:
##pE:   0.6238 (sd = 0.0367)
##pH:   0.0182 (sd = 0.0308)
##pS:   0.2718 (sd = 0.0549)
##pX:   0.0862 (sd = 0.0407)
##rr.x: 1.587  (sd = 0.1907)
## NIMBLE:
##           means        sds
##pE   0.600743362 0.03484688
##pH   0.005349068 0.01306911
##pS   0.320298241 0.05209477
##pX   0.073609329 0.04107101
##rr.x 1.537977214 0.20651663


## hand-implement the "ranked" statistics to verify against BUGS book
colnames(samples)
rrnames <- paste0('RR[', 1:56, ']')
rrsamples <- samples[, rrnames]
colnames(rrsamples)
dim(rrsamples)
apply(rrsamples, 2, effectiveSize)
 
a <- rrsamples[50001:100000,]
low <- 6
high <- 51
out <- array(NA, c(dim(a)[1], 2))
 
for(i in 1:dim(a)[1]) {
    vals <- a[i,]
    sor <- sort(vals)
    out[i,1] <- sor[low]
    out[i,2] <- sor[high]
}
 
ratio <- out[,2] / out[,1]
mean(ratio)
median(ratio)
sd(ratio)







## test models for dcar_normal distribution

library(nimble)
N <- 15
num <- c(9,    rep(1,9), 0,0,0,0,0)
adj <- c(2:10, rep(1,9))
L <- length(adj)
weights <- rep(1, L)
code <- nimbleCode({
    t ~ dunif(0, 100)
    x[1:N] ~ dcar_normal(adj[1:L], weights[1:L], num[1:N], tau=t)
    y1 ~ dexp(x[1])
    y2 ~ dnorm(x[2], 1)
    z <- x[3] + 3
    y3 ~ dnorm(z, 1)
    y4 ~ dnorm(x[4]*x[4], 1)
    y51 ~ dnorm(x[5]+5, 1)
    y52 ~ dnorm(x[5]/2, 2)
    for(i in 1:9) {
        mu[i] <- i
    }
    mu[10] <- x[6]
    y6[1:10] ~ dmnorm(mu[1:10], C[1:10,1:10])
})
constants <- list(N = N, L = L, adj = adj, weights = weights, num = num, C = diag(10))
data <- list(y1=1, y2=1, y3=1, y4=1, y51=1, y52=1, y6=rep(0,10))
inits <- list(x = rep(1,N), t = 1)
Rmodel <- nimbleModel(code, constants, data, inits)


library(nimble)
N <- 5
num <- c(0, 0, 0, 1, 1)
adj <- c(5, 4)
L <- length(adj)
weights <- rep(1, L)
code <- nimbleCode({
    t ~ dunif(0, 100)
    x[1:N] ~ dcar_normal(adj[1:L], weights[1:L], num[1:N], tau = t)
    y2 ~ dnorm(x[2], 1)
    y3 ~ dexp(x[3])
})
constants <- list(N = N, L = L, adj = adj, weights = weights, num = num)
data <- list(y2 = 2, y3 = 3)
inits <- list(x = 1:N, t = 1)
Rmodel <- nimbleModel(code, constants, data, inits)



conf <- configureMCMC(Rmodel)
##conf <- configureMCMC(Rmodel, control=list(adaptInterval=1000000))
##conf <- configureMCMC(Rmodel, control=list(carUseConjugacy=FALSE))
##conf$printSamplers(displayControlDefaults = TRUE)
conf$printSamplers()
##conf$addMonitors('x')
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel, showCompilerOutput = TRUE)

Rmodel$calculate('x')
Cmodel$calculate('x')
## -8.270447
## -1.837877  (3 islands model)
## -47.7848   (Scottish lip cancer)

niter <- 10
set.seed(0); Rmcmc$run(niter)
set.seed(0); Cmcmc$run(niter)
Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)
sampnames <- dimnames(Rsamples)[[2]]
Rsamples[, sampnames]
Csamples[, sampnames]
Rsamples[, sampnames] - Csamples[, sampnames]

niter <- 100000
##niter <- 10
##set.seed(0); Rmcmc$run(niter)
set.seed(0); Cmcmc$run(niter)

samples <- as.matrix(Cmcmc$mvSamples)

samples[niter,]
##         t       x[1]       x[2]       x[3]       x[4]       x[5]       x[6] 
## 0.1287901  0.5934936 -0.4500610 -1.7883696 -1.6046201 -0.9227700  0.6587114 
##      x[7]       x[8]       x[9]      x[10] 
## 4.7227748 -0.3847551  0.7452862  2.3125083 

apply(samples, 2, mean)
apply(samples, 2, sd)

samplesPlot(samples, 1)
samplesPlot(samples, 2:6)
samplesPlot(samples, c(3,4))

library(coda)
apply(samples, 2, effectiveSize)




> apply(samples, 2, mean)
          t        x[1]        x[2]        x[3]        x[4]        x[5] 
 0.10855950  1.15413864  0.99105430 -1.73847094  0.08252756 -1.81795004 
       x[6]        x[7]        x[8]        x[9]       x[10] 
 0.08001419  1.15626026  1.14893480  1.16751225  1.13389828 

> apply(samples, 2, sd)
        t      x[1]      x[2]      x[3]      x[4]      x[5]      x[6]      x[7] 
0.1131858 0.8444364 0.9587355 0.9704785 0.9258072 0.8092274 0.9547935 5.8191558 
     x[8]      x[9]     x[10] 
5.6441207 5.6973708 5.8381872 

> apply(samples, 2, effectiveSize)
        t      x[1]      x[2]      x[3]      x[4]      x[5]      x[6]      x[7] 
 2106.414  7062.471 93635.441 44922.136 20619.570 48727.286 22121.622 87259.953 
     x[8]      x[9]     x[10] 
79181.284 64071.703 78410.282 




## example of summing 0-length vector in nimble
library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)

nfDef <- nimbleFunction(
    setup = function(model, nodes) {},
    run = function() {
        a <- values(model, nodes)
        b <- sum(a)
    }
)

Rnf <- nfDef(Rmodel, character(0))

Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(Rnf, project = Rmodel)

Rnf$run()
Cnf$run()





## timing comparison of named vs. indexed lookup for named vectors
N <- 5
n <- 10^N
vec <- 1:n
nn <- paste0('x', 1:n)
names(vec) <- nn
##system.time(for(i in 1:n) { thisname <- paste0('x', i); out <- vec[thisname] })
system.time(for(i in 1:n) { thisname <- paste0('x', i); out <- vec[i] })   ## WAYYYYY FASTER !!!!!!!



## making conditional code block for samplerAssignmentRules

library(nimble)
nimbleOptions('MCMCuseSamplerAssignmentRules')
nimbleOptions(MCMCuseSamplerAssignmentRules = TRUE)
nimbleOptions('MCMCuseSamplerAssignmentRules')

rules <- nimbleOptions('MCMCdefaultSamplerAssignmentRules')
rules


code <- nimbleCode({
  a ~ dnorm(0, rs)
  b <- a + 1
  c ~ dnorm(b, 1)
  d <- c-1
})
constants <- list()
data <- list()
inits <- list(a = 0, c=0, rs=1)
Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)

conf$printSamplers()




## very quick test of compilation of dcar_normal(), for Chris P.
library(nimble)
code <- nimbleCode({
    x[1:3] ~ dcar_normal(adj[1:3], weights[1:3], num[1:3], t)
})
constants <- list(adj = 1:3, weights = 1:3, num = 1:3, t = 0)
data <- list()
inits <- list()


Rmodel <- nimbleModel(code, constants, data, inits, calculate=FALSE)

Cmodel <- compileNimble(Rmodel, showCompilerOutput = TRUE)




## bug in getDependencies(..., self=FALSE) !
library(nimble)
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
nimbleOptions(saveMCMChistory = TRUE)

code <- nimbleCode({
    x[1:N] ~ dmnorm(mu[1:N], C[1:N,1:N])
    y ~ dexp(x[1])
})
N <- 15
constants <- list(mu = rep(0,N), C = diag(N), N = N)
data <- list(y=10)
inits <- list(x = rep(5, N))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 1000)

Cmcmc$samplerFunctions$contentsList[[1]]$getScaleHistory()
Cmcmc$samplerFunctions$contentsList[[1]]$getAcceptanceHistory()
Cmcmc$samplerFunctions$contentsList[[1]]$getPropCovHistory()

length(Cmcmc$samplerFunctions$contentsList[[1]]$getScaleHistory())
length(Cmcmc$samplerFunctions$contentsList[[1]]$getAcceptanceHistory())
dim(Cmcmc$samplerFunctions$contentsList[[1]]$getPropCovHistory())


Rmodel$getDependencies('x[1:4]')
Rmodel$getDependencies('x[1:4]', self=FALSE)
Rmodel$getDependencies('x[1:4]', returnScalarComponents = TRUE)
Rmodel$getDependencies('x[1:4]', self=FALSE, returnScalarComponents = TRUE)

Rmodel$getDependencies('x[1]')
Rmodel$getDependencies('x[1]', self=FALSE)
Rmodel$getDependencies('x[1]', returnScalarComponents = TRUE)
Rmodel$getDependencies('x[1]', self=FALSE, returnScalarComponents = TRUE)


## looking at how it always lifts chol()
nms <- ls(Rmodel$nodes)
i <- 2
ls(Rmodel$nodes[[nms[i]]])
Rmodel$nodes[[nms[i]]]$calculate
Rmodel$nodes[[nms[i]]]$simulate

## bug in model building!
library(nimble)

code <- nimbleCode({
    x[1:4] ~ dmnorm(mu[1:4], C[1:4,1:4])
    y[1] ~ dnorm(x[2], 1)
})
constants <- list(mu = rep(0,4), C = diag(4))
Rmodel <- nimbleModel(code, constants)


## testing speed comparisons of new sampler assignment rules system


library(nimble)
nimbleOptions('MCMCuseSamplerAssignmentRules')

nimbleOptions(MCMCuseSamplerAssignmentRules = FALSE)
nimbleOptions(MCMCuseSamplerAssignmentRules = TRUE)

nimbleOptions('MCMCuseSamplerAssignmentRules')

timer <- function(n) {
    N <- 10^n
    library(nimble)
    code <- nimbleCode({
        for(i in 1:N) {
            a[i] ~ dnorm(0, 1)
            b[i] ~ dexp(a[i])
        }
    })
    constants <- list(N = N)
    data <- list(b = rep(10,N))
    inits <- list(a = rep(0,N))
    Rmodel <- nimbleModel(code, constants, data, inits)
    t <- system.time(conf <- configureMCMC(Rmodel))
    ##conf$printSamplers()
    return(as.numeric(t[3]))
}

sapply(1:4, function(n) timer(n))


## old system:
old <- c(0.049, 0.255, 2.986, 21.344)
## new sampler assignment rules:
new <- c(0.061, 0.460, 5.660, 52.530)   ## original
0.078  0.505  5.552 49.853  ## ruleSelectionCodeBlock, and only 1 eval()
0.066  0.474  5.688 49.230  ##
0.164  0.430  4.555 43.934  ## with isEndNode[], and nodeDistributions[] defined
0.139  0.376  3.271 32.278  ## is isEndNode[i], isBinary[i], isMultivariate[i], isConjugate[i]
0.034  0.276  3.316 35.299
0.082  0.393  3.625 35.707
0.105  0.287  3.261 32.724  ## ruleSelectFunction()
0.033  0.282  3.384 33.004

new/old

plot(1:4, new, col='red', type='l')
lines(1:4, old)


## my attempt at profiling the time for samplerAssignmentRules
?Rprof
?summaryRprof

N <- 5000
code <- nimbleCode({
    for(i in 1:N) {
        a[i] ~ dnorm(0, 1)
        b[i] ~ dexp(a[i])
    }
})
constants <- list(N = N)
data <- list(b = rep(10,N))
inits <- list(a = rep(0,N))
Rmodel <- nimbleModel(code, constants, data, inits)

profFile <- '~/temp/prof.txt'
Rprof(profFile)

system.time(conf <- configureMCMC(Rmodel))

Rprof(NULL)

profFile <- '~/temp/prof.txt'
summaryRprof(profFile)



## table of quantiles of t-distribution

dfs <- c(1,2,5,10,100,1000)
coverages <- c(.9, .95, .99)

ps <- 1-(1-coverages)/2
dfarg <- rep(dfs, each=length(coverages))
probarg <- rep(ps, length(dfs))
tvals <- qt(probarg, dfarg)
tab <- t(matrix(tvals, nrow=length(coverages), ncol=length(dfs)))
dimnames(tab) <- list(`df` = dfs, `CI Coverage` = paste0(100*coverages,'%'))
tab





## testing that nimble MCMC doesn't put samplers on deterministic nodes

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, rs)
    b <- a + 1
    c ~ dnorm(b, 1)
    d <- c-1
})
constants <- list()
data <- list()
inits <- list(a = 0, c=0, rs=1)
Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()

conf$addSampler('a', 'RW')
conf$addSampler('b', 'RW')
conf$addSampler('d', 'RW')
conf$addSampler('rs', 'RW')
conf$addSampler(c('a','rs'), 'RW_block')

Rmodel$getNodeNames()

conf <- configureMCMC(Rmodel, nodes = c('a', 'b', 'c', 'rs'))
conf$printSamplers()
conf <- configureMCMC(Rmodel, nodes = Rmodel$getNodeNames(includeRHSonly=TRUE))


Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

## example of Central Limit Theorem (CLT) for STAT201
unifMeans <- function(n, rep=500, breaks=100, curve=TRUE) {
    um <- replicate(rep, mean(runif(n)))
    hist(um, xlim=c(0,1), breaks=breaks, prob=TRUE, main=paste0('Uniform sample mean (n = ', n, ')'))
    if(curve) curve(dnorm(x, mean(um), sd(um)), col='red', add=TRUE)
}

n <- 4
par(mfrow=c(n,1), mar=c(2,2,2,2))
unifMeans(1, curve=FALSE)
##unifMeans(2)
##unifMeans(3)
unifMeans(4)
unifMeans(16)
unifMeans(64)





## playing with and testing new MCMC sampler assignment rules
library(nimble)
getNimbleOption('MCMCdefaultSamplerAssignmentRules')

code <- nimbleCode({
    for(i in 1:10) {
        a[i] ~ dnorm(0, 1)
    }
    for(i in 1:5) {
        b[i] ~ dexp(a[i] + 1)
    }
    x[1:10] ~ dmnorm(mu[1:10], C[1:10,1:10])
    y ~ dexp(x[1])
})
constants <- list(mu = rep(0,10), C = diag(10))
Rmodel <- nimbleModel(code, constants = constants)

conf <- configureMCMC(Rmodel)
conf$printSamplers()

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b1 ~ dnorm(0, 1)
    b2 ~ dnorm(0, 1)
    c1 ~ dnorm(0, 1)
    c2 ~ dnorm(0, 1)
    c3 ~ dnorm(0, 1)
    d ~ dnorm(0, 1)
    e1 ~ dnorm(0, 1)
    e2 ~ dnorm(0, 1)
    f ~ dnorm(0, 1)
    g ~ dnorm(0, 1)
    h ~ dnorm(0, 1)
    i ~ dnorm(0, 1)
    j ~ dnorm(0, 1)
    k1 ~ dnorm(0, 1)
    k2 ~ dnorm(0, 1)
    l1 ~ dnorm(0, 1)
    l2 ~ dnorm(0, 1)
    l3 ~ dnorm(0, 1)
    z1 ~ dnorm(0, 1)
    z2 ~ dnorm(0, 1)
})
Rmodel <- nimbleModel(code)

a_sampler <- nimbleFunction(setup = function() {}, run = function() 2)
x_sampler <- nimbleFunction(setup = function() {}, run = function() 3)

em <- samplerAssignmentRules(empty=TRUE, print=TRUE)
em$addRule(quote(grepl('^a', node)), 'a_sampler')
em$addRule(quote(grepl('^b1', node)), a_sampler)
em$addRule(quote(grepl('^b2', node)), a_sampler, name = 'sampler_for_node_b')
em$addRule(quote(grepl('^c1', node)), nimbleFunction(setup = function() {}, run = function() 4))
ll <- list()
ll[[1]] <- nimbleFunction(setup = function() {}, run = function() 5)
em$addRule(quote(grepl('^c2', node)), ll[[1]])
em$addRule(quote(grepl('^c3', node)), ll[[1]], name = 'sampler_for_c3')
em$addRule(quote(grepl('^d', node)), 'RW')
em$addRule(quote(grepl('^e1', node)), 'sampler_RW')
em$addRule(quote(grepl('^e2', node)), 'sampler_RW', name = 'sampler_for_e2')
em$addRule(quote(grepl('^f', node)), sampler_RW)
em$addRule(quote(grepl('^g', node)), quote(addSampler(target=node, type=sampler_RW)))
em$addRule(quote(grepl('^h', node)), quote(addSampler(target=node, type='RW')))
em$addRule(quote(grepl('^i', node)), quote(addSampler(target=node, type='sampler_RW')))
em$addRule(quote(grepl('^j', node)), quote(addSampler(target=node, type='x_sampler')))
em$addRule(quote(grepl('^k1', node)), quote(addSampler(target=node, type=x_sampler)))
em$addRule(quote(grepl('^k2', node)), quote(addSampler(target=node, type=x_sampler, name='sampler_for_k2')))
em$addRule(quote(grepl('^l1', node)), quote(addSampler(target=node, type=nimbleFunction(setup = function() {}, run = function() 6))))
em$addRule(
    quote(grepl('^l2', node)),
    quote(addSampler(
        target=node,
        type=nimbleFunction(setup = function() {}, run = function() 6),
        name='sampler_for_node_l2')))
em$addRule(
    quote(grepl('^l3', node)),
    quote(addSampler(
        target=node,
        type=nimbleFunction(setup = function() {}, run = function() 6))),
          name='sampler_for_node_l3')
em

conf <- configureMCMC(Rmodel, rules = em)
conf$printSamplers()

conf <- configureMCMC(Rmodel, rules = em, warnNoSamplerAssigned = FALSE)
warnings()
conf$printSamplers()

nimbleOptions(MCMCdefaultSamplerAssignmentRules = em)
getNimbleOption('MCMCdefaultSamplerAssignmentRules')

conf <- configureMCMC(Rmodel)
warnings()
conf$printSamplers()

nimbleOptions(MCMCdefaultSamplerAssignmentRules = samplerAssignmentRules())
getNimbleOption('MCMCdefaultSamplerAssignmentRules')

conf <- configureMCMC(Rmodel)
conf$printSamplers()


samplerAssignmentRules()

defaults$ruleList[[3]]
defaults$printRules(3)
defaults$printRules(c(3,5))

defaults$addRule(1, 5, print=TRUE, position=12)
defaults
defaults$orderRules(1:5, print=TRUE)

defaults$addRule(quote({a+b;TRUE;4}), 5, print=TRUE, position=3)
defaults$addRule(quote({a+b;TRUE;4}), quote({for(i in 1:10) {a + b; TRUE}}), print=TRUE)
defaults

defaults$addRule(TRUE, mean, print=TRUE)
defaults$addRule(TRUE, sampler_RW, print=TRUE)

defaults$reorder(c(1,4,5), print=TRUE)
defaults


defaults$reorder(c(1,3,2))
defaults$reorder(2:12)
defaults$reorder(c(1,4,0,3))
defaults$reorder(c(1,4,5.5,3))
defaults$addRule(quote(grepl('^a', node)), 'a_sampler', position=1, print=TRUE)


em <- samplerAssignmentRules(empty=TRUE, print=TRUE)
em$addRule(quote(grepl('^a', node)), 'a_sampler', position=1, print=TRUE)

em <- samplerAssignmentRules(empty=TRUE, print=TRUE)
em$addRule(quote(grepl('^a', node)), a_sampler, position=1, print=TRUE)

em$printRules()

conf <- configureMCMC(Rmodel, rules = em)
conf$printSamplers()


conf$printSamplers()


Rmodel$isMultivariate('a[1]')
Rmodel$isMultivariate('a[1:2]')
Rmodel$isMultivariate('x')
Rmodel$isMultivariate('x[1:5]')
Rmodel$isMultivariate(c('x[1:5]', 'x', 'a'))


## bug in copying models?

library(nimble)

code <- nimbleCode({
    for(i in 1:N) {
        x[i] ~ dnorm(0, 1)
    }
})
constants <- list(N=10)

def <- nimbleModel(code, constants, returnDef = TRUE)

Rmodel1 <- def$newModel(check = FALSE)

Rmodel2 <- def$newModel(check = FALSE)

Rmodel1$newModel(check=FALSE)
                 

## read and XML document

library(XML)

url <- 'https://www.w3schools.com/xml/plant_catalog.xml'
url

?xmlTreeParse
formals(xmlTreeParse)
args(xmlTreeParse)

xmlfile <- xmlTreeParse(url)
xmlfile <- xmlTreeParse(url, isURL=TRUE)

                                        # the xml file is now saved as an object you can easily work with in R:
class(xmlfile)
# Use the xmlRoot-function to access the top node
xmltop = xmlRoot(xmlfile)
# have a look at the XML-code of the first subnodes:
print(xmltop)[1:2]


## error in getDependencies

library(nimble)

code <- nimbleCode({
    x[1:10] ~ dmnorm(mu[1:10], C[1:10,1:10])
    for(i in 1:10) {
        y[i] ~ dnorm(x[i], 1)
    }
})

constants <- list(mu = rep(0,10), C = diag(10))
data <- list()
inits <- list(x = rep(0,10), y = rep(0,10))

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$getDependencies('x[5]')

Rmodel$calculate('x')

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)
##Cmcmc$run(10000)
##samples <- as.matrix(Cmcmc$mvSamples)

colnames(samples)
apply(samples, 2, mean)

samplesPlot(samples)

library(coda)
apply(samples, 2, effectiveSize)






## testing output of printSamplers() and getSamplers()
## and getSamplerDefinition()

library(nimble)

N <- 10
code <- nimbleCode({
    mu[1:N] ~ dmnorm(ones[1:N], C[1:N,1:N])
    a[1:N] ~ dmnorm(mu[1:N], C[1:N,1:N])
    x ~ dnorm(z, 1)
    y ~ dnorm(x, 1)
    z ~ dgamma(1, 1)
    for(i in 1:N) {
        qq[i] ~ dnorm(a[i], 1)
    }
})
constants <- list(N=N, C=diag(N), ones=rep(0,N))
data <- list(y = 0)
inits <- list(a = rep(0,N), x=0, qq=rep(0,N))
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)

conf$printSamplers()
conf$getSamplerDefinition(2)
conf$getSamplerDefinition(3)


conf$addSampler('a', 'RW_block', control=list(scale=2, adaptInterval=500, propCov=10*diag(N)))
conf$addSampler('z', 'RW', control=list(scale=3))
conf$addSampler('z', 'RW', control=list(scale=1:10))
conf$addSampler('z', 'RW', control=list(scale=c(1,4,6,7)))

conf$printSamplers()
conf$printSamplers(displayConjugateDep = TRUE)
conf$printSamplers(displayControlDefaults = TRUE)
conf$printSamplers(displayNonScalars = TRUE)
conf$getSamplers()

conf$samplerConfs[[1]]
class(conf$samplerConfs[[1]])


## testing using if() statements inside BUGS code

library(nimble)

code <- nimbleCode({
    if(orderx == 1) {
        y ~ dnorm(0,1)
    } else {
        y ~ dpois(1)
    }
})

orderx <- 1

m <- nimbleModel(code, constants=list(orderx=1))


## demo of using splines
library(splines)

data <- data.frame(value = c(0.01590000, 0.04040333, 0.08487666,
                             0.36868000, 0.31937660),
                   depth = c(10, 25, 50, 100, 180),
                   t = 1:5)

value.sp <- interpSpline(value ~ t, data)
depth.sp <- interpSpline(depth ~ t, data)
class(value.sp)
value.sp
plot(value.sp)
plot(depth ~ value, data)
lines(predict(value.sp)$y, predict(depth.sp)$y)

## demo of coin flips and LLN

set.seed(0); n <- 15; flips <- rbinom(n, 1, 0.5); flips
cumsum(flips)
running_avg <- cumsum(flips) / 1:n; running_avg
par(mfrow=c(1,1)); plot(running_avg, type='l'); abline(h=0.5, col='red', lty=3)
set.seed(0); n <- 100; flips <- rbinom(n, 1, 0.5); running_avg <- cumsum(flips) / seq_along(flips); plot(running_avg, type='l'); abline(h=0.5, col='red', lty=3)
ks <- 2:4; par(mfrow=c(length(ks),1), mar=c(2,2,2,2)); for(i in ks) { set.seed(0); n <- 10^i; flips <- rbinom(n, 1, 0.5); running_avg <- cumsum(flips) / seq_along(flips); plot(running_avg, type='l'); abline(h=0.5, col='red', lty=3) }



## testing use of rep() in NIMBLE DSL code

library(nimble)

code <- nimbleCode({
    for(i in 1:5) {
        a[i] ~ dnorm(0, 1)
    }
})
constants <- list()
data <- list()
inits <- list(a = 1:5)

Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)

nfDef <- nimbleFunction(
    setup = function(model) {
        d <- 5
        node <- 'a[1:5]'
    },
    run = function(a = double()) {
        model[[node]] <<- rep(a, d)
    }
)

Rnf <- nfDef(Rmodel)

Cnf <- compileNimble(Rnf, project=Rmodel)

Rmodel$a
Rnf$run(3)
Rmodel$a

Cmodel$a
Cnf$run(3)
Cmodel$a



## looking at Hurricane Damage data file for STAT201 takehome midterm

df <- read.csv("~/github/courses/stat201/data/HurricaneDamage.csv")
head(df)
dim(df)
df
with(df, plot(Year, Damage))
i <- which.max(df$Damage)
i
df[i,]

with(df, plot(Year[-1], Damage[-1]))


## trying to figure out what's meant by github issue for setData()

library(nimble)

code <- nimbleCode({
    for(i in 1:3) {
        a[i] ~ dnorm(0,1)
    }
})

constants <- list()
data <- list()
inits <- list(a = 1:3)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$a
Rmodel$getNodeNames(dataOnly=TRUE)

Rmodel$setData('a')

Rmodel$getNodeNames(dataOnly=TRUE)

1



## examining why the slope is *not* inverted
## when you switch x & y

set.seed(0)
x <- 1:200
y <- rnorm(x, 2*x, 100)#200)
x <- x-mean(x)
y <- y-mean(y)
lim <- c(-400, 400)
plot(x,y, pch=20, xlim=lim, ylim=lim)
points(y,x, pch=20, col='red')
abline(a=0,b=1,col='blue')
m1 <- lm(y~x)
m2 <- lm(x~y)
b1 <- m1$coef[2]
b2 <- m2$coef[2]
abline(m1)
abline(m2, col='red')
b1
1/b2
b1*b2

sd(x)
sd(y)

b1
b2




## STAT 201 class survey dataset

survey <- read.csv("~/Downloads/STAT201ClassInfo.csv")

names(survey)

attach(survey)
plot(Height, Minutes)
plot(Height, Friends)
cor(Height, Friends)
lm(Height ~ Friends)

str(survey)

survey$Siblings
table(survey$Siblings)
prop.table(table(survey$Siblings))


## warning messing in NIMBLE for forgetting type declaration

library(nimble)

## forgot type declaration.  This case is caught
nfDef <- nimbleFunction(
    setup = TRUE,
    run = function(a) { }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)  # useful warning message:
 ## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compiler details.
## Error: Type declaration missing for argument(s) a

## forgot type declaration, but gave a default value.
## this case gives a totally inconprehensible error
nfDef <- nimbleFunction(
    setup = TRUE,
    run = function(a = 1) { }
)

Rnf <- nfDef()
Cnf <- compileNimble(Rnf)  # would love a good error message here:
compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compiler details.
Error in AT$default : $ operator is invalid for atomic vectors




## playing with calculating r^2

a <- 1
b <- 0.5
sigma <- 1
n <- 10
x <- 1:n
y <- rnorm(n, a+b*x, sigma)
plot(x,y)

r <- cor(x,y)
r^2

m <- lm(y~x)
summary(m)
r^2


yhat <- m$fitted.values
ybar <- mean(y)

1 - sum((y-yhat)^2)/sum((y-ybar)^2)
r^2




#### NIMBLE occupancy model for ZIB GLMM model of potato psyllid occupancy
#### Originally developed by Daniel Turek
## trouble-shooting his issues with NIMBLE v0.6-3
library(nimble)

dCustom <- nimbleFunction(
    run = function(x = double(), a = double(), log = integer(0, default=0)) {
        returnType(double())
        if(log) return(0) else return(1)
    }
)
rCustom <- nimbleFunction(
    run = function(n = integer(), a = double()) {
        returnType(double())
        return(0)
    }
)
registerDistributions(list(
    dCustom = list(
        BUGSdist = "dCustom(a)",
        discrete = TRUE
    )
))

code <- nimbleCode({
    a ~ dnorm(0, 1)
    y ~ dCustom(0.5)
})
constants <- list()
data <- list()
inits <- list(y = 0, a=0)
Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)

spec <- configureMCMC(Rmodel)




library(nimble)
setwd("~/Downloads")

inputData <- readRDS('output/data_nimble_zib.rds')
source('R_functions/nimble_definitions.R')



code <- nimbleCode({
    mu_alpha ~ dnorm(0, 0.001)
    sigma_alpha ~ dunif(0, 1000)
    for(j in 1:nsite) { 
        alpha[j] ~ dnorm(mu_alpha, sd = sigma_alpha)  ## site random effect
    }
    for(i in 1:9) {
        beta[i] ~ dnorm(0, 0.001)
    }
    for(i in 1:N) {
        logit(p_occ[i]) <- alpha[siteID[i]] + beta[4]*aet[i] + beta[5]*tmn[i] + beta[6]*tmx[i] + beta[7]*year[i] + beta[8]*month[i] + beta[9]*month2[i]
        logit(p_obs[i]) <- beta[1] + beta[2]*list_length[i] + beta[3]*year_list_length[i]
        y[i] ~ dOccupancy(p_occ[i], p_obs[i])
    }
})

constants <- with(inputData,
                  list(N=N, nsite=nsite, 
                       aet=aet, tmn=tmn, tmx=tmx, 
                       year=year, 
                       month=month,
                       month2=month2,
                       list_length=list_length, 
                       year_list_length=year_list_length, 
                       siteID=siteID))

data <- with(inputData, list(y=y))

inits <- list(mu_alpha=0, sigma_alpha=1, alpha=rep(0,inputData$nsite), beta=rep(0,9))#, betaseason=rep(0,4))

modelInfo_month <- list(code=code, constants=constants, data=data, inits=inits, name='month_model')

Rmodel <- nimbleModel(modelInfo_month$code,
                      modelInfo_month$constants,
                      modelInfo_month$data,
                      modelInfo_month$inits)

Cmodel <- compileNimble(Rmodel)

spec <- configureMCMC(Rmodel)

#### Best configuration of samplers for random effect occupancy model
spec$removeSamplers('beta[1:9]')
spec$addSampler('beta[1:3]', 'RW_block') # detection sub-model sampler
spec$addSampler('beta[4:9]', 'RW_block') # occupancy sub-model sampler
spec$removeSamplers('sigma_alpha')
spec$addSampler('sigma_alpha', 'RW_log_shift', list(shiftNodes='alpha')) # random effect sampler
spec$getSamplers() # Check samplers
spec$addMonitors(c('p_occ')) # add a monitor to get p_occ in output
#spec$addMonitors(c('p_obs')) # add a monitor to get p_obs in output

#### Compile MCMC in R and C++
Rmcmc <- buildMCMC(spec)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)


#### Run MCMC with 150,000 iterations and 50,000 burn-in
niter <- 15000
burnin <- 5000

ti <- Sys.time()
samplesList <- lapply(3, mcmcClusterFunction)
tf <- Sys.time()

# The time it took to run MCMC
tf-ti




## helping to debug Dao's autoAdapt algorithm.
## NOTE: need to build and install package from his codess branch
## main function is MCMC_autoAdapt.R / autoAdapt() .... (I think??)

library(nimble)
setwd('~/Downloads')
load('./data/model_spatial.RData')
Rmodel <- nimbleModel(code, constants, data, inits)

CandidateSamplerList <- list(
#  sampler_conjugate= list(type = "sampler_conjugate", name = "conjugate"),
  sampler_RW = list(type = "sampler_RW", control =  list(adaptive = TRUE,adaptInterval = 20), name = "adaptive"),
  sampler_RWlog = list(type = "sampler_RW", control =  list(adaptive = TRUE,adaptInterval = 20,  log =TRUE), name = "adaptivelog"),
  sampler_slice = list(type = "sampler_slice", control =  list(adaptive = TRUE,adaptInterval = 20), name = "Myslice"),
  sampler_RW_block = list(type = "sampler_RW_block", control =  (list(adaptive = TRUE, adaptInterval = 20)), name = "block_RW"),
  sampler_RW_block = list(type = "sampler_AF_slice", control =  (list(adaptive = TRUE, adaptInterval = 20)), name = "block_AFS"),
  sampler_RW_block = list(type = "RW_rotated_block", control =  (list(adaptive = TRUE, adaptInterval = 20)), name = "block_Rotated")
)
##
Candidates<-list(2,3,4,5,6,7)  
##
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$getSamplers()
##
monitor <- Rmodel$getNodeNames(stochOnly=TRUE, includeData=FALSE, returnScalarComponents=TRUE)
monitor
##
n =length(monitor)
print(n)            
##
DefaultSamplerList <- vector(mode="list", length=n)
DefaultSamplerList
##
for(i in 1: n){
  names(DefaultSamplerList)[i]<-'RW'
  DefaultSamplerList[[i]]$name <-monitor[i]
  DefaultSamplerList[[i]]$target <-monitor[i]
  DefaultSamplerList[[i]]$control <-list()
  DefaultSamplerList[[i]]$type <-'sampler_RW'
  DefaultSamplerList[[i]]$oldtype <-DefaultSamplerList[[i]]$type
}
##
DefaultSamplerList[1:5]
##
DefaultSamplerList
Candidates
monitor


debug(autoAdapt)
debug(autoAdaptClass)
debug(ab$run)

ab <- autoAdapt(Rmodel, niter=1000, DefaultSamplerList=DefaultSamplerList, Candidates = Candidates, monitor=monitor, iteration =5)




## tobit model for Chris, Ann Raiho <ann.raiho@gmail.com>, Mike Dietze <dietze@bu.edu>
## experimenting with "turning off samplers" in the MCMC

## Daniel, Perry, the idea is that in the following code, 'y.ind' is an indicator vector that says whether each element of a data vector is positive or zero.  Then in the BUGS code, y.censored is either the actual data value, or if the data value is 0, y.censored[i] is NA.  In that case, we want to do MCMC sampling on that element of y.censored using a univariate sampler (we recognize that y.censored has a dmnorm distribution -- in this case we only want to update the values for which the corresponding data values are 0).
## When building the model, y.ind is flagged as 'data', y.censored is not flagged as 'data'. The sampler on y.censored is removed, and then a univariate sampler is assigned to each element of y.censored that contains an NA (which corresponds to elements of y.ind that are 0).
## This same model is used multiple times during a data assimilation, with the input data vector (and therefore the y.ind and y.censored vectors) changing at each time. Goal is to using the same compiled MCMC at each time, but dynamically controlling which components of y.censored are sampled.


library(nimble)

sampler_toggle <- nimbleFunction(
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        type <- control$type
        nested_sampler_name <- paste0('sampler_', type)
        control_new <- nimbleOptions('MCMCcontrolDefaultList')
        control_new[[names(control)]] <- control
        nested_sampler_list <- nimbleFunctionList(sampler_BASE)
        nested_sampler_list[[1]] <- do.call(nested_sampler_name, list(model, mvSaved, target, control_new))
        toggle <- 1
    },
    run = function() {
        if(toggle == 1)
            nested_sampler_list[[1]]$run()
    },
    methods = list(
        reset = function()
            nested_sampler_list[[1]]$reset()
    )
)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a*a + 1, 2)
    c ~ dnorm(a + b*b, 5)
})
constants <- list()
data <- list()
inits <- list(a = 0, b=0, c=0)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel, nodes=NULL)
conf$addMonitors('a','b','c')
conf$printMonitors()
conf$printSamplers()

conf$addSampler('a', 'RW')
conf$addSampler('b', 'RW')
conf$addSampler('c', 'RW')

conf$addSampler('c', 'slice')

conf$addSampler('a', 'toggle', control = list(type='RW'))
conf$addSampler('b', 'toggle', control = list(type='RW'))
conf$addSampler('c', 'toggle', control = list(type='RW'))

conf$addSampler('c', 'toggle', control = list(type='slice'))

conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Rmcmc$samplerFunctions$contentsList[[3]]$toggle
valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[3]], 'toggle')
Rmcmc$samplerFunctions$contentsList[[3]]$toggle <- 0
valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[3]], 'toggle', 0)
Rmcmc$samplerFunctions$contentsList[[3]]$toggle
valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[3]], 'toggle')

niter <- 50
set.seed(0); Rsamples <- runMCMC(Rmcmc, niter)
set.seed(0); Csamples <- runMCMC(Cmcmc, niter)
Rsamples
Csamples
Rsamples - Csamples

saveSamples <- Rsamples
Rsamples - saveSamples

##Cmcmc$run(10000)
##samples <- as.matrix(Cmcmc$mvSamples)

colnames(samples)
apply(samples, 2, mean)
samplesPlot(samples)


code <- nimbleCode({ 
    ##q[1:N,1:N]  ~ dwish(R = aq[1:N,1:N], df = bq) ## aq and bq are estimated over time
    ##Q[1:N,1:N] <- inverse(q[1:N,1:N])
    ##X.mod[1:N] ~ dmnorm(muf[1:N],prec = pf[1:N,1:N]) ## Model Forecast ##muf and pf are assigned from ensembles
    #### add process error
    X[1:N]  ~ dmnorm(X.mod[1:N],prec = q[1:N,1:N])
    ## Analysis
    y.censored[1:N] ~ dmnorm(X[1:N], prec = r[1:N,1:N]) 
    for(i in 1:N){
        y.ind[i] ~ dconstraint(y.censored[i] > 0)
    }
})
N <- 5
constants <- list(N=N, X.mod=rep(10,N), q=diag(N), r=diag(N))
## value of y.ind and y.censored are just placeholders...
## BUT, important, y.censored *must* be specified as data at this point.
## it's a long story why, has to do with the initializeModel routine
## at the beginning of MCMC execution
data <- list(y.ind=rep(1,N), y.censored=rep(10,N))  
inits <- list(X=rep(10,N))

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel, print=TRUE)

## important!
## this is needed for correct indexing later
samplerNumberOffset <- length(conf$getSamplers())

for(i in 1:N) {
    node <- paste0('y.censored[',i,']')
    conf$addSampler(node, 'toggle', control=list(type='RW'))
    ## could instead use slice samplers, or any combination thereof, e.g.:
    ##conf$addSampler(node, 'toggle', control=list(type='slice'))
}

conf$printSamplers()

conf$printMonitors()
## could monitor y.censored, if you wish, to verify correct behaviour
conf$addMonitors('y.censored')

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

## new dataset (1)
y.ind <- c(1, 1, 1, 0, 0)
## important!
## change: rather than providing NA for the non-data values (those to be sampled),
## you'll have to provide some values here.
## that's because we effective disabled the model initialization routine earlier
y.censored <- c(9, 9, 11, 20, 20)

Cmodel$y.ind <- y.ind
Cmodel$y.censored <- y.censored

for(i in 1:N) {
    ## ironically, here we have to "toggle" the value of y.ind[i]
    ## this specifies that when y.ind[i] = 1,
    ## indicator variable is set to 0, which specifies *not* to sample
    valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[samplerNumberOffset+i]], 'toggle', 1-y.ind[i])
}

## check the values of indicator variables, if you wish:
##for(i in 1:N) {
##    print(valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[samplerNumberOffset+i]], 'toggle'))
##}

niter <- 1000
set.seed(0)
samples <- runMCMC(Cmcmc, niter)

head(samples)
tail(samples)


## new dataset (2)
y.ind <- c(1, 1, 1, 1, 1)   ## everything is data
## important!
## change: rather than providing NA for the non-data values (those to be sampled),
## you'll have to provide some values here.
## that's because we effective disabled the model initialization routine earlier
y.censored <- c(9, 9, 11, 11, 12)

Cmodel$y.ind <- y.ind
Cmodel$y.censored <- y.censored

for(i in 1:N) {
    ## ironically, here we have to "toggle" the value of y.ind[i]
    ## this specifies that when y.ind[i] = 1,
    ## indicator variable is set to 0, which specifies *not* to sample
    valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[samplerNumberOffset+i]], 'toggle', 1-y.ind[i])
}

## check the values of indicator variables, if you wish:
##for(i in 1:N) {
##    print(valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[samplerNumberOffset+i]], 'toggle'))
##}

niter <- 1000
set.seed(0)
samples <- runMCMC(Cmcmc, niter)

head(samples)
tail(samples)



## new dataset (3)
y.ind <- c(0, 0, 0, 0, 0)   ## nothing is data
## important!
## change: rather than providing NA for the non-data values (those to be sampled),
## you'll have to provide some values here.
## that's because we effective disabled the model initialization routine earlier
y.censored <- c(20, 20, 20, 20, 20)

Cmodel$y.ind <- y.ind
Cmodel$y.censored <- y.censored

for(i in 1:N) {
    ## ironically, here we have to "toggle" the value of y.ind[i]
    ## this specifies that when y.ind[i] = 1,
    ## indicator variable is set to 0, which specifies *not* to sample
    valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[samplerNumberOffset+i]], 'toggle', 1-y.ind[i])
}

## check the values of indicator variables, if you wish:
##for(i in 1:N) {
##    print(valueInCompiledNimbleFunction(Cmcmc$samplerFunctions[[samplerNumberOffset+i]], 'toggle'))
##}

niter <- 1000
set.seed(0)
samples <- runMCMC(Cmcmc, niter)

head(samples)
tail(samples)






## prep for STAT 201 lecture

df <- read.csv('~/Downloads/TVhours.csv')
df <- read.delim('~/github/courses/stat201/data/Global.Temperature.txt')
df <- read.csv('~/Downloads/Global.Temperature.csv')
df
str(df)
barplot(df$animal)
df$animal
df

hist(df$tvhours)
hist(df$tvhours, breaks=30)
mean(df$tvhours)
sd(df$tvhours)


## testing speed of new RW sampler, which checks prior logProb first,
## to see if it's a valid proposal

library(nimble)
sampler_RW_new <- nimbleFunction(
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        ## control list extraction
        logScale      <- control$log
        reflective    <- control$reflective
        adaptive      <- control$adaptive
        adaptInterval <- control$adaptInterval
        scale         <- control$scale
        ## node list generation
        targetAsScalar <- model$expandNodeNames(target, returnScalarComponents = TRUE)
        calcNodes      <- model$getDependencies(target)
        depNodes       <- model$getDependencies(target, self = FALSE)
        ## numeric value generation
        scaleOriginal <- scale
        timesRan      <- 0
        timesAccepted <- 0
        timesAdapted  <- 0
        ##scaleHistory  <- c(0, 0)   ## scaleHistory
        optimalAR     <- 0.44
        gamma1        <- 0
        ## checks
        if(length(targetAsScalar) > 1)   stop('cannot use RW sampler on more than one target; try RW_block sampler')
        if(model$isDiscrete(target))     stop('cannot use RW sampler on discrete-valued target; try slice sampler')
        if(logScale & reflective)        stop('cannot use reflective RW sampler on a log scale (i.e. with options log=TRUE and reflective=TRUE')
    },
    run = function() {
        currentValue <- model[[target]]
        propLogScale <- 0
        if(logScale) { propLogScale <- rnorm(1, mean = 0, sd = scale)
                       propValue <- currentValue * exp(propLogScale)
        } else         propValue <- rnorm(1, mean = currentValue,  sd = scale)
        if(reflective) {
            lower <- model$getBound(target, 'lower')
            upper <- model$getBound(target, 'upper')
            while(propValue < lower | propValue > upper) {
                if(propValue < lower) propValue <- 2*lower - propValue
                if(propValue > upper) propValue <- 2*upper - propValue
            }
        }
        model[[target]] <<- propValue
############### revisons starting here
        ##logMHR <- calculateDiff(model, calcNodes) + propLogScale
        ##jump <- decide(logMHR)
        priorLP0 <- getLogProb(model, target)
        priorLP1 <- calculate(model, target)
        if(is.nan(priorLP1) | priorLP1 == -Inf ) { jump <- FALSE
                               ##print('automatic rejection')
                           } else { logMHR <- calculateDiff(model, depNodes) + priorLP1 - priorLP0 + propLogScale
                                    ##print(logMHR)
                                    jump <- decide(logMHR) }
############### same after here
        if(jump) nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
        else     nimCopy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE)
        if(adaptive)     adaptiveProcedure(jump)
    },
    methods = list(
        adaptiveProcedure = function(jump = logical()) {
            timesRan <<- timesRan + 1
            if(jump)     timesAccepted <<- timesAccepted + 1
            if(timesRan %% adaptInterval == 0) {
                acceptanceRate <- timesAccepted / timesRan
                timesAdapted <<- timesAdapted + 1
                ##setSize(scaleHistory, timesAdapted)         ## scaleHistory
                ##scaleHistory[timesAdapted] <<- scale        ## scaleHistory
                gamma1 <<- 1/((timesAdapted + 3)^0.8)
                gamma2 <- 10 * gamma1
                adaptFactor <- exp(gamma2 * (acceptanceRate - optimalAR))
                scale <<- scale * adaptFactor
                timesRan <<- 0
                timesAccepted <<- 0
            }
        },
        reset = function() {
            scale <<- scaleOriginal
            timesRan      <<- 0
            timesAccepted <<- 0
            timesAdapted  <<- 0
            gamma1 <<- 0
        }
    )
)
##
N <- 100
code <- nimbleCode({
    a ~ dnorm(1, 1)
    ##a ~ T(dnorm(0, 1), -1, 1)
    ##for(i in 1:N) {
    ##    b[i] ~ dnorm(a, 0.001)
    ##}
    ##b ~ dgamma(a, 1)
})
constants <- list()
data <- list()##b = rnorm(N))
inits <- list(a = 0)
##
Rmodel1 <- nimbleModel(code, constants, data, inits)
Cmodel1 <- compileNimble(Rmodel1)
Rmodel2 <- nimbleModel(code, constants, data, inits)
Cmodel2 <- compileNimble(Rmodel2)
##
conf1 <- configureMCMC(Rmodel1, nodes=NULL)
conf1$addSampler('a', 'RW')
conf1$printSamplers()
Rmcmc1 <- buildMCMC(conf1)
Cmcmc1 <- compileNimble(Rmcmc1, project = Rmodel1)
##
conf2 <- configureMCMC(Rmodel2, nodes=NULL)
conf2$addSampler('a', 'RW_new')
conf2$printSamplers()
Rmcmc2 <- buildMCMC(conf2)
Cmcmc2 <- compileNimble(Rmcmc2, project = Rmodel2)


niter <- 100

set.seed(0); Rmcmc1$run(niter); Rsamples1 <- as.matrix(Rmcmc1$mvSamples)
set.seed(0); Cmcmc1$run(niter); Csamples1 <- as.matrix(Cmcmc1$mvSamples)
set.seed(0); Rmcmc2$run(niter); Rsamples2 <- as.matrix(Rmcmc2$mvSamples)
set.seed(0); Cmcmc2$run(niter); Csamples2 <- as.matrix(Cmcmc2$mvSamples)


Rsamples1 - Csamples1
Rsamples2 - Csamples2
Rsamples1 - Rsamples2  ## *should be* different when there's truncation



niter <- 1e7

t1 <- system.time(Cmcmc1$run(niter))[1]
t2 <- system.time(Cmcmc2$run(niter))[1]

t1
t2

(t2-t1)/t1*100




library(nimble)
code <- nimbleCode({
    a ~ dbeta(1,1)
    b ~ dgamma(a, 3)
})
constants <- list()
data <- list()
inits <- list(a = -1, b=3)
Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)


Rmodel$a
Cmodel$a
calculate(Rmodel, 'a')
calculate(Cmodel, 'a')

Rmodel$b
Cmodel$b
calculate(Rmodel, 'b')
calculate(Cmodel, 'b')

    



## demo of how to use autoBlock on spatial model
## for Dao

library(nimble)
load('~/github/automated-blocking-examples/data/model_spatial.RData')
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel, autoBlock=TRUE)
conf$printSamplers()



## sampler to record scale and acceptanceRate history, for Dao

sampler_RW_record <- nimbleFunction(
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        ## control list extraction
        logScale      <- control$log
        reflective    <- control$reflective
        adaptive      <- control$adaptive
        adaptInterval <- control$adaptInterval
        scale         <- control$scale
        ## node list generation
        targetAsScalar <- model$expandNodeNames(target, returnScalarComponents = TRUE)
        calcNodes  <- model$getDependencies(target)
        ## numeric value generation
        scaleOriginal <- scale
        timesRan      <- 0
        timesAccepted <- 0
        timesAdapted  <- 0
        scaleHistory  <- c(0, 0)   ## scaleHistory
        acceptanceRateHistory <- c(0, 0)   ## scaleHistory
        optimalAR     <- 0.44
        gamma1        <- 0
        ## checks
        if(length(targetAsScalar) > 1)   stop('cannot use RW sampler on more than one target; try RW_block sampler')
        if(model$isDiscrete(target))     stop('cannot use RW sampler on discrete-valued target; try slice sampler')
        if(logScale & reflective)        stop('cannot use reflective RW sampler on a log scale (i.e. with options log=TRUE and reflective=TRUE')
    },
    run = function() {
        currentValue <- model[[target]]
        propLogScale <- 0
        if(logScale) { propLogScale <- rnorm(1, mean = 0, sd = scale)
                       propValue <- currentValue * exp(propLogScale)
                   } else         propValue <- rnorm(1, mean = currentValue,  sd = scale)
        if(reflective) {
            lower <- model$getBound(target, 'lower')
            upper <- model$getBound(target, 'upper')
            while(propValue < lower | propValue > upper) {
                if(propValue < lower) propValue <- 2*lower - propValue
                if(propValue > upper) propValue <- 2*upper - propValue
            }
        }
        model[[target]] <<- propValue
        logMHR <- calculateDiff(model, calcNodes) + propLogScale
        jump <- decide(logMHR)
        if(jump) nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
        else     nimCopy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE)
        if(adaptive)     adaptiveProcedure(jump)
    },
    methods = list(
        adaptiveProcedure = function(jump = logical()) {
            timesRan <<- timesRan + 1
            if(jump)     timesAccepted <<- timesAccepted + 1
            if(timesRan %% adaptInterval == 0) {
                acceptanceRate <- timesAccepted / timesRan
                timesAdapted <<- timesAdapted + 1
                setSize(scaleHistory,          timesAdapted)         ## scaleHistory
                setSize(acceptanceRateHistory, timesAdapted)         ## scaleHistory
                scaleHistory[timesAdapted]          <<- scale        ## scaleHistory
                acceptanceRateHistory[timesAdapted] <<- acceptanceRate        ## scaleHistory
                gamma1 <<- 1/((timesAdapted + 3)^0.8)
                gamma2 <- 10 * gamma1
                adaptFactor <- exp(gamma2 * (acceptanceRate - optimalAR))
                scale <<- scale * adaptFactor
                timesRan <<- 0
                timesAccepted <<- 0
            }
        },
        getScaleHistory = function()          { returnType(double(1)); return(scaleHistory) },          ## scaleHistory
        getAcceptanceRateHistory = function() { returnType(double(1)); return(acceptanceRateHistory) },          ## scaleHistory
        ##getScaleHistoryExpanded = function() {                                                 ## scaleHistory
        ##    scaleHistoryExpanded <- numeric(timesAdapted*adaptInterval, init=FALSE)            ## scaleHistory
        ##    for(iTA in 1:timesAdapted)                                                         ## scaleHistory
        ##        for(j in 1:adaptInterval)                                                      ## scaleHistory
        ##            scaleHistoryExpanded[(iTA-1)*adaptInterval+j] <- scaleHistory[iTA]         ## scaleHistory
        ##    returnType(double(1)); return(scaleHistoryExpanded) },                             ## scaleHistory
        reset = function() {
            scale <<- scaleOriginal
            timesRan      <<- 0
            timesAccepted <<- 0
            timesAdapted  <<- 0
            scaleHistory           <<- scaleHistory * 0    ## scaleHistory
            acceptanceRateHistory  <<- acceptanceRateHistory * 0    ## scaleHistory
            gamma1 <<- 0
        }
    ), where = getLoadingNamespace()
)





## Perry's demo of making R and C external calls in NIMBLE
## This is a demo for beta (or gamma?) users of external call features in nimble
## I have actually built TWO kinds of external calls:
## 1. Call arbitrary (up to limited argument conventions) compiled code as long as you can provide a .h file and a .o file.  Chris suggests maybe we should also (instead) allow the user (that's you) to provide a DLL (.so in Linx / OS X, .dll in Windows).
## 2. Call arbitrary R functions from within NIMBLE (including compiled NIMBLE), as long as argument and return types are guaranteed in advance.  Obviously if you have a need to call external code that cannot be handled by mechanism 1, you could use mechanism 2 and have your R function call your external code in whatever complicated way is needed.  For the case of deSolve, one could call a function that calls deSolve from R.  There is some overhead to using mechanism 2 since it requires copies of arguments and return value to be made, and of course the R steps of execution will be at the speed of R.

## You will need to install from the "call_external_c_function" branch (now mis-named because it also includes calling R functions)
library(devtools)
install_github("nimble-dev/nimble", ref = "call_external_c_function", subdir = "packages/nimble")

## Throughout this demo, ignore warning messages about functions that may not be defined.  I haven't updated the list of keywords used in that error-trapping.

library(nimble)

## Calling external C:

## Say we want a C function that adds 1.5 to a vector of values:
## We can only give non-scalars results by pointer argument.
## Any NIMBLE non-scalar can become a double* of contiguously allocated memory.
## Like in LAPACK or similar low-level libraries, you need a separate argument to say how long the allocated memory is.
sink('add1p5.h')
cat('
extern "C" {
  void my_internal_function(double *p, double*ans, int n);
}
')
sink()

sink('add1p5.cpp') 
cat('
#include "stdio.h"
#include "add1p5.h"

void my_internal_function(double *p, double *ans, int n) {
  printf("In my_internal_function\\n"); /* cat reduces the double slash to single slash */ 
  for(int i = 0; i < n; i++) 
    ans[i] = p[i] + 1.5;
}
')
sink()

## A major limitation is that we cannot have compiled code return anything except a scalar.  So returned non-scalars will have to be via arguments.  That meansto use this in BUGS code we'll have to wrap it in another nimbleFunction.  It will make sense below.

## compile that to .o
## I am not an expert on all compilation twists, but on my system I need to do it with g++ instead of gcc (even though it is pure C) in order for the linker to be later happy linking it to compiled C++ from NIMBLE.  Using g++ on C code gives me a clang warning about deprecated behavior, but it's ok.
system('g++ add1p5.cpp -c -o add1p5.o')

## now to create a nimbleFunction that interfaces to add1p5:

Radd1p5 <- nimbleExternalCall(function(x = double(1), ans = double(1), n = integer()){}, Cfun = 'my_internal_function', headerFile = 'add1p5.h', oFile = 'add1p5.o')
## The first argument uses the format of a nimbleFunction with argument type declarations, but the body of the function can be empty ('{}')
## You can choose any names for the arguments in R. They don't have to match the C code.
## Ignore the warning here and later warnings
Radd1p5 ## this is a nimbleFunction with some internal special sauce, but from here on out it should behave like a nimbleFunction
## We'll wait to use it in BUGS code

## Radd1p5 doesn't return anything and would only be able to return a scalar, so we can wrap it like this:
wrappedRadd1p5 <- nimbleFunction(
    run = function(x = double(1)) {
        ans <- numeric(length(x))
        Radd1p5(x, ans, length(x))
        return(ans)
        returnType(double(1))
    })
## Chris, if you are reading this, it looks like the DSLcode check is weird here.

## calling an R function
## Say we want an R function that adds 2.5 to every value in a vector
add2p5 <- function(x) {
    x + 2.5 ## This can be pure R
}

## now create a nimbleFunction that interfaces to add2p5, not necessary when running uncompiled but necessary when running compiled
Radd2p5 <- nimbleRcall(function(x = double(1)){}, Rfun = 'add2p5', returnType = double(1), envir = .GlobalEnv)
## Similar to above.  The function prototype and the returnType represent a promise that add2p5 will always take and return these types.  Also an environment is needed and it defaults to R's global environment but I'm showing it explicitly.
## Again, ignore the more extensive warnings
Radd2p5 ## this is also a nimbleFunction with more special sauce

## Now let's use these in a model

demoCode <- nimbleCode({
    for(i in 1:4) {x[i] ~ dnorm(0,1)} ## just to get a vector
    y[1:4] <- wrappedRadd1p5(x[1:4])
    z[1:4] <- Radd2p5(x[1:4])
    })

demoModel <- nimbleModel(demoCode, inits = list(x = rnorm(4)))
## Again ignore the error during checking.  We'll have to trap and handle that, but right now I'm focused on core functionality.
## The model will not work uncompiled!

nimbleOptions(showCompilerOutput = TRUE) ## so you can see what is happening
CdemoModel <- compileNimble(demoModel, dirName = '.') ## last arg puts the C++ code in your working directory so you can look at it if you like

CdemoModel$x
CdemoModel$calculate()
CdemoModel$y - CdemoModel$x
CdemoModel$z - CdemoModel$x

## The correct calculations happened.




library(nimble)
nimbleOptions(MCMCprogressBar = FALSE)


code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)

Cmcmc <- compileNimble(Rmcmc, project = Rmodel)


progressBarOption = TRUE
progressBarOption = FALSE

Cmcmc$run(30, progressBar = progressBarOption)
Cmcmc$run(30000, progressBar = progressBarOption)




library(nimble)

sampler_RW_dirichlet2 <- nimbleFunction(
    name = 'sampler_RW_dirichlet2',
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        ## control list extraction
        adaptive      <- if(!is.null(control$adaptive))      control$adaptive      else TRUE
        adaptInterval <- if(!is.null(control$adaptInterval)) control$adaptInterval else 200
        scaleOriginal <- if(!is.null(control$scale))         control$scale         else 1
        ## node list generation
        calcNodes <- model$getDependencies(target)
        depNodes  <- model$getDependencies(target, self = FALSE)
        targetScalarNodes <- model$expandNodeNames(target, returnScalarComponents = TRUE)
        ## numeric value generation
        d <- length(targetScalarNodes)
        thetaVec         <- rep(0, d)
        alphaVec         <- rep(0, d)    ## NEW
        scaleVec         <- rep(scaleOriginal, d)
        timesRan         <- 0
        timesAcceptedVec <- rep(0, d)
        timesAdapted     <- 0
        gamma1           <- 0
        ## checks
        if(length(model$expandNodeNames(target)) > 1)    stop('RW_dirichlet sampler only applies to one target node')
        if(model$getDistribution(target) != 'ddirch')    stop('can only use RW_dirichlet sampler for dirichlet distributions')
    },
    run = function() {
        ##if(thetaVec[1] == 0)   thetaVec <<- values(model, target)   ## initialization
        thetaVec <<- values(model, target)   ## NEW
        alphaVec <<- model$getParam(target, 'alpha')
        for(i in 1:d) {
            currentValue <- thetaVec[i]
            propLogScale <- rnorm(1, mean = 0, sd = scaleVec[i])
            propValue <- currentValue * exp(propLogScale)
            ##if(propValue != 0) {    ## NEW
            thetaVecProp <- thetaVec
            thetaVecProp[i] <- propValue
            values(model, target) <<- thetaVecProp / sum(thetaVecProp)
            ##logMHR <- (alphaVec[i]-1)*propLogScale + propLogScale + currentValue - propValue + calculateDiff(model, depNodes)
            ## TEMP:
            ##lpTargetCurrent <- getLogProb(model, target)
            ##lpTargetProp <- calculate(model, target)
            logMHR <- ##lpTargetProp - lpTargetCurrent + ##(alphaVec[i]-1)*propLogScale +
                ##calculateDiff(model, depNodes) +
                calculateDiff(model, calcNodes) +
                    propLogScale + currentValue - propValue
            ## end TEMP
            jump <- decide(logMHR)
            ##} else jump <- FALSE    ## NEW
            if(adaptive & jump)   timesAcceptedVec[i] <<- timesAcceptedVec[i] + 1
            if(jump) { thetaVec <<- thetaVecProp
                       nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
                   } else   { nimCopy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE) }
            model$calculate(target)                                                         ## update target logProb
            nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)    ##
        }
        if(adaptive) {
            timesRan <<- timesRan + 1
            if(timesRan %% adaptInterval == 0) {
                acceptanceRateVec <- timesAcceptedVec / timesRan
                timesAdapted <<- timesAdapted + 1
                gamma1 <<- 1/((timesAdapted + 3)^0.8)
                adaptFactorVec <- exp(10 * gamma1 * (acceptanceRateVec - 0.44))   ## optimalAR = 0.44
                scaleVec <<- scaleVec * adaptFactorVec
                timesRan <<- 0
                timesAcceptedVec <<- numeric(d, 0)
            }
        }
    },
    methods = list(
        reset = function() {
            thetaVec         <<- numeric(d, 0)
            scaleVec         <<- numeric(d, scaleOriginal)
            timesRan         <<- 0
            timesAcceptedVec <<- numeric(d, 0)
            timesAdapted     <<- 0
            gamma1           <<- 0
        }
    )
)


## testing new sampler function: RW_dirichlet
## for ddirch or ddirich Dirichlet nodes
library(nimble)
##nimbleOptions(showCompilerOutput=TRUE)
n <- 100
##alpha <- c(10, 30, 15)#, 60, 1)
alpha <- c(0.2, 0.5, 0.9)#, 60, 1)
K <- length(alpha)
p <- c(.12, .24, .09)#, .54, .01)
set.seed(0)
y <- rmulti(1, n, p)
code <- quote({
    p[1:K]  ~ ddirch(alpha[1:K])
    p2[1:K] ~ ddirch(alpha[1:K])
    p3[1:K] ~ ddirch(alpha[1:K])
    y[1:K]  ~ dmulti(p[1:K], n)
    y2[1:K] ~ dmulti(p2[1:K], n)
    y3[1:K] ~ dmulti(p3[1:K], n)
    ##x ~ dnorm(p[1], 1)
    ##x2 ~ dnorm(p2[1],1)
    ##for(i in 1:K) {
    ##    alpha[i] ~ dgamma(.001, .001);
    ##}
})
inits <- list(p = rep(1/K, K), alpha = alpha, p2 = rep(1/K,K), p3=rep(1/K,K))##, x=0)
constants <- list(n=n, K=K)
data <- list(y = y, y2 = y, y3=y)
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

##Rmodel$alpha
##Rmodel$alpha2
##Rmodel$getParam('p2', 'alpha')
##Rmodel$getParam('p[5:6]', 'alpha')

conf <- configureMCMC(Rmodel, nodes = NULL)
conf$printSamplers()
##conf$addSampler('p[1:3]',  'RW_dirichlet', print=TRUE)
##conf$addSampler('p2[1:3]', 'conjugate',    print=TRUE)
conf$addSampler('p[1:3]', 'conjugate')
conf$addSampler('p2[1:3]',  'RW_dirichlet')
conf$addSampler('p3[1:3]',  'RW_dirichlet2')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Rmodel$alpha
Rmodel$p
Rmodel$p2
Rmodel$p3
Rmodel$y
Rmodel$y2
Rmodel$y3

##debug(Rmcmc$run)
##Rmcmc$run(1001)

##debug(samplerFunctions[[1]]$reset)
##debug(samplerFunctions[[1]]$run)

##Cmodel <- compileNimble(Rmodel)
##Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

compiledList <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmodel <- compiledList$model; Cmcmc <- compiledList$mcmc

Cmodel$calculate()

niter <- 100000

##set.seed(0)
##Rsamples <- runMCMC(Rmcmc, niter)

set.seed(0)
Csamples <- runMCMC(Cmcmc, niter)

##colnames(Csamples)
means <- apply(Csamples, 2, mean)
round(matrix(means, ncol = 3), 3)

ss <- samplesSummary(Csamples)
ss[1:3,] - ss[4:6,]
ss[1:3,] - ss[7:9,]

for(i in 1:5) {
    samplesPlot(Csamples, paste0(c('p[', 'p2['), i, c(']')))
}


Rmodel$getLogProb()
Cmodel$getLogProb()
Rmodel$calculate()
Cmodel$calculate()


Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)

dimnames(Rsamples)
dimnames(Csamples)

Rsamples - Csamples
Csamples[1:200,]

mean(diff(Csamples[,1])==0)
(1-0.44)^3



## reproducible example of problem with model$values(...) <- ...

library(nimble)
code <- nimbleCode({
    for(i in 1:5) {
        a[i] ~ dnorm(0, 1)
    }
})
constants <- list()
data <- list()
inits <- list(a = rep(0,5))
Rmodel <- nimbleModel(code, constants, data, inits)

nfDef <- nimbleFunction(
    setup = function(model) {
        newValues <- rep(1, 5)
        nodes <- 'a'
    },
    run = function() {
        ##values(model, nodes) <<- newValues    ## this one works fine
        model$values(nodes) <<- newValues    ## this causes failure
    }
)

##Error in makeAssgnFcn(e[[1]]) : 
##  'model$values' is not a valid function in complex assignments

Rnf <- nfDef(Rmodel)

Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(Rnf, project = Rmodel)

Rmodel$a
Rnf$run()
Rmodel$a

Cmodel$a
Cnf$run()
Cmodel$a





## trying to use nodes[i] functionality,
## works for: model[[ multivariateNodeName ]][i] <- scalar
## does not work for:
## model[[ nodes[i] ]] <- x, or
## values(model, nodes[i]) <- x

library(nimble)

code <- nimbleCode({
    for(i in 1:3) {
        a[i] ~ dnorm(0, 1)
    }
})
constants <- list()
data <- list()
inits <- list(a = rep(0,3))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$a

nfDef <- nimbleFunction(
    setup = function(model) {
        nodes <- 'a'
        d <- length(model$expandNodeNames(nodes))
    },
    run = function() {
        for(i in 1:d) {
            model[[nodes]][i] <<- i*10
        }
    }
)

Rnf <- nfDef(Rmodel)

Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(Rnf, project = Rmodel)  ## compilation fails

Rmodel$a
Rnf$run()
Rmodel$a

Cmodel$a
Cnf$run()
Cmodel$a


## looking into nimble gthub issue 250, possible infinite
## recursion in getDependencyPaths in checkConjugacy()
## raised by Chris P.

library(nimble)

code <- nimbleCode({
  for(r in 1:R){ 
    for(i in 1:I){
      beta[r,i] ~ dnorm(0,.04)
      beta.pine[r,i] ~ dnorm(0,.04)
    }  
  }
  # if loop over 2nd index, number of dependencies would be greatly reduced
  for(i in 1:I) {
  phi.first[1:J,i] <- Z[1:J,1:R]%*%beta[1:R,i]
  pine.phi[1:J,i] <- Z[1:J,1:R]%*%beta.pine[1:R,i]
  exp.phi[1:J, i] <- exp(phi.first[1:J, i])
  exp.pine.phi[1:J, i] <- exp(pine.phi[1:J, i])
  }
    for(j in 1:J){
    	p.true[j,1] ~ dbeta(exp.phi[j,1],exp.pine.phi[j,1])
    for(i in 2:(I-1)){
        p.rel[j,i]  ~ dbeta(exp.phi[j,i],exp.pine.phi[j,i]) 
        p.true[j,i] <-  p.rel[j,i] * (1 - sum(p.true[j,1:(i-1)]))
    }	
       p.true[j,21] <- 1 - sum(p.true[j,1:20])
    }  
  for(j in 1:J){
    Y[j,1:I] ~ dmulti(size = n[j], prob = p.true[j,1:I])
  }
})

J <- 1
I <- 21
R <- 5

data = list(Y = matrix(0, nrow = J, ncol = I))

constants = list(n = rep(500, J), R = R, I = I, J = J,  Z =  matrix(0, nrow = J, ncol = R))
# try with Z as variable

model <- nimbleModel(code, constants = constants, data = data)

## infinite loop?:
model$checkConjugacy('p.rel[1, 2]')




## generating correlated chains of samples for acf acfplot question
## in stat365 final exam

library(coda)
set.seed(0)
n <- 10000
r1 <- 0.45
r2 <- 0.85
x <- y <- numeric(n)
x[1] <- y[1] <- 0
for(i in 2:n) {
    x[i] <- r1*x[i-1] + rnorm(1)
    y[i] <- r2*y[i-1] + rnorm(1)
}
samp <- cbind(alpha=x, beta=y)
effectiveSize(samp)
setwd('~/github/private/courses/stat365/exams')
pdf('acfPlot.pdf',width=6,height=4,paper='special')
acfplot(as.mcmc(samp), ylim=c(-0.05, 1), lag.max=35)
dev.off()




## looking at interval-censored Weibull regression model
## for Intekhab

library(nimble)
file <- read.csv('~/Downloads/365Project_data.csv')

##dim(file)
##colnames(file)
##head(file)
##str(file)
##file$t

code <- nimbleCode({
    b0 ~ dnorm(0, sd = 1000)
    b_T ~ dnorm(0, sd = 1000)
    b_karnof~ dnorm(0, sd = 1000)
    b_cd4 ~ dnorm(0,sd=1000)
    b_age ~ dnorm(0, sd=1000)
    sigma_N ~ dunif(0,100)
    for (i in 1:N) {
        error[i] ~ dnorm(0,sd=sigma_N)  #random effect
        eta[i] <- b0 + b_T*Tx[i] + b_karnof*karnof[i]+ b_cd4*cd4[i] + b_age*age[i]+error[i]
        mu[i] <- exp(eta[i])
        censored[i] ~ dinterval(t[i], c[i])
        t[i] ~ dweib(1, mu[i]) 
    }
})

file[c(1,14), c('censor','c','t')]

constants <- list(N = nrow(file), Tx=file$tx, karnof=file$karnof, cd4=file$cd4, age=file$age,c=file$c)
data <- list(censored =file$censor,t=file$t)
inits <- list(b0 = 0, b_T = 0, b_karnof=0, b_cd4=0, b_age=0, sigma_N=5, error=rnorm(nrow(file),0,5))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$simulate('t')
Rmodel$calculate()
Rmodel$calculate('censored')

i <- 1
Rmodel$c[i]
Rmodel$t[i]
Rmodel$censored[i]
Rmodel$mu[i]
Rmodel$logProb_t[i]
Rmodel$logProb_censored[i]


constants
data
inits

Rmodel$c
range(Rmodel$mu)
i <- 1
Rmodel$censored[1]
Rmodel$logProb_censored[14]
range(Rmodel$logProb_error)


Rmodel$getNodeNames(dataOnly=TRUE)

Rmodel$calculate()
Rmodel$simulate()
Rmodel$calculate()


conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 500)

samples
##Cmcmc$run(10000)
##samples <- as.matrix(Cmcmc$mvSamples)

colnames(samples)
apply(samples, 2, mean)
samplesPlot(samples)


## facebook use time comparison example for STAT201


df <- read.csv('~/Downloads/timeFB_FRvsSO.csv')

head(df)

hist(df$FBtime)

dim(df)

t.test(df$FBtime)
t.test(df$FBtime, mu=64)
t.test(df$FBtime, mu=64, alternative='greater')

fr <- subset(df$FBtime, df$class=='FR')
so <- subset(df$FBtime, df$class=='SO')
length(fr)
length(so)

t.test(fr, so)


prop.test(x=6, n=10, correct=FALSE)
prop.test(x=60, n=100, correct=FALSE)  ## by default, p0 = 0.5
prop.test(x=600, n=1000, correct=FALSE, p=.6)




head(df)

## trying different samplers for
## "seizures" example for STAT365

nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)

library(nimble)
library(coda)
load('~/Downloads/seizures.RData')
N <- dim(seizures$Counts)[1]


code <- nimbleCode({
    b0 ~ dnorm(0, sd=1000)
    bbase ~ dnorm(0, sd=1000)
    bage ~ dnorm(0, sd=1000)
    btrt ~ dnorm(0, sd=1000)
    sigma ~ dunif(0, 1000)
    sigma_patient ~ dunif(0, 1000)
    for(i in 1:N) {
        g[i] ~ dnorm(0, sd=sigma_patient)
        for(j in 1:4) {
            eps[i, j] ~ dnorm(0, sd=sigma)
            log(lambda[i,j]) <- b0 + bbase * log(baseline[i]) + bage * age[i] + btrt * treatment[i] + eps[i,j] + g[i]
            y[i,j] ~ dpois(lambda[i,j])
        }
    }
})
constants <- list(N = dim(seizures$Counts)[1], baseline=seizures$Baseline, age=seizures$Age, treatment=seizures$Treatment)
data <- list(y=seizures$Counts)
inits <- list(b0=1, bbase=0, bage=0, btrt=0, sigma=1, sigma_patient=1, g=rep(0,N), eps = array(0,c(N,4)))
Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf <- configureMCMC(Rmodel, onlySlice=TRUE)
conf <- configureMCMC(Rmodel, autoBlock=TRUE)
conf <- configureMCMC(Rmodel, onlySlice=TRUE, control=list(adaptInterval=100))

conf$addSampler(c('b0','bage','bbase'), 'RW_block')

conf$printSamplers()
##conf$removeSamplers('b0')
##conf$addSampler('b0', 'slice')
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)

system.time(samples <- runMCMC(Cmcmc, 10000, nburnin=2000))

samplesPlot(samples)


effectiveSize(samples)
## default RW, took 2 seconds
##           b0          bage         bbase          btrt         sigma 
##     9.639895     10.662738     13.488713     59.645279    553.986654 
##sigma_patient 
##    24.535715 




## fitting classification model of oak tree heights
## STAT 365
## Fall 2016

load('~/Downloads/oak_heights.RData')
ls()

hist(oak_heights, breaks=20)

library(nimble)

N <- length(oak_heights)

code <- nimbleCode({
    mu0 ~ dnorm(0, sd=10000)
    mu1 ~ dnorm(0, sd=10000)
    sigma0 ~ dunif(0, 10000)
    sigma1 ~ dunif(0, 10000)
    con ~ dconstraint(mu0 < mu1)
    for(i in 1:N) {
        z[i] ~ dbern(0.5)
        means[i]  <- equals(z[i],0)*mu0    + equals(z[i],1)*mu1
        sigmas[i] <- equals(z[i],0)*sigma0 + equals(z[i],1)*sigma1
        y[i] ~ dnorm(means[i], sd = sigmas[i])
    }
})

constants <- list(N=N)
data <- list(y = oak_heights, con=1)
inits <- list(mu0=0, mu1=1, sigma0=1, sigma1=1, z=rep(0,N))

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$calculate()




## experimenting with t.test and prop.test()
## for STAT201

x1 <- 347
n1 <- 11535
x2 <- 327
n2 <- 14035
p1 <- x1/n1
p2 <- x2/n2
p1
p2

se <- sqrt(p1*(1-p1)/n1 + p2*(1-p2)/n2)
se
ci <- p1-p2 + c(-1,1) * 1.96*se
ci

p_pooled <- (x1+x2)/(n1+n2)
p_pooled
se_pooled = sqrt(p_pooled*(1-p_pooled)*(1/n1+1/n2))
se_pooled

p1-p2
z <- (p1-p2) / (se_pooled)
z
1-pnorm(z)
2*(1-pnorm(z))

pnorm(3.4, lower.tail = FALSE)


prop.test(c(x1,x2), c(n1,n2), correct=FALSE)

prop.test(220,400,correct=FALSE)
prop.test(219,400,correct=FALSE)
prop.test(c(219,220),c(400,400),correct=FALSE)

xbar1 <- 33
xbar2 <- 19.9
n1 <- 476
n2 <- 496
s1 <- 21.9
s2 <- 14.6

se <- sqrt(s1^2/n1 + s2^2/n2)
se
ci <- xbar1-xbar2 + c(-1,1)*1.96*se
ci
z99 <- qnorm(0.995)
z99
ci99 <- xbar1-xbar2 + c(-1,1)*z99*se
ci99


f <- function(..., a=3) {
    browser()
    x <- list(...)
    print(x)
    print(a)
}

f(a=3, 5)


## experimentation with dconstraint and truncation

library(nimble)
code <- nimbleCode({
    a ~ dnorm(0, 1)
    con1 ~ dconstraint(a > 0)
    b ~ T(dnorm(.5, 1),a,)
    c ~ dnorm(0, 1)
    d ~ dnorm(c, 1)
    e ~ dexp(c+d)
    f ~ dexp(e^2)
})
constants <- list()
data <- list(con1=1)
inits <- list(a=1, b=2.5, c=0, d=0, e=1, f=1)
Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$getDependencies('a')

Rmodel$a
Rmodel$con1
exp(Rmodel$calculate('a'))
exp(Rmodel$calculate('con1'))

Rmodel$a <- -1
Rmodel$a
Rmodel$calculate()
exp(Rmodel$calculate('a'))
exp(Rmodel$calculate('con1'))

Rmodel$a <- 1
Rmodel$a
Rmodel$con1 <- 0
Rmodel$con1
Rmodel$calculate()
exp(Rmodel$calculate('a'))
exp(Rmodel$calculate('con1'))

Rmodel$a <- -10
Rmodel$a
Rmodel$con1 <- 0
Rmodel$con1
Rmodel$calculate()
exp(Rmodel$calculate('a'))
exp(Rmodel$calculate('con1'))

conf <- configureMCMC(Rmodel)

conf$getMonitors()
conf$getMonitors2()
conf$printMonitors()

conf$resetMonitors()
conf$addMonitors(c('b'), 'e', c('f', 'a'), print=FALSE)
conf$addMonitors2(c('d', 'e', 'f'), print=FALSE)
conf$addMonitors()
conf$getMonitors()


Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
samples <- runMCMC(Cmcmc, 500000)


colnames(samples)
##apply(samples, 2, mean)
##samplesPlot(samples)
par(mfrow=c(2,1), mar=c(2,1,1,1))
hist(samples[,'a'], breaks=200, xlim=c(-3,6))
hist(samples[,'b'], breaks=200, xlim=c(-3,6))


## testing new initializeModel() function

library(nimble)
code <- nimbleCode({
    p1 ~ dnorm(a, b)
    p2 <- p1 + 1
    p3 <- p2 + p2
    x[1] ~ dnorm(0,10)
    for(i in 2:10) {
        mu[i] <- x[i-1] + p3
        mu2[i] <- mu[i] + p2
        x[i] ~ dnorm(mu2[i], 1)
    }
    muy <- x[10] +100
    y ~ dnorm(muy, 1)
})
constants <- list()
data <- list(y = 100)
inits <- list()
Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)
RinitModel <- initializeModel(Rmodel)
CinitModel <- compileNimble(RinitModel, project=Rmodel)

model <- Rmodel
ini <- RinitModel

model <- Cmodel
ini <- CinitModel

model$a <- 0
model$b <- 3

set.seed(0)
ini$run()
model$calculate()

model$a
model$b
model$lifted_d1_over_sqrt_oPb_cP
##model$simulate('p1')
model$p1
model$p2
model$p3
model$x
model$mu
model$mu2
model$muy
model$y

model$getDependencies('a')
model$getDependencies('b')





## group membership / classification problem for STAT365
## generating data
## from Fall 206:

mu1 <- 24
sd1 <- 4
mu2 <- 32
sd2 <- .5
n1 <- 300
n2 <- 80
set.seed(0)
y1 <- rnorm(n1, mu1, sd1)
y2 <- rnorm(n2, mu2, sd2)
y <- c(y1, y2)
y <- sort(y)
hist(y, breaks=20)

oak_heights <- y
save(oak_heights, file='~/Downloads/oak_heights.RData')

## fit classification model
## from Fall 206:

library(nimble)

load('~/Downloads/oak_heights.RData')
N <- length(oak_heights)
y <- oak_heights

code <- nimbleCode({
    mu0 ~ dnorm(0, sd=10000)
    mu1 ~ dnorm(0, sd=10000)
    constraint ~ dconstraint(mu0 < mu1)
    sigma0 ~ dunif(0, 10000)
    sigma1 ~ dunif(0, 10000)
    for(i in 1:N) {
        x[i] ~ dbinom(size=1, prob=0.5)
        mu[i] <- equals(x[i],0)*mu0 + equals(x[i],1)*mu1
        sigma[i] <- equals(x[i],0)*sigma0 + equals(x[i],1)*sigma1
        y[i] ~ dnorm(mu[i], sd=sigma[i])
    }
})
constants <- list(N=N)
data <- list(y=oak_heights, constraint=1)
inits <- list(mu0=1, mu1=1, sigma0=1, sigma1=1, x=rep(0,N))

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$getMonitors()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 50000, nburnin=10000)
dim(samples)

topLevel <- c('mu0','mu1','sigma0','sigma1')
apply(samples[,topLevel], 2, mean)
samplesPlot(samples, topLevel)

hist(y, breaks=20)

c(y[253], y[260], y[275])
apply(samples[, c('x[253]','x[260]','x[275]')], 2, mean)


samplesPlot(samples, c('x[253]', 'x[260]', 'x[275]'), ind=1001:1050)




## this is from Fall 2016:
## doing continuous-valued state-space model
## for STAT365 problem set 12

## PHASE 1
## CREATE YOUR DATA

n <- 100
a <- 6
b <- 0.8
sigmaPN <- 2
sigmaOE <- 4

set.seed(0)
x <- numeric(n)
y <- numeric(n)
x[1] <- 1
y[1] <- rnorm(1, x[1], sigmaOE)

for(i in 2:n) {
    x[i] <- rnorm(1, a+b*x[i-1], sigmaPN)
    y[i] <- rnorm(1, x[i], sigmaOE)
}

tail(y)

plot(1:n, y, type='l')
lines(1:n, x, col='blue', lwd=2)

## NOW PHASE TWO
## ALL WE HAVE ARE THE Y's (the data)
## ANALYZE


library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, sd=10000)
    b ~ dnorm(0, sd=10000)
    sigmaPN ~ dunif(0, 10000)
    sigmaOE ~ dunif(0, 10000)
    x[1] ~ dnorm(0, sd=10000)   ## x1 technically a top-level param.
    y[1] ~ dnorm(x[1], sd=sigmaOE)
    for(t in 2:n) {
        x[t] ~ dnorm(a+b*x[t-1], sd=sigmaPN)
        y[t] ~ dnorm(x[t], sd=sigmaOE)
    }
})

constants <- list(n = length(y))
data <- list(y=y)
inits <- list(a=0, b=0, sigmaOE=1, sigmaPN=1, x=y)

Rmodel <- nimbleModel(code, constants, data, inits)






## error "Negative Probability Given to Rank Sample" sent to users list
## by Colin <clewisbeck@gmail.com>
## never did figure this one out...

library(nimble)
## Simulate Data#
set.seed(10)
t<-1:(365)
v<-rnorm(365,sd=0.5)
seasonal<-sin(2*pi*t/365) + cos(pi*2*t/365)
y<- seasonal + v
##plot(y,type="p")
##Create Constant W, G and F matrices to pass to NIMBLE
W<-diag(2)
F<-matrix(c(1,0), nrow = 1, ncol = 2)
##G matrix; can generalize this later
G<-matrix(c(cos(2*pi/365),-sin(2*pi/365),sin(2*pi/365),cos(2*pi/365)),nrow=2,ncol=2)
smosCode <- nimbleCode({
    ##Initial Values for State: x0 ~ Normal(c(0,0),diag(1000))
    m0[1]<-0
    m0[2]<-0
    C0[1:2,1:2]<-1000*W[1:2,1:2]
    ##x0[1:2]~dmnorm(m0[1:2],C0[1:2,1:2])
    x0[1:2]~dmnorm(m0[1:2],cov=C0[1:2,1:2])              ## using covariance
    ##initial values
    m1[1:2,1]<-G[1:2,1:2]%*%x0[1:2]
    C1[1:2,1:2]<-sigmaSquaredInvw*W[1:2,1:2]
    ##x[1:2,1]~dmnorm(m1[1:2,1],C1[1:2,1:2])
    x[1:2,1]~dmnorm(m1[1:2,1],cov=C1[1:2,1:2])           ## using covariance
    y[1]~dnorm(inprod(F[1,1:2],x[1:2,1]),sigmaSquaredInvv)
    ##Model
    for (t in 2:T){
        mu[1:2,t]<-G[1:2,1:2]%*%x[1:2,t-1]
        sigs[1:2,1:2]<-sigmaSquaredInvw*W[1:2,1:2]
        ##x[1:2,t] ~ dmnorm(mu[1:2,t],sigs[1:2,1:2])
        x[1:2,t] ~ dmnorm(mu[1:2,t],cov=sigs[1:2,1:2])   ## using covariance
        y[t] ~ dnorm(inprod(F[1,1:2],x[1:2,t]),sigmaSquaredInvv)
    }
    ##Priors 
    sigmaSquaredInvv~dgamma(5,20)
    sigmaSquaredInvw~dgamma(5,20)
})
smosModel<-nimbleModel(code=smosCode,constants=list(T=365,pi=pi,G=G,W=W,F=F),data=list(y=y), inits=list(sigmaSquaredInvv=1,sigmaSquaredInvw=1))
smosLiuWestFilter<-buildLiuWestFilter(model=smosModel,nodes='x',params=c('sigmaSquaredInvv','sigmaSquaredInvw'))

csmosLiuWestFilter <- compileNimble(smosModel, smosLiuWestFilter)

set.seed(0); smosLiuWestFilter$run(100)
head(as.matrix(smosLiuWestFilter$mvWSamples))[,c(3,4,5,1,2)]
##     sigmaSquaredInvv[1] sigmaSquaredInvw[1]      wts[1]     x[1]      x[2]
##[1,]         0.001055908           0.3644654 -0.04219564 35.55898 -15.77173
##[2,]         0.001056058           0.3644703 -0.04545424 35.64636 -15.62196
##[3,]         0.001056334           0.3644903 -0.04317495 35.58381 -15.96475
##[4,]         0.001056401           0.3644907  0.02075697 35.27854 -14.09417
##[5,]         0.001055928           0.3644734  0.02530712 35.15605 -15.57586
##[6,]         0.001056553           0.3645051  0.03630814 34.85197 -14.91214
tail(as.matrix(smosLiuWestFilter$mvWSamples))[,c(3,4,5,1,2)]
##       sigmaSquaredInvv[1] sigmaSquaredInvw[1]       wts[1]     x[1]      x[2]
##[95,]          0.001065339           0.3649603 -0.025964321 35.14237 -15.52160
##[96,]          0.001065207           0.3649535 -0.015406686 34.85541 -15.56082
##[97,]          0.001065683           0.3649811 -0.060543006 36.06590 -15.44988
##[98,]          0.001065355           0.3649594 -0.048862101 36.16558 -13.87091
##[99,]          0.001065157           0.3649503  0.003280856 34.76580 -14.95481
##[100,]         0.001065097           0.3649547  0.009111265 34.93088 -15.21369

set.seed(0); csmosLiuWestFilter[[2]]$run(10000)
head(as.matrix(csmosLiuWestFilter[[2]]$mvWSamples))
##     sigmaSquaredInvv[1] sigmaSquaredInvw[1]      wts[1]     x[1]      x[2]
##[1,]         0.001055908           0.3644654 -0.04219564 35.55898 -15.77173
##[2,]         0.001056058           0.3644703 -0.04545424 35.64636 -15.62196
##[3,]         0.001056334           0.3644903 -0.04317495 35.58381 -15.96475
##[4,]         0.001056401           0.3644907  0.02075697 35.27854 -14.09417
##[5,]         0.001055928           0.3644734  0.02530712 35.15605 -15.57586
##[6,]         0.001056553           0.3645051  0.03630814 34.85197 -14.91214
tail(as.matrix(csmosLiuWestFilter[[2]]$mvWSamples))
##       sigmaSquaredInvv[1] sigmaSquaredInvw[1]       wts[1]     x[1]      x[2]
##[95,]          0.001065339           0.3649603 -0.025964321 35.14237 -15.52160
##[96,]          0.001065207           0.3649535 -0.015406686 34.85541 -15.56082
##[97,]          0.001065683           0.3649811 -0.060543006 36.06590 -15.44988
##[98,]          0.001065355           0.3649594 -0.048862101 36.16558 -13.87091
##[99,]          0.001065157           0.3649503  0.003280856 34.76580 -14.95481
##[100,]         0.001065097           0.3649547  0.009111265 34.93088 -15.21369



## playing with NIMBLE model dinterval, dconstraint

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, sd=10)
    d ~ dinterval(a, int)
    c ~ dconstraint(a > 0 & int==1)
})

Rmodel <- nimbleModel(code)
Rmodel$getNodeNames()
Rmodel$getNodeNames(includeRHSonly=TRUE)
Rmodel$getDependencies('a')
ls(Rmodel$nodes)
Rmodel$nodes$lifted_a_gt_0_and_int__1
Rmodel$getDependencies('d')
Rmodel$getDependencies('c')

Rmodel$a <- 0
Rmodel$a
exp(Rmodel$calculate('a'))

Rmodel$d <- 0
Rmodel$int <- -2
Rmodel$d
Rmodel$int
exp(Rmodel$calculate('d'))

Rmodel$simulate('d'); Rmodel$d
exp(Rmodel$calculate('d'))

Rmodel$int <- 1
Rmodel$c <- 1
Rmodel$a <- .1
Rmodel$a
Rmodel$int
Rmodel$c
invisible(Rmodel$calculate()); exp(Rmodel$calculate('c'))




## doing wwwhours, web usage problem for STAT201

##df <- read.csv('~/Downloads/wwwhours.csv')
df <- read.csv('~/github/courses/stat201/data/wwwhours.csv')
head(df)
summary(df)
dim(df)
hours <- df$hours
n <- length(hours)
n
hist(hours)
mean(hours)
se <- sd(hours)/sqrt(n)
z <- qnorm(0.975)
ci <- mean(hours) + c(-1,1) * z * se
ci

iter <- 10000
means <- numeric(iter)
for(i in 1:iter) {
    ind <- sample(1:n, replace=TRUE)
    bootstrapSample <- hours[ind]
    means[i] <- mean(bootstrapSample)
}
hist(means)
abline(v=ci, lwd=2, col='red')


bootstrapCI <- quantile(means, c(0.025, 0.975))
bootstrapCI
ci
abline(v=bootstrapCI, lwd=2, col='blue')


## starting to think about BUGS example mice, which is
## a Weibull regression survival analysis with censoring, censored data

l <- list(t = structure(.Data = 
                            c(12, 1, 21, 25, 11, 26, 27, 30, 13, 12, 21, 20, 23, 25, 23, 29, 35, NA, 31, 36, 
                              32, 27, 23, 12, 18, NA, NA, 38, 29, 30, NA, 32, NA, NA, NA, NA, 25, 30, 37, 27, 
                              22, 26, NA, 28, 19, 15, 12, 35, 35, 10, 22, 18, NA, 12, NA, NA, 31, 24, 37, 29, 
                              27, 18, 22, 13, 18, 29, 28, NA, 16, 22, 26, 19, NA, NA, 17, 28, 26, 12, 17, 26),
              .Dim = c(4, 20)),
          t.cen = structure(.Data = 
                                c( 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 40, 0, 0, 
                                  0, 0, 0, 0, 0, 40, 40, 0, 0, 0, 40, 0, 40, 40, 40, 40, 0, 0, 0, 0, 
                                  0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 0, 40, 40, 0, 0, 0, 0, 
                                  0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 29, 10, 0, 0, 0, 0, 0, 0),
              .Dim = c(4, 20)),
          M = 4, N = 20)



t <- l$t
t.censored <- l$t.cen



t
t.censored
censored <- t.censored==0


library(nimble)

code <- nimbleCode({
    for(i in 1:4) {
        for(j in 1:20) {
            t[i,j] ~ dweib(r, mu[i])
            t[i,j] 
        }
    }
})

constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)
##Cmcmc$run(10000)
##samples <- as.matrix(Cmcmc$mvSamples)

colnames(samples)
apply(samples, 2, mean)
samplesPlot(samples)



## generating Dolphin dolphin capture-recapture data for STAT365
## Fall 2016:

phi <- .8
p <- 0.5
k <- 10
n <- 145
y <- array(NA, c(n,k))
for(i in 1:n) {
    y[i,1] <- 1
    x <- 1
    for(t in 2:k) {
        x <- rbinom(1, prob=x*phi, size=1)
        y[i,t] <- rbinom(1, prob=p*x, size=1)
    }
}

sightings <- y
save(sightings, file='~/Downloads/dolphins.RData')


load('~/github/courses/stat365/data/dolphins.RData')
load('~/downloads/dolphins.RData')



## generate time series data from
## first-order autoregressive process

## simulation parameters
n <- 100
a <- 6
b <- .8
sigmaOE <- 4
sigmaPN <- 2

## generate time-series data: y1, y2, ..., y100
set.seed(0)
x <- numeric(n)
y <- numeric(n)
x[1] <- 1
y[1] <- rnorm(1, x[1], sigmaOE)
for (i in 2:n) {
    x[i] <- rnorm(1, a + b*x[i-1], sigmaPN)
    y[i] <- rnorm(1, x[i], sigmaOE)
}

## visualise the data
plot(1:n, y, type='l')
lines(1:n, x, col='blue', lwd=2)

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 0.0001)
    b ~ dnorm(0, 0.0001)
    sigmaOE ~ dunif(0, 10000)
    sigmaPN ~ dunif(0, 10000)
    x[1] ~ dnorm(0, 0.00001)
    y[1] ~ dnorm(x[1], sd=sigmaOE)
    a[1] ~ dnorm(0, 0.00001)
    for(i in 2:n) {
        a[i] <- p*i
        x[i] ~ dnorm(a[i] + b * x[i-1], sd=sigmaPN)
        y[i] ~ dnorm(x[i], sd=sigmaOE)
    }
})
constants <- list(n=n)
data <- list(y=y)
inits <- list(c=.1, b=0, sigmaOE=1, sigmaPN=1, x=rep(0, n))

Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$resetMonitors()
conf$addMonitors(c('p', 'b', 'sigmaOE', 'sigmaPN'))
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 100000, nburnin=20000)
##Cmcmc$run(10000)
##samples <- as.matrix(Cmcmc$mvSamples)

colnames(samples)
apply(samples, 2, mean)
samplesPlot(samples, burnin=1000)



## modelling dolphin capture-recapture data for STAT 365
## Fall 2016:

load('~/Downloads/dolphins.RData')

library(nimble)

dim(sightings)
nind <- dim(sightings)[1]
nocc <- dim(sightings)[2]

code <- nimbleCode({
    p   ~ dunif(0,1)
    phi ~ dunif(0,1)
    for(ind in 1:nind) {
        x[ind,1] <- 1
        y[ind,1] <- 1
        for(t in 2:nocc) {
            x[ind,t] ~ dbinom(size=1, prob=phi*x[ind,t-1])
            y[ind,t] ~ dbinom(size=1, prob=p*x[ind,t])
        }
    }
})
constants <- list(nind=nind, nocc=nocc)
data <- list(y=sightings)
inits <- list(x=array(1,c(nind,nocc)), p=0.5, phi=0.5)

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()

conf <- configureMCMC(Rmodel)

conf$printSamplers('p')
conf$printSamplers('phi')
conf$printSamplers()
conf$getMonitors()





## testing new addition to NIMBLE: conf$addSampler('node', 'conjugate')
library(nimble)

code <- nimbleCode({
    x ~ dgamma(1, 1)       # should satisfy 'gamma' conjugacy class
    a  ~ dnorm(0, x)     # should satisfy 'norm' conjugacy class
    a2 ~ dnorm(0, tau = 3*x+0)
    b  ~ dpois(0+5*x)
    b2 ~ dpois(1*x*1)
    c ~ dgamma(1, 7*x*5)
    for(i in 2:3) {
        jTau[i] <- 1
        jNorm[i] ~ dnorm(c * (a+3) - i, var = jTau[i])
        kTauSd[i] <- 2
        kLogNorm[i] ~ dlnorm(0 - a - 6*i, kTauSd[i])
    }
})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)

conf$printSamplers()
##[1]  conjugate_dgamma_dnorm_dpois_dgamma sampler: x,  dep_dnorm: a, a2,  dep_dpois: b, b2,  dep_dgamma: c
##[2]  conjugate_dnorm_dnorm_dlnorm sampler: a,  dep_dnorm: jNorm[2], jNorm[3],  dep_dlnorm: kLogNorm[2], kLogNorm[3]
##[3]  posterior_predictive sampler: a2
##[4]  posterior_predictive sampler: b
##[5]  posterior_predictive sampler: b2
##[6]  RW sampler: c
##[7]  posterior_predictive sampler: kLogNorm[2]
##[8]  posterior_predictive sampler: kLogNorm[3]
##[9]  posterior_predictive sampler: jNorm[2]
##[10] posterior_predictive sampler: jNorm[3]

pr <- TRUE
pr <- FALSE
nd <- 'x'
nd <- 'a'
nd <- 'c'
nd <- 'a2'
nd <- 'kLogNorm[3]'
conf$addSampler(nd, 'RW',        print=pr)
conf$addSampler(nd, 'slice',     print=pr)
conf$addSampler(nd, 'conjugate', print=pr)

conf$printSamplers()

conf$addSampler(nd, print=TRUE)
conf$addSampler(nd, print=TRUE)
conf$addSampler(nd, print=TRUE)





par(mfrow=c(1,3))
ns <- c(50, 500, 5000)
for(n in ns) {
    x <- rbinom(n=10000, size=n, prob=0.8) / n
    hist(x, xlim=c(0,1))
}




library(nimble)
df <- read.csv('~/Downloads/UsedCars.csv')
code <- nimbleCode({
    b0  ~ dnorm(0, sd=10000)
    bage ~ dnorm(0, sd=10000)
    bhp ~ dnorm(0, sd=10000)
    btype ~ dnorm(0, sd=10000)
    sigma ~ dunif(0, 50000)
    for(i in 1:N) {
        y[i] ~ dnorm(mu[i], sd = sigma)
        mu[i] <- b0 + bage*age[i] + bhp*hp[i] + btype*type[i]
    }
})
constants <- list(N = dim(df)[1], age=df$Age, hp=df$HP, type=df$Type)
data <- list(y = df$Price)
inits <- list(b0=0, bage=0, bhp=0, btype=0, sigma=1)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)


samples <- runMCMC(Cmcmc, 20000, nburnin=10000)
dim(samples)

samplesPlot(samples)
samplesPlot(samples, 'btype')

apply(samples, 2, effectiveSize)






1

a <- acf(samples)
a
a$acf
plot(a)
acfplot(as.mcmc(samples), thin=10)

library(coda)
geweke.diag(codaMCMC[[1]][-(1:1000),])
geweke.plot(codaMCMC[[1]])
samplesPlot(codaMCMC[[1]])





?acf
acfplot

colnames(samples)
quantile(samples[, 'bage'], c(0.025, 0.975))

dim(samples)
colnames(samples)
samplesPlot(samples)
samplesPlot(samples, var='btype', burnin=2000)
samplesPlot(samples, var='btype', ind=1:200)
samplesPlot(samples, var='btype', ind=2001:10000)
samplesPlot(samples, var=c('btype', 'bage'), ind=2001:10000)
samplesPlot(samples, var=c('btype', 'bage'), ind=2001:10000, densityplots=FALSE)

samplesPlot(samples, burnin=2000)
samplesPlot(samples, var=1:3, burnin=2000)
samplesPlot(samples, var=4, burnin=5000)
samplesPlot(samples, var=1, burnin=2000)

quantile(samples[-(1:2000), 1], c(0.025, 0.975))








## "seizures" example for STAT365

library(nimble)
load('~/Downloads/seizures.RData')
N <- dim(seizures$Counts)[1]


code <- nimbleCode({
    b0 ~ dnorm(0, sd=1000)
    bbase ~ dnorm(0, sd=1000)
    bage ~ dnorm(0, sd=1000)
    btrt ~ dnorm(0, sd=1000)
    sigma ~ dunif(0, 1000)
    sigma_patient ~ dunif(0, 1000)
    for(i in 1:N) {
        g[i] ~ dnorm(0, sd=sigma_patient)
        for(j in 1:4) {
            eps[i, j] ~ dnorm(0, sd=sigma)
            log(lambda[i,j]) <- b0 + bbase * log(baseline[i]) + bage * age[i] + btrt * treatment[i] + eps[i,j] + g[i]
            y[i,j] ~ dpois(lambda[i,j])
        }
    }
})
constants <- list(N = dim(seizures$Counts)[1], baseline=seizures$Baseline, age=seizures$Age, treatment=seizures$Treatment)
data <- list(y=seizures$Counts)
inits <- list(b0=1, bbase=0, bage=0, btrt=0, sigma=1, sigma_patient=1, g=rep(0,N), eps = array(0,c(N,4)))
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
##conf <- configureMCMC(Rmodel, onlySlice=TRUE)
conf$printSamplers()
##conf$removeSamplers('b0')
##conf$addSampler('b0', 'slice')
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
samplesList <- runMCMC(Cmcmc, 500000, nburnin=10000, nchains=3, returnCodaMCMC=TRUE)

samples <- samplesList[[1]]
samplesPlot(samples)
samplesPlot(samples, 'btrt')
samplesPlot(samples, 'b0')
##samplesPlot(samples, 'bbase')
library(coda)
apply(samples, 2, effectiveSize)
##dim(samples)
gelman.diag(samplesList)

apply(samples, 2, effectiveSize)
apply(samples, 2, mean)
sqrt(apply(samples, 2, var)) / sqrt(apply(samples, 2, effectiveSize))



codalist <- runMCMC(Cmcmc, 100000, nchains=3, returnCodaMCMC=TRUE)

gelman.diag(codalist)
geweke.diag(codalist)
geweke.plot(codalist)

apply(codalist[[1]], 2, effectiveSize)
apply(codalist[[1]], 2, var) / apply(codalist[[1]], 2, effectiveSize)


## "seeds" example for STAT365
## from Fall 2016
write.csv(df, '~/Downloads/seeds.csv')


df <- read.csv('~/Downloads/seeds.csv')
df

cucumber <- as.numeric(df$plant) - 1
fertB <- as.numeric(df$fertilizer) - 1
y <- df$germinations
n <- df$seeds
N <- dim(df)[1]

library(nimble)


sds <- c(.1, .5, 1, 5, 10)
samps <- array(0, c(10000, length(sds)))

for(i in seq_along(sds)) {
    sdC <- sds[i]
    code <- nimbleCode({
        sigma ~ dunif(0, 100)
        b0 ~ dnorm(0, sd=1000)
        bCuc ~ dnorm(0, sd=sdC)
        bFertB ~ dnorm(0, sd=1000)
        for(i in 1:N) {
            mu[i] <- b0 + bCuc * cucumber[i] + bFertB * fertB[i]
            logit(p[i]) ~ dnorm(mu[i], sd=sigma)
            y[i] ~ dbinom(size=n[i], prob=p[i])
        }
    })
    constants <- list(N=N, cucumber=cucumber, fertB=fertB, n=n, sdC=sdC)
    data <- list(y=y)
    inits <- list(sigma=1, b0=1, bCuc=0, bFertB=0)
    Rmodel <- nimbleModel(code, constants, data, inits)
    Rmodel$mu
    calculate(Rmodel)
    conf <- configureMCMC(Rmodel)
    conf$printSamplers()
    Rmcmc <- buildMCMC(conf)
    Cmodel <- compileNimble(Rmodel)
    Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
    set.seed(0)
    samples <- runMCMC(Cmcmc, 100000)
    ##Cmcmc$run(10000)
    ##samples <- as.matrix(Cmcmc$mvSamples)
    samps[, i] <- samples[ 50001:60000, 'bCuc']
}

dim(samps)
dimnames(samps)
colnames(samps) <- paste0('sd', as.character(sds))
dimnames(samps)

samplesPlot(samples)

## prior sensitity analysis for different
## parametrizations of the 'sigma' term
niter <- 100000
samps <- array(0, c(niter, 3))
colnames(samps) <- c('sd', 'var', 'tau')

code <- nimbleCode({
    sigma ~ dunif(0, 1000)
    b0 ~ dnorm(0, sd=1000)
    bCuc ~ dnorm(0, sd=1000)
    bFertB ~ dnorm(0, sd=1000)
    for(i in 1:N) {
        mu[i] <- b0 + bCuc * cucumber[i] + bFertB * fertB[i]
        logit(p[i]) ~ dnorm(mu[i], sd=sigma)
        y[i] ~ dbinom(size=n[i], prob=p[i])
    }
})
constants <- list(N=N, cucumber=cucumber, fertB=fertB, n=n)
data <- list(y=y)
inits <- list(sigma=1, b0=1, bCuc=0, bFertB=0)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
samples <- runMCMC(Cmcmc, niter)
samps[, 1] <- samples[, 'sigma']
code <- nimbleCode({
    v ~ dunif(0, 1000)
    b0 ~ dnorm(0, sd=1000)
    bCuc ~ dnorm(0, sd=1000)
    bFertB ~ dnorm(0, sd=1000)
    for(i in 1:N) {
        mu[i] <- b0 + bCuc * cucumber[i] + bFertB * fertB[i]
        logit(p[i]) ~ dnorm(mu[i], var=v)
        y[i] ~ dbinom(size=n[i], prob=p[i])
    }
})
constants <- list(N=N, cucumber=cucumber, fertB=fertB, n=n)
data <- list(y=y)
inits <- list(v=1, b0=1, bCuc=0, bFertB=0)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
samples <- runMCMC(Cmcmc, niter)
samps[, 2] <- samples[, 'v']
code <- nimbleCode({
    tau ~ dgamma(0.001, 0.001)
    b0 ~ dnorm(0, sd=1000)
    bCuc ~ dnorm(0, sd=1000)
    bFertB ~ dnorm(0, sd=1000)
    for(i in 1:N) {
        mu[i] <- b0 + bCuc * cucumber[i] + bFertB * fertB[i]
        logit(p[i]) ~ dnorm(mu[i], tau=tau)
        y[i] ~ dbinom(size=n[i], prob=p[i])
    }
})
constants <- list(N=N, cucumber=cucumber, fertB=fertB, n=n)
data <- list(y=y)
inits <- list(tau=1, b0=1, bCuc=0, bFertB=0)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
samples <- runMCMC(Cmcmc, niter)
samps[, 3] <- samples[, 'tau']

samps[, 'var'] <- sqrt(samps[, 'var'])
samps[, 'tau'] <- 1/sqrt(samps[, 'tau'])


head(samps)

dim(samps)
dimnames(samps)

samplesPlot(samps, burnin=50000)








## "pairs"
## studying estimation of normal variance / covariance,
## from paired observations from a common variance
## (and possibly different mean)


## this shows that for pairs x1,x2 ~ N(mean=[changing], var= constant v)
## using the average of (x2-x1)^2 / 2
## gives an unbiased estimate of the common variance v
estimateV <- function(n, v) {
    pairs <- t(replicate(n, rnorm(2, mean=runif(1,0,100), sd=sqrt(v))))
    vest <- apply(pairs, 1, function(x) ((x[2]-x[1])^2)/2)
    mean(vest)
}
n <- 1000
v <- 2
n.average.over=1000
estimates <- replicate(n.average.over, estimateV(n=n, v=v))
mean(estimates)

## this shows that the above (vest = (x2-x1)^2 / 2)
## is *not* the same as a traditional sd(x), if the x are used to
## generate a vector of n consecutive "pairs"
n <- 10
v <- 2
xs <- rnorm(n, sd=sqrt(v))
sd(xs)^2
pairs <- array(NA, c(n,2))
for(i in 1:(n-1)) pairs[i,] <- xs[i:(i+1)]
pairs[n,] <- xs[c(n,1)]
xs
pairs
vest <- apply(pairs, 1, function(x) ((x[2]-x[1])^2)/2)
sum(vest) / (n-1)
sd(xs)^2


## this shows that for multivariate-normal pairs
## x1,x2 ~ MVN(mean=[changing], Sigma = constant V)
## using the average of dif=x2-x1, dif %*% t(dif) / 2
## gives an unbiased estimate of the common covariance matrix V
estimateV <- function(n, V) {
    pairs <- lapply(1:n, function(x) {
        mus <- runif(2,0,100)
        list(rmvnorm(1, mus, sigma=V)[1,],
             rmvnorm(1, mus, sigma=V)[1,])
    })
    vest <- lapply(pairs, function(x) {
        dif <- x[[2]] - x[[1]]
        (dif %*% t(dif)) / 2
    })
    apply(array(unlist(vest), dim=c(2,2,n)), c(1,2), sum) / n
}
## this new way does the "better" "newer" way of estimating
## empirical covariance.  I'm really just implementing it,
## because if it works, will be easier to modify the block sampler.
estimateV2 <- function(n, V) {
    pairs <- lapply(1:n, function(x) {
        mus <- runif(2,0,100)
        list(rmvnorm(1, mus, sigma=V)[1,],
             rmvnorm(1, mus, sigma=V)[1,])
    })
    ## original:
    ##vest <- lapply(pairs, function(x) {
    ##    dif <- x[[2]] - x[[1]]
    ##    (dif %*% t(dif)) / 2
    ##})
    ##apply(array(unlist(vest), dim=c(2,2,n)), c(1,2), sum) / n
    ## following is, essentially, vest2:
    difs <- lapply(pairs, function(x) { x[[2]] - x[[1]] })
    ## keep track of array of the after-before differences:
    empirSamp <- matrix(unlist(difs), nrow=n, ncol=2, byrow=TRUE)
    ## estimate covariance as:
    empirCov <- t(empirSamp) %*% empirSamp / (2*n)  ## this works!
    empirCov
}
library(mvtnorm)
n <- 200
v1 <- 2
v2 <- 5
rho <- 0.6
V <- array(c(v1^2, rho*v1*v2, rho*v1*v2, v2^2), c(2,2))
n.average.over <- 200
set.seed(0); estimates  <- replicate(n.average.over, estimateV( n=n, V=V))
set.seed(0); estimates2 <- replicate(n.average.over, estimateV2(n=n, V=V))
V
## two methods are identical:
apply(estimates,  c(1,2), mean)
apply(estimates2, c(1,2), mean)

## still working on "pairs"
## now, testing that the new block sampler, RW_block_new
## appears to be correctly saving before/after samples, etc..

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a/2, 1)
    ##c ~ dgamma(a^2, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0, b=0)
Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel, nodes=NULL)
conf$printSamplers()
conf$addSampler(c('a', 'b'), 'RW_block_new', control=list(adaptInterval=10))
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

iter <- 5
set.seed(0); samples <- runMCMC(Rmcmc, iter)
set.seed(0); samples <- runMCMC(Cmcmc, iter)



## still continuing "pairs"
## now, running correlated state space model, and seeing if
## new RW_block_new sampler adapts towards a new, different covariance?
## and whether it achieves good sampling quicker???
## re-creating the deep-dive deep dive into correlated state-space state space model
## to figure out why the block sampler is taking so long to adapt (from Dao).
## where 'a' and 'b' don't mix until after 150,000 iterations
## this time trying the RW_block_new sampler

library(nimble)
library(coda)
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
#####nimbleOptions(showCompilerOutput=TRUE)
##
code <- nimbleCode({
    a ~ dunif(-0.9999, 0.9999)
    b ~ dnorm(0, sd = 1000)
    sigPN ~ dunif(1e-04, 1)
    sigOE ~ dunif(1e-04, 1)
    x[1] ~ dnorm(b/(1 - a), sd = sqrt(sigPN^2 + sigOE^2))
    y[1] ~ dnorm(x[1], sd = sigOE)
    for (i in 2:t) {
        x[i] ~ dnorm(x[i - 1] * a + b, sd = sigPN)
        y[i] ~ dnorm(x[i], sd = sigOE)
    }
})
constants <- list(t = 100)
data <- list(y = c(20.24405,20.57693,20.49357,20.34159,20.45759,20.43326,20.20554,20.12860,20.14756,20.20781,20.23022,20.26766,20.22984,20.37703,20.13641,20.05309,19.95709,20.19303,20.30562,20.54443,20.91010,20.70580,20.42344,20.19795,20.28816,20.31894,20.76939,20.77023,20.83486,20.29335,20.40990,20.19601,20.04083,19.76056,19.80810,19.83129,19.69174,19.90069,19.87623,19.63371,19.62360,19.72630,19.64450,19.86779,20.17104,20.34797,20.32968,20.48027,20.46694,20.47006,20.51676,20.40695,20.18715,19.97552,19.88331,19.67831,19.74702,19.47502,19.24408,19.37179,19.38277,19.15034,19.08723,19.37051,19.14274,19.46433,19.62459,19.77971,19.54194,19.39081,19.61621,19.51307,19.34745,19.17019,19.26829,19.58943,19.77143,19.83582,19.71198,19.67746,19.75053,20.40197,20.49363,20.37079,20.19005,20.55862,20.48523,20.33071,19.97069,19.79758,19.83811,19.79728,19.86277,19.86836,19.92481,19.88095,20.24899,20.55165,20.22707,20.11235))
inits <- list(a = 0.95, b=1, sigPN = 0.2, sigOE=0.05, x = c(20.26036,20.51331,20.57057,20.35633,20.33736,20.47321,20.22002,20.14917,20.19216,20.26969,20.21135,20.22745,20.20466,20.41158,20.13408,20.08023,19.98956,20.13543,20.32709,20.55840,20.88206,20.74740,20.47671,20.14012,20.29953,20.33778,20.80916,20.75773,20.84349,20.35654,20.41045,20.20180,20.02872,19.74226,19.80483,19.81842,19.69770,19.84564,19.88211,19.70559,19.56090,19.73728,19.66545,19.88158,20.13870,20.39163,20.37372,20.47429,20.39414,20.42024,20.55560,20.40462,20.15831,19.89425,19.79939,19.72692,19.74565,19.42233,19.22730,19.36489,19.37289,19.19050,19.00823,19.35738,19.14293,19.48812,19.67329,19.82750,19.58979,19.43634,19.61278,19.56739,19.38584,19.19260,19.32732,19.65500,19.65295,19.84843,19.68285,19.69620,19.77497,20.31795,20.45797,20.32650,20.24045,20.60507,20.51597,20.30076,19.98100,19.86709,19.85965,19.74822,19.86730,19.90523,19.86970,19.87286,20.28417,20.46212,20.22618,20.13689))
##
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()   ## [1] 183.3436
##Rmodel2 <- Rmodel$newModel(replicate=TRUE)
##Rmodel2$calculate()   ## [1] 183.3436
##Rmodel3 <- Rmodel$newModel(replicate=TRUE)
##Rmodel3$calculate()   ## [1] 183.3436
##
conf <- configureMCMC(Rmodel, nodes = NULL)
conf$addSampler(c('a', 'b'), 'RW_block')
conf$printSamplers()
conf$addSampler('sigOE', 'RW')
conf$addSampler('sigPN', 'RW')
for(node in Rmodel$expandNodeNames('x'))
    conf$addSampler(node, 'RW')
conf$resetMonitors()
conf$addMonitors(c('a', 'b', 'sigOE', 'sigPN'))
conf$getMonitors()
##
##conf2 <- configureMCMC(Rmodel2, nodes = NULL)
##conf2$addSampler(c('a', 'b'), 'RW_block_new', control=list(useAcceptedOnly=FALSE))
####conf2$addSampler(c('a', 'b'), 'AF_slice', control=list(sliceWidths=c(1,1)))
##conf2$printSamplers()
##conf2$addSampler('sigOE', 'RW')
##conf2$addSampler('sigPN', 'RW')
##for(node in Rmodel$expandNodeNames('x'))
##    conf2$addSampler(node, 'RW')
##conf2$resetMonitors()
##conf2$addMonitors(c('a', 'b', 'sigOE', 'sigPN'))
##conf2$getMonitors()
##
##conf3 <- configureMCMC(Rmodel3, nodes = NULL)
##conf3$addSampler(c('a', 'b'), 'RW_block_new', control=list(useAcceptedOnly=TRUE))
####conf3$addSampler(c('a', 'b'), 'AF_slice', control=list(sliceWidths=c(1,1)))
##conf3$printSamplers()
##conf3$addSampler('sigOE', 'RW')
##conf3$addSampler('sigPN', 'RW')
##for(node in Rmodel$expandNodeNames('x'))
##    conf3$addSampler(node, 'RW')
##conf3$resetMonitors()
##conf3$addMonitors(c('a', 'b', 'sigOE', 'sigPN'))
##conf3$getMonitors()


Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmodel$calculate()   ## [1] 183.3436
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##
##Rmcmc2 <- buildMCMC(conf2)
##Cmodel2 <- compileNimble(Rmodel2)
##Cmodel2$calculate()   ## [1] 183.3436
##Cmcmc2 <- compileNimble(Rmcmc2, project = Rmodel2)
####
##Rmcmc3 <- buildMCMC(conf3)
##Cmodel3 <- compileNimble(Rmodel3)
##Cmodel3$calculate()   ## [1] 183.3436
##Cmcmc3 <- compileNimble(Rmcmc3, project = Rmodel3)

##niter <- 100000
##niter <- 300000
##niter <- 500000
niter <- 1000000

set.seed(0); system.time(Cmcmc$run(niter))
##set.seed(0); system.time(Cmcmc2$run(niter))
##set.seed(0); system.time(Cmcmc3$run(niter))

samples  <- as.matrix(Cmcmc$mvSamples)
##samples2 <- as.matrix(Cmcmc2$mvSamples)
##samples3 <- as.matrix(Cmcmc3$mvSamples)

#### Plot1
dev.new(width=8, height=6)
par(mfrow=c(3,2), mar=c(1,1,1,1))
indToPlot <- 1:700000
plot( samples[indToPlot,'a'], type='l', ylab='a')
plot( samples[indToPlot,'b'], type='l', ylab='b')
plot(samples2[indToPlot,'a'], type='l', ylab='a')
plot(samples2[indToPlot,'b'], type='l', ylab='b')
plot(samples3[indToPlot,'a'], type='l', ylab='a')
plot(samples3[indToPlot,'b'], type='l', ylab='b')

plot( samples[,'a'], type='l', ylab='a')
plot( samples[,'b'], type='l', ylab='b')
plot(samples2[,'a'], type='l', ylab='a')
plot(samples2[,'b'], type='l', ylab='b')
plot(samples3[,'a'], type='l', ylab='a')
plot(samples3[,'b'], type='l', ylab='b')

## plots of sigma term sampling look just fine, in both cases
##samplesPlot(samples, var=c('sigOE','sigPN'))
##samplesPlot(samples2, var=c('sigOE','sigPN'))

pdf('~/Downloads/RWblock.pdf')
par(mfrow=c(2,1), mar=c(2,1,2,1))
indToPlot <- 1:200000
plot( samples[indToPlot,'a'], type='l', ylab='a', main = 'MCMC Samples of Parameter 1')
plot( samples[indToPlot,'b'], type='l', ylab='b', main = 'MCMC Samples of Parameter 2')
dev.off()


block_scales <- Cmcmc$samplerFunctions$contentsList[[1]]$getScaleHistory()
length(block_scales)
block_propCovHistory <- Cmcmc$samplerFunctions$contentsList[[1]]$getPropCovHistory()
## create block_propCovScale
block_propCovScale <- block_propCovHistory
for(i in 1:length(block_scales))   block_propCovScale[i,,] <- block_scales[i] * block_propCovHistory[i,,]
block_scale_a <- apply(block_propCovScale, 1, function(x) sqrt(x[1,1]))
block_scale_b <- apply(block_propCovScale, 1, function(x) sqrt(x[2,2]))
block_cors <- apply(block_propCovHistory, 1, function(x) cov2cor(x)[1,2])
ar <- cbind(block_scales, block_scale_a, block_scale_b, block_cors)
colnames(ar) <- c('scale', 'sig_a', 'sig_b', 'cor')
samplesPlot(ar, ind = 1:2000, density = FALSE)

expandFactor <- 200
dim(ar)[1] * expandFactor
arNew <- array(0, c(dim(ar)[1] * expandFactor, dim(ar)[2]))
dimnames(arNew) <- dimnames(ar)
for(i in 1:dim(ar)[1]) {
    for(j in 1:expandFactor) {
        arNew[(i-1)*expandFactor+j, ] <- ar[i, ]
    }
}

samplesPlot(arNew, var = c('scale', 'cor'), ind = 1:200000, density = FALSE, legend = FALSE)
##savePlot('RWblock_adapt', 'jpeg')

 

##block_scales2 <- Cmcmc2$samplerFunctions$contentsList[[1]]$getScaleHistory()
##length(block_scales2)
##block_propCovHistory2 <- Cmcmc2$samplerFunctions$contentsList[[1]]$getPropCovHistory()
#### create block_propCovScale
##block_propCovScale2 <- block_propCovHistory2
##for(i in 1:length(block_scales2))   block_propCovScale2[i,,] <- block_scales2[i] * block_propCovHistory2[i,,]
##block_scale_a2 <- apply(block_propCovScale2, 1, function(x) sqrt(x[1,1]))
##block_scale_b2 <- apply(block_propCovScale2, 1, function(x) sqrt(x[2,2]))
##block_cors2 <- apply(block_propCovHistory2, 1, function(x) cov2cor(x)[1,2])
##ar2 <- cbind(block_scales2, block_scale_a2, block_scale_b2, block_cors2)
##colnames(ar2) <- c('scale', 'sig_a', 'sig_b', 'cor')
##samplesPlot(ar2)

##block_scales3 <- Cmcmc3$samplerFunctions$contentsList[[1]]$getScaleHistory()
##length(block_scales3)
##block_propCovHistory3 <- Cmcmc3$samplerFunctions$contentsList[[1]]$getPropCovHistory()
#### create block_propCovScale
##block_propCovScale3 <- block_propCovHistory3
##for(i in 1:length(block_scales3))   block_propCovScale3[i,,] <- block_scales3[i] * block_propCovHistory3[i,,]
##block_scale_a3 <- apply(block_propCovScale3, 1, function(x) sqrt(x[1,1]))
##block_scale_b3 <- apply(block_propCovScale3, 1, function(x) sqrt(x[2,2]))
##block_cors3 <- apply(block_propCovHistory3, 1, function(x) cov2cor(x)[1,2])
##ar3 <- cbind(block_scales3, block_scale_a3, block_scale_b3, block_cors3)
##colnames(ar3) <- c('scale', 'sig_a', 'sig_b', 'cor')
##samplesPlot(ar3)


## here's propCov that RW_block adapts towards:
block_propCovScale[dim(block_propCovScale)[1],,]
##            [,1]        [,2]
##[1,]  0.00293348 -0.05870338
##[2,] -0.05870338  1.17509114
##(from values of scale):
block_scales[length(block_scales)]
##[1] 1.686427
##(and propCovHistory):
block_propCovHistory[dim(block_propCovHistory)[1],,]
##             [,1]        [,2]
##[1,]  0.001739464 -0.03480932
##[2,] -0.034809317  0.69679328

## empirical covariance between a&b (what RW_block adapts towards)
ind <- 800001:1000000
cov(samples[ind, c('a','b')])
##             a           b
##a  0.001805492 -0.03613168
##b -0.036131683  0.72328234
cov(samples2[ind, c('a','b')])  ## same!
##             a          b
##a  0.001802017 -0.0360598
##b -0.036059804  0.7217960

## here's propCov that RW_block_NEW adapts towards (conditional cov):
block_propCovScale2[dim(block_propCovScale2)[1],,]
##             [,1]        [,2]
##[1,]  0.001016699 -0.02033899
##[2,] -0.020338986  0.40709698
block_scales2[length(block_scales2)]
##[1] 3.65329
##(and propCovHistory):
block_propCovHistory2[dim(block_propCovHistory2)[1],,]
##              [,1]         [,2]
##[1,]  0.0002782968 -0.005567306
##[2,] -0.0055673059  0.111432961


## what we're calling the "conditional covariance" of a&b:
ind <- 800001:1000000
difpairs <- diff(samples[ind,c('a','b')])
(t(difpairs) %*% difpairs) / (2*dim(difpairs)[1])
##             a            b
##a  0.000400004 -0.008005672
##b -0.008005672  0.160272494
difpairs2 <- diff(samples2[ind,c('a','b')])
(t(difpairs2) %*% difpairs2) / (2*dim(difpairs2)[1])
##              a            b
##a  0.0003152922 -0.006307686
##b -0.0063076856  0.126247785


## using these FINAL ADAPTED scale and propCov values, how about ESS??
dim(samples)
dimnames(samples)
ind <- 800001:1000000
## for original RW_block:
ess <- effectiveSize(samples[ind,])
round(ess)
##    a     b sigOE sigPN 
##22954 22969   251 16831 
## for NEW RW_block_NEW (adapting to conditional covariance):
ess2 <- effectiveSize(samples2[ind,])
round(ess2)
##    a     b sigOE sigPN 
##18452 18443   185 15305 
ess3 <- effectiveSize(samples3[ind,])
round(ess3)
##    a     b sigOE sigPN 
##21923 21937   147 17625

##
## conclusion: mixing is WORSE using RW_block_NEW, quite reasonably so.....
## conclusion2: with using accepted proposals only, it isn't quite as bad.


## 
#### final constant that scale approaches:
##block_scales[length(block_scales)]
####propCov adapts very nicely to true covariance between 'a' and 'b'
##cov(samples[(dim(samples)[1]/2):(dim(samples)[1]), c('a','b')])
##block_propCovHistory[dim(ar)[1],,]
#### final adapted (and scaled) proposal corrleation is very accurate:
##cor(samples[(dim(samples)[1]/2):(dim(samples)[1]), c('a','b')])
##cov2cor(block_scales[length(block_scales)] * block_propCovHistory[dim(ar)[1],,])
#### final adapted (and scaled) proposal standard deviations for 'a' and 'b':
##sqrt((block_scales[length(block_scales)] * block_propCovHistory[dim(ar)[1],,])[1,1])
##sqrt((block_scales[length(block_scales)] * block_propCovHistory[dim(ar)[1],,])[2,2])
## 
#### expand cor, sig_a, and sig_b by adaptInterval:
##length(block_scale_a)
##aI <- 200
##block_scale_a_ex <- rep(block_scale_a, each=aI)
##block_scale_b_ex <- rep(block_scale_b, each=aI)
##block_cors_ex    <- rep(block_cors,    each=aI)
##block_scales_ex  <- rep(block_scales,  each=aI)
##samples_block_info <- cbind(samples[,'a'], samples[,'b'], block_scale_a_ex, block_scale_b_ex, block_cors_ex, block_scales_ex)
##dimnames(samples_block_info)[[2]] <- c('a', 'b', 'sig_a', 'sig_b', 'cor', 'scale')
## 
####samplesPlot(samples_block_info, ind=1:300000, var=c('b', 'a'))
####samplesPlot(samples_block_info, ind=1:300000, var=c('sig_a', 'sig_b', 'cor', 'scale'))
## 
#### this one is best:
#### artificially trim 'b' samples:
##samples_block_info_trim <- samples_block_info
##samples_block_info_trim[,'b'] <- pmin(samples_block_info_trim[,'b'], 2)
##samplesPlot(samples_block_info_trim, ind=1:300000, var=c('b', 'a', 'sig_a', 'sig_b', 'cor', 'scale'))
##dev.copy2pdf(file='~/Downloads/plot2.pdf')
## 
#### same thing, on the "early" time scale
##samplesPlot(samples_block_info_trim, ind=1:2000, var=c('b', 'a', 'sig_a', 'sig_b', 'cor', 'scale'), densityplot=FALSE)
##dev.copy2pdf(file='~/Downloads/plot3.pdf')
## 
##samplesPlot(samples_block_info_trim, ind=1:20000, var=c('b', 'a', 'sig_a', 'sig_b', 'cor', 'scale'), densityplot=FALSE)
##dev.copy2pdf(file='~/Downloads/plot4.pdf')





## testing the new adaptive properties for covariance in RW_block sampler,
## not adapting until acceptance rate >= 0.15 at least once
## uses scaleHistory and propCovHistory

library(nimble)
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)

code <- nimbleCode({
    a ~ dnorm(0, sd=100)
    b ~ dnorm(0, sd=100)
    c ~ dnorm(a/2, sd=.1)
    for(i in 1:n1) {
        y1[i] ~ dnorm(a, sd=0.1)
    }
    for(i in 1:n2) {
        y2[i] ~ dnorm(b, sd=0.1)
    }
})
n1 <- 10
n2 <- n1
y1 <- rnorm(n1, 3, 0.1)
y2 <- rnorm(n2, y1+5, 0.01)
constants <- list(n1=n1, n2=n2)
data <- list(y1=y1, y2=y2)
inits <- list(a = 0, b=0, c=0)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel, nodes=NULL)
##conf$addSampler('a', 'RW')
conf$addSampler(c('a', 'b', 'c'), 'RW_block')
conf$printSamplers()
conf$addMonitors(c('a', 'b', 'c'))
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(4000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)

samplesPlot(samples)

Cmcmc$samplerFunctions$contentsList[[1]]$scaleHistory
a <- Cmcmc$samplerFunctions$contentsList[[1]]$propCovHistory
dim(a)
for(i in 1:dim(a)[1]) print(a[i,,])


samplesPlot(samples)

Cmcmc$samplerFunctions$contentsList[[1]]$scaleHistory
a <- Cmcmc$samplerFunctions$contentsList[[1]]$propCovHistory
dim(a)
for(i in 1:dim(a)[1]) print(a[i,,])








## getting model running for Colin Lewis-Beck at Iowa State

## Create Constant W, G and F matrices to pass to NIMBLE
library(nimble)
y1 <- rep(0,2190)
W<-diag(2)
F1<-matrix(c(1,0), nrow = 1, ncol = 2)
G<-matrix(c(cos(2*pi/365),-sin(2*pi/365),sin(2*pi/365),cos(2*pi/365)),nrow=2,ncol=2)
smosCode <- nimbleCode({
    ## Initial Values for States
    c0[1]<-0
    c0[2]<-0
    m0[1:2,1:2]<-sigmaSquaredInvw*W[1:2,1:2]   ## CHANGE 1: add full indexing to m0[...]
    x0[1:2]~dmnorm(c0[1:2],m0[1:2,1:2])        ## CHANGE 2: add full indexing to x0[...]
    ## initial values
    m[1:2,1]<-G[1:2,1:2] %*% x0[1:2]           ## CHANGE 3: add indexing to m[...], inprod() only returns a scalar (not a vector or matrix) so use %*%
    var0[1:2,1:2]<-sigmaSquaredInvw*W[1:2,1:2] ## CHANGE 4: add full indexing to var0[...]
    x[1:2,1]~dmnorm(m[1:2,1],var0[1:2,1:2])
    y[1]~dnorm(inprod(F1[1,1:2],x[1:2,1]),sigmaSquaredInvv)
    ## Model
    for (t in 2:T){
        mu[1:2, t] <- G[1:2,1:2] %*% x[1:2,t-1]     ## CHANGE 5: something was funny with your mu[] declaration.  In addition to not having
                                                    ## the required indexing, it should should be indexed by t
        sigs[1:2,1:2]<-sigmaSquaredInvw*W[1:2,1:2]  ## CHANGE 6: add full indexing to sigs[...]
        x[1:2,t] ~ dmnorm(mu[1:2,t],sigs[1:2,1:2])  ## CHANGE 7: added appropriate 't' indexing to mu[...]
        y[t] ~ dnorm(inprod(F1[1,1:2],x[1:2,t]),sigmaSquaredInvv)
    }
    ## Priors
    sigmaSquaredInvv~dgamma(5,20)
    sigmaSquaredInvw~dgamma(5,200)
})
smosModel<-nimbleModel(code=smosCode,name='2harm',constants=list(T=2190,pi=pi,W=W,G=G,F1=F1),data=list(y=y1), inits=list(sigmaSquaredInvv=1,sigmaSquaredInvw=1))
Cmodel <- compileNimble(smosModel)




## doing the Surgeries example for STAT 365 homework





## spatial NIMBLE model example from Abhirup Datta
library(nimble)
library(mvtnorm)
nimbleOptions(showCompilerOutput=TRUE)

## model code ###
gpCode <- nimbleCode({
    for(i in 1:n){ y[i] ~ dnorm(w[i],sd=tau)}
    Vw[,] <- sigma*chol(G[,])
    w[] ~ dmnorm(muw[], cholesky=Vw[,], prec_param=0)
    sigma ~ dunif(0, 100)
    tau ~ dunif(0, 10)
})

## data ###
set.seed(1)
n=100
sigs=4
w=as.vector(rmvnorm(1,rep(0,n),sigs*0.5^as.matrix(dist(1:n))))
Y=w+sqrt(0.5*sigs)*rnorm(n)

### setting up nimble model ####
gpModel <- nimbleModel(gpCode, constants = list(n = n, G=0.5^as.matrix(dist(1:n)), muw=rep(0,n)), 
    dimensions=list(y=n, Vw=c(n, n), w=n, Vy=c(n, n), G = c(n, n)), check=FALSE)

gpModel$setData(list(y = Y))

gpModel$sigma <- 1
gpModel$tau <- 0.1
gpModel$w <- rep(0,n)

CgpModel <- compileNimble(gpModel)

### MCMC ####

gpconf <- configureMCMC(CgpModel, print=TRUE)


gpconf$printSamplers()
gpconf$removeSamplers('w')
gpconf$printSamplers()
for(node in gpModel$expandNodeNames('w', returnScalarComponents=TRUE)) {
    gpconf$addSampler(node, 'RW')
}
gpconf$printSamplers()



gpconf$addMonitors(c('w'))	

gpMCMC <- buildMCMC(gpconf)

CgpMCMC <- compileNimble(gpMCMC, project = CgpModel)

##Nmcmc=100000
Nmcmc=1000
set.seed(1)
CgpMCMC$run(Nmcmc)

gpMCMCsamples <- as.matrix(CgpMCMC$mvSamples)

plot(gpMCMCsamples[,"w[1]"])

dimnames(gpMCMCsamples)

samplesPlot(gpMCMCsamples, var=3:10)


## implementing RJMCMC for a simple 2 models, logistic regression
## yi ~ bernoulli(logit(p) = a + bx)
set.seed(0)
n <- 50
x <- runif(n, -10, 10)
a <- .3
b <- 0.1
p <- 1/(1 + exp(-1*(a + b*x)))
y <- rbinom(n, prob=p, size=1)
##plot(x,y)
m1 <- glm(y~1, family=binomial())
m1
m2 <- glm(y~x, family=binomial())
m2
aic1 <- AIC(m1)  ## for this AIC function, "lower values indicate better fit"
aic2 <- AIC(m2)
aics <- c(aic1, aic2)
aic_min <- min(aics)
daics <- aics - aic_min
w1_aic <- exp(-daics[1]/2) / sum(exp(-daics/2))
w2_aic <- exp(-daics[2]/2) / sum(exp(-daics/2))
bic1 <- BIC(m1)  ## for this AIC function, "lower values indicate better fit"
bic2 <- BIC(m2)
bics <- c(bic1, bic2)
bic_min <- min(bics)
dbics <- bics - bic_min
w1_bic <- exp(-dbics[1]/2) / sum(exp(-dbics/2))
w2_bic <- exp(-dbics[2]/2) / sum(exp(-dbics/2))
w1_aic
w1_bic

## NEED TO KEEP WORKING ON LOGISTIC EXAMPLE FROM HERE
prior_sd <- 10
rj_proposal_sd <- 10
prior_m1 <- 0.5
{
    decide <- function(lMHR) {
        if(is.nan(lMHR)) return(FALSE)
        if(log(runif(1,0,1)) < lMHR) return(TRUE) else return(FALSE)
    }
    prior <- function(x) return(dnorm(x, 0, sd=prior_sd, log=TRUE))
    ll <- function(m, a, b, y, x) {
        if(m==1) return(sum(dnorm(y, a, 1, log=TRUE)))
        if(m==2) return(sum(dnorm(y, a+b*x, 1, log=TRUE)))
    }
    updateM <- function(mab, y, x) {
        ##browser()
        if(mab[1] == 1) {
            bprop <- rnorm(1, 0, rj_proposal_sd)   ## proposal for new slope 'b'
            lMHR <- prior(bprop) + ll(2,mab[2],bprop,y,x)  -  (ll(1,mab[2],0,y,x) + dnorm(bprop,0,rj_proposal_sd,log=TRUE)) + log((1-prior_m1)/prior_m1)
            if(decide(lMHR)) return(c(2,mab[2],bprop)) else return(mab)
        }
        if(mab[1] == 2) {
            lMHR <- ll(1,mab[2],0,y,x) + dnorm(mab[3],0,rj_proposal_sd,log=TRUE)  -  (prior(mab[3]) + ll(2,mab[2],mab[3],y,x)) - log((1-prior_m1)/prior_m1)
            if(decide(lMHR)) return(c(1,mab[2],0)) else return(mab)
        }
    }
    updateAB <- function(mab, y, x) {
        ## update a
        sd.aprop <- 0.1
        aprop <- rnorm(1,mab[2],sd.aprop)
        lMHR <- prior(aprop) + ll(mab[1],aprop,mab[3],y,x) - prior(mab[2]) - ll(mab[1],mab[2],mab[3],y,x)
        if(decide(lMHR)) mab <- c(mab[1],aprop,mab[3])
        ## update b
        if(mab[1] == 2) {
            sd.bprop <- 0.1
            bprop <- rnorm(1,mab[3],sd.bprop)
            lMHR <- prior(bprop) + ll(mab[1],mab[2],bprop,y,x) - prior(mab[3]) - ll(mab[1],mab[2],mab[3],y,x)
            if(decide(lMHR)) mab <- c(mab[1],mab[2],bprop) else mab
        }
        return(mab)
    }
}

set.seed(0)
iter <- 100000
mab <- c(1, 0, 0)  ## c(1, 0, 0)
##samp <- data.frame(a=rep(NA,iter), b=NA, c=NA)
samp <- array(NA, c(iter,3))
colnames(samp) <- c('m', 'a', 'b')
for(i in 1:iter) {
    mab <- updateM(mab, y, x)
    mab <- updateAB(mab, y, x)
    samp[i,] <- mab
}
df <- as.data.frame(samp)

mean(df$m==1)
##head(df,10)

c(mean(df$a[df$m==1]), m1$coef[1])
c(mean(df$a[df$m==2]), m2$coef[1])
c(mean(df$b[df$m==2]), m2$coef[2])

##plot(df$a, type='l')
##plot(1:iter, df$b)

prod(dnorm(y, 0, sd=sqrt(prior_sd^2 + 1^2)/10))   ## wrong for p(y|m1), don't know why

pym1 <- mean(replicate(500000, prod(dnorm(y, rnorm(1,0,prior_sd), 1))))
pym2 <- mean(replicate(500000, prod(dnorm(y, rnorm(1,0,prior_sd) + rnorm(1,0,prior_sd)*x, 1))))
pym1
pym2
ppm1 <- prior_m1*pym1 / (prior_m1*pym1 + (1-prior_m1)*pym2)
ppm2 <- (1-prior_m1)*pym2 / (prior_m1*pym1 + (1-prior_m1)*pym2)
ppm1
ppm2

w1_aic
w1_bic
mean(df$m==1)
ppm1

library(BMA)
bma <- bic.glm(x=data.frame(x=x), y=y, glm.family='gaussian')
##class(bma)
##str(bma)
bma$postprob[1]
bma$deviance
bma$label
bma$size
bma$which
bma$probne0
bma$postmean
bma$condpostmean
bma$mle





## implementing RJMCMC for a simple 2 models
## yi ~ N(a + bx, 1)
set.seed(0)
n <- 10
x <- runif(n, -1, 1)
a <- .3
b <- 0.1
y <- rnorm(n, a+b*x, 1)
##plot(x,y)
m1 <- lm(y~1)
m1
m2 <- lm(y~x)
m2
aic1 <- AIC(m1)  ## for this AIC function, "lower values indicate better fit"
aic2 <- AIC(m2)
aics <- c(aic1, aic2)
aic_min <- min(aics)
daics <- aics - aic_min
w1_aic <- exp(-daics[1]/2) / sum(exp(-daics/2))
w2_aic <- exp(-daics[2]/2) / sum(exp(-daics/2))
bic1 <- BIC(m1)  ## for this AIC function, "lower values indicate better fit"
bic2 <- BIC(m2)
bics <- c(bic1, bic2)
bic_min <- min(bics)
dbics <- bics - bic_min
w1_bic <- exp(-dbics[1]/2) / sum(exp(-dbics/2))
w2_bic <- exp(-dbics[2]/2) / sum(exp(-dbics/2))
w1_aic
w1_bic

prior_sd <- 10
rj_proposal_sd <- 10
prior_m1 <- 0.5
{
    decide <- function(lMHR) {
        if(is.nan(lMHR)) return(FALSE)
        if(log(runif(1,0,1)) < lMHR) return(TRUE) else return(FALSE)
    }
    prior <- function(x) return(dnorm(x, 0, sd=prior_sd, log=TRUE))
    ll <- function(m, a, b, y, x) {
        if(m==1) return(sum(dnorm(y, a, 1, log=TRUE)))
        if(m==2) return(sum(dnorm(y, a+b*x, 1, log=TRUE)))
    }
    updateM <- function(mab, y, x) {
        ##browser()
        if(mab[1] == 1) {
            bprop <- rnorm(1, 0, rj_proposal_sd)   ## proposal for new slope 'b'
            lMHR <- prior(bprop) + ll(2,mab[2],bprop,y,x)  -  (ll(1,mab[2],0,y,x) + dnorm(bprop,0,rj_proposal_sd,log=TRUE)) + log((1-prior_m1)/prior_m1)
            if(decide(lMHR)) return(c(2,mab[2],bprop)) else return(mab)
        }
        if(mab[1] == 2) {
            lMHR <- ll(1,mab[2],0,y,x) + dnorm(mab[3],0,rj_proposal_sd,log=TRUE)  -  (prior(mab[3]) + ll(2,mab[2],mab[3],y,x)) - log((1-prior_m1)/prior_m1)
            if(decide(lMHR)) return(c(1,mab[2],0)) else return(mab)
        }
    }
    updateAB <- function(mab, y, x) {
        ## update a
        sd.aprop <- 0.1
        aprop <- rnorm(1,mab[2],sd.aprop)
        lMHR <- prior(aprop) + ll(mab[1],aprop,mab[3],y,x) - prior(mab[2]) - ll(mab[1],mab[2],mab[3],y,x)
        if(decide(lMHR)) mab <- c(mab[1],aprop,mab[3])
        ## update b
        if(mab[1] == 2) {
            sd.bprop <- 0.1
            bprop <- rnorm(1,mab[3],sd.bprop)
            lMHR <- prior(bprop) + ll(mab[1],mab[2],bprop,y,x) - prior(mab[3]) - ll(mab[1],mab[2],mab[3],y,x)
            if(decide(lMHR)) mab <- c(mab[1],mab[2],bprop) else mab
        }
        return(mab)
    }
}

set.seed(0)
iter <- 100000
mab <- c(1, 0, 0)  ## c(1, 0, 0)
##samp <- data.frame(a=rep(NA,iter), b=NA, c=NA)
samp <- array(NA, c(iter,3))
colnames(samp) <- c('m', 'a', 'b')
for(i in 1:iter) {
    mab <- updateM(mab, y, x)
    mab <- updateAB(mab, y, x)
    samp[i,] <- mab
}
df <- as.data.frame(samp)

mean(df$m==1)
##head(df,10)

c(mean(df$a[df$m==1]), m1$coef[1])
c(mean(df$a[df$m==2]), m2$coef[1])
c(mean(df$b[df$m==2]), m2$coef[2])

##plot(df$a, type='l')
##plot(1:iter, df$b)

prod(dnorm(y, 0, sd=sqrt(prior_sd^2 + 1^2)/10))   ## wrong for p(y|m1), don't know why

pym1 <- mean(replicate(500000, prod(dnorm(y, rnorm(1,0,prior_sd), 1))))
pym2 <- mean(replicate(500000, prod(dnorm(y, rnorm(1,0,prior_sd) + rnorm(1,0,prior_sd)*x, 1))))
pym1
pym2
ppm1 <- prior_m1*pym1 / (prior_m1*pym1 + (1-prior_m1)*pym2)
ppm2 <- (1-prior_m1)*pym2 / (prior_m1*pym1 + (1-prior_m1)*pym2)
ppm1
ppm2

w1_aic
w1_bic
mean(df$m==1)
ppm1

library(BMA)
bma <- bic.glm(x=data.frame(x=x), y=y, glm.family='gaussian')
##class(bma)
##str(bma)
bma$postprob[1]
bma$deviance
bma$label
bma$size
bma$which
bma$probne0
bma$postmean
bma$condpostmean
bma$mle





## testing samplers used by JAGS for multivariate normal (dmnorm) nodes
## also, modified code in JAGS MNormal.cc (block sampler)
## to see what's going on

## (see http://danielturek.github.io/public/jags_block_adaptation/jags_block_adaptation.html)
## to build jags from source:
##   $ cd ~/Downloads/JAGS-4.2.0
##   $ ./configure; make -j 4; sudo make install
## then restart R

##remove.packages('rjags')
##install.packages('rjags')
library(rjags)
code <- quote({
    C[1,1] <- 1
    C[1,2] <- 0
    C[2,1] <- 0
    C[2,2] <- 1
    mu[1] <- 0
    mu[2] <- 0
    y[1:2] ~ dmnorm(mu[1:2], C[1:2, 1:2])
    d[1] <- exp(y[1]) + 4 * pow(y[1], 2)
    d[2] <- exp(y[2]) + 10 * pow(y[1], 2) + 3
    dat1 ~ dexp(d[1])
    dat2 ~ dexp(d[2])
})
constants <- list()
data <- list(dat1=3, dat2=6)
inits <- list(y = c(0,0))
##Rmodel <- nimbleModel(code, constants, data, inits)
##conf <- configureMCMC(Rmodel)
##conf$printSamplers()
niter <- 100
monitorVars <- c('y')
constsAndData <- c(constants, data)
modelfile <- file.path(tempdir(), 'model.txt')
writeLines(paste0('model\n', paste0(deparse(code, width.cutoff=500L), collapse='\n')), con=modelfile)
set.seed(0); jags_mod <- jags.model(file=modelfile, data=constsAndData, inits=inits, n.chains=1, quiet=FALSE, n.adapt=40)

set.seed(0); jags_out <- rjags::coda.samples(model=jags_mod, variable.names=monitorVars, n.iter=niter, thin=1)

list.samplers(jags_mod)



## doing the midterm question about normal mean hypothesis test
## "weights of sheep raised on a farm"
## now also with predictive distribution

library(nimble)

y <- c(78, 81, 77, 76, 75, 74, 78, 75, 77, 75)
n <- length(y)
np <- 5

code <- nimbleCode({
    mu ~ dnorm(75, sd=10)
    for(i in 1:n) {
        y[i] ~ dnorm(mu, sd=3)
    }
    for(i in 1:np) {
        p[i] ~ dnorm(mu, sd=3)
    }
    pmean <- mean(p[1:np])
})
constants <- list(n=n, np=np)
data <- list(y=y)
inits <- list(mu = 75, p = rep(0,np))

Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$addMonitors(c('mu', 'p', 'pmean'))
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel, resetFunctions=TRUE)

set.seed(0)
Cmcmc$run(1000000)
samples <- as.matrix(Cmcmc$mvSamples)
colnames(samples)
dim(samples)
samples[1:20,]
apply(samples, 2, mean)
apply(samples, 2, sd)
apply(samples, 2, var)
1/apply(samples, 2, var)

tau0 <- 1/100
mu0 <- 75
ybar <- mean(y)
tau <- 1/9
tau_n <- tau0 + n*tau
mu_n <- (mu0*tau0 + ybar*n*tau)/(tau0+n*tau)
mu_n
tau_n
1/tau_n
var(samples[,'mu'])

v <- 1/tau
v_n <- 1/tau_n
v_p1 <- v_n + v
v_p1
apply(samples, 2, var)

v_p1/5
var(samples[,'pmean'])
v_n/5 + v
v_n + v/5

var(apply(matrix(sample(as.numeric(samples[,2:6])), ncol=5), 1, mean))




## working on used cars regression example for Bayes course 365
df <- read.csv('~/Downloads/UsedCars.csv')
str(df)
hist(df$Age)
hist(df$HP)
dim(df)
head(df)

m <- lm(Price ~ Age + HP | Type, df=df)
summary(m)

summary(lm(Price ~ Age + HP, df=subset(df, Type==0)))
summary(lm(Price ~ Age + HP, df=subset(df, Type==1)))
summary(lm(Price ~ Age + HP + Type, df=df))


sd(residuals(lm(Price ~ Age + HP, df=subset(df, Type==0))))
sd(residuals(lm(Price ~ Age + HP, df=subset(df, Type==1))))




## deep-dive deep dive into correlated state-space state space model
## to figure out why the block sampler is taking so long to adapt.
## recreating and fixing Dao's issue with the correlated SSM,
## where 'a' and 'b' don't mix until after 150,000 iterations
library(nimble)
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
##
code <- nimbleCode({
    a ~ dunif(-0.9999, 0.9999)
    b ~ dnorm(0, sd = 1000)
    sigPN ~ dunif(1e-04, 1)
    sigOE ~ dunif(1e-04, 1)
    x[1] ~ dnorm(b/(1 - a), sd = sqrt(sigPN^2 + sigOE^2))
    y[1] ~ dnorm(x[1], sd = sigOE)
    for (i in 2:t) {
        x[i] ~ dnorm(x[i - 1] * a + b, sd = sigPN)
        y[i] ~ dnorm(x[i], sd = sigOE)
    }
})
constants <- list(t = 100)
data <- list(y = c(20.24405,20.57693,20.49357,20.34159,20.45759,20.43326,20.20554,20.12860,20.14756,20.20781,20.23022,20.26766,20.22984,20.37703,20.13641,20.05309,19.95709,20.19303,20.30562,20.54443,20.91010,20.70580,20.42344,20.19795,20.28816,20.31894,20.76939,20.77023,20.83486,20.29335,20.40990,20.19601,20.04083,19.76056,19.80810,19.83129,19.69174,19.90069,19.87623,19.63371,19.62360,19.72630,19.64450,19.86779,20.17104,20.34797,20.32968,20.48027,20.46694,20.47006,20.51676,20.40695,20.18715,19.97552,19.88331,19.67831,19.74702,19.47502,19.24408,19.37179,19.38277,19.15034,19.08723,19.37051,19.14274,19.46433,19.62459,19.77971,19.54194,19.39081,19.61621,19.51307,19.34745,19.17019,19.26829,19.58943,19.77143,19.83582,19.71198,19.67746,19.75053,20.40197,20.49363,20.37079,20.19005,20.55862,20.48523,20.33071,19.97069,19.79758,19.83811,19.79728,19.86277,19.86836,19.92481,19.88095,20.24899,20.55165,20.22707,20.11235))
inits <- list(a = 0.95, b=1, sigPN = 0.2, sigOE=0.05, x = c(20.26036,20.51331,20.57057,20.35633,20.33736,20.47321,20.22002,20.14917,20.19216,20.26969,20.21135,20.22745,20.20466,20.41158,20.13408,20.08023,19.98956,20.13543,20.32709,20.55840,20.88206,20.74740,20.47671,20.14012,20.29953,20.33778,20.80916,20.75773,20.84349,20.35654,20.41045,20.20180,20.02872,19.74226,19.80483,19.81842,19.69770,19.84564,19.88211,19.70559,19.56090,19.73728,19.66545,19.88158,20.13870,20.39163,20.37372,20.47429,20.39414,20.42024,20.55560,20.40462,20.15831,19.89425,19.79939,19.72692,19.74565,19.42233,19.22730,19.36489,19.37289,19.19050,19.00823,19.35738,19.14293,19.48812,19.67329,19.82750,19.58979,19.43634,19.61278,19.56739,19.38584,19.19260,19.32732,19.65500,19.65295,19.84843,19.68285,19.69620,19.77497,20.31795,20.45797,20.32650,20.24045,20.60507,20.51597,20.30076,19.98100,19.86709,19.85965,19.74822,19.86730,19.90523,19.86970,19.87286,20.28417,20.46212,20.22618,20.13689))
##
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()   ## [1] 183.3436
##

conf <- configureMCMC(Rmodel, nodes = NULL)

conf$addSampler(c('a', 'b'), 'RW_block')
##conf$addSampler(c('a', 'b'), 'RW_block', control=list(adaptInterval=100))
##conf$addSampler(c('a', 'b'), 'RW_block', control=list(propCov=array(c(1,-.99,-0.99,1), c(2,2))))
##conf$addSampler(c('a', 'b'), 'RW_block', control=list(propCov=array(c(1,-.99,-0.99,1), c(2,2)), scale=0.01))
##conf$addSampler(c('a', 'b'), 'RW_block', control=list(propCov=array(c(0.001709168, -0.0341986, -0.0341986, 0.6844844), c(2,2))))


conf$printSamplers(c('a','b'))

conf$addSampler('sigOE', 'RW')
conf$addSampler('sigPN', 'RW')
for(node in Rmodel$expandNodeNames('x'))
    conf$addSampler(node, 'RW')
conf$resetMonitors()
conf$addMonitors(c('a', 'b', 'sigOE', 'sigPN'))
##conf$getMonitors()
##
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmodel$calculate()   ## [1] 183.3436
nimbleOptions(showCompilerOutput = TRUE)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##
niter <- 500000
system.time(Cmcmc$run(niter))
##
samples <- as.matrix(Cmcmc$mvSamples)
dim(samples)
dimnames(samples)
##

## Plot1
dev.new(width=8, height=5)
par(mfrow=c(1,2))
plot(samples[1:300000,'a'], type='l', ylab='a')
plot(samples[1:300000,'b'], type='l', ylab='b')
par(mfrow=c(1,1))
getwd()
dev.copy2pdf(file='~/Downloads/plot1.pdf')

samplesPlot(samples, var=c('b','a'), ind=1:300000)

##samplesPlot(samples, var=c('sigOE','sigPN'))

##xs <- Rmodel$expandNodeNames('x')
##samplesPlot(samples, var=xs[ 1:10])
##samplesPlot(samples, var=xs[11:20])
##samplesPlot(samples, var=xs[21:30])
##samplesPlot(samples, var=xs[31:40])

##i <-  4:13   ## x[ 1]...x[10]
##i <- 14:23   ## x[11]...x[20]
##i <-  1:8    ## (a,b), sigOE, sigPN, x[1]...x[5]
##i <-  2:3    ## sigOE, sigPN

## all the latent state scales follow sigOE scale
##i <-  c(2,4:8)    ## sigOE, x[1]...x[5]
##nms <- sapply(conf$samplerConfs[i], function(x) x$target)
##nms
##ar <- do.call(cbind, lapply(Cmcmc$samplerFunctions$contentsList[i], function(x) x$scaleHistory))
##colnames(ar) <- nms
##samplesPlot(ar)
##dim(ar)
##samplesPlot(ar, burnin=500)

block_scales <- Cmcmc$samplerFunctions$contentsList[[1]]$scaleHistory
block_propCovHistory <- Cmcmc$samplerFunctions$contentsList[[1]]$propCovHistory
## create block_propCovScale
block_propCovScale <- block_propCovHistory
for(i in 1:length(block_scales))   block_propCovScale[i,,] <- block_scales[i] * block_propCovHistory[i,,]
##dim(block_propCovScale)
block_scale_a <- apply(block_propCovScale, 1, function(x) sqrt(x[1,1]))
block_scale_b <- apply(block_propCovScale, 1, function(x) sqrt(x[2,2]))
block_cors <- apply(block_propCovHistory, 1, function(x) cov2cor(x)[1,2])
ar <- cbind(block_scales, block_scale_a, block_scale_b, block_cors)
colnames(ar) <- c('scale', 'sig_a', 'sig_b', 'cor')
samplesPlot(ar)

## final constant that scale approaches:
block_scales[length(block_scales)]
##propCov adapts very nicely to true covariance between 'a' and 'b'
cov(samples[(dim(samples)[1]/2):(dim(samples)[1]), c('a','b')])
block_propCovHistory[dim(ar)[1],,]
## final adapted (and scaled) proposal corrleation is very accurate:
cor(samples[(dim(samples)[1]/2):(dim(samples)[1]), c('a','b')])
cov2cor(block_scales[length(block_scales)] * block_propCovHistory[dim(ar)[1],,])
## final adapted (and scaled) proposal standard deviations for 'a' and 'b':
sqrt((block_scales[length(block_scales)] * block_propCovHistory[dim(ar)[1],,])[1,1])
sqrt((block_scales[length(block_scales)] * block_propCovHistory[dim(ar)[1],,])[2,2])

## expand cor, sig_a, and sig_b by adaptInterval:
length(block_scale_a)
aI <- 200
block_scale_a_ex <- rep(block_scale_a, each=aI)
block_scale_b_ex <- rep(block_scale_b, each=aI)
block_cors_ex    <- rep(block_cors,    each=aI)
block_scales_ex  <- rep(block_scales,  each=aI)
samples_block_info <- cbind(samples[,'a'], samples[,'b'], block_scale_a_ex, block_scale_b_ex, block_cors_ex, block_scales_ex)
dimnames(samples_block_info)[[2]] <- c('a', 'b', 'sig_a', 'sig_b', 'cor', 'scale')

##samplesPlot(samples_block_info, ind=1:300000, var=c('b', 'a'))
##samplesPlot(samples_block_info, ind=1:300000, var=c('sig_a', 'sig_b', 'cor', 'scale'))

## this one is best:
## artificially trim 'b' samples:
samples_block_info_trim <- samples_block_info
samples_block_info_trim[,'b'] <- pmin(samples_block_info_trim[,'b'], 2)
samplesPlot(samples_block_info_trim, ind=1:300000, var=c('b', 'a', 'sig_a', 'sig_b', 'cor', 'scale'))
dev.copy2pdf(file='~/Downloads/plot2.pdf')

## same thing, on the "early" time scale
samplesPlot(samples_block_info_trim, ind=1:2000, var=c('b', 'a', 'sig_a', 'sig_b', 'cor', 'scale'), densityplot=FALSE)
dev.copy2pdf(file='~/Downloads/plot3.pdf')

samplesPlot(samples_block_info_trim, ind=1:20000, var=c('b', 'a', 'sig_a', 'sig_b', 'cor', 'scale'), densityplot=FALSE)
dev.copy2pdf(file='~/Downloads/plot4.pdf')







## testing addition of scaleHistory and propCovHistory
## back into RW and RW_block samplers


library(nimble)
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0, b=1)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel, nodes=NULL)
conf$addSampler('a', 'RW')
conf$addSampler(c('a', 'b'), 'RW_block')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)

Cmcmc$samplerFunctions$contentsList[[1]]$scaleHistory
Cmcmc$samplerFunctions$contentsList[[2]]$scaleHistory
a <- Cmcmc$samplerFunctions$contentsList[[2]]$propCovHistory
dim(a)
for(i in 1:dim(a)[1]) print(a[i,,])

set.seed(0)
Cmcmc$run(1000)

Cmcmc$samplerFunctions$contentsList[[1]]$scaleHistory
Cmcmc$samplerFunctions$contentsList[[2]]$scaleHistory
Cmcmc$samplerFunctions$contentsList[[2]]$propCovHistory
a <- Cmcmc$samplerFunctions$contentsList[[2]]$propCovHistory
dim(a)
for(i in 1:dim(a)[1]) print(a[i,,])







## playing with colorRamp and colorRampPalette
## to get gradients of colors in plots

FLcrime <- read.csv("~/github/courses/stat201/data/fl_crime.csv")
head(FLcrime[, 1:4])
FLcrime <- FLcrime[, 2:4]
colnames(FLcrime) <- c("Crime", "Education", "Urbanization")
head(FLcrime)
pairs(FLcrime, panel=panel.smooth)
pairs(FLcrime)
plot(FLcrime$Education, FLcrime$Crime)
hist(FLcrime$Urbanization)
boxplot(FLcrime$Urbanization)
quantile(FLcrime$Urbanization)

plot(FLcrime$Education, FLcrime$Crime, pch=19)
with(subset(FLcrime, Urbanization<20), points(Education, Crime, col='green', pch=19))
with(subset(FLcrime, Urbanization>80), points(Education, Crime, col='red', pch=19))


pal <- colorRampPalette(c('blue', 'red'))
cols <- pal(nrow(FLcrime))[cut(FLcrime$Urbanization, nrow(FLcrime))]
plot(FLcrime$Education, FLcrime$Crime, col=cols, pch=19)
with(subset(FLcrime, Urbanization<20), points(Education, Crime, col='green', pch=19))
with(subset(FLcrime, Urbanization>80), points(Education, Crime, col='red', pch=19))


x <- runif(100)
dat <- data.frame(x = x,y = x^2 + 1)

?colorRampPalette
rbPal <- colorRampPalette(c('red','blue'))

cc <- rbPal(100)[cut(dat$x, 100)]
plot(dat$x, dat$y, col = cc)

cc <- colorRampPalette(1:2)
plot(1:10, 1:10, col=cc(1:10/10))


plot(FLcrime$Education, FLcrime$Crime)

## practice problem 6 for STAT 365

## likelihood: yi ~ Normal(mu, sigma)
n <- 4
y <- c(100, 110, 112, 118)
sigma <- 10

## prior: mu ~ Normal(mu0, sigma0)
mu0 <- 100
sigma0 <- 10

## derive precisions
tau0 <- sigma0^-2
tau <- sigma^-2

## posterior distribution
tau_n <- tau0 + n*tau
mu_n <- (tau0*mu0 + n*tau*mean(y)) / tau_n

sigma_n <- 1/sqrt(tau_n)

mu_n
tau_n
sigma_n

## prior, likelihood, posterior plots

## 95% BCI

## one-sided hypothesis test
## H0: mu|y <= 100
## Ha: mu|y >  100

## two-sided hypothesis test
## H0: mu|y  = 100
## Ha: mu|y != 100

## samples from posterior

## 95% BCI

## one-sided hypothesis test
## H0: mu|y <= 100
## Ha: mu|y >  100


## two-sided hypothesis test
## H0: mu|y  = 100
## Ha: mu|y != 100


.





file <- 'HurricaneDamage.csv'
data <- read.csv(paste0('~/github/courses/stat201/data/', file))
Year <- data$Year
Damage <- data$Damage
Year2 <- Year[-1]
Damage2 <- Damage[-1]
m2 <- lm(Damage2~Year2)

res <- residuals(m2)

par(mfrow=c(1,1))
hist(res, breaks=12)

par(mfrow=c(2,1))
plot(Year2, res)
plot(fitted(m2), res)






## modifying hurricanes.csv data file into Hurricane_Damage.csv,
## for use in the STAT201 miderm
file <- 'hurricanes.csv'
data <- read.csv(paste0('~/Downloads/', file))
str(data)
names(data)
names(data)[1] <- 'Rank'
names(data)[2] <- 'Tropical.Cyclone'
names(data)[3] <- 'Year'
names(data)
dim(data)
data <- data[-(2:5),]
dim(data)
mtemp <- lm(data$Damage ~ data$Year)
coef(mtemp)
mtemp <- lm(data$Damage[-1] ~ data$Year[-1])
coef(mtemp)
fitted(mtemp)
newY <- c(data$Damage[1], 0.4*data$Damage[-1] + 0.6*fitted(mtemp))
data$Damage <- newY
write.csv(data, file='~/Downloads/HurricaneDamage.csv', row.names=FALSE)


file <- 'HurricaneDamage.csv'
data <- read.csv(paste0('~/Downloads/', file))
str(data)
x <- data$Year
y <- data$Damage
m <- lm(y~x)
summary(m)
plot(x, y)
abline(m, col='red')
coef(m)

sort(y)
which(y>40000)
i <- which(y>40000)
x2 <- x[-i]
y2 <- y[-i]
m2 <- lm(y2~x2)
summary(m2)
plot(x2, y2)
abline(m2, col='red')
coef(m2)
cor(x2,y2)
cor(x2,y2)^2





## doing LAX flight departures problem
## from the STAT201 midterm

x <- 3648
y <- 25843
w <- 3407
z <- 26134
N <- x+y+w+z

year <- c(rep(2014, x+y), rep(2015, w+z))
delay <- c(rep('ayes',x), rep('no',y), rep('ayes',w), rep('no',z))
tab <- table(year, delay)
tab

## (a) display contingency table
rbind(cbind(tab, margin.table(tab,1)), c(margin.table(tab,2), N))
##     ayes    no      
##2014 3648 25843 29491
##2015 3183 26284 29467
##     6831 52127 58958

## (b) percentage in 2014?
(x+y)/N * 100
prop.table(margin.table(tab, 1))[1]
## 50.02035 %

## (c) percentage of delayed departures in 2015?
w/(w+x) * 100
## 46.5964 %

## (d) diff. in prop, delays in 2015 relative to 2014?
ptab <- prop.table(tab, 1)
ptab[2,1] - ptab[1,1]
## -0.01567962 = -1.567962 %

## (e) interpret this diff. in prop.
## The fraction of LAX December 2015 departures that were delayed was 1.57 *percentage points* lower than the fraction of LAX December 2016 departures that were delayed.

iter <- 1000
data <- data.frame(year=year, delay=delay)
nr <- nrow(data)
ratio <- numeric(iter)
for(i in 1:iter) {
    ind <- sample(1:nr)
    tab <- table(data$year, data$delay[ind])
    cond.tab <- prop.table(tab, 1)
    ratio[i] <- cond.tab[2,1] - cond.tab[1,1]
}

hist(ratio, breaks=15)
tab <- table(data$year, data$delay)
cond.tab <- prop.table(tab, 1)
observed_ratio <- cond.tab[2,1] - cond.tab[1,1]
abline(v = observed_ratio, col = 'red', lwd = 2)







## testing seq_along in NIMBLE run code
library(nimble)

nfDef <- nimbleFunction(
    setup = function() {
        a <- 1:10
    },
    run = function() {
        for(i in seq_along(a)) {
            print(i)
        }
    }
)

Rnf <- nfDef()

Rnf$run()

Cnf <- compileNimble(Rnf)

Cnf$run()




## trying out MCMC with two Gibbs samplers for Normal

y <- c(4.3, 2.5, 3.2, 3.8, 2.9, 3.1, 4.2, 4.0)
n <- length(y)
n
y
mean(y)
sd(y)

## mu ~ dnorm(mu0=0, tau0=0.001)
## tau ~ dgamma(r0=0.001, v0=0.001)
## yi ~ dnorm(mu, tau)

mu0 <- 0
tau0 <- 0.001
r0 <- 0.001
v0 <- 0.001

## iter sampling iterations to run
iter <- 100000
samp <- cbind(mu = rep(NA,iter), tau = rep(NA,iter))

## inits: mu=0, tau=1
mu <- 0
tau <- 1

set.seed(0)
for(i in 1:iter) {
    mu <- rnorm(1, (tau0*mu0+tau*sum(y))/(tau0+n*tau), (tau0+n*tau)^-0.5)
    tau <- rgamma(1, r0+n/2, v0+0.5*sum((y-mu)^2))
    samp[i, 1] <- mu
    samp[i, 2] <- tau
}


1/2 * sum((y-mu)^2)
n/2 * (mean(y)-mu)^2


head(samp)

samp <- cbind(samp, sd = samp[,'tau']^-0.5)

apply(samp, 2, mean)
apply(samp, 2, median)
mean(y)
sd(y)
sd(y)^-2

samplesPlot(samp)

library(nimble)

code <- nimbleCode({
    mu ~ dnorm(0, 0.001)
    tau ~ dgamma(0.001, 0.001)
    for(i in 1:n) {
        y[i] ~ dnorm(mu, tau)
    }
})
constants <- list(n=n)
data <- list(y=y)
inits <- list(mu=0, tau=1)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)

set.seed(0)
Cmcmc$run(10)

Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)

head(Rsamples, 10)
head(Csamples, 10)
head(samp, 10)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)




## fixing bug in conjugacy

library(nimble)
set.seed(0)
n <- 10
##y <- c(rnorm(n,0,sd=1), rnorm(n,10,sd=1))
y <- c(rnorm(n,0,sd=1), rnorm(n,10,sd=2), rnorm(n,20,sd=3))

code <- nimbleCode({
    for(i in 1:3) {
        ##sig[i] ~ dunif(0, 100)
        tau[i] ~ dgamma(0.001, 0.001)   ## using TAU
    }
    mu[1] ~ dnorm(0, sd=1000)
    mu[2] <- mu[1] + delta1
    delta1 ~ T(dnorm(0, sd=1000), 0, 1000)
    mu[3] <- mu[2] + delta2
    delta2 ~ T(dnorm(0, sd=1000), 0, 1000)
    for(i in 1:N) {
        ##means[i] <- equals(z[i],1)*mu[1] + equals(z[i],2)*mu[2]
        means[i] <- equals(z[i],1)*mu[1] + equals(z[i],2)*mu[2] + equals(z[i],3)*mu[3]
        ##sigmas[i] <- equals(z[i],1)*sig[1] + equals(z[i],2)*sig[2] + equals(z[i],3)*sig[3]
        taus[i] <- equals(z[i],1)*tau[1] + equals(z[i],2)*tau[2] + equals(z[i],3)*tau[3]   ## using TAU
        ##y[i] ~ dnorm(means[i], sd=sigmas[i])
        y[i] ~ dnorm(means[i], taus[i])   ## using TAU
        z[i] ~ dcat(pi[1:3])
    }
    for(i in 1:3) {
        pi0[i] ~ dgamma(1, 1)
        pi[i] <- pi0[i] / (pi0[1] + pi0[2] + pi0[3])
    }
})

N <- length(y)
constants <- list(N=N)
data <- list(y=y)
##inits <- list(mu=c(1,2,3), delta1=1, delta2=1, pi0=c(1,1,1), sig=c(1,1,1), z=rep(1:3, each=n))
inits <- list(mu=c(1,2,3), delta1=1, delta2=1, pi0=c(1,1,1), tau=c(1,1,1), z=rep(1:3, each=n))  ## using TAU
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$pi0
Rmodel$pi
Rmodel$z
Rmodel$mu
Rmodel$means
Rmodel$sigmas
Rmodel$taus

##undebug(Rmodel$checkConjugacy)
##Rmodel$checkConjugacy('mu[1]')
## 
##undebug(Rmodel$checkConjugacy2)
##Rmodel$checkConjugacy2('mu[1]')

conf <- configureMCMC(Rmodel)
##conf <- configureMCMC(Rmodel, useConjugacy=FALSE)
conf$resetMonitors()
##conf$addMonitors(c('mu', 'z', 'sig'))
conf$addMonitors(c('mu', 'z', 'tau'))
conf$printSamplers('mu')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
calculate(Rmodel)
calculate(Cmodel)

##iter <- 20
## 
##set.seed(0)
##Rmcmc$run(iter)
## 
##set.seed(0)
##Cmcmc$run(iter)
## 
##Rsamples <- as.matrix(Rmcmc$mvSamples)
##Csamples <- as.matrix(Cmcmc$mvSamples)
## 
##sampNames <- colnames(Rsamples)
## 
##Rsamples[, sampNames]
##Csamples[, sampNames]
##Rsamples[, sampNames] - Csamples[, sampNames]

set.seed(0)
Cmcmc$run(50000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)

mus <- Rmodel$expandNodeNames(c('mu'))
sigs <- Rmodel$expandNodeNames(c('sig'))
taus <- Rmodel$expandNodeNames(c('tau'))
zs <- Rmodel$expandNodeNames(c('z'))

ind <- 1:dim(samples)[1]
ind <- 1:5000
samplesPlot(samples, col = mus, ind = ind)
samplesPlot(samples, col = sigs, ind = ind)
samplesPlot(samples, col = taus, ind = ind)
samplesPlot(samples, col = c('mu[3]','sig[3]'), ind = ind)
samplesPlot(samples, col = c('mu[3]','tau[3]'), ind = ind)
samplesPlot(samples, col = c(        'tau[3]'), ind = ind)

samplesPlot(samples, col = zs, ind = ind)

samplez <- samples[, zs]
samplezsum <- cbind(samplez, z1=apply(samplez, 1, function(x) sum(x==1)), z2=apply(samplez, 1, function(x) sum(x==2)), z3=apply(samplez, 1, function(x) sum(x==3)))
dim(samplez)
dim(samplezsum)

samplesPlot(samplezsum, col = c('z1','z2','z3'), ind=1:5000)
samplesPlot(samplezsum, col = c('z1','z2','z3'), ind=1:15000)

mean(y[1:n])
mean(y[(n+1):(2*n)])
mean(y[(2*n+1):(3*n)])



## doing the midterm question about normal mean hypothesis test
## "weights of sheep raised on a farm"

tau0 <- 1/100
mu0 <- 75

y <- c(78, 81, 77, 76, 75, 74, 78, 75, 77, 75)
n <- length(y)
ybar <- mean(y)
ybar
tau <- 1/9

taup <- tau0 + n*tau
mup <- (mu0*tau0 + ybar*n*tau)/(tau0+n*tau)

mup
taup
pnorm(75, mup, 1/sqrt(taup))


library(nimble)

code <- nimbleCode({
    mu ~ dnorm(75, sd=10)
    for(i in 1:n) {
        y[i] ~ dnorm(mu, sd=3)
    }
})
constants <- list(n=n)
data <- list(y=y)
inits <- list(mu = 75)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(500000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)
mup
mean(samples[,1] < 75)
pnorm(75, mup, 1/sqrt(taup))






## inplementing a finite mixture model in NIMBLE

library(nimble)

set.seed(0)
n <- 500
y <- c(rnorm(n,0,sd=1), rnorm(n,10,sd=2), rnorm(n,20,sd=3))
##hist(y, breaks=30)

code <- nimbleCode({
    for(i in 1:3) {
        sig[i] ~ dunif(0, 100)
    }
    mu[1] ~ dnorm(0, sd=1000)
    mu[2] <- mu[1] + delta1
    delta1 ~ T(dnorm(0, sd=1000), 0, 1000)
    mu[3] <- mu[2] + delta2
    delta2 ~ T(dnorm(0, sd=1000), 0, 1000)
    for(i in 1:N) {
        means[i] <- equals(z[i],1)*mu[1] + equals(z[i],2)*mu[2] + equals(z[i],3)*mu[3]
        sigmas[i] <- equals(z[i],1)*sig[1] + equals(z[i],2)*sig[2] + equals(z[i],3)*sig[3]
        y[i] ~ dnorm(means[i], sd=sigmas[i])
        z[i] ~ dcat(pi[1:3])
    }
    ##pi[1:3] ~ ddirch(alpha[1:3])
    for(i in 1:3) {
        pi0[i] ~ dgamma(1, 1)
        pi[i] <- pi0[i] / (pi0[1] + pi0[2] + pi0[3])
    }
})

N <- length(y)
constants <- list(N=N)
data <- list(y=y)
inits <- list(sig=rep(1,3), mu=c(1,2,3), delta1=1, delta2=1, z=rep(1,N), pi0=c(1,1,1))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$pi0
Rmodel$pi
Rmodel$z
Rmodel$mu
Rmodel$means
Rmodel$sigmas

conf <- configureMCMC(Rmodel)
##conf <- configureMCMC(Rmodel, useConjugacy = FALSE)

conf$printSamplers()
conf$printSamplers('mu')
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

calculate(Rmodel)
calculate(Cmodel)

##set.seed(0)
##Rmcmc$run(2)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)


                   

## bootstrapping the slope 

data <- read.csv('~/Downloads/Animals\ (1).csv')
names(data)
head(data)

plot(data$gestation, data$longevity)

m <- lm(data$longevity ~ data$gestation)
m
summary(m)

s <- numeric(10000)
n <- dim(data)[1]
for(i in 1:10000) {
    ind <- sample(1:n, replace = TRUE)
    dat <- data[ind,]
    m <- lm(dat$longevity ~ dat$gestation)
    s[i] <- unname(m$coef[2])
}

hist(s)
m <- lm(data$longevity ~ data$gestation)
abline(v = unname(m$coef[2]), col='red')

              



## doing the bigmac problem

data <- read.delim('~/Downloads/bigmac.txt')
names(data)
head(data)
with(data, plot(EngSal, BigMac))
rownames(data)
colnames(data)
dimnames(data)

data <- read.delim('~/Downloads/bigmac.txt')
with(data, plot(EngSal, BigMac))
with(data, text(EngSal, BigMac, labels=rownames(data), cex=0.5, pos=4))

i <- -c(17,21)

m <- lm(data$EngSal[i] ~ data$BigMac[i])
coef(m)
summary(m)

plot(data$EngSal[i], residuals(m))

dim(data)
length(data$EngSal)
length(unname(residuals(m)))
names(m)
unname(residuals(m))
data$EngSal
data$BigMac

library(MASS)
boxcox
boxcox(m)

plot(data$EngSal[i], (data$BigMac[i])^.01)
plot(data$EngSal[i], log(data$BigMac[i]))

## likelihood: y ~ Normal(mu, sigma)
## what prior for sigma?
## sigma ~ Uniform(0, 1000)

sigma <- runif(100000, 0, 1000)

hist(sigma)



## verifying the change of variable formula

a <- runif(100000, 0, 1000)

par(mfrow=c(2,1))
plot(density(a))
plot(density(sqrt(a)))
curve(2*x/1000, col='red', add=TRUE)



## STAT 365 Monte Carlo Exercise 9.1
## comparing Bayesian and Frequentist estimators of pi

n <- 10
msef <- numeric()
mseb <- numeric()
biasf <- numeric()
biasb <- numeric()
varf <- numeric()
varb <- numeric()
pis <- c(1:5/100, 1:9/10, 95:99/100)
for(i in 1:length(pis)) {
    pi <- pis[i]
    samp <- rbinom(10000, size=n, prob=pi)
    pihatf <- samp / n
    pihatb <- (samp+1)/(n+2)
    biaspif <- mean(pihatf) - pi
    biaspib <- mean(pihatb) - pi
    varpif <- var(pihatf)
    varpib <- var(pihatb)
    msef[i] <- biaspif^2 + varpif
    mseb[i] <- biaspib^2 + varpib
    biasf[i] <- biaspif
    biasb[i] <- biaspib
    varf[i] <- varpif
    varb[i] <- varpib
}
par(mfrow = c(3,1))
plot(pis, biasf, main = 'Bias', col='red', type = 'b', ylim = range(c(biasf,biasb)))
lines(pis, biasb, col='blue', type = 'b')
plot(pis, varf, main = 'Variance', col='red', type = 'b', ylim = range(c(varf,varb)))
lines(pis, varb, col='blue', type = 'b')
plot(pis, msef, main = 'MSE', col='red', type = 'b', ylim = range(c(msef,mseb)))
lines(pis, mseb, col='blue', type = 'b')



## trying HW for STAT201

data <- read.delim('~/Downloads/Saratoga.txt')
names(data)
str(data)
attach(data)
boxplot(Price)
hist(Price)
boxplot(Living.Area)
hist(Living.Area)
plot(Living.Area, Price)

data <- read.delim('~/Downloads/Mauna-Loa-and-DJIA.txt')
attach(data)
ind <- Year>=1982
plot(DJIA, CO2.Avg)
plot(DJIA[ind], CO2.Avg[ind])
cor(DJIA[ind], CO2.Avg[ind])
str(data)

pairs(data, panel = panel.smooth)
?pairs

data <- read.delim('~/Downloads/Kentucky_Derby_2014.txt')
names(data)
str(data)
attach(data)
plot(data$Year, data$Speed..mph.)


## pulling together pieces of not having to recompile models and MCMCs, for Dao

library(nimble)
nimbleOptions(showCompilerOutput = TRUE) ### DELETE THIS later
code <- nimbleCode({
    a ~ dbern(0.5)
    b ~ dnorm(0, 1)
    c ~ dnorm(0, 1)
})
Rmodel <- nimbleModel(code, inits = list(a=0, b=0, c=0))
conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

model_orig <- Cmodel

## recover the R (uncompiled) model object
if(inherits(model_orig, 'CmodelBaseClass'))
    model_orig <- model_orig$Rmodel


##md <<- Rmodel_orig$modelDef
Rmodel <- model_orig$newModel(replicate = TRUE, check = FALSE)

conf_initial <- configureMCMC(Rmodel)

monitorsVector <- Rmodel$getNodeNames(stochOnly=TRUE, includeData=FALSE)
conf_initial$addMonitors(monitorsVector, print=FALSE)

scalarNodeVector <- Rmodel$getNodeNames(stochOnly=TRUE, includeData=FALSE, returnScalarComponents=TRUE)
discreteInd <- sapply(scalarNodeVector, function(n) Rmodel$isDiscrete(n), USE.NAMES=FALSE)
scalarNodeVectorContinuous <<- scalarNodeVector[!discreteInd]
scalarNodeVectorContinuous

firstScalarNode <- scalarNodeVectorContinuous[1]
firstScalarNode

conf_initial$printSamplers()

samplersWeMightUse <- c('RW', 'slice', 'RW_block')
for(sampler in samplersWeMightUse)
    conf_initial$addSampler(target = firstScalarNode, type = sampler)

conf_initial$printSamplers()

Rmcmc_initial <- buildMCMC(conf_initial)
Cmodel <- compileNimble(Rmodel)
Cmcmc_initial <- compileNimble(Rmcmc_initial, project = Rmodel)

conf_new <- configureMCMC(oldConf = conf_initial)

conf_new$setSamplers()  ## remove all samplers
conf_new$printSamplers()

nodes <- c('a', 'b', 'c')
for(node in nodes)
    conf_new$addSampler(target = node, type = 'slice')
conf_new$printSamplers()

Rmcmc_new <- buildMCMC(conf_new)
Cmcmc_new <- compileNimble(Rmcmc_new, project = Rmodel)


nimCopy(from = model_orig, to = Cmodel, logProb = TRUE)
calculate(Cmodel)


## doing the random drug abuse survey using NIMBLE MCMC

library(nimble)

N <- 29
y <- 14


code <- nimbleCode({
    theta ~ dunif(0, 1)
    y ~ dbinom(size = N, prob = 0.25 + 0.5*theta)
})

constants <- list(N = N)
data <- list(y = y)
inits <- list(theta = 0.5)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(100000)
samples <- as.matrix(Cmcmc$mvSamples)

samplesPlot(samples, col='theta')
theta <- samples[,'theta']
mean(theta)
median(theta)
quantile(theta, c(0.25, 0.5, 0.75))


## testing random drug abuse survey strategy, the variance of the results

f <- function(n, iter) {
    res <- rbinom(n=iter, size=n, prob=0.25)
    var(res)
}

## expect variance is 3*n/16
iter <- 100000

n <- 100
f(n, iter) - 3*n/16


g <- function(n, iter) {
    res <- numeric(iter)
    for(i in 1:iter) {
        a <- rbinom(n=1, size=n, prob=1/2)
        res[i] <- rbinom(n=1, size=a, prob=1/2)
    }
    var(res)
}

g(n, iter) - 3*n/16



test <- function(iter, n, theta) {
    res <- numeric()
    c <- numeric()
    d <- numeric()
    for(i in 1:iter) {
        a <- rbinom(1, size=n, prob=1/2)
        b <- n - a
        c[i] <- rbinom(1, size=a, prob=1/2)
        d[i] <- rbinom(1, size=b, prob=theta)
        res[i] <- c[i] + d[i]
    }
    var(d)
}

iter <- 10000
n <- 100
theta <- .2
test(iter, n, theta)

3*n/16

theta*n*(2-theta)/4



## solving 5 statisticians problem

one <- function() {
    cur <- 1  ## 1, 2, 3, 4, 5
    while(TRUE) {
        n <- runif(1)  # stay, +1, -1
        if(n < 1/3) {
            return(cur)
        } else if(n < 2/3) {
            cur <- ((cur - 1) - 1) %% 5 + 1
        } else cur <- ((cur + 1) - 1) %% 5 + 1
    }
}

many <- function(n) {
    ret <- numeric(n)
    for(i in 1:n)
        ret[i] <- one()
    return(ret)
}

prop.table(table(many(1000000))) * 11

##        1        2        3        4        5 
## 5.002679 2.000020 0.996589 0.999779 2.000933 
##     5/11,    2/11,    1/11,    1/11,    2/11


## playihg with Rmd

setwd('~/temp/lecTEMP')
getwd()
library(methods)
library(knitr)
library(rmarkdown)

list.files()

render('Lecture1Slides.rmd')

## testing Nick Michaud's question about nimbleFunctionLists
library(nimble)

bigFunction <- nimbleFunction(
    setup = function(N) {
        functions <- nimbleFunctionList(littleFunction_virtual)  ## CHANGE
        for(n in 1:N)
            functions[[n]] <-littleFunction(n)
    },
    run = function() {
        returnType(integer(0))
        sum_N <- 0
        for(n in 1:N) 
            sum_N <- sum_N + functions[[n]]$run()
        return(sum_N)
    }
)

## NEW
littleFunction_virtual <- nimbleFunctionVirtual(
    run = function() {
        returnType(integer(0))
    }
)

littleFunction <- nimbleFunction(
    contains = littleFunction_virtual,
    setup = function(n){},
    run = function(){
        returnType(integer(0))
        return(n)
    }
)

testFunction <- bigFunction(5)
testFunction$run()
CtestFunction <- compileNimble(testFunction)
CtestFunction$run()


## make configureMCMC respect dconstraint()
library(nimble)

code <- nimbleCode({
    for(j in 1:J) {
        for(i in 1:n[j]) {
            y[j,i] ~ dconstraint(w[j,i] > 0)
            w[j,i] ~ dnorm(theta[j], 1)
        }
        theta[j] ~ dnorm(mu, itau2)
    }
    itau2 ~ dgamma(a, b)
    mu ~ dnorm(0, .00001)
})

J <- 3
n <- rep(2, J)

y <- matrix( sample(c(0,1), sum(n), replace = TRUE), nrow = J)
m <- nimbleModel(code, constants = list(n = n, J = J), data = list(y = y))

conf <- configureMCMC(m)
conf$printSamplers()


## nested sampler function wrapper for Dao
library(nimble)

sampler_record_wrapperNEW <- nimbleFunction(
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control){
      numSamples <- 0
      before <- c(0, 0)
      after <- c(0, 0)
      samplerFunctionList <- nimbleFunctionList(sampler_BASE)
    ###### make sure to provide *named* arguments to this function
    ###### shouldn't require anything in control$control, if you don't want
    controlListForNestedSampler <- mcmc_generateControlListArgument(samplerFunction = control$sampler_function, control = control$control)
    samplerFunctionList[[1]] <- eval(call( control$sampler_function, model = model, mvSaved = mvSaved, target = target, control =  controlListForNestedSampler))}, 
    run = function() {
      ## these lines are new:
      numSamples <<- numSamples + 1
      setSize(before, numSamples)
      setSize(after, numSamples)
      before[numSamples] <<- model[[target]]
      ## back to the original sampler function code:
      samplerFunctionList[[1]]$run()
      ## this line new:
      after[numSamples] <<- model[[target]]
    },
    methods = list(
        reset = function() {samplerFunctionList[[1]]$reset()}
    ))

code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:10) {
        x[i] ~ dnorm(mu*mu, sd = sigma)
    }
})
Rmodel <- nimbleModel(code)

conf <- configureMCMC(Rmodel)
conf$printSamplers()

conf$removeSamplers('sigma')
conf$printSamplers()

## SEE CHANGES HERE
conf$addSampler(target = 'sigma', type = sampler_record_wrapperNEW, control = list(sampler_function = 'sampler_slice', control=list()))
conf$printSamplers()

## SEE CHANGES HERE
conf$addSampler(target = 'sigma', type = 'sampler_record_wrapperNEW', control = list(sampler_function = 'sampler_RW', control = list()))
conf$printSamplers()

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)



## trying out a recursive nimble function
library(nimble)

Rnf <- nimbleFunction(
    run = function(x = double()) {
        if(x == 0 || x == 1) {
            return(1)
        } else {
            a <- 1
        }
    })



## testing the new NIMBLE function: runMCMC()
library(nimble)
code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:10) {
        x[i] ~ dnorm(mu*mu, sd = sigma)
    }
})
Rmodel <- nimbleModel(code)
Rmodel$setData(list(x = c(2, 5, 3, 4, 1, 0, 1, 3, 5, 3)))
conf <- configureMCMC(Rmodel)
conf$getMonitors()
conf$setThin(10)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)


## testing new functions for addSampler(), 'name', 'libraryTag', etc...
library(nimble)
code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:10) {
        x[i] ~ dnorm(mu*mu, sd = sigma)
    }
})
Rmodel <- nimbleModel(code)
conf <- configureMCMC(Rmodel)

debug(conf$addSampler)
undebug(conf$addSampler)
conf$printSamplers()
conf$removeSamplers('sigma')
conf$printSamplers()
conf$addSampler(target = 'sigma', type = sampler_slice, name='slice1')
conf$addSampler(target = 'sigma', type = 'slice', name='slice2')
conf$addSampler(target = 'sigma', type = sampler_slice)
conf$addSampler(target = 'sigma', type = 'slice')
conf$addSampler(target = 'sigma', type = sampler_RW, name='slice1')
conf$addSampler(target = 'sigma', type = 'RW', name='slice2')
conf$addSampler(target = 'sigma', type = sampler_RW)
conf$addSampler(target = 'sigma', type = 'RW')
conf$printSamplers()
conf$controlNamesLibrary



##debug(buildMCMC)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)



            

mc <- Cmcmc
mc <- Rmcmc

pb <- TRUE
pb <- FALSE

si <- TRUE
si <- FALSE

ni <- 10
ni <- 100

nb <- 5
nb <- 9



inits <- function() list(mu = rnorm(1,0,1000), sigma = runif(1,0,10))
##inits <- function() list(mu = 1:2, sigma = runif(1,0,10), x = 3)

initsList <- list(inits(), inits(), inits())
initsList <- list(inits(), inits())
initsList <- list(inits())

debug(runMCMC)
undebug(runMCMC)

debug(mcmc$run)

runMCMC(Rmcmc, niter = 3, nchains = 3, inits = inits())
runMCMC(Cmcmc, niter = 3, nchains = 3, inits = inits())

runMCMC(Rmcmc, niter = 3, nchains = 3, inits = initsList)
runMCMC(Cmcmc, niter = 3, nchains = 3, inits = initsList)

runMCMC(Rmcmc, niter = 300, nchains = 3, inits = inits())
runMCMC(Cmcmc, niter = 300, nchains = 3, inits = ii, nburnin=10)
a <- runMCMC(Rmcmc, niter = 300, nburnin=10, nchains = 4)

runMCMC(Rmcmc, niter = 300, nchains = 3, inits = inits(), nburnin=295)
runMCMC(Cmcmc, niter = 30000, nchains = 3, inits = inits, nburnin=29995)

runMCMC(Cmcmc, niter = 30000, nchains = 3, inits = inits, nburnin=29995, progressBar=TRUE, silent=TRUE, returnCodaMCMC = TRUE)

runMCMC(Cmcmc, niter = 30000, inits = inits, nburnin=29995, progressBar=TRUE, silent=TRUE, returnCodaMCMC = TRUE)

runMCMC(Cmcmc, niter = 30000, inits = inits, nburnin=29995, progressBar=TRUE, silent=TRUE)

runMCMC(Cmcmc, niter = 30000, nchains = 3, inits = inits, nburnin=29999, progressBar=TRUE, silent=TRUE, setSeed=TRUE)

runMCMC(Rmcmc, niter = 30, nchains = 3, inits = inits, nburnin=29, silent=TRUE, setSeed=TRUE)

runMCMC(Cmcmc, niter = 30000, nchains = 3, nburnin=29999, progressBar=TRUE, silent=TRUE, setSeed=TRUE)

runMCMC(Rmcmc, niter = 30, nchains = 3, nburnin=29, silent=TRUE, setSeed=TRUE)



## testing inconsistancy in dgamma() between R and C

library(nimble)

Rnf <- nimbleFunction(
    run = function(x = double(), a = double(), b = double()) {
        lp <- dgamma(x, a, b, log = 1)
        returnType(double())
        return(lp)
    }
)

Cnf <- compileNimble(Rnf)

x <- 6e-100
a <- 0.001
b <- 1.0
Rnf(x, a, b)
Cnf(x, a, b)




## making utility function for MCMC sample traceplots and density histograms

library(nimble)   ## get samples from birats2 model
Rmodel <- readBUGSmodel('birats2.bug', dir = getBUGSexampleDir('birats'), data = 'birats-data.R', inits = 'birats-inits.R')
Rmcmc <- buildMCMC(Rmodel)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
Cmcmc$run(20000)
samples <- as.matrix(Cmcmc$mvSamples)

library(nimble)   ## get samples from Dave Pleydel's multinomial test model
codeTest <- nimbleCode ({
    X[1:nGroups] ~ dmultinom(size=N, prob=pVecX[1:nGroups])
    Y[1:nGroups] ~ dmultinom(size=N, prob=pVecY[1:nGroups])
    for (ii in 1:nGroups)
        Z[ii] ~ dbeta(1 + X[ii], 1 + Y[ii]) })
nGroups   <- 5
N         <- 1E6
pVecX     <- rdirch(1, rep(1, nGroups))
pVecY     <- rdirch(1, rep(1, nGroups))
X         <- rmultinom(1, N, pVecX)[,1]
Y         <- rmultinom(1, N, pVecY)[,1]
Z         <- rbeta(nGroups, 1+X, 1+Y)
Xini      <- rmultinom(1, N, sample(pVecX))[,1]
Yini      <- rmultinom(1, N, sample(pVecY))[,1]
Constants <- list(nGroups=nGroups)
Inits     <- list(X=Xini, Y=Yini, pVecX=pVecX, pVecY=pVecY, N=N)
Data      <- list(Z=Z)
modelTest <- nimbleModel(codeTest, constants=Constants, inits=Inits, data=Data)
mcmcTest  <- buildMCMC(modelTest) 
cModelTest <- compileNimble(modelTest)
cMcmcTest <- compileNimble(mcmcTest, project=modelTest)
cModelTest$N     <- N <- 1E3
cModelTest$pVecX <- sort(rdirch(1, rep(1, nGroups)))
cModelTest$pVecY <- sort(rdirch(1, rep(1, nGroups)))
simulate(cModelTest, c('X','Y','Z'), includeData=TRUE)
niter  <- 1E4
cMcmcTest$run(niter)
samples <- as.matrix(cMcmcTest$mvSamples)

samplesPlot <- function(samples, ind=1:ncol(samples), burnin=NULL, width=7, height=4, legend=TRUE, legend.location='topright') {
    ## device window and plotting parameters
    dev.new(height=height, width=width)
    par(mfrow=c(1,2), cex=0.7, cex.main=1.5, lab=c(3,3,7), mgp=c(0,0.6,0), mar=c(2,1,2,1), oma=c(0,0,0,0), tcl=-0.3, yaxt='n', bty='l')
    ## process samples
    samples <- samples[, ind, drop=FALSE]
    if(!is.null(burnin))
        samples <- samples[(burnin+1):dim(samples)[1], , drop=FALSE]
    nparam <- ncol(samples)
    rng <- range(samples)
    ## traceplots
    plot(1:nrow(samples), ylim=rng, type='n', main='Traceplots', xlab='', ylab='')
    for(i in 1:nparam)
        lines(samples[,i], col=rainbow(nparam, alpha=0.75)[i])
    ## posterior densities
    xMin <- xMax <- yMax <- NULL
    for(i in 1:nparam) {
        d <- density(samples[,i])
        xMin <- min(xMin,d$x); xMax <- max(xMax,d$x); yMax <- max(yMax, d$y) }
    plot(1, xlim=c(xMin,xMax), ylim=c(0,yMax), type='n', main='Posterior Densities', xlab='', ylab='')
    alpha_density <- 0.2
    for(i in 1:nparam)
        polygon(density(samples[,i]), col=rainbow(nparam, alpha=alpha_density)[i], border=rainbow(nparam, alpha=alpha_density)[i])
    if(legend & !is.null(dimnames(samples)) & is.character(dimnames(samples)[[2]]))
        legend(legend=dimnames(samples)[[2]], fill=rainbow(nparam, alpha=0.5), bty='n', x=legend.location)
}


dim(samples)
dimnames(samples)
samplesPlot(samples)

apply(samples, 2, mean)

samplesPlot(samples, ind=c(1,2,3,7), burnin=1000, legend.location='topleft')
samplesPlot(samples, ind=c(4,6), burnin=1000)
samplesPlot(samples, ind=c(5), burnin=1000)

## better to just use the plotting functions in coda package!!!
library(coda)
mcmcSamples <- as.mcmc(samples)
acfplot(mcmcSamples)
plot(mcmcSamples)



## playing with plot(density(x))
x <- rnorm(10000)
plot(density(x))
d <- density(x)
class(d)
ls(d)
length(d$x)
length(d$y)
plot(d$x, d$y, type='l')  ## this creates the standard plot(density(x)) plot

      plot(prior ~ param.x, ylim = yLims, type = "l", xlim = range(param.x), 
            xlab = "", ylab = "", main = "", axes = FALSE, ...)
        polygon(param.x, prior, col = "red")
        box()
        r = legend("topleft", legend = "Prior", lty = 1, bty = "n", 
            plot = FALSE)$text
        text(r$x, r$y, "Prior", adj = 0)
        plot(likelihood ~ param.x, type = "l", xlab = "", ylab = "", 
            main = "", axes = FALSE, ...)
        polygon(param.x, likelihood, col = "green")
        box()
        r = legend("topleft", legend = "Prior", lty = 1, bty = "n", 
            plot = FALSE)$text
        text(r$x, r$y, "Likelihood", adj = 0)
        plot(posterior ~ param.x, ylim = yLims, type = "l", xlab = "", 
            ylab = "", main = "", axes = F, ...)
        polygon(param.x, posterior, col = "blue")




## testing funny behavior of DSL round() and nimRound()
library(nimble)

Rnf <- nimbleFunction(
    run = function() {
        for(i in 0:50) {
            x <- i/10
            xRound <- round(x)
            print('x: ', x, ',   round(x): ', xRound)
        }
    }
)

Cnf <- compileNimble(Rnf)

Rnf()
Cnf()



## testing making my own progress bar in NIMBLE DSL nimbleFunction

library(nimble)

rfun <- nimbleFunction(
    run = function(pb = logical(default=TRUE)) {
        ##print('|')
        ##for(i in 1:20)   { a <- i %% 7; print(a) }
        ##print('|')
        ##cat('|')
        ##for(i in 1:3)   cat('-', i)
        ##cat('|')
        a <- 1
        pb <- pb & 0
        print(pb)
    }
)

##rfun()
cfun <- compileNimble(rfun)
##cfun()
cfun()


library(nimble)
code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)


library(nimble)
Rmodel <- readBUGSmodel('birats2.bug', dir = getBUGSexampleDir('birats'), data = 'birats-data.R', inits = 'birats-inits.R')
Rmcmc <- buildMCMC(Rmodel)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Cmcmc$run(20000)
Cmcmc$run(30007, reset = FALSE)         ## continue previous run
Cmcmc$run(10000, progressBar = FALSE)   ## turn off progress bar
Cmcmc$run(100003, reset = FALSE)        ## sort of slow run
Cmcmc$run(5000)     ## faster
Cmcmc$run(2000)     ## faster still
Cmcmc$run(1000)     ## ...
Cmcmc$run(500)
Cmcmc$run(100)
Cmcmc$run(40)  ## no bar when too few iterations



## playing with R progress bars: txtProgressBar

f <- function(n = 1e6, frac = 0.01) {
    pb <- txtProgressBar(style = 3, char = '-')
    nupdate <- floor(frac * n) 
    for(i in 1:n) {
        a <- rnorm(1)
        if(i %% nupdate == 0) {
            setTxtProgressBar(pb, i/n)
        }
    }
    setTxtProgressBar(pb, 1)
    close(pb)
}

f(n=2e6, frac = .01)



library(Bolstad)

## datasets in library(Bolstad):
## bears (exercise in chapter 3)
## slug  (exercise in chapter 14)

## functions that I might possibly use:
## decomp: makes plots of prior, likelihood, posterior, but only for class=Bolstad.
##         maybe steal a bunch of the code, make one that works for general samples?
##         BUT WAIT -- this only works for sorted (x,y) pairs -- not for samples.


## getting MCMC samples for logProbs of variables

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$getMonitors()
conf$addMonitors('logProb_a')
conf$getMonitors()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

##paste0('logProb_', letters)

set.seed(0)
Cmcmc$run(10)
samples <- as.matrix(Cmcmc$mvSamples)
samples
apply(samples, 2, mean)


## tracking down error in conjugacy checking system,
## this model was submitted by: Eduardo Martins <egmartins@gmail.com>
## during the ISEC 2016 NIMBLE workshop
library(nimble)
load('~/Downloads/data_inits.RData')
fec_code <- nimbleCode({
    ## Likelihood
    ## Fecundity
    for (i in 1:ny.fec) {
        y.fec[i] ~ dlnorm(log(z.fec[i]), tau.y.fec) # observation model
        z.fec[i] ~ dnegbin(p.fec[i], size.fec) # sampling model
        p.fec[i] <- size.fec / (size.fec + mu.fec[i])
        log(mu.fec[i]) <- a.fec.raw + b.fec * z.sl[i] + nu.fec.k.raw[idx.k[i]] + nu.fec.l.raw[idx.l[i]]
    }
    a.fec <- a.fec.raw + mean(nu.fec.k.raw[1:nk]) + mean(nu.fec.l.raw[1:nl])
    for (k in 1:nk){
        nu.fec.k.raw[k] ~ dnorm(mu.nu.fec.k, tau.nu.fec.k)
        nu.fec.k[k] <- nu.fec.k.raw[k] - mean(nu.fec.k.raw[1:nk])
    }
    for (l in 1:nl){
        nu.fec.l.raw[l] ~ dnorm(mu.nu.fec.l, tau.nu.fec.l)
        nu.fec.l[l] <- nu.fec.l.raw[l] - mean(nu.fec.l.raw[1:nl])
    }
    ## Length
    for(i in 1:ny.fec) {
        z.sl[i] ~ dnorm(mu.sl[i], tau.sl.i)
        mu.sl[i] <- a.sl.raw + b.sl * z.ocr[i] + nu.sl.k.raw[idx.k[i]] + nu.sl.l.raw[idx.l[i]]
    }
    a.sl <- a.sl.raw + mean(nu.sl.k.raw[1:nk]) + mean(nu.sl.l.raw[1:nl])
    for (k in 1:nk){
        nu.sl.k.raw[k] ~ dnorm(mu.nu.sl.k, tau.nu.sl.k)
        nu.sl.k[k] <- nu.sl.k.raw[k] - mean(nu.sl.k.raw[1:nk])
    }
    for (l in 1:nl){
        nu.sl.l.raw[l] ~ dnorm(mu.nu.sl.l[l], tau.nu.sl.l)
        mu.nu.sl.l[l] <- b.nu.sl.l[1] * sst6.std[l] + b.nu.sl.l[2] * pdo6.std[l] + b.nu.sl.l[3] * nsal.std[l]
        nu.sl.l[l] <- nu.sl.l.raw[l] - mean(nu.sl.l.raw[1:nl])
    }
    ## Years of ocean residence
    for(i in 1:ny.fec){
        z.ocr[i] ~ dbern(p.z.ocr[i])
        logit(p.z.ocr[i]) <- a.ocr.raw + nu.ocr.k.raw[idx.k[i]] + nu.ocr.l.raw[idx.l[i]]
    }
    a.ocr <- a.ocr.raw + mean(nu.ocr.k.raw[1:nk]) + mean(nu.ocr.l.raw[1:nl])
    for (k in 1:nk){
        nu.ocr.k.raw[k] ~ dnorm(mu.nu.ocr.k, tau.nu.ocr.k)
        nu.ocr.k[k] <- nu.ocr.k.raw[k] - mean(nu.ocr.k.raw[1:nk])
    }
    for (l in 1:nl){
        nu.ocr.l.raw[l] ~ dnorm(mu.nu.ocr.l, tau.nu.ocr.l)
        nu.ocr.l[l] <- nu.ocr.l.raw[l] - mean(nu.ocr.l.raw[1:nl])
    }
    ## Priors
    ## Fecundity
    tau.y.fec <- pow(sig.y.fec, -2)
    sig.y.fec ~ dunif(0, 100)
    num.size.fec ~ dnorm(0, 0.0016)
    den.size.fec ~ dnorm(0, 1)
    size.fec <- abs(num.size.fec / den.size.fec)
    mu.nu.fec.k ~ dnorm(0, 0.0001)
    tau.nu.fec.k <- pow(sig.nu.fec.k, -2)
    sig.nu.fec.k ~ dunif(0, 100)
    mu.nu.fec.l ~ dnorm(0, 0.0001)
    tau.nu.fec.l <- pow(sig.nu.fec.l, -2)
    sig.nu.fec.l ~ dunif(0, 100)
    a.fec.raw ~ dnorm(mean.z.fec, tau.z.fec)
    b.fec ~ dnorm(0, 0.0001)
    ## Length
    tau.sl.i <- pow(sig.sl.i, -2)
    sig.sl.i ~ dunif(0, 100)
    mu.nu.sl.k ~ dnorm(0, 0.0001)
    tau.nu.sl.k <- pow(sig.nu.sl.k, -2)
    sig.nu.sl.k ~ dunif(0, 100)
    tau.nu.sl.l <- pow(sig.nu.sl.l, -2)
    sig.nu.sl.l ~ dunif(0, 100)
    a.sl.raw ~ dnorm(0, 0.0001)
    b.sl ~ dnorm(0, 0.0001)
    for (b in 1:nb.nu.sl.l) {
        b.nu.sl.l[b] ~ dnorm(0, 0.0001)
    }
    ## Years of ocean residence
    mu.nu.ocr.k ~ dnorm(0, 0.0001)     
    tau.nu.ocr.k <- pow(sig.nu.ocr.k, -2)
    sig.nu.ocr.k ~ dunif(0, 100)
    mu.nu.ocr.l ~ dnorm(0, 0.0001)
    tau.nu.ocr.l <- pow(sig.nu.ocr.l, -2)
    sig.nu.ocr.l ~ dunif(0, 100)
    a.ocr.raw ~ dnorm(0, 0.0001)
    ## Residuals and metrics for model assessment
    ## Fecundity
    for(i in 1:ny.fec){
        res.y.fec[i] <- y.fec[i] - z.fec[i]
        new.y.fec[i] ~ dlnorm(log(z.fec[i]), tau.y.fec)
    }
    for(i in 1:ny.fec){
        pres.z.fec[i] <- (z.fec[i] - mu.fec[i])/sqrt(mu.fec[i] + pow(mu.fec[i], 2) / size.fec)
        new.z.fec[i] ~ dnegbin(p.fec[i], size.fec)
        pres.new.z.fec[i] <- (new.z.fec[i] - mu.fec[i])/sqrt(mu.fec[i] + pow(mu.fec[i], 2) / size.fec)
        d.z.fec[i] <- pow(pres.z.fec[i], 2)
        d.new.z.fec[i] <- pow(pres.new.z.fec[i], 2)
    }
    fit.z.fec <- sum(d.z.fec[1:ny.fec])
    fit.new.z.fec <- sum(d.new.z.fec[1:ny.fec])
    ## Length
    for (i in 1:ny.fec) {
        res.z.sl[i] <- z.sl[i] - mu.sl[i]
        new.z.sl[i] ~ dnorm(mu.sl[i], tau.sl.i)
    }
    for (l in 1:nl){
        res.nu.sl.l[l] <- nu.sl.l[l] - mu.nu.sl.l[l]
        new.nu.sl.l[l] ~ dnorm(mu.nu.sl.l[l], tau.nu.sl.l)
    }
})
fec_mod <- nimbleModel(fec_code, constants = dat, inits = inits())
fec_cmod <- compileNimble(fec_mod)

##options(error = recover)

## next line errors out in conjugacy check:
## Error in x[[1]] : subscript out of bounds
mcmcConf <- configureMCMC(fec_mod)

mcmcConf$printSamplers()
mcmcConf$getMonitors()
fecMCMC <- buildMCMC(mcmcConf)
cfecMCMC <- compileNimble(fecMCMC, project = fec_mod)

cfecMCMC$run(10)

## testing of new setData() functionality:
## taking a character vector of variable names

library(nimble)
Rmodel <- readBUGSmodel('birats2.bug', dir = getBUGSexampleDir('birats'), data = 'birats-data.R', inits = 'birats-inits.R')

Rmodel$getNodeNames(includeData = FALSE)

modelLs <- readBUGSmodel('birats2.bug', dir = getBUGSexampleDir('birats'), data = 'birats-data.R', inits = 'birats-inits.R', returnModelComponents = TRUE)
modelLs$model

Rmodel$beta
Rmodel$tau.c
Rmodel$r

Rmodel$getNodeNames(dataOnly = TRUE)
Rmodel$setData(c('beta', 'tau.c', 'r'))
Rmodel$getNodeNames(dataOnly = TRUE)
Rmodel$resetData()
Rmodel$getNodeNames(dataOnly = TRUE)
Rmodel$setData(c('beta', 'tau.c', 'r'))
Rmodel$getNodeNames(dataOnly = TRUE)
Rmodel$setData(c('Y'))





## testing new MCMC runtime argument: time
library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a, 1)
    c ~ dnorm(b + b^2, 1)
})
constants <- list()
data <- list(b=0, c=0)
inits <- list(a=0)

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$printSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(100000, time = TRUE)
Cmcmc$getTimes()
Cmcmc$run(100000, time = TRUE, reset = FALSE)
Cmcmc$getTimes()



## error from Perry about not finding nimArray
## but I can't reproduce this error

library(nimble)

Rmodel <- readBUGSmodel('birats2.bug', dir = getBUGSexampleDir('birats'), data = 'birats-data.R', inits = 'birats-inits.R')
##spec <- configureMCMC(Rmodel)
##spec$printSamplers()
##spec$getSamplerDefinition(1)
##spec$getSamplerDefinition(4)
##mcmc <- buildMCMC(spec)
mcmc <- buildMCMC(Rmodel)
mcmc$run(1)
##Error in samplerFunctions[[i]]$run() : could not find function "nimArray"

source(system.file(file.path('tests', 'test_utils.R'), package = 'nimble'))
test_mcmc('birats', model = 'birats2.bug', inits = 'birats-inits.R', data = 'birats-data.R', numItsC = 1000, resampleData = TRUE)




## Soledad's compilation error

library(nimble)
x <- c(2, 2, 3)

## works
test1 <- nimbleFunction ( run = function (indx = double(1) ) {
    if(  length(indx)==2 & indx[1]==indx[2] ) {
        aa <- indx[1] 
    } else {
        newindx <- indx
        aa         <- length(newindx)
    }
    output <- 1 + aa
    returnType(double(0))
    return(output)
})

## doesn't work
test1 <- nimbleFunction(run = function(indx = double(1)) {
    if(  length(indx)==2 & indx[1]==indx[2] ) {
        newindx <- indx[1]
        aa          <- newindx     ## THIS IS THE MAIN CHANGE IN THIS FUNCTION
    } else {
        newindx <- indx
        aa          <- length(newindx)
    }
    output <- 1 + aa
    returnType(double(0))
    return(output)
})

Ctest1 <- compileNimble(test1)


test1(x)
Ctest1(x)





## using a custom distribution to have flexible length data
## for floriane plard population models

library(nimble)


dxxx <- nimbleFunction(
    run = function(x = double(1), mu = double(1), sigma = double(1), length = integer(), log.p = double()) {
        ll <- 0
        for(i in 1:length) {
            ll <- ll + dnorm(x[i], mu[i], sd=sigma[i], log=TRUE)
        }
        returnType(double())
        if(log.p) return(ll) else return(exp(ll))
    }
)

rxxx <- nimbleFunction(
    run = function(n = integer(), mu = double(1), sigma = double(1), length = integer()) {
        print('this should never run')
        x <- numeric(length)
        return(x)
    }
)

registerDistributions(list(
    dxxx = list(
        BUGSdist = 'dxxx(mu, sigma, length)',
        types    = c('value = double(1)', 'mu = double(1)', 'sigma = double(1)', 'length = integer()')
    )
))

code <- nimbleCode({
    y[1:N] ~ dxxx(mu[1:N], sigma[1:N], length)
})
constants <- list(N = 10)
data <- list()
inits <- list(length = 10)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$length
Rmodel$mu
Rmodel$sigma
Rmodel$y
Rmodel$getNodeNames(dataOnly = TRUE)

length <- 10
length
mu <- ((1:10)/2)[1:length]
mu
sigma <- (1+(1:10)/10)[1:length]
sigma
y <- c(1,0,1,3,2,5,4,6,7,5)[1:length]
y
yaug <- c(y, rep(as.numeric(NA), 10-length))
yaug

Rmodel$length <- length
Rmodel$length
Rmodel$mu[1:length] <- mu
Rmodel$mu
Rmodel$sigma[1:length] <- sigma
Rmodel$sigma
Rmodel$resetData()
Rmodel$getNodeNames(dataOnly = TRUE)
Rmodel$setData(list(y=yaug))
Rmodel$getNodeNames(dataOnly = TRUE)
Rmodel$y

sum(dnorm(y, mu, sd=sigma, log=TRUE))
calculate(Rmodel)


debug(exprClasses_setSizes)



## testing MCEM for pump model
library(nimble)

pumpCode <- nimbleCode({ 
  for (i in 1:N){
      theta[i] ~ dgamma(alpha,beta) 
      lambda[i] <- theta[i]*t[i]
      x[i] ~ dpois(lambda[i]) 
  }
  alpha ~ dexp(1.0) 
  beta ~ dgamma(0.1,1.0) 
}) 
pumpConsts <- list(N = 10,
                   t = c(94.3, 15.7, 62.9, 126, 5.24,
                       31.4, 1.05, 1.05, 2.1, 10.5)) 
pumpData <- list(x = c(5, 1, 5, 14, 3, 19, 1, 1, 4, 22)) 
pumpInits <- list(alpha = 1, beta = 1,
                  theta = rep(0.1, pumpConsts$N)) 
newPump <- nimbleModel(code = pumpCode, name = 'pump', constants = pumpConsts,
                    data = pumpData, inits = pumpInits) 

pumpMCEM <- buildMCEM(model = newPump,
                      latentNodes = 'theta',
                      burnIn = 100,
                      mcmcControl = list(adaptInterval = 20),
                      boxConstraints = list( list( c('alpha', 'beta'), 
                          limits = c(0, Inf) ) ), 
                      buffer = 1e-6)

pumpMCEM(maxit = 20, m1 = 250, m2 = 500)
pumpMCEM(maxit = 50, m1 = 1000, m2 = 5000)


## testing adding numeric(), integer(), array(), matrix()
library(nimble)
Rfun <- nimbleFunction(run = function() {
    ##ans <- numeric(10, value = 2)
    ##
    #x <- 100 
    #ans <- integer(x) 
    #for(i in 1:x) {
    #    ans[i] <- i 
    #}
    ##
    ##ans <- matrix(1, nrow = 10, ncol = 1)
    ##y <- 3
    ##ans <- numeric(10, value = y) 
    ##ans <- array(y, dim = 10) 
    ##ans <- array(y, dim = c(10))
    ##z <- numeric(10)
    ##z[5] <- 3
    ##x <- 20
    ##y <- 30
    ##ans <- integer(z[5], value = x + y) 
    ##ans <- array(x+y, dim = z[5], type = 'integer')
    ##ans <- array(x+y, dim = c(z[5]), type = 'integer')
    ##
    ##x <- array(0, c(4,5))
    ##ans <- matrix(0, nrow = dim(x)[1], ncol = dim(x)[2]) 
    ##ans <- array(0, dim = c(dim(x)[1], dim(x)[2]))
    ##
    x <- 1
    y <- 2
    z <- 3
    ans <- array(0, dim = c(x, y, z))
    ##returnType(double(1))
    ##returnType(integer(1))
    ##returnType(double(2))
    returnType(double(3))
    return(ans)
})
Cfun <- compileNimble(Rfun)

Rfun()
Cfun()
class(Rfun()[1])
class(Cfun()[1])
class(Rfun()[1,1])
class(Cfun()[1,1])
class(Rfun()[1,1,1])
class(Cfun()[1,1,1])

## creates a length-100 integer vector, containing 1, 2, ..., 100 

## creates a 10x1 ones-matrix 

## the following three lines are equivalent 
## each creates a length-10 vector, with all elements equal to y 

## the following two lines are equivalent 
## each creates an integer vector of length z[5], with all elements equal to x+y 

## the following two lines are equivalent 
## each one creates a matrix of 0's of the same size as matrix x 

## the following creates a 3-dimensional array of 0's 



## testing adding numeric(), integer(), array(), matrix()
library(nimble)
library(testthat)
##source(system.file(file.path('tests', 'test_utils.R'), package = 'nimble'))

expected <- numeric(10)
Rfun <- nimbleFunction(run = function() {
    ans <- numeric(10)
    returnType(double(1))
    return(ans)
})
Cfun <- compileNimble(Rfun)
test_that('numeric', expect_equal(Rfun(), expected))
test_that('numeric', expect_equal(Cfun(), expected))
test_that('numeric', expect_identical(class(Rfun()[1]), 'numeric'))
test_that('numeric', expect_identical(class(Cfun()[1]), 'numeric'))


expected <- rep(3, length = 2)
Rfun <- nimbleFunction(run = function() {
    ans <- numeric(value = 3, length = 2)
    returnType(double(1))
    return(ans)
})
Cfun <- compileNimble(Rfun)
test_that('numeric', expect_equal(Rfun(), expected))
test_that('numeric', expect_equal(Cfun(), expected))
test_that('numeric', expect_identical(class(Rfun()[1]), 'numeric'))
test_that('numeric', expect_identical(class(Cfun()[1]), 'numeric'))

expected <- rep(9, 3)
Rfun <- nimbleFunction(run = function() {
    x <- numeric(10, value = 3)
    ans <- integer(x[2], x[2]+x[3]*2)
    returnType(integer(1))
    return(ans)
})
Cfun <- compileNimble(Rfun)
test_that('integer', expect_equal(Rfun(), expected))
test_that('integer', expect_equal(Cfun(), expected))
test_that('integer', expect_identical(class(Rfun()[1]), 'integer'))
test_that('integer', expect_identical(class(Cfun()[1]), 'integer'))

expected <- array(4, c(10,11))
Rfun <- nimbleFunction(run = function() {
    ans <- array(4, c(10,11))
    returnType(double(2))
    return(ans)
})
Cfun <- compileNimble(Rfun)
test_that('integer', expect_equal(Rfun(), expected))
test_that('integer', expect_equal(Cfun(), expected))
test_that('integer', expect_identical(class(Rfun()[1]), 'numeric'))
test_that('integer', expect_identical(class(Cfun()[1]), 'numeric'))

expected <- matrix(as.integer(0), nrow=4, ncol=5)
Rfun <- nimbleFunction(run = function() {
    x <- 4
    y <- 5
    ans <- matrix(init=FALSE, nrow=x, ncol=y, type='integer')
    returnType(integer(2))
    return(ans)
})
Cfun <- compileNimble(Rfun)
test_that('integer', expect_equal(Rfun(), expected))
test_that('integer', expect_equal(Cfun(), expected))
test_that('integer', expect_identical(class(Rfun()[1,1]), 'integer'))
test_that('integer', expect_identical(class(Cfun()[1,1]), 'integer'))




expected
Rfun()
Cfun()


## testing adding numeric(), integer(), array(), matrix()
library(nimble)
nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        x <- 2
        bbb <- array(3, x+10, type='integer')
        ##print(bbb)
        bbb2 <- array(value=3, dim=c(3,4), type='double')
        ##print(bbb2)
        ccc <- array(value=3, dim=c(3,4), type='double', init=FALSE)
        ##print(ccc)
        x <- 5
        y <- matrix(2, ncol = 3, nrow = x)
        ##print(y)
        zxc <- array(2, c(x, dim(y)[2], y[2,2]))
        zxc[1,1,1] <- 98
        ##returnType(double(3));  return(zxc)
        zz <- numeric(4)
        zz[1] <- 99
        ##print(zz)
        z <- numeric(value=.5, 4)
        ##print(z)
        zz[2:4] <- z[1:3] + numeric(3, 10)
        ##print(zz)
        mat <- matrix(0, 5, 5)
        mat[1, 1] <- 97
        ##print(mat)
        arr <- array(4, type = 'integer', init = FALSE, dim=c(3,4))
        ##print(arr)
        ar2 <- array(dim = dim(z)[1])
        ##print(ar2)
        ar3 <- array(dim = c(dim(z)[1]))
        ##print(ar3)
        ar4 <- array(dim = c(dim(arr)[1], dim(arr)[2]), type = 'integer')
        ##print(ar4)
    }
)
Rnf <- nfDef()
##Rnf$run
Cnf <- compileNimble(Rnf, dirName)

Rnf$run()

Cnf$run()



tempdir()

##Cnf$run()



## ???

library(nimble)
sampler_dt <- nimbleFunction(
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        browser()
        1
        2
        3
        ###  node list generation  ###
        calcNodes  <- model$getDependencies(target)
    },
    run = function() {
        simulate(model, target)
        calculate(model, calcNodes)
        nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
    },
    methods = list(
        reset = function() { }
    ), where = getLoadingNamespace()
)
pumpCode <- nimbleCode({
	for(i in 1:N){
		theta[i] ~ dgamma(alpha,beta)
		lambda[i] <- theta[i]*t[i]
		x[i] ~ dpois(lambda[i])
	}
	alpha ~ dexp(1.0)
        alpha2 ~ dnorm(alpha, 1)
	beta ~ dpois(lambda=3)  # Make beta discrete valued
})
pumpConsts <- list(N=10,t=c(94.3,15.7,62.9,126,5.24,31.4,1.05,1.05,2.1,10.5))
pumpData <- list(x=c(5,1,5,14,3,19,1,1,4,22))
pumpInits <- list(alpha=1,beta=1,theta=rep(0.1,pumpConsts$N))
Rmodel <- nimbleModel(code=pumpCode,name='pump',constants=pumpConsts,data=pumpData,inits=pumpInits)

spec <- configureMCMC(Rmodel)
spec$getMonitors()
spec$addMonitors(c('theta', 'alpha2'))
spec$getMonitors()
spec$printSamplers()
spec$addSampler(type = 'dt', target = 'alpha')
spec$printSamplers()

Rmcmc <- buildMCMC(spec)

## testing new function in modelBaseClass: model$getNodeFunctions()
library(nimble)
pumpCode <- nimbleCode({
	for(i in 1:N){
		theta[i] ~ dgamma(alpha,beta)
		lambda[i] <- theta[i]*t[i]
		x[i] ~ dpois(lambda[i])
	}
	alpha ~ dexp(1.0)
        alpha2 ~ dnorm(alpha, 1)
	beta ~ dpois(lambda=3)  # Make beta discrete valued
})
pumpConsts <- list(N=10,t=c(94.3,15.7,62.9,126,5.24,31.4,1.05,1.05,2.1,10.5))
pumpData <- list(x=c(5,1,5,14,3,19,1,1,4,22))
pumpInits <- list(alpha=1,beta=1,theta=rep(0.1,pumpConsts$N))
Rmodel <- nimbleModel(code=pumpCode,name='pump',constants=pumpConsts,data=pumpData,inits=pumpInits)
Cmodel <- compileNimble(Rmodel)

nodes <- c('theta[3]')
nodes <- c('theta[3]', 'x[10]')
nodes <- c('theta[3]', 'x[9:10]')
Rmodel$getNodeFunctions(nodes)$calculate
Cmodel$getNodeFunctions(nodes)[[1]]$calculate

Rmodel$getNodeNames()
modelDef <- Rmodel$getModelDef()
gids <- mdef$nodeName2GraphIDs(nodes)
gids
mdef$graphIDs2indexedNodeInfo(gids)
Rmodel$nodeFunctions[[2]]$calculate


## making dmnorm conjugate sampler work with dependent
## nodes of different size from target node
library(nimble)
source(system.file(file.path('tests', 'test_utils.R'), package = 'nimble'))

code <- nimbleCode({
    x[1:3] ~ dmnorm(mu0[1:3], prec = ident[1:3,1:3])
    mu_y2[1:2] <- asCol(a[1:2]) + B[1:2,1:3] %*% asCol(x[1:3])
    mu_y3[1:3] <- asCol(a[1:3]) + B[1:3,1:3] %*% asCol(x[1:3])
    mu_y5[1:5] <- asCol(a[1:5]) + B[1:5,1:3] %*% asCol(x[1:3])
    y2[1:2] ~ dmnorm(mu_y2[1:2], prec = prec_y[1:2,1:2])
    y3[1:3] ~ dmnorm(mu_y3[1:3], prec = prec_y[1:3,1:3])
    y5[1:5] ~ dmnorm(mu_y5[1:5], prec = prec_y[1:5,1:5])
})

mu0 <- rep(0,3)
ident <- diag(3)
a <- 11:15
B <- matrix(1:15, nrow=5, ncol=3, byrow=TRUE)
prec_y <- diag(1:5)

constants <- list(mu0=mu0, ident=ident, a=a, B=B, prec_y=prec_y)
data <- list(y2=1:2, y3=1:3, y5=1:5)
inits <- list(x=rep(0,3))

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
##spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)

set.seed(0)
Cmcmc$run(10)

Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rsamples - Csamples

Rsamples[c(1,10),]
Csamples[c(1,10),]
##         x[1]       x[2]      x[3]
##[1,] 4.966869 -1.1252473 -3.922769
##[2,] 4.972376 -0.5288582 -4.511282


##node <- quote(mu0[1:3])
##node <- quote(prec_y[1:2, 1:2])
##node <- quote(prec_y[1:3, 1:3])
##node <- quote(prec_y[1:5, 1:5])
##nimble:::cc_expandDetermNodesInExpr(Rmodel, node)
##debug(nimble:::cc_expandDetermNodesInExpr)
##undebug(nimble:::cc_expandDetermNodesInExpr)
##debug(Rmcmc$samplerFunctions$contentsList[[1]]$run)
##a <- spec$getSamplerDefinition(1)
##createNamedObjectsFromList(a, writeToFile = '~/temp/del.R')







## ???
library(nimble)

d <- read.csv("http://personal.bgsu.edu/~albert/data/gateway.csv")
library(dplyr)
d$month <- as.numeric(d$Month)
d$year <- d$Year - 2002

worshipCode <- nimbleCode({ 
  for (i in 1:N){
  y[i] ~ dpois(lambda[i])
  log(lambda[i]) <- mu + epsilon[i] + alpha[month[i]]
  epsilon[i] ~ dnorm(0, sd=sigma.y)
  }
  for (j in 1:J){
  alpha[j] ~ dnorm(0, sd=sigma.month)
}
mu ~ dnorm(0, sd=1000)
sigma.y ~ dunif(0, 100)
sigma.month ~ dunif(0, 100)
})

worshipConsts <- list(N=484,
                      J=12,
                      K=10,
                      month=d$month)
worshipData <- list(y=d$Count)
worshipInits <- list(mu=5, sigma.y=.4, sigma.month=.4,
                     alpha=rep(0, 12))

worship <- nimbleModel(code = worshipCode, name = 'worship', 
                    constants = worshipConsts,
                    data = worshipData,
                    inits=worshipInits)

Rmodel <- worship

spec <- configureMCMC(Rmodel)
spec$printSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)


## error produced in model$check() when using equals()
library(nimble)
code <- nimbleCode({
    for (i in 1:5) {
        x[i] ~ dbern(0.5)
        ##y[i] <- equals(x[i], 0)
    }
    sss <- sum(x[1:5])
    none <- equals(sss, 0)
})
constants <- list()
data <- list()
inits <- list(x = c(0,0,1,1,1))

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$x
Rmodel$sss
Rmodel$none

## testing new log=TRUE option of RW sampler

library(nimble)
code <- nimbleCode({
    sigma ~ dunif(0,10)
    a ~ dnorm(0, sd=sigma)
    b ~ dnorm(sigma, tau=3)
})
constants <- list()
data <- list(a=1, b=4)
inits <- list(sigma=1)

Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)
spec <- configureMCMC(Rmodel, nodes=NULL)

spec$addSampler('sigma', 'RW_log')
### --- or ---
spec$addSampler('sigma', 'RW', control=list(log=TRUE))

Rmcmc <- buildMCMC(spec)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
as.matrix(Rmcmc$mvSamples)

set.seed(0)
Cmcmc$run(10)
as.matrix(Cmcmc$mvSamples)

##         sigma
## [1,] 3.535852
## [2,] 3.535852
## [3,] 3.535852
## [4,] 5.352671
## [5,] 5.352671
## [6,] 3.986347
## [7,] 3.963423
## [8,] 3.963423
## [9,] 3.963423
##[10,] 3.963423




## Github issue #107
## initializeModel issues false warning about RHSonly variable not initialized
## reduced case
library(nimble)

code <- nimbleCode({
    for (n in 1:N){
        a0[n,1:3] ~ dmnorm(mu[1:3], Sigma[1:3,1:3])
        ##lambda[n,1:3] <- exp(a0[n,1:3]) ## alternative, which shifts the problem to lambda
        for (k in 1:K){
            lambda[n,k] <- exp(a0[n,k])
            y[n,k]~dpois(lambda[n,k])
        }
    }
})

constants <- list(N = 3, K = 3)
data <- list(y = matrix(rpois(9, 3), nrow=3))
inits <- list(mu = rep(0,3), Sigma = diag(3))

Rmodel <- nimbleModel(code, constants, data, inits)

Cmodel <- compileNimble(Rmodel)

Rmcmc <- buildMCMC(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Rmcmc$run(1)
Cmcmc$run(1)



## testing new binary sampler
library(nimble)

code <- nimbleCode({
    a ~ dbern(0.5)
    b ~ dbern(0.6)
    c ~ dbern(0.05)
    d ~ dbin(prob=0.2, size=1)
    e ~ dbinom(prob=0.9, size=1)
    f ~ dbern(0.5)
    g ~ dbern(0.5)
    h ~ dbern(0.5)
    for(i in 1:10)
        yf[i] ~ dnorm(f, sd = 1)
    for(i in 1:10)
        yg[i] ~ dnorm(g, sd = 1)
    for(i in 1:10)
        yh[i] ~ dnorm(h, sd = 1)
    ##x ~ dnorm(0,1)
    ##y ~ dnorm(x*x, 1)
    ##z ~ dnorm(x*y, 1)
    ##zz ~ dnorm(z + z, 1)
})
constants <- list()
data <- list(yf = c(rep(0,2), rep(1,8)), yg = c(rep(0,8), rep(1,2)), yh = c(rep(0,5), rep(1,5)))
inits <- list(a=0, b=0, c=0, d=0, e=0, f=0, g=0, h=0)
Rmodel <- nimbleModel(code, constants, data, inits)

##spec <- configureMCMC(Rmodel, autoBlock = TRUE)
##spec$printSamplers()

Rmodel$isBinary('a')
Rmodel$isBinary('b')
Rmodel$isBinary('c')
Rmodel$isBinary('d')
Rmodel$isBinary('e')
Rmodel$isBinary('f')
Rmodel$isBinary('g')
Rmodel$isBinary('h')

spec <- configureMCMC(Rmodel, nodes = NULL)
spec$addSampler('a', 'binary', print=FALSE)
spec$addSampler('b', 'binary', print=FALSE)
spec$addSampler('c', 'binary', print=FALSE)
spec$addSampler('d', 'binary', print=FALSE)
spec$addSampler('e', 'binary', print=FALSE)
spec$addSampler('f', 'binary', print=FALSE)
spec$addSampler('g', 'binary', print=FALSE)
spec$addSampler('h', 'binary', print=FALSE)
spec$printSamplers()
Rmcmc <- buildMCMC(spec)
##Rmcmc$run(3)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(100000)
samples <- as.matrix(Cmcmc$mvSamples)
means <- apply(samples, 2, mean)
means


# Slice sampler name conflict with library(bbmle) 'slice' function
library(bbmle)
library(nimble)  # No messages about slice are generated

pumpCode <- nimbleCode({
	for(i in 1:N){
		theta[i] ~ dgamma(alpha,beta)
		lambda[i] <- theta[i]*t[i]
		x[i] ~ dpois(lambda[i])
	}
	alpha ~ dexp(1.0)
        alpha2 ~ dnorm(alpha, 1)
	beta ~ dpois(lambda=3)  # Make beta discrete valued
})

pumpConsts <- list(N=10,t=c(94.3,15.7,62.9,126,5.24,31.4,1.05,1.05,2.1,10.5))
pumpData <- list(x=c(5,1,5,14,3,19,1,1,4,22))
pumpInits <- list(alpha=1,beta=1,theta=rep(0.1,pumpConsts$N))
Rmodel <- nimbleModel(code=pumpCode,name='pump',constants=pumpConsts,data=pumpData,inits=pumpInits)

spec <- configureMCMC(Rmodel)

spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(2000000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)


## testing passing a compiled model as a setup argument to a NF

library(nimble)

code <- nimbleCode({
     a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)

nfDef <- nimbleFunction(
    setup = function(model, node) {},
    run = function() {
        simulate(model, node)
    }
)

Rnf <- nfDef(Rmodel, 'a')
Rnf <- nfDef(Cmodel, 'a')

Cnf <- compileNimble(Rnf, project = Rmodel)

set.seed(0)
Rmodel$a
Rnf$run()
Rmodel$a

set.seed(0)
Cmodel$a
Cnf$run()
Cmodel$a



## testing passing a compiled model object to configureMCMC(), or buildMCMC()

library(nimble)

code <- nimbleCode({
     a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)

###
Rmcmc <- buildMCMC(Cmodel)

###
spec <- configureMCMC(Cmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
as.matrix(Rmcmc$mvSamples)

set.seed(0)
Cmcmc$run(10)
as.matrix(Cmcmc$mvSamples)


## testing problem with covariance matrix in BUGS model causing a chol() error at time of model checking

library(nimble)

code <- nimbleCode({
    C[1,1] <- 1
    C[1,2] <- 0
    C[2,1] <- 0
    C[2,2] <- 1
    mu[1] <- 0
    mu[2] <- 0
    y[1:2] ~ dmnorm(mu[1:2], prec = C[1:2, 1:2])
})
constants <- list()
data <- list(y = c(0,0))
inits <- list()

Rmodel <- nimbleModel(code, constants, data, inits)





##If you're interested, here's the ecological/scientific background to the project: the potato psyllid spreads a disease called Zebra Chip Disease that is damaging potato and tomato crops in California and western US and Mexico. The disease is new, having only been described about 20 years ago, but the potato psyllid is native to the area. So what's changed to cause the disease outbreaks? I'm using data from where and when potato psyllid museum specimens were collected in California to test if psyllid populations have increased over the last century and if climate change may play a role. If so, this could explain the disease outbreaks. It's complicated though because museum data is messy, specifically, the museum specimens give me presence-only information. So I'm using lists of related species collected at the same time as the psyllids to infer psyllid absences, based on an assumed correlation between number of species collected and collecting effort. This is the "list_length" covariate that's in the model.

##BLOCK SAMPLE PARAMS THAT GO IN THE SAME LINEAR PREDICTOR,
##i.e. all the betapN terms in this one

{
    ## Priors
    ## For the site random effect
    for(j in 1:nsite) { 
        alpha[j] ~ dnorm(mu.alpha, tau.alpha) 
    }
    mu.alpha ~ dnorm(0, 0.001)
    tau.alpha <- 1 / (sigma.alpha * sigma.alpha)
    sigma.alpha ~ dunif(0, 5)
    
    ## Grand mean
    muq ~ dnorm(0, 0.001)
    
    ## For the fixed effect coefficients
    betaq ~ dnorm(0, 0.001)
    betap1 ~ dnorm(0, 0.001)
    betap2 ~ dnorm(0, 0.001)
    betap3 ~ dnorm(0, 0.001)
    betap4 ~ dnorm(0, 0.001)
    betap5 ~ dnorm(0, 0.001)
    betap6 ~ dnorm(0, 0.001)
    betap7 ~ dnorm(0, 0.001)
    betap8 ~ dnorm(0, 0.001)

    ## Likelihood
    for (i in 1:nlist){ # i = events (year-months)
        for(j in 1:nsite) { # j = sites

            logit(p[i,j]) <- betap1*year[i,j] + betap2*pow(year[i,j],2) + # Year quadratic effects
                betap2*month[i,j] + betap3*pow(month[i,j],2) + # month quadratic effects
                    betap4*aet[i,j] + betap5*cwd[i,j] + betap6*tmn[i,j] + betap7*tmx[i,j] + # Climate effects
                        alpha[j] # Random effects

            Y[i,j] ~ dbern(p[i,j]) # Occupancy probability

            logit(q[i,j]) <- Y[i,j] + muq + betaq*list_length[i,j] # Logistic regression for detection

            detectionMatrix[i,j] ~ dbern(q[i,j])  # Distribution for random part; observed presences relating to detection probability

        } #j
    } #i
}



## bug with forwardsolve in nimbleFunctions

library(nimble)
A <- matrix(c(2,1,0,3), nrow = 2)
b <- c(1,5)

nf <- nimbleFunction(
    run = function(A = double(2), b = double(1)) {
        declare(x, double(1))
        setSize(x, 2)
        x[1:2] <- forwardsolve(A[1:2, 1:2], b[1:2])
        ##x <- forwardsolve(A[1:2,1:2], b[1:2])
        x[1:2,1:2] <- chol(A[1:2,1:2])
        returnType(double(1))
        ##declare(x, double(2))
        ##setSize(x, 2, 2)
        ##returnType(double(2))
        return(x)
    })

cnf <- compileNimble(nf, dirName = '.')

nf(A,b)
cnf(A,b)



## bug with forwardsolve in BUGS models

library(nimble)

code <- nimbleCode({
    A[1,1] <- 2
    A[2,1] <- 1
    A[1,2] <- 0
    A[2,2] <- 3
    b[1] <- 1
    b[2] <- 5
    x[1:2] <- forwardsolve(A[1:2, 1:2], b[1:2])
})

Rmodel <- nimbleModel(code)
Cmodel <- compileNimble(Rmodel)

Cmodel$A
Cmodel$b
Cmodel$x
calculate(Cmodel)
## Process R floating point exception: 8 at Thu Apr  7 14:38:02 2016



## testing using forwardsolve and backsolve in a user-defined function
## in a BUGS model

library(nimble)
A <- array(NA, c(2,2))
A[1,1] <- 2
A[2,1] <- 1
A[1,2] <- 0
A[2,2] <- 3
b <- c(1,5)

myFS <- nimbleFunction(
    run = function(A = double(2), b = double(1)) {
        returnType(double(1))
        ret <- forwardsolve(A, b)
        return(ret)
    }
)

myChol <- nimbleFunction(
    run = function(A = double(2)) {
        returnType(double(2))
        ret <- chol(A)
        return(ret)
    }
)

myInverse <- nimbleFunction(
    run = function(A = double(2)) {
        returnType(double(2))
        ret <- inverse(A)
        return(ret)
    }
)

code <- nimbleCode({
    x1[1:2] <- forwardsolve(A[1:2, 1:2], b[1:2])  ## forwardsolve() directly here does NOT work
    x2[1:2] <- myFS(A = A[1:2, 1:2], b = b[1:2])
    C1[1:2,1:2] <- chol(A[1:2, 1:2])
    C2[1:2,1:2] <- myChol(A[1:2, 1:2])
    I1[1:2,1:2] <- inverse(A[1:2, 1:2])
    I2[1:2,1:2] <- myInverse(A[1:2, 1:2])
    xx1[1:2,1:2] <- chol(A[1:2, 1:2]) + 2 * inverse(A[1:2, 1:2])
    xx2[1:2,1:2] <- myInverse(A[1:2, 1:2]) * 2 + myChol(A[1:2, 1:2])
})

Rmodel <- nimbleModel(code, constants = list(A=A, b=b))
Cmodel <- compileNimble(Rmodel)

m <- Rmodel
m <- Cmodel

calculate(m)
m$x1; m$x2
m$C1; m$C2
m$I1; m$I2
m$xx1; m$xx2

## another example of MVN conjugate sampler, for test-mcmc.R
## using both cov and prec parametrizaions of MVN,
## and various linear links

library(nimble)

set.seed(0)
mu0 <- rep(0,5)
ident <- diag(5)
a <- array(rnorm(20), c(4,5))
B <- array(NA, c(4, 5, 5))
for(i in c(1,3)) {
    A <- array(rnorm(25,i), c(5,5))
    A <- A + t(A) + 5*i*diag(5)
    B[i,,] <- A
}
B[2,,] <- diag(5)
B[4,,] <- diag(5)
M_y <- array(NA, c(4, 5, 5))
for(i in 1:4)
    M_y[i,,] <- i * diag(5)
x <- array(0, c(4,5))
y <- array(0, c(4,5))

code <- nimbleCode({
    for(i in 1:2)
        x[i,1:5] ~ dmnorm(mu0[1:5], prec = ident[1:5,1:5])
    for(i in 3:4)
        x[i,1:5] ~ dmnorm(mu0[1:5], cov  = ident[1:5,1:5])
    for(i in 1:4)
        mu_y[i,1:5] <- asCol(a[i,1:5]) + B[i,1:5,1:5] %*% asCol(x[i,1:5])
    y[1,1:5] ~ dmnorm(mu_y[1,1:5], prec = M_y[1,1:5,1:5])
    y[2,1:5] ~ dmnorm(mu_y[2,1:5], cov  = M_y[2,1:5,1:5])
    y[3,1:5] ~ dmnorm(mu_y[3,1:5], prec = M_y[3,1:5,1:5])
    y[4,1:5] ~ dmnorm(mu_y[4,1:5], cov  = M_y[4,1:5,1:5])
})
constants <- list(mu0=mu0, ident=ident, a=a, B=B, M_y=M_y)
data <- list(y=y)
inits <- list(x=x)
Rmodel <- nimbleModel(code, constants, data, inits)
spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
Rsamples <- as.matrix(Rmcmc$mvSamples)

set.seed(0)
Cmcmc$run(10)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rsamples - Csamples

Rsamples[c(1,10),]
Csamples[c(1,10),]
##        x[1, 1]     x[2, 1]     x[3, 1]    x[4, 1]     x[1, 2]   x[2, 2]
##[1,] -0.7098220 -0.92579244  0.02859492 -0.9271968 -0.03440439 0.1133790
##[2,] -0.6642612  0.09915454 -0.13079585  0.6298495 -0.08432850 0.8806305
##        x[3, 2]    x[4, 2]     x[1, 3]   x[2, 3]     x[3, 3]    x[4, 3]
##[1,] 0.09213596  0.3257091  0.27465408 -1.410726 -0.12937422 -0.2311789
##[2,] 0.01777368 -0.6051182 -0.09322287 -1.723685  0.07759651  0.4119755
##       x[1, 4]     x[2, 4]     x[3, 4]     x[4, 4]   x[1, 5]  x[2, 5]
##[1,] 0.5071505 0.140652780  0.01778143  0.51383003 0.3639871 2.146307
##[2,] 0.4781541 0.002361055 -0.02517465 -0.06995162 0.4202336 1.225317
##         x[3, 5]    x[4, 5]
##[1,] -0.02913630 -0.2563026
##[2,]  0.04303507  0.2716980




## Richard McElreath Statistical Rethinking package,
## might have lots of good data and hierarchical (Bayesian) modeling examples
install.packages('rethinking')
library(rethinking)



## tesing MVN conjugate sampler

library(nimble)

set.seed(0)
mu0 = 1:3
Q0 = matrix(c(1, .2, .8, .2, 2, 1, .8, 1, 2), nrow = 3)
Q = solve(matrix(c(3, 1.7, .9, 1.7, 2, .6, .9, .6, 1), nrow = 3))
a = c(-2, .5, 1)
B = matrix(rnorm(9), 3)

code <- nimbleCode({
  mu[1:3] ~ dmnorm(mu0[1:3], Q0[1:3, 1:3])
  y_mean[1:3] <- asCol(a[1:3]) + B[1:3, 1:3] %*% asCol(mu[1:3])
  y[1:3] ~ dmnorm(y_mean[1:3], Q[1:3, 1:3])
})
mu <- mu0 + chol(solve(Q0)) %*% rnorm(3)
y <- c(a + B%*%mu + chol(solve(Q)) %*% rnorm(3))
data = list(mu0 = mu0, Q0 = Q0, Q = Q, a = a, B = B, y = y)
muQtrue = t(B) %*% Q%*%B + Q0
muMeanTrue = c(solve(muQtrue, crossprod(B, Q%*%(y-a)) + Q0%*%mu0))

Rmodel <- nimbleModel(code, data= data)
spec <- configureMCMC(Rmodel)
spec$getSamplers()
##spec$getSamplerDefinition(1)
Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)

Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
Rsamples <- as.matrix(Rmcmc$mvSamples)

set.seed(0)
Cmcmc$run(10)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rsamples
Csamples
##         mu[1]    mu[2]    mu[3]
## [1,] 3.877574 2.201065 1.084173
## [2,] 2.537805 1.916830 2.084021
## [3,] 4.426193 2.326982 1.567054
## [4,] 2.456652 1.921560 1.892777
## [5,] 2.959890 2.133226 1.506502
## [6,] 3.084360 1.560040 1.941621
## [7,] 3.178667 2.072212 2.611881
## [8,] 2.996492 2.210513 2.795391
## [9,] 2.494759 1.539610 2.118232
##[10,] 2.880285 1.826356 1.805385

Rsamples - Csamples
##              mu[1] mu[2] mu[3]
## [1,] -8.881784e-16     0     0
## [2,]  0.000000e+00     0     0
## [3,] -8.881784e-16     0     0
## [4,] -4.440892e-16     0     0
## [5,] -4.440892e-16     0     0
## [6,]  0.000000e+00     0     0
## [7,] -4.440892e-16     0     0
## [8,] -4.440892e-16     0     0
## [9,]  0.000000e+00     0     0
##[10,] -4.440892e-16     0     0


## inconsistent behavior between R and C chol()

n <- 4
set.seed(0)
tmp <- array(rnorm(n^2), c(n,n)) + n*diag(n)
A <- tmp + t(tmp)
R <- chol(A)
L <- t(R)
B <- array(as.numeric(1:n^2), c(n,n))
b <- as.numeric(1:n)
mat <- A + L

sym <- function(mat, lower) {
    ind <- if(lower) lower.tri(mat) else upper.tri(mat)
    ans <- mat
    ans[!ind] <- 0
    ans <- ans + t(ans) + diag(diag(mat))
    ans
}

## R execution
chol(mat)
chol(sym(mat, FALSE))
chol(mat) - chol(sym(mat, FALSE)) ## R takes the UPPER part of the matrix

## Rnf execution
Rnf$chol_oPA_plus_L_cP() - chol(sym(mat, FALSE))  ## Rnf also UPPER part of matrix

## Cnf execution
Cnf$chol_oPA_plus_L_cP() - chol(sym(mat, FALSE))  ## Cnf now also uses UPPER part of matrix





## test case for forwardsolve, backsolve in the DSL
## A.triangularView<Eigen::Upper>().solve(b);
## could make a file called nimble_macros.h:
##define ......

library(nimble)
## inputs
n <- 4
tests <- c(
    'chol(A)',
    'forwardsolve(A, b)', 'forwardsolve(A, B)', 'forwardsolve(L, b)', 'forwardsolve(L, B)',
       'backsolve(A, b)',    'backsolve(A, B)',    'backsolve(R, b)',    'backsolve(R, B)',
           'solve(A, b)',        'solve(A, B)',        'solve(R, b)',        'solve(R, B)'
)
## derived objects
testNames <- as.character(sapply(tests, nimble:::Rname2CppName))
testDims <- sapply(grepl('_b_', testNames), function(x) if(x) 1 else 2)
methods <- mapply(function(test, dim) eval(substitute(function() { ans <- TEST; returnType(double(DIM)); return(ans) }, list(TEST = parse(text=test)[[1]], DIM = dim))), tests, testDims)
names(methods) <- testNames
set.seed(0)
tmp <- array(rnorm(n^2), c(n,n)) + n*diag(n)
A <- tmp + t(tmp)
R <- chol(A)
L <- t(R)
B <- array(as.numeric(1:n^2), c(n,n))
b <- as.numeric(1:n)
nfDef <- nimbleFunction(
    setup = function(A, B, b) {
        R <- chol(A)
        L <- t(R)
    },
    run = function() {},
    methods = methods
)
Rnf <- nfDef(A, B, b)

Cnf <- compileNimble(Rnf)

testOneCase <- function(test, testName, Rnf, Cnf) {
    Rans <- eval(parse(text=test)[[1]])
    Rnfans <- eval(substitute(Rnf$TEST(), list(TEST=as.name(testName))))
    Cnfans <- eval(substitute(Cnf$TEST(), list(TEST=as.name(testName))))
    dif <- max(abs(Rans - Rnfans))
    if(dif > 1E-15) return(1)
    dif <- max(abs(Rans - Cnfans))
    if(dif > 1E-15) return(1)
    return(0)
}
for(i in seq_along(tests)) {
    test <- tests[i]
    testName <- testNames[i]
    testResult <- testOneCase(test, testName, Rnf, Cnf)
    if(testResult != 0) message('failed test: ', test)
}




library(nimble)

n <- 3
set.seed(0)
tmp <- array(rnorm(n^2), c(n,n)) + diag(n)
A <- tmp + t(tmp)
B <- array(1:n^2, c(n,n))
b <- as.numeric(1:n)


Rnf <- nimbleFunction(
    run = function(A = double(2), b = double(1), B = double(2)) {
        ##ans2 <- forwardsolve(A, b)
        ##ans3 <- forwardsolve(A, B)
        ans4 <- chol(A + A)
    }
)

Cnf <- compileNimble(Rnf)   ## this works




nfDef <- nimbleFunction(
    setup = function(A, B, b) {},
    run = function() {
        ans2 <- forwardsolve(A, b)
        ##ans3 <- forwardsolve(A, B)
    }
)

Rnf <- nfDef(A, B, b)
Cnf <- compileNimble(Rnf)   ## FAIL


library(nimble)

n <- 3
set.seed(0)
tmp <- array(rnorm(n^2), c(n,n)) + diag(n)
A <- tmp + t(tmp)
B <- array(1:n^2, c(n,n))
b <- as.numeric(1:n)


Rnf <- nimbleFunction(
    run = function(A = double(2), b = double(1), B = double(2)) {
        ans1 <- chol(A)
        print('chol(A)')
        print(ans1)
        ##
        ##ans2 <- forwardsolve(A, b)
        ##print('forwardsolve(A, b)')
        ##print(ans2)
        ####
        ##ans3 <- forwardsolve(A, B)
        ##print('forwardsolve(A, B)')
        ##print(ans3)
        ####
        ##ans4 <- backsolve(A, b)
        ##print('backsolve(A, b)')
        ##print(ans4)
        ####
        ans5 <- backsolve(A, B)
        print('backsolve(A, B)')
        print(ans5)
    }
)

Cnf <- compileNimble(Rnf)


set.seed(0)
tmp <- array(rnorm(9), c(3,3)) + diag(3)
A <- tmp + t(tmp)
B <- array(1:9, c(3,3))
b <- 1:3

chol(A)
forwardsolve(A, b)
forwardsolve(A, B)
backsolve(A, b)
backsolve(A, B)

Rnf(A, b, B)
Cnf(A, b, B)

##debug(exprClasses_labelForEigenization)
##undebug(exprClasses_labelForEigenization)
##debug(exprClasses_eigenize)
##undebug(exprClasses_eigenize)
##writeCode(nimDeparse(code))
##writeCode(nimDeparse(compileInfo$nimExpr))


## using compareMCMCs, and MCMC comparisons

library(nimble)

code1 <- nimbleCode({ ## really toy model
    a ~ dnorm(0, 1)
    b ~ dnorm(a, 1)
    sigma ~ dunif(0.5, 1)
    for(i in 1:3) y[i] ~ dnorm(b, sd = sigma)
})

input1 <- list(code = code1, data = list(y = 1:3), inits = list(a = 0.5)) ## can also provide constants
results1 <- compareMCMCs(input1, MCMCs = c('nimble')) ## run a single case

results2 <- compareMCMCs(input1, MCMCs = c('nimble','noConj')) ## run two more cases
results2[[1]]$efficiency ## inspect
results2[[1]]$summary
results2[[1]]$timing
make_MCMC_comparison_pages(results2, 'model1b') ## could generate a comparison of these

## can combine results as follows
## If the same name (â€œnimbleâ€ in this case) was used multiple times one will have to be renamed:
results2[[1]] <- rename_MCMC_comparison_method('nimble', 'another nimble', results2[[1]])
results3 <- combine_MCMC_comparison_results(results1[[1]], results2[[1]], name = 'combined results')
make_MCMC_comparison_pages(results3, 'model1c')



## testing calc_dmnormConjugacyContributions

nimble:::identityMatrix(3)

calc_dmnormConjugacyContributions(diag(3), diag(3), 1)
calc_dmnormConjugacyContributions(diag(3), 7*diag(3), 1)
calc_dmnormConjugacyContributions(diag(3), 8*diag(3), 2)
calc_dmnormConjugacyContributions(3*diag(3), 1+diag(3), 1)
calc_dmnormConjugacyContributions(3*diag(3), 2+diag(3), 2)

Cnf <- compileNimble(calc_dmnormConjugacyContributions)

Cnf(diag(3), diag(3), 1)
Cnf(diag(3), 7*diag(3), 1)
Cnf(diag(3), 8*diag(3), 2)
Cnf(3*diag(3), 1+diag(3), 1)
Cnf(3*diag(3), 2+diag(3), 2)



## compiler lesson 101
library(nimble)

nf <- nimbleFunction(
    setup = function() {
        a <- 1
    },
    run = function(arg = double()) {
        b <- a + arg
        returnType(double())
        return(b)
    })

Rnf <- nf()

## nfProc object is created every time we compile a new NF generator
## nfCompileInfo data structure for compilation info RCfunctionCompileClass"
## RCfunProcessing class to manage compilation info for a *single* run/member function
proj <- nimble:::nimbleProjectClass()
nfProc <- nimble:::nfProcessing(Rnf, project = proj)
nfProc$process(control = list(debug = TRUE))

## place to define 1st arg switching at top of genCpp_sizeProcessing
## switch name to nimArr_xxx in genCpp_processSpecificCalls






## testing of new parsed posterior text in conjugacy system
posteriorText = '{a<-1;
b
c
dnorm(mean = (prior_mean*prior_tau + contribution_mean) / (prior_tau + contribution_tau),
                            sd   = (prior_tau + contribution_tau)^(-0.5))
}'

posteriorText = '{ R <- chol(prior_prec + contribution_prec)
                        A <- prior_prec %*% asCol(prior_mean) + asCol(contribution_mean)
                        mu <- backsolve(R, forwardsolve(t(R), A))[,1]
                        dmnorm_chol(mean = mu, cholesky = R, prec_param = 1) }'

parsedTotalPosterior <- parse(text = posteriorText)[[1]]
parsedTotalPosterior
if(parsedTotalPosterior[[1]] != '{') parsedTotalPosterior <- substitute({POST}, list(POST = parsedTotalPosterior))
parsedTotalPosterior
prePosteriorCodeBlock <- parsedTotalPosterior[-length(parsedTotalPosterior)]
prePosteriorCodeBlock
posteriorExpr <- parsedTotalPosterior[[length(parsedTotalPosterior)]]
posteriorExpr
rDistribution <- cc_makeRDistributionName(as.character(posteriorExpr[[1]]))
rDistribution
dDistribution <- as.character(posteriorExpr[[1]])
dDistribution
argumentExprs <- as.list(posteriorExpr)[-1]
argumentExprs
argumentNames <- names(argumentExprs)
argumentNames
rCallExpr <- as.call(c(as.name(rDistribution), 1, argumentExprs))
rCallExpr
dCallExpr <- as.call(c(as.name(dDistribution), quote(VALUE), argumentExprs, log = 1))
dCallExpr
posteriorVars <- all.vars(parsedTotalPosterior)
posteriorVars
neededPriorParams <- gsub('^prior_', '', posteriorVars[grepl('^prior_', posteriorVars)])
neededPriorParams
neededContributionNames <- posteriorVars[grepl('^contribution_', posteriorVars)]
neededContributionNames
neededContributionDims <- inferContributionTermDimensions(prior)


## cases of dmnorm() parametrizations, and possible getParam() calls
library(nimble)

code <- nimbleCode({ x ~ dnorm(mu, var = v) })
inits <- list(x=0, mu=0, v=1)

set.seed(0)
tmp <- array(rnorm(9), c(3,3)) + diag(3)
A <- tmp + t(tmp)
code <- nimbleCode({ x[1:3] ~ dmnorm(mu[1:3], prec = Q[1:3, 1:3]) })
inits <- list(x=rep(0,3), mu=rep(0,3), Q=A)

md <- nimbleModel(code, inits = inits, returnDef = TRUE, debug=TRUE)

Rmodel <- nimbleModel(code, inits = inits)

Rmodel$getNodeNames()
nn <- Rmodel$getNodeNames(stochOnly = TRUE)
lifted <- Rmodel$getNodeNames(determOnly = TRUE)
nn
lifted
Rmodel$nodes[[lifted]]$simulate  ## calculates chol(Q) into lifted node
Rmodel$nodes[[nn]]$simulate
Rmodel$nodes[[nn]]$get_var
Rmodel$nodes[[nn]]$get_sd
Rmodel$nodes[[nn]]$get_tau
Rmodel$nodes[[nn]]$get_cholesky
Rmodel$nodes[[nn]]$get_prec
Rmodel$nodes[[nn]]$get_cov


## cases of dmnorm() parametrizations, and possible getParam() calls
library(nimble)

set.seed(0)
tmp <- array(rnorm(9), c(3,3)) + diag(3)
A <- tmp + t(tmp)

## parametrize in terms of prec matrix (Q):
code <- nimbleCode({
    x[1:3] ~ dmnorm(mu[1:3], prec = Q[1:3, 1:3])
})
inits <- list(x=rep(0,3), mu=rep(0,3), Q=A)

Rmodel <- nimbleModel(code, inits = inits)
Cmodel <- compileNimble(Rmodel)

Rmodel$getNodeNames()
nn <- Rmodel$getNodeNames(stochOnly = TRUE)
lifted <- Rmodel$getNodeNames(determOnly = TRUE)
nn
lifted
Rmodel[[lifted]]

Q <- A
ch <- chol(Q)
V <- solve(Q)
I <- diag(3)
Q
ch
V
t(ch) %*% ch
Q
backsolve(ch, forwardsolve(t(ch), I))
V

Rmodel$nodes[[nn]]$get_mean
Rmodel$nodes[[nn]]$get_mean()
Rmodel$getParam(nn, 'mean')

Rmodel$nodes[[nn]]$get_cholesky
Rmodel$nodes[[nn]]$get_cholesky()
Rmodel$getParam(nn, 'cholesky')
ch

Rmodel$nodes[[nn]]$get_prec
Rmodel$nodes[[nn]]$get_prec()
Rmodel$getParam(nn, 'prec')
Q

Rmodel$nodes[[nn]]$get_cov
Rmodel$nodes[[nn]]$get_cov()
Rmodel$getParam(nn, 'cov')
V

xx <- forwardsolve(t(ch), I)
t(xx) %*% xx

xx <- backsolve(ch, I)  ## would this work too???
xx %*% t(xx)

t(ch) %*% ch %*% V
t(ch) %*% ch
ch %*% V
Q


nfDef <- nimbleFunction(
    setup = function(model, node) {},
    run = function() {
        print('chol:')
        chh <- model$getParam(node, 'cholesky')
        print(chh)
        print('prec:')
        q <- model$getParam(node, 'prec')
        print(q)
        print('cov:')
        vv <- model$getParam(node, 'cov')
        print(vv)
    }
)

Rnf <- nfDef(Rmodel, nn)
Cnf <- compileNimble(Rnf, project=Rmodel)


ch
Q
V
Rnf$run()
Cnf$run()

## store something into model variable 'Q'
Rmodel$Q <- Q

## calculate deterministic dependents
Rmodel$nodes[[lifted]]$simulate  ## calculates chol(Q) into lifted node

Rmodel$nodes[[nn]]$simulate
Rmodel$nodes[[nn]]$get_cholesky
Rmodel$nodes[[nn]]$get_cholesky()
ch

## request 'prec'
Rmodel$nodes[[nn]]$get_prec  ## direct: model$Q
Rmodel$nodes[[nn]]$get_prec()
Q

## request 'cov'
## here's what we want to get cov: backsolve(ch,  forwardsolve(t(ch), I)  )
Rmodel$nodes[[nn]]$get_cov
Rmodel$nodes[[nn]]$get_cov()
V







    


## figuring out R / nimble functions:
## inverse, solve, forwardsolve, backsolve

library(nimble)
set.seed(0)
A <- array(rnorm(9), c(3,3))
A <- t(A) %*% A

nfDef <- nimbleFunction(
    setup <- function(A) {
        D <- diag(3)
        b <- 1:3
    },
    methods = list(
        inv = function() {
            Ainv <- inverse(A)
            print(Ainv)
        },
        ch = function() {
            R <- chol(A)
            print(R)
        },
        fs = function() {
            R <- chol(A)
            L <- t(R)
            Linv <- forwardsolve(L, D)
            print(Linv)
        },
        bs = function() {
            R <- chol(A)
        }
    )
)

Rnf <- nfDef(A)
Cnf <- compileNimble(Rnf)

print('inverse of A')
solve(A)
Rnf$inv()
Cnf$inv()

print('chol of A')
chol(A)
Rnf$ch()
Cnf$ch()

R <- chol(A)
L <- t(R)

print('forwardward solve')
forwardsolve(L, diag(3))

print('backward solve')
backsolve(R, diag(3))



## example of using nf$getDefinition() and nf_getDefinition()
library(nimble)
nfDef <- nimbleFunction(
    setup = function(a) { },
    run = function(b = double()) {
        a <<- a + b
        returnType(double())
        return(a)
    },
    methods = list(
        nothing = function() {
            print(a)
        })
)
Rnf <- nfDef(3)
Cnf <- compileNimble(Rnf)
getDefinition(nfDef)
Rnf$getDefinition()
Cnf$getDefinition()

## example of spec$getSamplerDefinition(..)
library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dgamma(a, 1)
    for(i in 1:140)
        c[i] ~ dnorm(0,1)
})

Rmodel <- nimbleModel(code, check = FALSE)

spec <- configureMCMC(Rmodel, print = TRUE)
spec$printSamplers()
spec$getSamplerDefinition(1)
spec$getSamplerDefinition('b')

spec$getSamplers()
spec$printSamplers()
spec$addSampler(type='RW_block', target=c('a', 'b'))
spec$getSamplers()
spec$printSamplers()


## performance comparison of of new dynamic conjugate samplers, rats example
library(nimble)
rats <- readBUGSmodel('rats', dir = getBUGSexampleDir('rats'), returnModelComponentsOnly = TRUE)
Rmodel <- nimbleModel(rats$model, rats$data[c('N','T','x')], rats$data['Y'], rats$inits)
spec <- configureMCMC(Rmodel, print = TRUE)
out <- MCMCsuite(
    rats$model, rats$data[c('N','T','x')], rats$data['Y'], rats$inits,
    niter = 100000,
    MCMCs = c('staticConj', 'dynamicConj'),
    MCMCdefs = list(
        staticConj  = quote({ nimbleOptions(useDynamicConjugacy = FALSE)
                              configureMCMC(Rmodel) }),
        dynamicConj = quote({ nimbleOptions(useDynamicConjugacy = TRUE)
                              configureMCMC(Rmodel) })),
    makePlot = FALSE
)
sampdiff <- out$samples['staticConj',,] - out$samples['dynamicConj',,]
all(sampdiff == 0)
## [1] TRUE                      ## samples from each MCMC are identical
out$timing
## staticConj    dynamicConj
##     26.309         17.801     ## ~ 33% decrease in conjugate sampler runtime



## testing proper dmnorm conjugate calculations for Chris

## code = nimbleCode({
##    y[1:3]  ~ dmnorm(mu [1:3], cov = covy [1:3,1:3])
##    mu[1:3] ~ dmnorm(mu0[1:3], cov = covmu[1:3,1:3])
## })
## Concerned about some explicit inverses in conjugate update for mu,
## compared to use of already existing Cholesky factors.  In the R sampler,
## 1) we have: inverse(prior_prec + contribution_prec)
## 2) we use get_prec, which when we specify a dmnorm with a covariance
## has code like the following (in calc_dmnormAltParams()):
##  tmp <- t(cholesky) %*% cholesky
##  return(inverse(tmp))

library(nimble)

nimbleOptions('buildInterfacesForCompiledNestedNimbleFunctions')
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
nimbleOptions('buildInterfacesForCompiledNestedNimbleFunctions')
nimbleOptions()

code = nimbleCode({
   mu[1:3] ~ dmnorm(mu0[1:3], cov=covmu[1:3,1:3])
   y[1:3] ~ dmnorm(mu[1:3], cov = covy[1:3,1:3])
   a ~ dnorm(y[1], y[1])
   b ~ dgamma(a, y[1])
})

cov <- diag(3)
constants <- list(covy = cov, covmu = cov, mu0 = rep(0,3))
data <- list(y = 1:3)
inits <- list(mu = 4:6, a = 1, b = 1)

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()
spec$printSamplers()
spec$getSamplerDefinition('mu')
Rmcmc <- buildMCMC(spec)


Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Rmcmc$samplerFunctions$contentsList[[2]]$scale
Cmcmc$samplerFunctions$contentsList[[2]]
Cmcmc$samplerFunctions$contentsList[[2]][['scale']]
Cmcmc$samplerFunctions$contentsList[[2]]$scale



set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)



## testing what works and what doesn't with nimbleFunctionLists, getParam, etc..

library(nimble)

getNimbleOption('checkModel')
nimbleOptions(checkModel = FALSE)
getNimbleOption('checkModel')

code <- quote({
    x ~ dgamma(1, 1)
    y[1] ~ dnorm(1, tau = 1*x)
    y[2] ~ dnorm(2, tau = 2*x)
    y[3] ~ dnorm(3, tau = 3*x)
})
Rmodel <- nimbleModel(code, data=list(), inits=list(x=1, y=11:13))

nfDef <- nimbleFunction(
    setup = function(model) {
        nl <- model$expandNodeNames('y')
        N <- length(nl)
        nfl <- nimbleFunctionList(node_stoch_dnorm)
        for (i in 1:N) nfl[[i]] <- model$nodeFunctions[[nl[i]]]
    },
    run = function() {
        for(i in 1:N) {
            print('i = ', i)
            val <- nfl[[i]]$get_mean()
            print('mean = ', val)
            val <- nfl[[i]]$get_tau()
            print('tau = ', val)
            val <- nfl[[i]]$get_var()
            print('var = ', val)
            val <- nfl[[i]]$get_value()
            print('value = ', val)
        }
        returnType(double(0))
        return(1)
    }
)
Rnf <- nfDef(Rmodel)
Rnf$run()
Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(Rnf, project = Rmodel)
Cnf$run()


## testing of new dynamic conjugate samplers
### demo2 of check conjugacy
library(nimble)
code <- BUGScode({
    x ~ dbeta(3, 13)
    y[1] ~ dbin(x, 10)
    y[2] ~ dbin(x, 20)
    z ~ dbeta(10, 10)
    zz[1] ~ dbin(z, 10)
    zz[2] ~ dbin(z, 20)
    aaa ~ dbeta(10, 10)
    bbb[1] ~ dbin(aaa, 10)
    bbb[2] ~ dbin(aaa, 20)
    bbb[3] ~ dbin(aaa, 20)
    d ~ dnorm(0, 1)
    dd ~ dnorm(0, 1)
    ddd ~ dnorm(0, 1)
    e ~ dnorm(d + dd + ddd, 1)
    f ~ dlnorm(d + dd + ddd, 1)
    ff ~ dlnorm(d + dd, 1)
    g ~ dnorm(0, 1)
    gg ~ dnorm(5+g, 1)
})
constants <- list()
inits <- list(x = 0.5, z=0.6, aaa=0.3, d=0, dd=0, ddd=0, g=0)
data = list(y = c(3,4), zz=c(5,5), bbb=c(3,3,3), ff=1, e=0, f=1, gg=0)
Rmodel <- nimbleModel(code, constants, data, inits)


spec <- configureMCMC(Rmodel)

spec$getSamplers()

Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
Rmcmc$run(3)
Rsamples <- as.matrix(Rmcmc$mvSamples)
set.seed(0)
Cmcmc$run(3)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rnames <- dimnames(Rsamples)[[2]]
Rsamples[, Rnames] - Csamples[, Rnames]
##     x z aaa d dd ddd g
##[1,] 0 0   0 0  0   0 0
##[2,] 0 0   0 0  0   0 0
##[3,] 0 0   0 0  0   0 0
Rsamples[, Rnames]
##             x         z       aaa         d         dd        ddd         g
##[1,] 0.3254630 0.3746455 0.3658422 0.6362147 -0.2698403 -1.1333402 -3.156596
##[2,] 0.1975665 0.3995488 0.3210974 0.3695457 -0.2843177 -0.2239394 -2.711577
##[3,] 0.1900000 0.4199454 0.2194685 0.5430496 -0.9140867  0.1178770 -2.233141


## testing of new dynamic conjugate samplers
### demo2 of check conjugacy
library(nimble)
code <- BUGScode({
    x ~ dbeta(3, 13)
    y[1] ~ dbin(x, 10)
    y[2] ~ dbin(x, 20)
})
data = list(y = c(3,4))
constants <- list()
inits <- list()

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
Rsamples <- as.matrix(Rmcmc$mvSamples)
set.seed(0)
Cmcmc$run(10)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rsamples - Csamples
as.numeric(Rsamples) - c(0.195510839527966, 0.332847482503424,0.247768152764931, 0.121748195439553, 0.157842271774841, 0.197566496350904, 0.216991517500577, 0.276609942874852, 0.165733872345582, 0.144695512780252)



## testing of new dynamic conjugate samplers
### checkConjugacy_demo3_run.R - various conjugacies
library(nimble)
code <- BUGScode({
    x ~ dgamma(1, 1)       # should satisfy 'gamma' conjugacy class
    a  ~ dnorm(0, x)     # should satisfy 'norm' conjugacy class
    a2 ~ dnorm(0, tau = 3*x+0)
    b  ~ dpois(0+5*x)
    b2 ~ dpois(1*x*1)
    c ~ dgamma(1, 7*x*5)
    for(i in 2:3) {
        jTau[i] <- 1
        jNorm[i] ~ dnorm(c * (a+3) - i, var = jTau[i])
        kTauSd[i] <- 2
        kLogNorm[i] ~ dlnorm(0 - a - 6*i, kTauSd[i])
    }
})
data = list()
constants <- list()
inits <- list()
Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel, monitors = c('x', 'c'), control = list(scale = 0.01))
spec$getSamplers()

Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
Rsamples <- as.matrix(Rmcmc$mvSamples)
set.seed(0)
Cmcmc$run(10)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rnames <- dimnames(Rsamples)[[2]]
Rsamples[, Rnames] - Csamples[, Rnames]

Rsamples[, Rnames] - cbind(c(3.950556165467749, 1.556947815895538, 1.598959152023738, 2.223758981790340, 2.386291653164086, 3.266282048060261, 3.064019155073057, 3.229661999356182, 1.985990552839427, 2.057249437940977), c( 0.010341199485849559, 0.010341199485849559, 0.003846483017887228, 0.003846483017887228, 0.007257679932131476, 0.009680314740728335, 0.012594777095902964, 0.012594777095902964, 0.018179641351556003, 0.018179641351556003))


## testing of new dynamic conjugate samplers
### Dirichlet-multinomial conjugacy
# as of v0.4, exact numerical results here have changed because
# ddirch now sometimes returns NaN rather than -Inf (when an
# alpha is proposed to be negative) -- this changes the RNG
# sequence because NaN values result in no runif() call in decide()
# single multinomial
library(nimble)
set.seed(0)
n <- 100
alpha <- c(10, 30, 15, 60, 1)
K <- length(alpha)
p <- c(.12, .24, .09, .54, .01)
y <- rmulti(1, n, p)
code <- quote({
    y[1:K] ~ dmulti(p[1:K], n);
    p[1:K] ~ ddirch(alpha[1:K]);
    for(i in 1:K) {
        alpha[i] ~ dgamma(.001, .001);
    }
               })
inits <- list(p = rep(1/K, K), alpha = rep(K, K))
constants <- list(n=n, K=K)
data <- list(y = y)
Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel, monitors = c('alpha', 'p'))
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)

apply(samples, 2, mean)[6:10]
p
apply(samples, 2, mean)[6:10] - p



## testing of new dynamic conjugate samplers
## rats example
library(nimble)
lst <- readBUGSmodel('rats', dir = getBUGSexampleDir('rats'), returnModelComponentsOnly = TRUE)
code <- lst$model
data <- lst$data[c('Y')]
constants <- lst$data[c('N', 'T', 'x')]
inits <- lst$inits

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()

Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
Cmcmc$run(5000)
samples <- as.matrix(Cmcmc$mvSamples)

a <- samples
b <- samples
a
b
a-b
any(a-b != 0)




## phase transition regression example for Mark.

## generate data
sigma <- 5
a <- 2
b <- 0.5
t0 <- 30
x <- 1:100
n <- length(x)
y <- rnorm(n, a + (x>t0) * (b*(x-t0)), sd=sigma)
plot(x, y)

## fit NIMBLE model
library(nimble)

code <- nimbleCode({
    a ~ dunif(-100, 100)
    b ~ dunif(-100, 100)
    t0 ~ dunif(0, 100)
    sigma ~ dunif(0, 100)
    for(i in 1:n) {
        phase[i] <- step(x[i] - t0)
        mu[i] <- a + phase[i] * b * (x[i]-t0)
        y[i] ~ dnorm(mu[i], sd = sigma)
    }
})
constants <- list(n=n, x=x)
data <- list(y=y)
inits <- list(sigma=1, a=0, b=1, t0=50)

## MCMC Suite will let us do a few MCMCs quickly, and look at results easily
out <- MCMCsuite(
    code, constants, data, inits,
    MCMCs = c('nimble', 'nimble_slice', 'bugs'),
    niter = 20000,
    burnin = 2000,
    makePlot = TRUE,
    savePlot = FALSE,
    calculateEfficiency = TRUE
)

out$summary
out$efficiency
dim(out$samples)
out$timing





## nimble problem with using 'sd' name for 'tau' parameter, e.g.
library(nimble)

code <- nimbleCode({
    sd ~ dunif(0, 1)
    y ~ dnorm(0, tau = sd)
    shape1 ~ dnorm(0, 1)
    f ~ dbeta(mean = y, sd = shape1)
})

m <- nimbleModel(code, inits = list(sd=.1))

m$sd

m$getModelDef()$printDI()

code <- nimbleCode({
    sd2 ~ dunif(0, 1)
    y ~ dnorm(0, tau = sd2)
})
m <- nimbleModel(code, debug=TRUE)

m$sd2






     
## Plot the fake MCMC output
denoverplot(fakemcmc, fakemcmc2)
     denoverplot(fakemcmc, fakemcmc2, style="plain",
                 col=mcmcplotsPalette(3, type="grayscale"),
                 ci=0.95, greek=TRUE)
     denoverplot(fakemcmc, fakemcmc2,
                 plot.title="Comparison of densities of fake data")
     denoverplot(fakemcmc, fakemcmc2,
                 plot.title="Comparison of densities of fake data", greek=TRUE)

setwd('/Users/dturek/GitHub/legacy/autoBlock/')
getwd()

source("autoBlock.R")
load(file.path("data", "model_test.RData"))
saveSamples <- TRUE
niter <- 10000
ab <- autoBlock(code, constants, data, inits, niter, runList, saveSamples = saveSamples)
dftest <- ab$summary
save(dftest, file = file.path("results_samples", "results_test.RData"))
if (saveSamples) {
    burnedSamplesList <- ab$samples
    for (i in 1:length(burnedSamplesList)) burnedSamplesList[[i]] <- burnedSamplesList[[i]][(floor(niter/2) + 1):niter, ]
    save(burnedSamplesList, niter, file = file.path("results_samples", "results_test_samples.RData"))
}

class(burnedSamplesList)
names(burnedSamplesList)

i <- 4
dim(burnedSamplesList[[i]])
dimnames(burnedSamplesList[[i]])

dimnames(ab$samples[[3]])


ret$summary

dftest <- autoBlock(code, constants, data, inits, 10000, runList)$summary
save(dftest, file = file.path("results_hclust_complete2", "results_test.RData"))


rm(list=ls())
getwd()
load('results_samples/results_test.RData')
ls()
dftest

rm(list=ls())
getwd()
load('results_samples/results_test_samples.RData')
ls()
niter
class(burnedSamplesList)
names(burnedSamplesList)
i <- 4
dim(burnedSamplesList[[i]])
dimnames(burnedSamplesList[[i]])







## renaming stupid data frame from the autoBlock results
setwd('~/GitHub/legacy/autoBlock')

rm(list=ls())
file <- 'results/results_samplingEfficiency.RData'
load(file)
ls()
dfsamplingEfficiency <- dfSamplingEfficiency
rm(dfSamplingEfficiency)
ls()
save(list='dfsamplingEfficiency', file = file)

rm(list=ls())
file <- 'results/results_computationalRequirement.RData'
load(file)
ls()
dfcomputationalRequirement <- dfComputationalRequirement
rm(dfComputationalRequirement)
ls()
save(list='dfcomputationalRequirement', file = file)





## working on the spatial capture-recapture (SCR) models

tr<-seq(15,85, length=10)

X<-cbind(rep(tr,each=length(tr)),rep(tr,times=length(tr))) # 100 coord. traps

plot(X, xlim=c(0,100), ylim=c(0,100), pch=3, cex=0.75)

set.seed(10)
xlim <- c(0,100); ylim <- c(0,100)      # Area 100*100=1e4
A <- (xlim[2]-xlim[1])*(ylim[2]-ylim[1])/10000
mu <- 50                 # Density
N <- rpois(1, mu*A) ;N   # Generate population


s <- cbind(runif(N, xlim[1], xlim[2]), runif(N, ylim[1], ylim[2]))
points(s, pch=16, col=2)

sigma <- 5
lambda0 <- 0.4
J <- nrow(X)
K <- 5
yy <- array(NA, c(N, J, K))
for(j in 1:J) {
    dist <- sqrt((X[j,1]-s[,1])^2 + (X[j,2]-s[,2])^2)
    lambda <- lambda0*exp(-dist^2/(2*sigma^2))
    for(k in 1:K) {
        yy[,j,k] <- rpois(N, lambda)
    }
}

n <- apply(yy, c(2,3), sum)

# Plot capture events
tot<-apply(n, 1,sum)
symbols(X, circles=tot, inches=F, bg="#00000022", add=T)
points(X, pch=3, cex=0.75); points(s, pch=16, col=2)


library(nimble)

## define the model
code <- nimbleCode({
    sigma ~ dunif(0,10)
    lam0 ~ dunif(0,5)
    psi ~ dbeta(1,1)
    for(i in 1:M) {
        z[i] ~ dbern(psi)
        s[i,1] ~ dunif(xlim[1], xlim[2])
        s[i,2] ~ dunif(ylim[1], ylim[2])
        for(j in 1:J) {# Number of traps
            dist[i,j] <- (s[i,1] - X[j,1])^2 + (s[i,2] - X[j,2])^2
            lam[i,j] <- lam0*exp(-dist[i,j]/(2*sigma^2))*z[i]
        }
    }
    for(j in 1:J){
        bigLambda[j] <- sum(lam[1:M,j])
        for(k in 1:K) {
            n[j,k] ~ dpois(bigLambda[j])
        }
    }
    N <- sum(z[1:M])
})


M<-200

constants <- list(M = M, K=K, J=J)
n1<-apply(n,1,sum)
data<-list(n=n, X=X, xlim=xlim, ylim=ylim)
s<-cbind(runif(M, xlim[1], xlim[2]), runif(M, ylim[1], ylim[2]))
z<-rep(1,M)
inits <- list (sigma=0.5, lam0=0.1, s=s, z=z)

Rmodel <- nimbleModel(code=code, constants=constants, data=data, inits=inits, check=FALSE) ## check=FALSE is faster

mcmcspec<-configureMCMC(Rmodel, print=TRUE, monitors = c("N", "lam0", "psi", "sigma"))
scrMCMC <- buildMCMC(mcmcspec)
Cmodel <- compileNimble(Rmodel) 
CscrMCMC <- compileNimble(scrMCMC, project = Rmodel)

## It's pretty slow to run so I just want an execution time first for a small sample
t1_100 <- system.time(CscrMCMC$run(100))

## And now I want a decent sample
t1_20k <- system.time(CscrMCMC$run(20000))
t1_20k_samples <- as.matrix(CscrMCMC$mvSamples)
save(t1_100, t1_20k, t1_20k_samples, file = "case1results.Rdata")


## checking dcat distribution for Chris

library(nimble)

code <- nimbleCode({
    y ~ dcat((p[1:2,1:2] %*% ones[1:2])[1:2,1])
})
constants <- list(ones = c(1,1))
data <- list()
inits <- list(y = 2, p = diag(c(.5,.5)))

Rmodel <- nimbleModel(code, constants, data, inits)




library(nimble)
Rmodel <- readBUGSmodel('rats', dir = '~/GitHub/nimble/nimble/packages/nimble/inst/classic-bugs/vol1/rats/')

customSpec <- configureMCMC(Rmodel)
customSpec$getSamplers()
customSpec$removeSamplers(1:65)
customSpec$getSamplers()
customSpec$addSampler(target = 'alpha.c', type = 'slice')
customSpec$addSampler(target = 'beta.c', type = 'slice')
customSpec$addSampler(target = 'tau.c', type = 'slice')
customSpec$addSampler(target = 'tau.alpha', type = 'slice')
customSpec$addSampler(target = 'tau.beta', type = 'slice')
customSpec$getSamplers()
alphaNames <- Rmodel$getDependencies('tau.alpha', self = FALSE, stochOnly = TRUE)
alphaNames
length(alphaNames)  ## 30
for(aN in alphaNames) customSpec$addSampler(target = aN, type = 'slice')
customSpec$getSamplers()
betaNames <- Rmodel$getDependencies('tau.beta', self = FALSE, stochOnly = TRUE)
betaNames
length(betaNames)  ## 30
for(bN in betaNames) customSpec$addSampler(target = bN, type = 'slice')
customSpec$getSamplers()
customSpec


## making a simple MCMC traceplot, and density plot

n <- 1000
x <- rnorm(n, 3, 1)
dev.new(width=4, height=3)
plot(1:n, x, type='l')
dev.copy2pdf(file = '~/Downloads/traceplot.pdf')
dev.new(width=4, height=3)
plot(density(x))
dev.copy2pdf(file = '~/Downloads/densityplot.pdf')


## make sure I get the empirical covariance right

x <- matrix(c(1,2,1,2,1,4,6,5,7,4), 5, 2)
timesRan <- 5
statSums <- apply(x, 2, sum)
statSums <- t(statSums)
statProds <- matrix(0, 2, 2)
for(i in 1:timesRan) statProds <- statProds + t(t(x[i,])) %*% t(x[i,])
statSums
statProds
(statProds - (t(statSums) %*% statSums)/timesRan) / (timesRan-1)
cov(x)
colMeans <- apply(x, 2, mean)
xCentered <- x - rep(colMeans, each=timesRan)
xCentered
(t(xCentered) %*% xCentered) / (timesRan-1)

set.seed(0)
library(nimble)
code <- nimbleCode({
    x[1:d] ~ dmnorm(mu[1:d], cov = Sigma[1:d, 1:d])
})
d <- 4
mu <- 1:d
ch <- array(rnorm(d^2), c(d,d))
Sigma <- ch %*% t(ch)
print(Sigma)
constants <- list(d=d, mu=mu, Sigma=Sigma)
data <- list()
inits <- list(x = rep(1, d))
Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel, nodes = NULL)
spec$addSampler('x', 'RW_block', print = FALSE)
spec$addSampler('x', 'RW_block_NEW', print = FALSE)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
Cmcmc$run(1000)
samples <- as.matrix(Cmcmc$mvSamples)
cov(samples)
           x[1]       x[2]     x[3]       x[4]
x[1]  2.0934846 -0.1465907 1.107342  1.1252451
x[2] -0.1465907  4.1696671 1.644427 -0.5950559
x[3]  1.1073420  1.6444267 2.581730  1.5097074
x[4]  1.1252451 -0.5950559 1.509707  1.8741393
samples[c(1,250,500,750,1000),]
           x[1]      x[2]       x[3]     x[4]
[1,]  1.0000000  1.000000  1.0000000 1.000000
[2,]  0.2162332  2.260594  1.9804354 2.913190
[3,]  1.8118014  3.966497  2.7563534 2.552825
[4,] -1.6015689 -1.004725 -0.9864974 1.065231
[5,]  0.8037007  2.496745  1.2659404 2.208456






## looking into RStudio hanging problem (once again... Nov 2015)

library(nimble)
code <- nimbleCode({
  a ~ dnorm(0, 1)
  sd ~ dunif(0, 100)
  for(i in 1:N) {
    b[i] ~ dnorm(a, sd = sd)
  }
})
N <- 100
constants <- list(N = N)
data <- list()
inits <- list(a = 0, sd = 1, b = rep(0, N))


Rmodel <- nimbleModel(code, constants, data, inits)


## NEED TO FIND THE PROBLEM IN MODEL CHECKING

## EXAMPLE 1:
rm(list = ls())
library(nimble)
classicDyesModel <- readBUGSmodel('dyes', dir = getBUGSexampleDir('dyes'))

## EXAMPLE 2:
set.seed(0)
library(nimble)
n_obs <- 50
### Parameters
mu <- 2
rho_a <- 1
rho_b <- 0.1
monitored_param <- c('mu', 'rho_a', 'rho_b', 'ngis')
SV =  rgamma(n=n_obs, 1, 1)
### Data generation
lambda <-  exp(mu)*SV
ngis  <-  rpois(n=n_obs, lambda=lambda )
Y <- rgamma(n=n_obs, ngis*rho_a, rho_b)
### Remove zero values from ngis
ngis <- ngis[which(ngis>0)]
abs <- which(Y==0)
n_abs <- length(abs)
pres <- which(Y>0)
n_pres <- length(pres)

# Install from source for OS X, Linux, or Windows:
#install.packages("nimble", repos = "http://r-nimble.org", type = "source")
M1_CPG_nimbleCode <- nimbleCode(
    {
        ## Covariates
        for (s in 1:n_obs){
            lambda[s] <- exp(mu)*SV[s]
        }
        ## Observation model
        ## Strictly positive Observation
        for ( s in 1:n_pres){
            ngis[s] ~ T(dpois(lambda[pres[s]]), 1, )  
            Y[pres[s]] ~ dgamma(rho_a * ngis[s], rho_b)
        }
        ## Zero Observation
        for ( s in 1:n_abs){
            proba[s] <- 1 - exp(-lambda[abs[s]])
            Y[abs[s]] ~ dbern(proba[s])
        }
        ## Prior
        rho_a ~ dgamma(0.01, 0.01)
        rho_b ~ dgamma(0.01, 0.01)
        mu ~ dnorm(0, 0.01)
    })

constants_list <- list(n_obs = length(Y), n_abs=n_abs, n_pres=n_pres, abs=abs, pres=pres)
nimble_data <- list(Y = Y, SV=SV)
## Initial values
inits_fc <- function(chain_id = 1){
  mu <- rnorm(1,0,1)
  rho_a <- runif(1,0,10)
  rho_b <- runif(1,0,10)
  ngis <- round(runif(n_pres, 2, 5), digits=0)
  list(mu=mu, rho_a=rho_a, rho_b=rho_b ,ngis=ngis)
}
init_ll <- lapply(1, function(id) inits_fc(chain_id = id))

Rmodel <- nimbleModel(code= M1_CPG_nimbleCode, name= 'M1_CPG_nimble', constants = constants_list, data =nimble_data, inits = init_ll[[1]], check = TRUE)

calculate(Rmodel, 'ngis[1]')

Rmodel$ngis

check <- TRUE
check <- FALSE

md <- nimbleModel(code= M1_CPG_nimbleCode, name= 'M1_CPG_nimble', constants = constants_list, data =nimble_data, inits = init_ll, returnDef = TRUE)

M1_CPG_nimble <- md$newModel(data =nimble_data, inits = init_ll, check = check)

debug(md$newModel)
md$newModel(data =nimble_data, inits = init_ll, check = TRUE)

debug(model$check)





## random stuff for CR paper....

rm(list=ls())
library(nimble)
##source('~/GitHub/legacy/dipper/dipperCode.R')

Rmodel <- nimbleModel(code_dipper, constants, data, inits)
lnodes <- Rmodel$expandNodeNames('x')
length(lnodes) - nind

rm(list=ls())
trunc <- FALSE
source('~/GitHub/userDistMCMC/defs.R')
source('~/GitHub/userDistMCMC/create_data.R')
Rmodel <- nimbleModel(orchidDHMM$code, orchidDHMM$constants, orchidDHMM$data, orchidDHMM$inits)
topN <- Rmodel$getNodeNames(topOnly = TRUE, stochOnly = TRUE)
topN
length(topN)

con <- orchidDHMM$constants
con$f
con$nind
#### actually 250 individuals in original dataset!!!

con$f
12 - con$f
sum(12 - con$f)

rm(list=ls())
load('~/GitHub/userDistMCMC/results.RData')
or <- results$df[results$df$model == 'orchid', ]
or
or[or$mcmc %in% c('jags', 'nimbleDHMM2'), ]
or1 <- or[or$param == 's[1]', ]
data.frame(mcmc = or1$mcmc, runtime_minutes = or1$timing/60)

rm(list=ls())
load('~/GitHub/userDistMCMC/results.RData')
go <- results$df[results$df$model == 'goose', ]
go
go1 <- go[go$param == 'p[1]', ]
data.frame(mcmc = go1$mcmc, runtime_minutes = go1$timing/60, runtime_hours = go1$timing/60/60)


rm(list=ls())
trunc <- FALSE
source('~/GitHub/userDistMCMC/defs.R')
source('~/GitHub/userDistMCMC/create_data.R')
Rmodel <- nimbleModel(gooseDHMM$code, gooseDHMM$constants, gooseDHMM$data, gooseDHMM$inits)
topN <- Rmodel$getNodeNames(topOnly = TRUE, stochOnly = TRUE)
topN
length(topN)



## testing the MVN conjugacy test from test-mcmc.R
library(nimble)
set.seed(0)
mu0 = 1:3
Q0 = matrix(c(1, .2, .8, .2, 2, 1, .8, 1, 2), nrow = 3)
Q = solve(matrix(c(3, 1.7, .9, 1.7, 2, .6, .9, .6, 1), nrow = 3))
a = c(-2, .5, 1)
B = matrix(rnorm(9), 3)

##### not currently working - see Perry's email of ~ 10/6/14
## code <- nimbleCode({
##   mu[1:3] ~ dmnorm(mu0[1:3], Q0[1:3, 1:3])
##   y[1:3] ~ dmnorm(asCol(a[1:3]) + B[1:3, 1:3] %*% asCol(mu[1:3]), Q[1:3, 1:3])
## })

code <- nimbleCode({
  mu[1:3] ~ dmnorm(mu0[1:3], Q0[1:3, 1:3])
  y_mean[1:3] <- asCol(a[1:3]) + B[1:3, 1:3] %*% asCol(mu[1:3])
  y[1:3] ~ dmnorm(y_mean[1:3], Q[1:3, 1:3])
})

## Simplest version of model w/o 'a' and 'B'
## a = rep(0,3)
## B = diag(rep(1,3))
## code <- nimbleCode({
##   mu[1:3] ~ dmnorm(mu0[1:3], Q0[1:3, 1:3])
##   y[1:3] ~ dmnorm(mu[1:3], Q[1:3, 1:3])
## })


mu <- mu0 + chol(solve(Q0)) %*% rnorm(3)
# make sure y is a vec not a 1-col matrix or get a dimensionality error
y <- c(a + B%*%mu + chol(solve(Q)) %*% rnorm(3))
data = list(mu0 = mu0, Q0 = Q0, Q = Q, a = a, B = B, y = y)

muQtrue = t(B) %*% Q%*%B + Q0
muMeanTrue = c(solve(muQtrue, crossprod(B, Q%*%(y-a)) + Q0%*%mu0))

constants <- list()
inits <- list()

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

##debug(Rmcmc$samplerFunctions$contentsList[[1]]$run)
set.seed(0)
Rmcmc$run(5)
Rsamples <- as.matrix(Rmcmc$mvSamples)
Rsamples

set.seed(0)
Cmcmc$run(5)
Csamples <- as.matrix(Cmcmc$mvSamples)
Csamples

Rsamples - Csamples

Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)
muMeanTrue

apply(samples, 2, mean) - muMeanTrue


source('~/GitHub/nimble/nimble/packages/nimble/inst/tests/test_utils.R')
##test_mcmc
test_mcmc(model = code, name = 'two-level multivariate normal', data = data, seed = 0, numItsC = 10000,
          results = list(mean = list(mu = muMeanTrue),
                           cov = list(mu = solve(muQtrue))),
          resultsTolerance = list(mean = list(mu = rep(.02,3)),
            cov = list(mu = matrix(.01, 3, 3))))



## working the basketball "streaks" problem emailed out by Aaron Strauss
p <- 0.1
ns <- 1
run <- function(p, ns) {
    out <- numeric()
    n <- 0
    while(n < ns) {
        nmade <- 1
        while(rbinom(1, 1, p) == 1) nmade <- nmade + 1
        out <- c(out, nmade)
        n <- length(out)
    }
    return(out)
}

pOfStreaks <- function(p, streaks, niter=1000) {
    pMissing <- missing(p)
    ns <- length(streaks)
    suc <- 0
    for(i in 1:niter) {
        if(pMissing) p <- runif(1)
        thisStreak <- run(p, ns)
        if(all(thisStreak == streaks)) suc <- suc+1
    }
    prob <- suc/niter
    return(prob)
}

pOfStreaks(p=0.2, streaks=4, niter=100000)
0.2^3*(.8)

p <- 0.7
n <- 4
m <- 7
pOfStreaks(p=p, streaks=c(n,m), niter=1000000)
p^(n+m-2)*(1-p)^2

n <- 2
m <- 1
pOfStreaks(streaks=c(n,m), niter=1000000)
1/12

pOfPlessthanPROB <- function(streaks, PROB, niter=1000) {
    ns <- length(streaks)
    suc <- 0
    ranSoFar <- 0
    while(ranSoFar < niter) {
        p <- runif(1)
        thisStreak <- run(p, ns)
        if(all(thisStreak == streaks)) {
            if(p < PROB) suc <- suc+1
            ranSoFar <- ranSoFar+1
        }
    }
    prob <- suc/niter
    return(prob)
}

pOfPlessthanPROB(streaks=c(2,1), PROB=0.5, niter=20000)
11/16

## continusing the "sreaks" basketball problem, now in NIMBLE

library(nimble)

code <- nimbleCode({
    ##p ~ dunif(0, 1)
    p ~ dbeta(1, 1)
    y[1] ~ dnegbin(prob=p, size=1)
    y[2] ~ dnegbin(prob=p, size=1)
})
constants <- list()
data <- list(y = 0:1)  ## the 'streak' history of {2,1} in the original problem
inits <- list(p = 0.5) ## translates into c(1,0) in terms of the negative-binomial distribution

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
#Rmcmc$samplerFunctions$contentsList[[1]]$run
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
debug(Rmcmc$run)
debug(samplerFunctions[[1]]$run)
Rmcmc$run(1)
set.seed(0)
Cmcmc$run(3)
Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)
Rsamples
Csamples
Rsamples - Csamples

apply(samples, 2, mean)

mean(samples[,1] > 0.5)
5/16

out <- MCMCsuite(
    code, constants, data, inits,
    MCMCs = c('nimble', 'nimble_noConj'),
    summaryStats = c('mean', 'median', 'sd', 'function(x) mean(x>0.5)'),
    calculateEfficiency = TRUE,
    makePlot = TRUE,
    savePlot = FALSE,
    niter = 100000
)

out$timing
out$summary
11/16


## creating a really small, blank figure, for use in captions in CR-MCMC paper
dev.new(width=3, height=2)
plot(1, 1)
dev.copy2pdf(file='~/GitHub/nimble/nimblePapers/CR-MCMC/blankFigure.pdf')

## testing the problem with naming a variable 'ans' in a nimbleFunction
library(nimble)

rcFunction1 <- nimbleFunction(
    run = function() {
        declare(ans, double(1, 5))
        for(i in 1:5) ans[i] <- i
        print(ans)
    })

rcFunction1()

rcFunction2 <- nimbleFunction(
    run = function() {
        declare(functionAsList, double(1, 5))
        for(i in 1:5) functionAsList[i] <- i
        print(functionAsList)
    })

rcFunction2()

code <- nimbleCode({
    mu ~ dnorm(0, 1)
    mu2 <- timesTwo(mu)
})

Rmodel <- nimbleModel(code, inits = list(mu=1), check=FALSE)

##undebug(calculate)
##debug(nimble:::rCalcNodes)
##Rmodel$nodes$mu2$calculate
##Rmodel$nodes$mu2$calculate()
##Rmodel$nodes$mu2$simulate
##Rmodel$nodes$mu2$simulate()

calculate(Rmodel)
Cmodel <- compileNimble(Rmodel)

Rmodel$mu
Rmodel$mu2
Cmodel$mu
Cmodel$mu2

Rmodel$mu <- 2
calculate(Rmodel)
Rmodel$mu2


## finding the posterior pairwise correlations of
## the blocked parameters for the orchid model
## (multistate CR-MCMC, userDistMCMC)

library(nimble)
rm(list=ls())
source('~/GitHub/userDistMCMC/defs.R')
load('~/GitHub/userDistMCMC/models.RData')

Rmodel <- nimbleModel(orchidDHMM$code, orchidDHMM$constants, orchidDHMM$data, orchidDHMM$inits)
spec <- configureMCMC(Rmodel)
spec$getSamplers()
spec$removeSamplers(numeric(0))
spec$removeSamplers(c('b[1:2]','c[1:2]'))
spec$addSampler(c('b[1]','b[2]'), 'RW_block')
spec$addSampler(c('c[1]','c[2]'), 'RW_block')
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
## n <- 10000
n <- 50000
system.time(Cmcmc$run(n)) / 60
samples <- as.matrix(Cmcmc$mvSamples)
samples2 <- samples[(n/2+1):n, ]   ## truncate to second half of samples
apply(samples2, 2, mean)
Cov <- cov(samples2)
Cov
Cor <- cov2cor(Cov)
Cor
sort(as.numeric(abs(Cor)))
Cor['b[1]', 'b[2]']
## [1] 0.9866091
Cor['c[1]', 'c[2]']
## [1] 0.9675666


## printing the numeric performance results for CR-MCMC paper, or userDistMCMC paper
load('~/GitHub/userDistMCMC/results.RData')
##load('~/GitHub/userDistMCMC/resultsNew.RData')
source('~/GitHub/userDistMCMC/defs.R')
options(digits = 3)
df <- results$df
for(mod in unique(df$model)) {
    dfmod <- df[df$model==mod, ]
    message('******************************************')
    message(mod, ' model')
    message('******************************************')
    for(mc in unique(dfmod$mcmc)) {
        dfmodmcmc <- dfmod[dfmod$mcmc==mc, ]
        message(mc, ' MCMC:')
        message('minimum: ',  min(dfmodmcmc$Efficiency))
        message('mean: ',    mean(dfmodmcmc$Efficiency))
    }
    message('******************************************')
}

df[df$model=='orchid',]
df[df$model=='goose',]

results$check()
results$quickplot()


## problem with multivariate conjugate samplers... ?
## fixed.  problem was in vector demoting in the conjugacy definition
library(nimble)
code <- nimbleCode({
    x[1:d] ~ dmnorm(mu[1:d], cov = Sigma[1:d, 1:d])
    y[1:d] ~ dmnorm(x[1:d], cov = Sigma[1:d, 1:d])
})
d <- 2
mu <- rep(0, d)
Sigma <- diag(d)
constants <- list(d=d, mu=mu, Sigma=Sigma)
Rmodel <- nimbleModel(code, constants, data=list(y=rep(0,d)), inits=list(x=rep(0, d)))
Cmodel <- compileNimble(Rmodel)

spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)
solve(cov(samples))

library(nimble)
nimble:::conjugateSamplerDefinitions$sampler_conjugate_dnorm
nimble:::conjugateSamplerDefinitions$sampler_conjugate_dbeta
nimble:::conjugateSamplerDefinitions$sampler_conjugate_dmnorm



## testing different priors for rho in package gpmanagement
library(gpmanagement)
library(ggplot2)
example_data <- function(p, x0, Tobs){
    f <- function (x, h){
        sapply(x, function(x) {
            x <- pmax(0, x - h)
            x * exp(p[1] * (1 - x/p[2]) * (x - p[3])/p[2])
        })
    }
    sigma_g <- 0.1
    pdfn <- function(x, mu, sigma = sigma_g){
        dlnorm(x, log(mu), sdlog = sigma)
    }
    z_g <- function() rlnorm(1, 0, sigma_g)
    x <- numeric(Tobs)
    x[1] <- x0
    for(t in 1:(Tobs-1))
        x[t+1] = z_g() * f(x[t], h=0)
    obs <- data.frame(x = c(0, 
                          pmax(rep(0,Tobs-1), x[1:(Tobs-1)])), 
                      y = c(0, 
                          x[2:Tobs]))
    obs
}
myfit <- function(xObs, yObs, priors = NULL, niter = 100000){    ## default value for priors
  xPred <- seq(0, 1.1 * max(xObs), length = 50)
  fit <- gp_setup(xObs, yObs, xPred, priors)     ## included priors argument
  Cmcmc <- fit$Cmcmc 
  Cpred <- fit$Cpred
  Cmodel <- fit$Cmodel
  system.time(Cmcmc$run(niter))
  samples <- as.matrix(Cmcmc$mvSamples)
  system.time(Cpred$run(samples))
  E <- Cpred$getE()
  V <- sqrt(diag(Cpred$getC()))
  list(samples = tidyr::gather(as.data.frame(samples)),
       pred = data.frame(x = xPred, y = E, ymin = E - V, ymax = E + V))
}

densPlot <- function(fit, paramName, paramPrior) {
        dev.new()
        samp <- fit$samples[fit$samples$key==paramName, 'value']
        plot(density(samp), main=paste0('black posterior, red prior: ', paramName, ' ~ ', deparse(paramPrior)))
        xs <- seq(range(samp)[1], range(samp)[2], length.out = 500)
        yCall <- as.call(c(list(as.name(paramPrior[[1]])), list(quote(xs)), as.list(paramPrior[-1])))
        ys <- eval(yCall, envir = environment())
        lines(xs, ys, col='red')
}

## Carrying capcacity at 10, critical point at 5, x0=6
## 100, 50, x0=60
plotPred <- function(ccap, crit, x0, Tobs=40, plotData=FALSE, rhoPrior, plotRhoDensity=TRUE, niter=100000, plotSigGPDensity=FALSE, plotSigOEDensity=FALSE, plotPredictionIntervals=TRUE) {
    set.seed(0)
    obs <- example_data(p=c(2,ccap,crit), x0=x0, Tobs=Tobs)
    if(plotData) {dev.new(); qplot(seq_along(obs$x[-1]), obs$x[-1]) + geom_line()}
    priors <- expression({
        rho ~ X
        sigGP ~ dunif(0, 1e5)
        sigOE ~ dunif(0, 1e5)
    })
    if(missing(rhoPrior)) stop('must specify a prior for rho')
    rhoPrior[-1] <- lapply(rhoPrior[-1], function(param)
        eval(eval(substitute(substitute(PARAM, list(xObs=obs$x[-1])), list(PARAM=param)))))
    print(rhoPrior)
    priors[[1]][[2]][[3]] <- rhoPrior
    fit <- myfit(obs$x, obs$y, priors = priors, niter=niter)
    if(plotRhoDensity) densPlot(fit, 'rho', rhoPrior)
    if(plotSigGPDensity) densPlot(fit, 'sigGP', priors[[1]][[3]][[3]])
    if(plotSigOEDensity) densPlot(fit, 'sigOE', priors[[1]][[4]][[3]])
    if(plotPredictionIntervals) {
        dev.new()
        print(ggplot2::ggplot(fit$pred) + 
                  geom_ribbon(aes(x = x, y = y, ymin = ymin, ymax = ymax), fill = "grey80") +
                      geom_line(aes(x = x, y = y), size = 1) + 
                          geom_point(data = obs, aes(x, y)) +
                              coord_cartesian(xlim = range(c(obs$x, fit$pred$x)), 
                                              ylim = range(c(obs$y, fit$pred$y))) +
                                                  ggtitle(paste0('rho ~ ', deparse(rhoPrior))))
    }
}

rp <- quote(dunif(0, diff(range(xObs))/sqrt(6)/2))

plotPred(rhoPrior = rp, ccap= 10, crit= 5, x0= 6)
plotPred(rhoPrior = rp, ccap=100, crit=50, x0=60)

plotPred(rhoPrior = rp, ccap=100, crit=50, x0=60, plotSigGPDensity=TRUE, plotSigOEDensity=TRUE)

plotPred(rhoPrior = quote(dunif(0,200)), ccap=10, crit=5, x0=6)

for(uMax in 5:6) {
    rp <- substitute(dunif(0, UMAX), list(UMAX = as.numeric(uMax)))
    plotPred(rhoPrior = rp, ccap=10, crit=5, x0=6, niter=10000)
}



## testing ggplot() inside a loop??
library(ggplot2)
df <- data.frame(a=1:2, b=3:4)
for(i in 1:3) {
    dev.new()
    ## need explict print(...) to make ggplot() plot, when inside a loop!!!
    print(ggplot(df, aes(a, b)) + geom_line() + ggtitle(i))
}



## testing use of substitute...
## which I used to be more comfortable with!

f <- function(a) {
    b <- substitute(a)
    print(b)
    print(class(b))
    vv <- eval(b)
    print(vv)
    print(class(vv))
}

f(dnorm(0,1))
f(4+5)

x <- f(quote(dnorm(0,1)))
x
class(x)


## testing NIMBLE compiler to handle pi
nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        a <- pi
        print(a)
        b <- 2 * pi
        print(b)
    }
)
Rnf <- nfDef()
Rnf$run()
Cnf <- compileNimble(Rnf)    ## ERROR
Cnf$run()


## example of trying to use NIMBLE functions in another package
## this uses my minimalist nimtest package; same thing I sent to Duncan
library(devtools)
install_github('danielturek/nimtest')
library(nimtest)
library(nimble)
## First Test:
## a nimbleFunction defined as a package function
nfDefinition
Rnf <- nfDefinition()
Cnf <- compileNimble(Rnf)
Rnf$run()
Cnf$run()
## Second Test:
## A package function which internally defines and returns a nimbleFunction definition
## This one gives ugly looking warnings
nextFunction
def <- nextFunction()        ## WARNINGS
Rnf <- def()
Cnf <- compileNimble(Rnf)    ## WARNINGS
Rnf$run()
Cnf$run()

## alternate.  from shell:
## R CMD BUILD nimtest
## R CMD install nimtest_0.0.0.9000.tar.gz
library(nimtest)
library(nimble)
nfDefinition    ## finds it -- it created the NIMBLE function at build time
Rnf <- nfDefinition()  ## can't use it
Cnf <- compileNimble(Rnf)
Rnf$run()
Cnf$run()
nextFunction
nextFunction()
def <- nextFunction()
Rnf <- def()  ## can't use it
Cnf <- compileNimble(Rnf)
Rnf$run()
Cnf$run()


## testing out the new elliptical slice sampler (ess)
library(nimble)
code <- nimbleCode({
    x[1:d] ~ dmnorm(mu[1:d], cov = Sigma[1:d, 1:d])
})
d <- 2
##mu <- rep(0, d)
mu <- 1:d
##Sigma <- diag(d);  print(Sigma)
ch <- array(c(1, 0.7, 0, 2), c(2,2));  Sigma <- ch %*% t(ch);  print(Sigma)
constants <- list(d=d, mu=mu, Sigma=Sigma)
data <- list()
inits <- list(x = rep(1, d))
Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)
spec <- configureMCMC(Rmodel, nodes = NULL)
spec$addSampler('x', 'ess', print=FALSE)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0); Cmcmc$run(20000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)
cov(samples)   ## empirical cov seems WRONG!  about half of what it should be?
Sigma


## conjugate MVN-MVN
set.seed(1)
library(nimble)
code <- nimbleCode({
    x[1:d] ~ dmnorm(mu_x[1:d], prec = prec_x[1:d, 1:d])
    y[1:d] ~ dmnorm(x[1:d], prec = prec_y[1:d, 1:d])
})
d <- 3
mu_x <- rnorm(d)
temp <- array(rnorm(d^2), c(d,d))
prec_x <- solve(temp %*% t(temp))
temp <- array(rnorm(d^2), c(d,d))
prec_y <- solve(temp %*% t(temp))
y <- rnorm(d)
constants <- list(d = d, mu_x = mu_x, prec_x = prec_x, prec_y = prec_y)
data <- list(y = y)
inits <- list(x = rep(0, d))
Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)

## block sampling
spec <- configureMCMC(Rmodel, nodes = NULL)
spec$addSampler('x', 'RW_block', print = FALSE)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0); Cmcmc$run(100000)
samples <- as.matrix(Cmcmc$mvSamples)
solve(prec_x + prec_y)
cov(samples)   ## empirical cov seems WRONG!  about half of what it should be?
solve(prec_x + prec_y) %*% (prec_y %*% y + prec_x %*% mu_x)
apply(samples, 2, mean)

## ess sampling
spec <- configureMCMC(Rmodel, nodes = NULL)
spec$addSampler('x', 'ess', print = FALSE)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0); Cmcmc$run(100000)
samples <- as.matrix(Cmcmc$mvSamples)
solve(prec_x + prec_y)
cov(samples)   ## empirical cov seems WRONG!  about half of what it should be?
solve(prec_x + prec_y) %*% (prec_y %*% y + prec_x %*% mu_x)
apply(samples, 2, mean)



## testing elliptical slice sampler on 'spatial' model
library(nimble)
load('~/GitHub/legacy/autoBlock/data/model_spatial.RData')

out <- MCMCsuite(
    code, constants, data, inits,
    MCMCs = c('block', 'ess'),
    MCMCdefs = list(
        block = quote({
            spec <- configureMCMC(Rmodel, nodes = NULL)
            spec$addSampler('mu', 'RW', print=FALSE)
            spec$addSampler(c('sigma', 'rho'), 'RW_block', print=FALSE)
            spec$addSampler('g[1:148]', 'RW_block', print=FALSE)
            spec
        }),
        ess = quote({
            spec <- configureMCMC(Rmodel, nodes = NULL)
            spec$addSampler('mu', 'RW', print=FALSE)
            spec$addSampler(c('sigma', 'rho'), 'RW_block', print=FALSE)
            spec$addSampler('g[1:148]', 'ess', print=FALSE)
            spec
        })),
    niter = 10000,
    burnin = 2000,
    monitors = c('mu', 'sigma', 'rho', 'g'),
    makePlot = FALSE
)

out$timing
out$summary[,,1:5]    ## WRONG again...
cov(t(out$samples['block', -(1:3), ])) - cov(t(out$samples['ess', -(1:3), ]))






## Taken from Carl's lab notebook:
## Gpdp Via MDPtoolbox Cont; 30 Jul 2015
## http://www.carlboettiger.info/2015/07/30/gpdp-via-mdptoolbox-cont.html
## (Markov Decision Processes (MDP) toolbox)
##knitr::opts_chunk$set(eval=FALSE)
##devtools::install_github("cboettig/gpmanagement@b3b765cbceb51c9b0b8cb2724e395353ec365df9")
library("MDPtoolbox")
library("gpmanagement")
library("tidyr")
library("dplyr")
library("ggplot2")

## True model
p <- c(2, 100, 50)
f <- function (x, h){
    sapply(x, function(x) {
        x <- pmax(0, x - h)
        x * exp(p[1] * (1 - x/p[2]) * (x - p[3])/p[2])      #### Allen model
    })
}

sigma_g <- 0.1
pdfn <- function(x, mu, sigma = sigma_g){
    dlnorm(x, log(mu), sdlog = sigma)
}

z_g <- function() rlnorm(1, 0, sigma_g)
set.seed(0)
Tobs <- 40

x <- numeric(Tobs)
x[1] <- 60
for(t in 1:(Tobs-1)) x[t+1] = z_g() * f(x[t], h=0)
obs <- data.frame(x = c(0, pmax(rep(0,Tobs-1), x[1:(Tobs-1)])), 
                  y = c(0, x[2:Tobs]))
xObs <- obs$x
yObs <- obs$y
xPred <- seq(0, 1.1 * max(xObs), length = 50)
qplot(seq_along(x), x) + geom_line()

## Now the GP estimation from NIMBLE. Letâ€™s emphasize shorter length-scales with the prior to compare:

## having this work with output from `nimbleCode` might be more natural than `expression`
priors <- expression({
  rho ~ dgamma(1, 1)
  sigGP ~ dunif(0, 1e5)
  sigOE ~ dunif(0, 1e5)
})

fit <- gp_setup(xObs, yObs, xPred)

Cmcmc <- fit$Cmcmc 
Cpred <- fit$Cpred
Cmodel <- fit$Cmodel
system.time(Cmcmc$run(100000))
samples <- as.matrix(Cmcmc$mvSamples)
## basic sanity check
testthat::expect_identical(Cmodel$getNodeNames(topOnly = TRUE), colnames(samples))

## predict from GP model using posterior MCMC samples
system.time(Cpred$run(samples))

## Posteriors
samples <- as.data.frame(as.matrix(Cmcmc$mvSamples))
df <- tidyr::gather(samples)

ggplot(df) + 
  geom_density(aes(value)) + 
  facet_wrap(~key, scale='free')

## extract predictions: E and C
E <- Cpred$getE()
C <- Cpred$getC()

obs <- data.frame(x = xObs, y = yObs)
pred <- data.frame(x = xPred, y = E, ymin = E - sqrt(diag(C)), ymax = E + sqrt(diag(C)))

ggplot2::ggplot(pred) + 
  geom_ribbon(aes(x = x,y = y, ymin = ymin, ymax = ymax), fill = "grey80") +
  geom_line(aes(x = x, y = y), size=1) + 
  geom_point(data = obs, aes(x,y)) +
  coord_cartesian(xlim = range(c(xObs, xPred)), ylim = range(c(yObs,E))) +
  theme_bw()

## Decision theory
states <- xPred # Vector of all possible states
actions <- states # Vector of actions: harvest
## Letâ€™s consider a slight variation of the most trivial utility function:
## one which explicitly adds a cost to completely exhausting the stock
## (or reducing the stock by more than, say 95% in this case.)
## This should be somewhat similar to the impact of no discount rate.

## Utility function
discount = 0.99

#get_utility <- function(x,h) pmin(x,h)
#R <- outer(states, actions, get_utility)

R <- sapply(actions, function(h){
    sapply(states, function(x){
        if(h < x) h else - 1 * max(states)
    })
})

## Implementing policy
z <- function() rlnorm(1, meanlog = 0, sdlog = sigma_g)

simulate_policy <- function(states, actions, policy, f, z, s0, steps = 50, utility = function(s,a) NA, discount = 1){
    s <- numeric(steps)
    a <- numeric(steps)
    u <- numeric(steps)
    s[1] <- s0
    for(t in 1:(steps-1)){
        a[t] <- actions[policy[which.min(abs(states - s[t]))]]
        s[t+1] <- z() * f(s[t], a[t])
        u[t] <- utility(s[t], a[t]) * discount ^ t
    }
    ## Final action determined but not implemented
    a[steps] <- actions[policy[which.min(abs(states - s[t]))]]
    data.frame(time = 1:steps, state = s, action = a, utility = u)
}

## GP model
gp_matrix <- function(states, actions, E, C){
    transition <- array(0, dim = c(length(states), length(states), length(actions)))
    K <- length(states)
    sigmas <- sqrt(diag(C))
    for (k in 1:length(states)) {
        for (i in 1:length(actions)) {
            nextpop <- E[k] - actions[i]
            if(nextpop <= 0) {
                transition[k, , i] <- c(1, rep(0, K - 1))
            } else {
                transition[k, , i] <- dnorm(states, nextpop, sigmas[i]) / sum(dnorm(states, nextpop, sigmas[i]))
            }
        }
    }
    transition
}

P_gp <- gp_matrix(states, actions, E, C)
mdp_check(P = P_gp, R = R)

gp <- mdp_value_iteration(P_gp, R, discount = discount, epsilon = 0.00001, max_iter = 5e3, V0 = numeric(length(states)))

plot(states, states - actions[gp$policy],  xlab="Population size", ylab="Escapement")

data.frame(reps = 1:50) %>% 
    group_by(reps) %>% 
        do(simulate_policy(states, actions, gp$policy, f, z, s0 = 100, steps = 20, utility = pmin, discount = discount)[c("time", "state", "utility")]) -> sims 

mean(sims$utility)
ggplot(sims) + geom_line(aes(time, state, group = reps), alpha = 0.3, col = "darkblue")
## With this amount of data, the gp solution is too cautious, and avoids any exploitation.

## Simulate under the true model

data.frame(reps = 1:50) %>% 
    group_by(reps) %>% 
        do(simulate_policy(states, actions, gp$policy, f, z, s0 = 100, steps = 20, utility = pmin, discount = discount)[c("time", "state", "utility")]) -> sims 

mean(sims$utility)
## (Average utility is approximate here since it does not include penalty;
## since a function and not a matrix is requred by this function at this time.)

ggplot(sims) + geom_line(aes(time, state, group = reps), alpha = 0.3, col = "darkblue")

P <- transition_matrix(states, actions, f, pdfn)
mdp_check(P = P, R = R)

mdp <- mdp_value_iteration(P, R, discount = discount, epsilon = 0.001, max_iter = 5e3, V0 = numeric(length(states)))
plot(states, states - actions[mdp$policy],  xlab="Population size", ylab="Escapement")

## Note that the altered award structure has almost no effect on the optimal policy
## given the true model, other than to avoid harvesting directly to zero even when
## the stock cannot persist, due to the explicit penalty for doing so.

data.frame(reps = 1:50) %>% 
    group_by(reps) %>% 
        do(simulate_policy(states, actions, mdp$policy, f, z, s0 = 100, steps = 20, utility = pmin, discount = discount)[c("time", "state", "utility")]) -> sims 

mean(sims$utility)

ggplot(sims) + geom_line(aes(time, state, group = reps), alpha = 0.3, col = "darkblue")





## Taken from Carl's lab notebook:
## MDPtoolbox Allen Model; 29 Jul 2015
## http://www.carlboettiger.info/2015/07/29/mdptoolbox-allen-model.html
## (Markov Decision Processes (MDP) toolbox)
library("MDPtoolbox", quietly = TRUE)
library("ggplot2", quietly = TRUE)
K <- 10 # state space limit
states <- 0:K # Vector of all possible states
actions <- states # Vector of actions: harvest

sigma_g = 0.1
p <- c(2, 15, 5)

f <- function (x, h){
    sapply(x, function(x) {
        x <- pmax(0, x - h)
        x * exp(p[1] * (1 - x/p[2]) * (x - p[3])/p[2])      #### Allen model
    })
}

pdfn <- function(x, mu, sigma = sigma_g){
    dlnorm(x, log(mu), sdlog = sigma)
}

## Utility function
discount = 0.95
get_utility <- function(x,h) {
    pmin(x,h)
}

R <- outer(states, actions, get_utility)

transition_matrix <- function(states, actions, f, pdfn){
    ## Initialize
    transition <- array(0, dim = c(length(states), length(states), length(actions)))
    K <- length(states)
    for (k in 1:length(states)) {
        for (i in 1:length(actions)) {
            ## Calculate the transition state at the next step, given the 
            ## current state k and action i (harvest H[i])
            nextpop <- f(states[k], actions[i])
            ## Population always extinct if this is negative.
            ## since multiplicitive shock z_t * f(n) < 0 for all f(n) < 0
            if(nextpop <= 0)
                transition[k, , i] <- c(1, rep(0, length(states) - 1))
            ## Implement demographic stochasticity 
            else {
                ## Cts distributions need long-tailed denominator as normalizing factor:
                fine_states <- seq(min(states), 10 * max(states), by = states[2] - states[1])
                N <- sum(pdfn(fine_states, nextpop))  
                transition[k, , i] <-pdfn(states, nextpop) / N
                ## We need to correct this density for the final capping state ("Pile on boundary")
                ## (discrete or cts case)
                ## this can be a tiny but negative value due to floating-point errors.
                ## so we take max(v,0) to avoid
                transition[k, K, i] <- max(1 - sum(transition[k, -K, i]), 0)
            }
        } 
    }
    transition
}

P <- transition_matrix(states, actions, f, pdfn)
apply(P, c(1,3), sum)  ## double-check

## Using toolbox
mdp_check(P = P, R = R)
mdp <- mdp_value_iteration(P, R, discount = discount, epsilon = 0.001, max_iter = 5e3, V0 = numeric(length(states)))
plot(states, states - actions[mdp$policy],  xlab="Population size", ylab="Escapement")

## Compare to Reed
## From Reed (1979) we know that the optimal solution is a constant-escapement rule
## when the growth function in convex. Note that this condition is violated by the
## growth function with alternative stable states (Allen/Ricker-Allee model),
## resulting in a very different optimal policy:
## f^prime(s^star) = 1/discount
## For growth-rate function f, where discount is the discount factor and sâˆ— the stock size
## for the constant escapement. Analytic solutions are clearly possible for certain
## growth functions, but here Iâ€™ve just implemented a generic numerical solution.
fun <- function(x) - f(x,0) + x / discount
out <- optimize(f = fun, interval = c(0,K))
S_star <- out$minimum
exact_policy <- sapply(states, function(x) if(x < S_star) 0 else x - S_star)
D <- actions[mdp$policy]
# The difference between Bellman and the analytical solution is small:
plot(states, states - D,  xlab="Population size", ylab="Escapement", ylim = c(0, 1.2*max(states-D)))
lines(states, states - exact_policy)




## Carl's examples using MDPtoolbox
## MDPtoolbox Ex 2; 14 Jul 2015
## (Markov Decision Processes (MDP) toolbox)
## http://www.carlboettiger.info/2015/07/14/mdptoolbox-ex-2.html
## Adapted from Marescot et al. appendix 5, to Reed optimal control problem, including direct comparison against (semi) analytic optimum.

## step 1: define objectives
## This is a conceptual step which does not require coding
## step 2: define states
##K <- 150 # state space limit
K <- 10 # state space limit
states <- 0:K # Vector of all possible states

## step 3: define control actions
## Vector of actions: harvest
H <- states

## step 4: define dynamic model (with demographic parameters)
##p <- c(6,0.05)
p <- c(3, 0.2)
f <- function(x, h){
  A <- p[1] 
  B <- p[2] 
  s <- pmax(x-h, 0)
  A * s/(1 + B * s)   #### Bellman equation
}
sigma_g = 0.1

## step 5: define utility
## Utility function
get_utility <- function(x,h) {
    pmin(x,h)
}

## step 6: solve bellman equation with value iteration
## Initialize transition matrix
transition <- array(0, dim = c(length(states), length(states), length(H)))
## Initialize utility matrix
utility <- array(0, dim = c(length(states), length(H)))
## Fill in the transition and utility matrix
## Loop on all states
for (k in 0:K) {
    ## Loop on all actions
    for (i in 1:length(H)) {
        ## Calculate the transition state at the next step, given the 
        ## current state k and the harvest H[i]
        nextpop <- f(k, H[i])
        if(nextpop <= 0)
            transition[k+1, , i] <- c(1, rep(0, length(states) - 1))
        ## Implement demographic stochasticity by drawing probability from a density function
        else {
            ## We need to correct this density for the final capping state ("Pile on boundary").
            ## For discrete probability distribution,
            ## this is easy if `states` includes all possible
            ## discrete states below the capping state.
            ## (e.g. all non-negative integers less than K).  
            ## For a continuous distribution, this is more problematic
            ## as we have to first normalize the densities.
            ## EDIT: this can be negative, due to floating-point errors.
            ## so we take max(v,0) to avoid.
            ## Get long-tailed denominator as normalizing factor (continuous distributions only):
            fine_states <- seq(min(states), 10 * max(states), by = states[2]-states[1])
            N <- sum(dlnorm(fine_states, log(nextpop), sdlog = sigma_g))
            transition[k+1, , i] <- dlnorm(states, log(nextpop), sdlog = sigma_g) / N
            ## We need to correct this density for the final capping state ("Pile on boundary")
            transition[k+1, K+1, i] <- max(1 - sum(transition[k+1, -(K+1), i]), 0)
        }
        ## Compute utility
        utility[k+1, i] <- get_utility(k, H[i])
    } # end of action loop
} # end of state loop

## Solution calculated explicitly:
## The backward iteration consists in storing action values in the vector
## Vt which is the maximum of utility plus the future action values for all
## possible next states. Knowing the final action values, we can then backwardly
## reset the next action value Vtplus to the new value Vt.  We start The backward
## iteration at time T-1 since we already defined the action value at Tmax.

## Discount factor
discount <- 0.95
## Action value vector at tmax
Vtmax <- numeric(length(states))
## Action value vector at t and t+1
Vt <- numeric(length(states))
Vtplus <- numeric(length(states))
## Optimal policy vector
D <- numeric(length(states))
## Time horizon
##Tmax <- 150
Tmax <- 4

for (t in (Tmax - 1):1) {
    ## We define a matrix Q that stores the updated action values for 
    ## all states (rows)
    ## actions (columns)
    Q <- array(0, dim = c(length(states), length(H)))
    for (i in 1:length(H)) {
        ## For each harvest rate we fill for all states values (row) 
        ## the ith column (Action) of matrix Q
        ## The utility of the ith action recorded for all states is 
        ## added to the product of the transition matrix of the ith 
        ## action by the action value of all states 
        Q[,i] <- utility[, i] + discount * (transition[,,i] %*% Vtplus)
    } # end of the harvest loop
    ## Find the optimal action value at time t is the maximum of Q
    Vt <- apply(Q, 1, max)
    ## After filling vector Vt of the action values at all states, we 
    ## update the vector Vt+1 to Vt and we go to the next step standing 
    ## for previous time t-1, since we iterate backward
    Vtplus <- Vt
} # end of the time loop

## Find optimal action for each state
for (k in 0:K) {
    ## We look for each state which column of Q corresponds to the 
    ## maximum of the last updated value 
    ## of Vt (the one at time t + 1). If the index vector is longer than 1 
    ## (if there is more than one optimal value we chose the minimum 
    ## harvest rate)
    D[k + 1] <- H[(min(which(Q[k + 1, ] == Vt[k + 1])))]
}

## plot solution
plot(states, states - D, xlab="Population size", ylab="Escapement")

## proof of optimality: compare with analytical solution
fun <- function(x) - f(x,0) + x / discount
out <- optimize(f = fun, interval = c(0,K))
S_star <- out$minimum
exact_policy <- sapply(states, function(x) if(x < S_star) 0 else x - S_star)

## The difference between Bellman equation solution and the analytical solution is small:
plot(states, states - D, xlab="Population size", ylab="Escapement", ylim = c(0, 1.2*max(states-D)))
lines(states, states - exact_policy)

## Using MDPtoolbox
library('MDPtoolbox')
mdp_check(P = transition, R = utility)
out <- mdp_value_iteration(transition, utility, discount = discount, epsilon = 0.001, max_iter = 5e3, V0 = Vtmax)
plot(states, states - D, xlab="Population size", ylab="Escapement")
lines(states, states - H[out$policy], col="red", lty=2)




## testing using new NIMBLE option: verbose
nimbleOptions('verbose')
nimbleOptions(verbose = FALSE)
nimbleOptions('verbose')

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)


## testing the new includeEfficiency option for MCMCsuite
## calculates N, ESS, and Eff

library(nimble)
load('~/GitHub/legacy/autoBlock/data/model_litters.RData')
load('~/GitHub/legacy/autoBlock/data/model_ice.RData')

out <- MCMCsuite(code, constants, data, inits,
##                 MCMCs = c('nimble', 'nimble_slice', 'autoBlock'),
                 MCMCs = c('nimble', 'nimble_slice'),
                 makePlot = FALSE,
                 calculateEfficiency = TRUE
                 )

out$timing

out$summary

apply(out$summary[, 'efficiency', ], 1, min)
apply(out$summary[, 'efficiency', ], 1, mean)

out$efficiency


## running OpenBUGS installation from OSX (using wine)

##library(R2OpenBUGS)
library(R2WinBUGS)
library(BRugs)

## schools data in the R2OpenBUGS library
data(schools)
schools

## define the model
nummodel <- function() {
    for (j in 1:J) {
        y[j] ~ dnorm(theta[j], tau.y[j])
        theta[j] ~ dnorm(mu.theta, tau.theta)
        tau.y[j] <- pow(sigma.y[j], -2)
    }
    mu.theta ~ dnorm(0, 1E-6)
    tau.theta <- pow(sigma.theta, -2)
    sigma.theta ~ dunif(0, 1000)
}

## write the model code out to a file
write.model(nummodel, 'nummodel.txt')
model.file1 = paste(getwd(), 'nummodel.txt', sep='/')
## and let's take a look:
file.show('nummodel.txt')

## prepare the data for input into OpenBUGS
J <- nrow(schools)
y <- schools$estimate
sigma.y <- schools$sd
data <- list(J = J, y = y, sigma.y = sigma.y)

## initialization of variables
inits <- function() {
    list(theta = rnorm(J, 0, 100), mu.theta = rnorm(1, 0, 100), sigma.theta = runif(1, 0, 100))
}

## set the WINE working directory and the directory to OpenBUGS
## change the OpenBUGS.exe location as necessary
WINE <- '/opt/local/bin/wine'
WINEPATH <- '/opt/local/bin/winepath'
##OpenBUGS.pgm <- '/Users/dturek/.wine/drive_c/Program Files/OpenBUGS/OpenBUGS323/OpenBUGS.exe'
bugs.directory <- '/Users/dturek/.wine/drive_c/Program Files/OpenBUGS/OpenBUGS323'

## these are the parameters to save
parameters = c('theta', 'mu.theta', 'sigma.theta')

## run the model
schools.sim <- bugs(data, inits, model.file = model.file1, parameters=parameters,
                    n.chains = 3, n.iter = 1000,
                    ##OpenBUGS.pgm = OpenBUGS.pgm,   ## syntax for R2OpenBUGS::bugs
                    program = 'OpenBUGS',
                    bugs.directory = bugs.directory,
                    WINE = WINE, WINEPATH = WINEPATH, useWINE = TRUE)

## R will pause
## When model is complete a prompt will reappear
print(schools.sim)


## debugging commands in R
## n: next
## c: continue
## s: step into function call
## f: step out of current function call



library(nimble)
modelName <- 'rats'
rats <- readBUGSmodel(model = modelName, dir = getBUGSexampleDir(modelName), returnModelComponentsOnly = TRUE)

code <- rats$model
constants <- rats$data[c('N','T','x')]
data <- rats$data[c('Y')]
inits <- rats$inits

Rmodel <- nimbleModel(code, constants, data, inits, dimensions = rats$dims)

spec <- configureMCMC(Rmodel)
spec$getSamplers()




## creating dendogram figure of autoBlocking for JSM presentation
library(nimble)
load('~/GitHub/legacy/autoBlock/data/model_litters.RData')
Rmodel <- nimbleModel(code, constants, data, inits)
Rmcmc <- buildMCMC(Rmodel, autoBlock = TRUE, makePlots = TRUE)




## working on the conjugacy system, to infer dimensions of 'contributions'
library(nimble)
getDistributionsInfo('dnorm')



## let's manually run all NIMBLE testing
## (to find what's crashing, easier than using travis)
library(nimble)
library(testthat)
setwd('~/GitHub/nimble/nimble/packages/nimble/inst/tests/')
source('test_utils.R')

testFiles <- list.files()
print(testFiles)

##### this listing is out of date!
test_package('nimble', 'copy')      ## pass
test_package('nimble', 'dsl_dists') ## pass
test_package('nimble', 'math')      ## pass
test_package('nimble', 'mcmc')      ## errors/warnings copied below
test_package('nimble', 'meta')      ## pass
test_package('nimble', 'models')    ## 
test_package('nimble', 'trunc')     ## 
test_package('nimble', 'user')      ## 



## fully rebuild NIMBLE user manual

setwd('~/GitHub/nimble/nimble-docs/UserManual/')
library(knitr) 
knit2pdf('NimbleUserManual.Rnw') 
system('open NimbleUserManual.pdf')


## investigating the bug in R M-H sampler

library(nimble)
code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
spec <- configureMCMC(Rmodel, nodes = NULL)
spec$addSampler('a', 'RW')
Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
niter <- 1000

set.seed(0); Rmcmc$run(niter)
set.seed(0); Cmcmc$run(niter)

Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rsamples
Csamples

d <- Rsamples - Csamples
all(d == 0)



Rmodel$nodes[['a']]$calculateDiff

Rmodel$logProb_a
Cmodel$logProb_a

calculateDiff(Rmodel, 'a')
calculateDiff(Cmodel, 'a')


nimble:::rCalcDiffNodes(Rmodel, 'a')
nimble:::rCalcDiffNodes(Cmodel, 'a')


## testing nimbleOptions() system
## and also the new nimble option: MCMCcontrolDefaultList

library(nimble)

getNimbleOption('MCMCcontrolDefaultList')

environment(sampler_conjugate_dnorm)$nfRefClassDef

getNimbleOption('verifyConjugatePosteriors')
nimble:::setNimbleOption('verifyConjugatePosteriors', TRUE)
getNimbleOption('verifyConjugatePosteriors')
environment(sampler_conjugate_dnorm)$nfRefClassDef
buildConjugateSamplerFunctions()
environment(sampler_conjugate_dnorm)$nfRefClassDef


code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dgamma(a, 2)
    c ~ dbin(b, 10)
    d ~ dnorm(c, 1)
})
constants <- list()
data <- list()
inits <- list(a = 1, b = .5, c = 4, d = 1)

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()

lst <- getNimbleOption('MCMCcontrolDefaultList')
lst
lst$sliceWidth <- 99
lst$scale <- 9999
lst
nimbleOptions(MCMCcontrolDefaultList = lst)
getNimbleOption('MCMCcontrolDefaultList')

spec <- configureMCMC(Rmodel)
spec$getSamplers()


## complete example of using RW_llFunction sampler,
## for the User Manual

library(nimble)

code <- nimbleCode({
    p ~ dunif(0, 1)
    y ~ dbin(p, n)
})

Rmodel <- nimbleModel(code, data = list(y = 3),
                      inits = list(p = 0.5, n = 10))

llFun <- nimbleFunction(
    setup = function(model) { },
    run = function() {
        y <- model$y
        p <- model$p
        n <- model$n
        ll <- lfactorial(n) - lfactorial(y) - lfactorial(n-y)
                 + y*log(p) + (n-y)*log(1-p)
        returnType(double())
        return(ll)
    }
)

RllFun <- llFun(Rmodel)

mcmcspec <- configureMCMC(Rmodel, nodes = NULL)

mcmcspec$addSampler(target = 'p', type = 'RW_llFunction',
    control = list(llFunction = RllFun, includesTarget = FALSE))

Rmcmc <- buildMCMC(spec)





## testing new nimble options system

library(nimble)
nimbleOptions()
ls(nimble:::.nimbleOptions)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dgamma(a, 1)
    c ~ dbin(b, 10)
    d ~ dnorm(c, 1)
})
constants <- list()
data <- list()
inits <- list(a = 1, b=.5, c=1, d=0)

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)



## working on dipper models

rm(list=ls())
library(nimble)
trunc <- TRUE
load('~/GitHub/userDistMCMC/dipperData.RData')
## optionally truncate data:
last <- apply(y, 1, function(hist) max(which(hist==1)))
yDHMM <- 2 - y
onesMatrix = matrix(1,nind,k)  ## 'ones' matrix for survival
onesVector = rep(1,nind)       ## 'ones' vector for chi
if(trunc) { ind <- 1:3;   nind<-length(ind);   first<-first[ind];   last<-last[ind];   y<-y[ind,,drop=FALSE];   yDHMM<-yDHMM[ind,,drop=FALSE];   x_init<-x_init[ind,,drop=FALSE];   onesMatrix<-onesMatrix[ind,,drop=FALSE];   onesVector<-onesVector[ind] }
code <- quote({
    phi ~ dunif(0, 1)
    p ~ dunif(0, 1)
    for(i in 1:nind) {
        for(t in (first[i] + 1):last[i]) {
            y[i,t] ~ dbin(p, 1)
            ##onesMatrix[i,t] ~ dbin(phi, 1)
        }
        ##onesVector[i] ~ dbin(chi[last[i]], 1)
    }
    ##chi[k] <- 1
    ##for(t in 1:(k-1)) {
    ##chi[k-t] <- (1-phi) + phi * (1-p) * chi[k-t+1]
    ##}
})
constants <- list(k=k, nind=nind, first=first, last=last)
data      <- list(y=y)##, onesMatrix=onesMatrix, onesVector=onesVector)
inits     <- list(phi=0.6, p=0.9)

md <- nimbleModel(code, constants, data, inits, returnDef = TRUE)


Rmodel <- nimbleModel(code, constants, data, inits)



## reproducible example of problem in new igraph package
## emailed this example to Gabor


remove.packages('igraph')
install.packages('igraph')

library(nimble)
code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a, 1)
})
constants <- list()
data <- list()
inits <- list(a=0, b=0)

Rmodel <- nimbleModel(code, constants, data, inits)


library(igraph)
graph <- graph.empty()
graph <- add.vertices(graph, 2, name = c('a', 'b'))
graph <- add.edges(graph, c(1, 2))
toposortReturn <- topological.sort(graph, mode = 'out')
print(toposortReturn)
class(toposortReturn)



oldGraphID_2_newGraphID <- sort(newGraphID_2_oldGraphID, index = TRUE)$ix
graph <<- permute.vertices(graph, oldGraphID_2_newGraphID)  # re-label vertices in the graph



## writing stan model file for RSBS
##source('http://mc-stan.org/rstan/install.R', echo = TRUE, max.deparse.length = 2000)
##install_rstan()

load('~/GitHub/legacy/automated-blocking-examples/data/model_redblue.RData')
constantsAndData <- c(constants, data)

library(rstan)

stan_mod <- stan_model(file = '~/temp/redblue.stan')

stan_out <- sampling(stan_mod, data=constantsAndData, chains=1, iter=10000, thin=1, init=list(inits))

tempArray <- extract(stan_out, permuted = FALSE, inc_warmup = TRUE)[, 1, ]
dimnames(tempArray)[[2]] <- gsub('_', '.', dimnames(tempArray)[[2]])
if(!all(monitorNodesBUGS %in% dimnames(tempArray)[[2]])) {
    missingNames <- setdiff(monitorNodesBUGS, dimnames(tempArray)[[2]])
    warning(paste0('Stan output is missing values for: ', paste0(missingNames,collapse=', ')))
}
samplesArray <- array(0, dim = c(nkeep, length(monitorNodesBUGS)))
dimnames(samplesArray)[[2]] <- monitorNodesBUGS
monitorsWeHave <- intersect(monitorNodesBUGS, dimnames(tempArray)[[2]])
samplesArray[, monitorsWeHave] <- tempArray[(burnin+1):floor(niter/thin), monitorsWeHave, drop=FALSE]
addToOutput('stan', samplesArray, timeResult)

## playing around to build, install, run, and test gpmanagement package

q('no')
library(devtools)
setwd('~/GitHub/forks/gpmanagement')
remove.packages('gpmanagement')
install_github('danielturek/gpmanagement')
##document('gpmanagement')
system('R CMD build gpmanagement')
system('R CMD INSTALL .')

library(nimble)
library(testthat)
library(gpmanagement)

check('.')  ## this gives me the errors 

gp_setup

test_package('gpmanagement')

source('~/GitHub/forks/gpmanagement/tests/testthat/test_gp.R')

##setwd('~/GitHub/forks/gpmanagement/tests/')
##test_check('gpmanagement')

nf <- gpmanagement:::nfd(3)
nf$run()

set.seed(0)
x <- 1:100
y <- sin(x/5) + rnorm(100, 0.1)
ind <- sort(sample(1:100, 40))
xObs <- x[ind]                ## input
yObs <- y[ind]                ## input
xPred <- c(x, 101:120)        ## input

fit <- gp_setup(xObs, yObs, xPred)


## testing of 3-D array arguments, and their automatic demotion to smaller dim arrays

library(nimble)

nf1 <- nimbleFunction(
    run = function(a = double(2), b = double(3)) {
        print('a11: ', a[1,1])
        print('b111: ', b[1,1,1])
        print(dim(a)[1])
        print(dim(a)[2])
        print(dim(b)[1])
        print(dim(b)[2])
        print(dim(b)[3])
    }
)

nf <- nimbleFunction(
    run = function() {
        a <- nimArray(0, 3, 3)
        declare(b, double(3, c(3,3,3)))
        for(i in 1:3)
            for(j in 1:3) {
                a[i,j] <- 10*i + j
                for(k in 1:3)
                    b[i,j,k] <- 100*i+10*j+k
            }
        print(a)
        ##print(b)
        nf1(a[2,1:2], b[,,])
    }
)

nf()
Cnf <- compileNimble(nf)
Cnf()


## testing distribution dDHMM on a multistate example

CdDHMM <- compileNimble(dDHMM)
CrDHMM <- compileNimble(rDHMM)

checkD <- function(ind) {
    y <- yDHMM
    print(y[ind,])
    x <- y[ind, first[ind]:k]
    length <- length(x)
    phi <- 0.6
    p <- 0.9
    T <- array(c(phi, 1-phi, 0, 1), c(2,2))
    Z <- array(c(p,   1-p,   0, 1), c(2,2))
    pi <- c(1, 0)
    condition <- c(1, 0)
    print(dDHMM(x, length, pi, Z, T, condition, 0))
    print(CdDHMM(x, length, pi, Z, T, condition, 0))
    print(exp(dDHMM(x, length, pi, Z, T, condition, 1)))
    print(exp(CdDHMM(x, length, pi, Z, T, condition, 1)))
}
checkD(225)   ## 0.2484
checkD(202)   ## 0.017496
checkD(167)   ## 0.1246882

checkR <- function(length) {
    phi <- 0.6
    p <- 0.9
    T <- array(c(phi, 1-phi, 0, 1), c(2,2))
    Z <- array(c(p,   1-p,   0, 1), c(2,2))
    pi <- c(1, 0)
    condition <- c(1, 0)
    print(rDHMM(1, length, pi, Z, T, condition))
    print(CrDHMM(1, length, pi, Z, T, condition))
}
checkR(1)
checkR(2)
checkR(3)
checkR(10)


Z <- array(c(.9,.05,.05,.1,.8,.1,0,1/2,1/2), c(3,3))
Z
T <- array(c(0,0,1,0,0,1,0,0,1), c(3,3))
T
pi <- c(1, 1, 1)
condition <- c(1, 1, 1)

y <- c(1,1)
length <- length(y)
dDHMM( y, length, pi, Z, T, condition, 0)
CdDHMM(y, length, pi, Z, T, condition, 0)


## testing new DSL functions:
## nimVector()
## nimArray()

library(nimble)

Rnf <- nimbleFunction(
    run = function(val = double(), len = double(), r = double(), c = double()) {
        onesVec <- nimVector(1, 5)
        print('onesVec:');   print(onesVec)
        zerosVec <- nimVector(0, 5)
        print('zerosVec:');   print(zerosVec)
        otherVec <- nimVector(val, len)
        print('otherVec:');   print(otherVec)
        sumVec <- zerosVec + onesVec + otherVec[1:5]
        print('sumVec:');   print(sumVec)
        arr <- nimArray(.4, r, c)
        print('arr:');   print(arr)
        print('sum of otherVec: ', sum(otherVec))
        print('sum of arrL: ', sum(arr))
    }
)

Cnf <- compileNimble(Rnf)

Rnf(10, 7, 3, 4)
Cnf(10, 7, 3, 4)

## trying nimVector() and nimArray() in BUGS code ?!?
## it works!!
code <- nimbleCode({
    a[1:4] <- nimVector(2, 4)
    phi ~ dnorm(0, 1)
    b[2:10] <- nimVector(10, 9)
    b[1] <- phi
})

Rmodel <- nimbleModel(code)
simulate(Rmodel, 'phi')
calculate(Rmodel)

Cmodel <- compileNimble(Rmodel)

model <- Rmodel
model <- Cmodel

model$a
model$phi
model$b

simulate(model)
calculate(model)

## re-creating NA's data example error
## 11 June 2015:
## Perry said he might get to this by July
## running tests/test-copy.R also gives this same error

library(nimble)

code <- nimbleCode({
    y[2:4] ~ dmnorm(mu3[1:3], cov = cov3[1:3, 1:3])
})
constants <- list(mu3=rep(0,3), cov3=diag(3))
data <- list(y = c(NA,0,0,0))

Rmodel <- nimbleModel(code, constants, data)

calculate(Rmodel)
simulate(Rmodel)

Cmodel <- compileNimble(Rmodel)
calculate(Rmodel)
simulate(Rmodel)


## same error from test-copy.R
library(testthat)
library(nimble)
test_package('nimble', 'copy')




## Zhenglei's example, trying MCMC suite on it

library(lme4)
library(ggplot2)
library(scales)
library(nimble)
library(coda)

I <- 25
J <- 5
beta <- c(-0.2, 0.1)
theta <- 1
sigma <-0.5

simfun <- function(substance=letters[1:10],experiment=factor(1:5),log10HQ=seq(-1,3,by=0.2),theta=1,sigma=0.1,beta=c(-0.2, 0.5)){
    expdat <- expand.grid(experiment = experiment,log10HQ=log10HQ,substance=substance)
    expdat$HQ <- 10^expdat$log10HQ
    expdat$obs <- factor(seq(nrow(expdat)))
    n <- nrow(expdat)
    nsub <- length(substance)
    re <- rnorm(nsub,0,theta)
    names(re) <- substance
    fixed <- beta[1]+beta[2]*expdat$log10HQ
    expdat$y <- re[expdat$substance]+rnorm(n,0,sigma)+fixed
    expdat$effect <- gtools::inv.logit(expdat$y)
    return(expdat)
}

expdat <- simfun(substance=factor(paste0('S',1:I)),experiment = factor(1:J),log10HQ=seq(-1,3,by=0.2),theta=0.5,beta=c(-0.2,0.5),sigma=0.3)

code <- nimbleCode({
    alpha ~ dnorm(0, 0.001)
    beta ~ dnorm( 0, 0.001 )
    tau.y ~ dgamma(0.001, 0.001)  
    tau.re ~ dgamma(0.001, 0.001)
    for(j in 1:M){
        ranef.v[j]~dnorm(0, tau.re)
    }
    for (i in 1:N){
        y[i] ~ dnorm ( mu.y[i], tau.y )
        mu.y[i] <- alpha+beta*x[i] + ranef.v[unit[i]]
    }
})

constants <- list(N = nrow(expdat),M=nlevels(expdat$substance),x = expdat$log10HQ,unit=as.numeric(expdat$substance))
##data <- data.frame(y = expdat$y)
data <- list(y = expdat$y)
inits <- list(alpha = 0, beta = 0.5, tau.y=1,tau.re=1)

out <- MCMCsuite(
    code, constants, data, inits,
    MCMCs = c('nimble', 'jags', 'noConj'),
    niter = 20000,
    makePlot = TRUE,
    savePlot = TRUE,
    summaryStats = c('mean', 'median', 'sd', 'CI95_low', 'CI95_upp', 'effectiveSize'),
    MCMCdefs = list(noConj = quote({ configureMCMC(Rmodel, useConjugacy=FALSE) }))
)

out$summary[, 'effectiveSize', ] / out$timing[c('nimble', 'jags', 'noConj')]


## helping Nick get started with using node$get_mean() function for particle filter

library(nimble)

code <- nimbleCode({
    x ~ dnorm(1, 1)
    y ~ dnorm(x, 1)
})

Rmodel <- nimbleModel(code)
simulate(Rmodel)
Rmodel$x
Rmodel$y
Rmodel$nodeFunctions[['x']]$get_mean()
Rmodel$nodeFunctions[['y']]$get_mean()

## only necessary for the 'higherLevel' example
virtual_NF <- nimbleFunctionVirtual(
    ## don't need to supply prototpye for run(),
    ## since the default is no args, and returnType void
    methods = list(  ## definitely need prototype for return_mean() function
        return_mean = function() {
            returnType(double())
        }
    )
)

nfDef <- nimbleFunction(
    contains = virtual_NF,  ## only necessary for the 'higherLevel' example
    setup = function(model, node) {
        nfList <- nimbleFunctionList(node_stoch_dnorm)
        nfList[[1]] <- model$nodeFunctions[[node]]
    },
    run = function() {
        print('node value is: ', model[[node]])
        print('(assumed to be dnorm) node mean is: ', nfList[[1]]$get_mean())
    },
    methods = list(                        ## only necessary for the 'higherLevel' example
        return_mean = function() {         ##
            returnType(double())           ##
            return(nfList[[1]]$get_mean()) ##
        }                                  ##
    )                                      ##
)

nfX <- nfDef(Rmodel, 'x')
nfY <- nfDef(Rmodel, 'y')
nfX$run()
nfY$run()

compiledList <- compileNimble(list(Rmodel, nfX, nfY))
Cmodel <- compiledList[[1]]
CnfX <- compiledList[[2]]
CnfY <- compiledList[[3]]

CnfX$run()
CnfY$run()

CnfX$return_mean()
CnfY$return_mean()

## this is the 'higherLevel' example
nfDefHigherLevel <- nimbleFunction(
    setup = function(model, nodes) {
        nfList <- nimbleFunctionList(virtual_NF)
        for(i in seq_along(nodes))
            nfList[[i]] <- nfDef(model, nodes[i])
    },
    run = function() {
        declare(meansVector, double(1, length(nfList)))  ## declares a vector of doubles, with length equal to the length of nfList
        for(i in seq_along(nfList)) {
            nfList[[i]]$run()
            meansVector[i] <- nfList[[i]]$return_mean()
        }
        print('vector of the means is: ', meansVector)
    }
)

nfHigherLevel <- nfDefHigherLevel(Rmodel, c('x', 'y'))

nfHigherLevel$run()

CnfHigherLevel <- compileNimble(nfHigherLevel, project = Rmodel)

CnfHigherLevel$run()


## fixing values(model, nodes) <- value reference class '<<-' warning


library(nimble)

nfdef <- nimbleFunction(
    setup = function(model) {},
    run = function(val = double(1)) {
        values(model, 'xxx') <<- val
    }
)

code <- nimbleCode({
    xxx ~ dnorm(0, 1)
})

Rmodel <- nimbleModel(code)

nf <- nfdef(Rmodel)

Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(nf, project = Rmodel)

Rmodel$xxx
nf$run(3)
Rmodel$xxx

Cmodel$xxx
Cnf$run(4)
Cmodel$xxx


## testing / trying to find some error in conjugacy system(??)  May 2015

library(nimble)
source('~/GitHub/nimble/packages/nimble/inst/tests/test_utils.R')

## run entire MCMC testing system
source('~/GitHub/nimble/packages/nimble/inst/tests/test-mcmc.R')

set.seed(0)
mu0 = 1:3
Q0 = matrix(c(1, .2, .8, .2, 2, 1, .8, 1, 2), nrow = 3)
Q = solve(matrix(c(3, 1.7, .9, 1.7, 2, .6, .9, .6, 1), nrow = 3))
a = c(-2, .5, 1)
B = matrix(rnorm(9), 3)
code <- nimbleCode({
  mu[1:3] ~ dmnorm(mu0[1:3], Q0[1:3, 1:3])
  y_mean[1:3] <- asCol(a[1:3]) + B[1:3, 1:3] %*% asCol(mu[1:3])
  y[1:3] ~ dmnorm(y_mean[1:3], Q[1:3, 1:3])
})
mu <- mu0 + chol(solve(Q0)) %*% rnorm(3)
# make sure y is a vec not a 1-col matrix or get a dimensionality error
y <- c(a + B%*%mu + chol(solve(Q)) %*% rnorm(3))
data = list(mu0 = mu0, Q0 = Q0, Q = Q, a = a, B = B, y = y)
muQtrue = t(B) %*% Q%*%B + Q0
muMeanTrue = c(solve(muQtrue, crossprod(B, Q%*%(y-a)) + Q0%*%mu0))

constants <- list(mu0 = mu0, Q0 = Q0, Q = Q, B = B, a=a)
data <- list(y=y)
m <- nimbleModel(code, constants = constants, data=data)

m <- nimbleModel(code, constants = data)

spec <- configureMCMC(m)
Rmcmc <- buildMCMC(spec)
spec$getSamplers()

options(error = recover)

test_mcmc(model = code, data = data, seed = 0, numItsC = 10000,
          results = list(mean = list(mu = muMeanTrue),
                           cov = list(mu = solve(muQtrue))),
          resultsTolerance = list(mean = list(mu = rep(.02,3)),
            cov = list(mu = matrix(.01, 3, 3))))


## testing distribution of particle filter likelihood estimates

ll <- log(sapply(muvec, function(mu) mean(dnorm(y, rnorm(n, mu, sigx), sigy))))

plot(muvec, varll)
varest <- 1/2*sigx^4/sigy^4 + muvec^2*sigx^2/sigy^4
lines(muvec, varest, col='red')



## library(nimble)
## Rpf <- buildPF(Rmodel, 'x')
## Cmodel <- compileNimble(Rmodel)
## Cpf <- compileNimble(Rpf, project = Rmodel)
## Rpf$run(10)
## Cpf$run(10)
## ll <- numeric()
## for(i in seq_along(muvec)) {
##     mu <- muvec[i]
##     Cmodel$mu <- mu
##     ll[i] <- Cpf$run(10000)
## }
## plot(muvec, ll)


library(nimble)
source('~/GitHub/pfLL/pfLL.R')
m <- 1000
rep <- 1000
n <- m*rep
muvec <- seq(-5, 5, by=0.2)
sigx <- 1
sigy <- 1
taux <- 1/sigx^2; tauy <- 1/sigy^2
y <- 0   ## observation
constants <- list()
inits <- list(mu = y, sigx = sigx, sigy = sigy)
data <- list(y = y)
code <- quote({
    mu ~ dnorm(0, 0.00001)  ## these should not be necessary
    sigx ~ dunif(0, 1000)  ## these should not be necessary
    sigy ~ dunif(0, 1000)  ## these should not be necessary
    x ~ dnorm(mu, sigx)
    y ~ dnorm(x, sigy)
})
Rmodel <- nimbleModel(code, constants, data, inits)

out <- pfLL(Rmodel, 'x', param = data.frame(mu=muvec), m=m, rep=rep, makePlot=FALSE)

ll <- apply(out$ll, 1, mean)
##ll <- log(sapply(muvec, function(mu) mean(dnorm(y, rnorm(n, mu, sigx), sigy))))
plot(muvec, ll)
ll.pred <- -1/2 * muvec^2 / (sigx^2+sigy^2) + log(1/sqrt(2*pi*(sigx^2+sigy^2)))
lines(muvec, ll.pred, col='red')


varll <- apply(out$ll, 1, var)
##varll <- varll - min(varll)  ### TEMPORARY
plot(muvec, varll)
var.pred <- exp(-8.8 + 1.2592*abs(muvec))
lines(muvec, var.pred, col='red')

1



n <- 100000
sigx <- 1
muvec <- seq(-3, 3, by=0.1)
nmu <- length(muvec)
X <- array(NA, c(n, nmu))
for(i in 1:nmu) X[,i] <- rnorm(n, muvec[i], sigx)

Y <- X^2
var <- apply(Y, 2, var)
pred <- muvec^2 + sigx^2
plot(muvec, var, type='p')
lines(muvec, pred, col='red')



## figuring out 3D plotting in R
rm(list=ls())
df <- expand.grid(list(x=1:10, y=1:10))
rep <- 1
ll <- array(NA, c(dim(df)[1], rep))
for(i in 1:dim(df)[1]) {
    for(j in 1:rep) {
        x <- df[i, 'x']
        y <- df[i, 'y']
        dist <- sqrt((x-4)^2 + (y-7)^2)
        ll[i,j] <- rnorm(1, 100-dist^2, sd = dist/10)
    }
}

library(plot3D)
?scatter3D

x <- rep(df$x, rep)
y <- rep(df$y, rep)
z <- as.numeric(ll)

scatter3D(x, y, z)


## minimally reproducible example of getDependencies error for Perry
library(nimble)
code <- nimbleCode({
    x[1] ~ dnorm(0, 1)
    x[2] ~ dnorm(0, 1)
    y[1] ~ dnorm(x[1], 1)
    y[2] ~ dnorm(x[2], 1)
})

Rmodel <- nimbleModel(code)

Rmodel$getDependencies('x[1]')   ## should be x[1], y[1]
## [1] 'x[1]' 'y[1]' 'y[2]'

Rmodel$getDependencies('x[2]')   ## should be x[2], y[2]
## [1] 'x[2]'



## trying to figure out conjugate sampling in 'equiv' BUGS example model
library(nimble)
code <- nimbleCode({
    tau[1] ~ dgamma(0.001, 0.001)
    tau[2] ~ dgamma(0.001, 0.001)
    pi ~ dnorm(0, 1.0E-06)
    phi ~ dnorm(0, 1.0E-06)
    mu ~ dnorm(0, 1.0E-06)
    for(i in 1:N) { 
        d[i] ~ dnorm(0,tau[2])  # Subject random effect
        for (k in 1:2) {
            Treat[i,k] <- group[i]*(k-1.5) + 1.5  # treatment given
            m[i,k] <- mu + pow(-1, Treat[i,k]-1)* phi /2 + pow(-1, k-1)* pi /2 + d[i]
            Y[i,k] ~ dnorm(m[i,k], tau[1])
        }
    }
    ##theta <- exp(phi)
    ##equivalence <- step(theta - 0.8) - step(theta - 1.2)
})
Y <- structure(c(1.4, 1.64, 1.44, 1.36, 1.65, 1.08, 1.09, 1.25, 1.25, 1.3, 1.65, 1.57, 1.58, 1.68, 1.69, 1.31, 1.43, 1.44, 1.39, 1.52), .Dim = c(10, 2))
group <- c(1, 1, -1, -1, -1, 1, 1, 1, -1, -1)
N <- 10
##T <- 2
constants <- list(N=N, group=group)
data <- list(Y=Y)
inits <- list(mu=0, phi=0, pi=0, tau=c(1,1))
Rmodel <- nimbleModel(code, constants, data, inits)
spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

niter <- 10

set.seed(0); Rmcmc$run(niter)
Rsamples <- as.matrix(Rmcmc$mvSamples)
Rsamples

set.seed(0); Cmcmc$run(niter)
Csamples <- as.matrix(Cmcmc$mvSamples)
Csamples <- Csamples[, dimnames(Rsamples)[[2]]]
Csamples

Rsamples-Csamples


## test equiv model
library(nimble)
source('~/GitHub/nimble/packages/nimble/inst/tests/test_utils.R')
test_mcmc('equiv', numItsC = 1000, resampleData = TRUE)


1

## trying Kalman Filter and calculating likelihood for a linear SSM

# Say state process is
# X(t+1) = r * X(t) + b + nu(t)
# nu(t) ~ dnorm(0, sig2nu)
# 
# Obs process is
# Y(t) = X(t) + eps(t)
# eps(t) ~ dnorm(0, sig2eps)
# 
# Time series goes from t = 1:T
# 
# This is the simplest possible model for us.
# 
# Direct mvn approach:
#   1. Stationary variance of states:
#   V[x] = r^2 V[x] + sig2nu
# V[x] = sig2nu / (1-r^2)
# 
# 2. Deriving entries in covariance matrix of Y (from 1:T)
# Cov[X(t), X(t+1)] = r V[x]
# Cov[X(t), X(t+d)] = r^d V[x]
# Therefore Cov[Y(t), Y(t+d)] = r^d V[x]
# 
# And on the diagonal:
#   V[y] = V[x] + sig2eps
# 
# And mu(t) = E[Y] = E[X] = b/(1-r) for all t
# 
# So with that mean vector and covariance matrix in hand 
# you can directly calculate mvnorm(Y, mu, Cov(Y))
# 
# Kalman filter approach and direct LL are shown in code below.  
# Sorry  I didn't think of this.  I had this from the early 
# explorations that led to Knape, Besbeas and de Valpine in 
# Ecology last year (or was it this year?).  
# 
# In this setup, you want numSites = 1 (we were considering time-series 
# with data from multiple sites).  The LL functions use the meanData 
# (mean over all the sites, which we were comparing to a more complete 
# site-specific model), which for you will be the same as the fullData.



simData <- function(mu=10, a=0.8, sigPN=0.2, sigOE=0.4, t=20) {
    b <- mu*(1-a)
    X <- mu
    for(i in 1:50) X <- a*X + b + rnorm(1,0,sigPN) ## get to stationary
    states <- numeric(t)
    states[1] <- X
    for(i in 2:t) states[i] <- a*states[i-1]+b + rnorm(1,0,sigPN)
    obsStates <- rnorm(t, states, sigOE)
    list(x = states, y = obsStates, mu=mu, a=a, b=b, sigOE=sigOE, sigPN=sigPN, t=t)
}

mvn_ll <- function(d) {
    require(mvtnorm)
    mu <- d$mu
    a <- d$a
    b <- mu*(1-a)
    sigPN <- d$sigPN
    sigPN2 <- sigPN*sigPN
    sigOE <- d$sigOE
    sigOE2 <- sigOE*sigOE
    y <- d$y
    t <- d$t
    mu_vec <- rep( b/(1-a), t)
    cov_mat <- matrix(nrow = t, ncol = t)
    var_x <- sigPN2 / (1-a^2)
    var_y <- var_x + sigOE2
    diag(cov_mat) <- var_y
    for(i in 2:t) {
        for(j in 1:(i-1)) {
            cov_mat[i,j] <- (a^(i-j))*var_x
            cov_mat[j,i] <- cov_mat[i,j]
        }
    }
    dmvnorm(y, mu_vec, cov_mat, log = TRUE)
}

KF_ll <- function(d) {
    mu <- d$mu
    a <- d$a
    b <- mu*(1-a)
    sigPN <- d$sigPN
    sigPN2 <- sigPN*sigPN
    sigOE <- d$sigOE
    sigOE2 <- sigOE*sigOE
    y <- d$y
    t <- d$t
    mu_x <- b/(1-a)
    var_x <- sigPN2 / (1-a^2)
    cov_xy <- var_x
    var_y <- var_x + sigOE2
    ll <- dnorm(mu_x, y[1], sqrt(var_y), log = TRUE)
    mu_x <- mu_x + (cov_xy / var_y) * (y[1] - mu_x)
    var_x <- var_x - cov_xy*cov_xy / var_y
    for(i in 2:t) {
        mu_x <- a*mu_x + b
        var_x <- a^2 * var_x + sigPN2
        if(!is.na(y[i])) {
            cov_xy <- var_x
            var_y <- var_x + sigOE2
            ll <- ll + dnorm(mu_x, y[i], sqrt(var_y), log = TRUE)
            mu_x <- mu_x + (cov_xy / var_y) * (y[i] - mu_x)
            var_x <- var_x - cov_xy*cov_xy / var_y
        }
    }
    ll
}

code <- quote({
    x[1] ~ dnorm(mu, sd = sqrt((sigPN^2) / (1 - a^2)))
    y[1] ~ dnorm(x[1], sd = sigOE)
    for(i in 2:t){
        x[i] ~ dnorm(x[i-1] * a + b, sd = sigPN)
        y[i] ~ dnorm(x[i], sd = sigOE)
    }
})

d <- simData()

library(nimble)
constants <- list(a=d$a, b=d$b, t=d$t, sigOE=d$sigOE, sigPN=d$sigPN, mu=d$mu)
data <- list(y = d$y)
inits <- list(x = d$x)
Rmodel <- nimbleModel(code = code, constants = constants, data=data, inits=inits)
calculate(Rmodel)
pf <- buildPF(Rmodel, 'x')
Cmodel <- compileNimble(Rmodel)
Cpf <- compileNimble(pf, project = Rmodel)

Cpf$run(100000)

mvn_ll(d)

KF_ll(d)



## exploring how truncation works in our system

library(nimble)
code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ T(dnorm(a, 1), 1.5, 1.6)
    c ~ T(dnorm(b, 1), 3, Inf)
    d ~ dnorm(c, 1)
    e ~ dnorm(d, 1)
})
constants <- list()
data <- list()
inits <- list()

md <- nimbleModel(code, constants, data, inits, returnDef = TRUE)
Rmodel <- md$newModel(data=data, inits=inits)

Rmodel$modelDef$printDI()

node <- 'c'
Rmodel$nodes[[node]]$calculate
Rmodel$nodes[[node]]$simulate
Rmodel$nodes[[node]]$getLogProb

lapply(Rmodel$modelDef$declInfo, function(di) di$truncation)
for(node in c('a', 'b', 'c')) print(Rmodel$isTruncated(node))
for(node in c('a', 'b', 'c')) print(Rmodel$getBounds(node))

spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)



## using rankSample

library(nimble)
##Cmodel <- compileNimble(Rmodel = nimbleModel(quote({a ~ dnorm(0, 1)})))
nfDef <- nimbleFunction(
    setup = function() {},
    run = function(wts = double(1), m = integer(), silent = logical()) {
        ##print('in nimbleFunction rankSample')
        declare(samp, integer(1))
        rankSample(wts, m, samp, silent)
        print(samp)
    }
)
nf <- nfDef()
Cnf <- compileNimble(nf)

nfDefOmit <- nimbleFunction(
    setup = function() {},
    run = function(wts = double(1), m = integer()) {
        ##print('in nimbleFunction rankSample')
        declare(samp, integer(1))
        rankSample(wts, m, samp)
        print(samp)
    }
)
nfOmit <- nfDefOmit()
CnfOmit <- compileNimble(nfOmit)
x <- 1:10

silent <- FALSE
silent <- TRUE

set.seed(1);   .Call('rankSample', c(.5,.4,.1), 10L, x, silent)
set.seed(1);            rankSample(c(.5,.4,.1), 10L, x, silent);    print(x)
set.seed(1);                nf$run(c(.5,.4,.1), 10L,    silent)
set.seed(1);               Cnf$run(c(.5,.4,.1), 10L,    silent)

set.seed(1);   .Call('rankSample', c( 0, 0, 0), 10L, x, silent)
set.seed(1);            rankSample(c( 0, 0, 0), 10L, x, silent);    print(x)
set.seed(1);                nf$run(c( 0, 0, 0), 10L,    silent)
set.seed(1);               Cnf$run(c( 0, 0, 0), 10L,    silent)

set.seed(1);   .Call('rankSample', c(.5,.4,-1), 10L, x, silent)
set.seed(1);            rankSample(c(.5,.4,-1), 10L, x, silent);    print(x)
set.seed(1);                nf$run(c(.5,.4,-1), 10L,    silent)
set.seed(1);               Cnf$run(c(.5,.4,-1), 10L,    silent)

set.seed(1);   .Call('rankSample', c(.5,.4,-1), 10L, x)
set.seed(1);            rankSample(c(.5,.4,-1), 10L, x);            print(x)
set.seed(1);            nfOmit$run(c(.5,.4,-1), 10L)
set.seed(1);           CnfOmit$run(c(.5,.4,-1), 10L)



## difference in rank sample
library(nimble)
nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        declare(wts, double(1, 1))
        wts[1] <- 1
        m <- 1L
        declare(samp, integer(1, 1))
        rand <- rnorm(1, 0, 1)
        print('random number before rankSample: ', rand)
        rankSample(wts, m, samp)
        rand <- rnorm(1, 0, 1)
        print('random number after rankSample: ', rand)
    }
)

nf <- nfDef()
Cnf <- compileNimble(nf)
set.seed(0)
nf$run()
set.seed(0)
Cnf$run()



## testing buildPF() particle filter (pf) algorithm
library(nimble)
code <- quote({
    mu ~ dnorm(0, sd = 1000)
    b ~ dnorm(0, sd = 1000)
    sigPN ~ dunif(0.0001, 1)
    sigOE ~ dunif(0.0001, 1)
    x[1] ~ dnorm(mu, sd = sqrt(sigPN^2 + sigOE^2))
    y[1] ~ dnorm(x[1], sd = sigOE)
    a <- 1-(b/mu)
    for(i in 2:t){
        x[i] ~ dnorm(x[i-1] * a + b, sd = sigPN)
        y[i] ~ dnorm(x[i], sd = sigOE)
    }
})
t <- 5
constants <- list(t = t)
Rmodel <- nimbleModel(code = code, constants = constants)
Rmodel$mu <- 1/(1-.95)
Rmodel$b <- 1
Rmodel$sigPN <- .2
Rmodel$sigOE <- .05
set.seed(0)
calculate(Rmodel, Rmodel$getDependencies(c('mu','b','sigPN','sigOE'), determOnly = TRUE))
simulate(Rmodel, Rmodel$getDependencies(c('x', 'y')))
data <- list(y = Rmodel$y)
inits <- list(mu = Rmodel$mu, b = Rmodel$b, sigPN = Rmodel$sigPN, sigOE = Rmodel$sigOE, x = Rmodel$x)
rm(Rmodel)
Rmodel <- nimbleModel(code, constants, data, inits)
pf <- buildPF(Rmodel, 'x')
Cmodel <- compileNimble(Rmodel)
Cpf <- compileNimble(pf, project = Rmodel)

m <- 100
set.seed(0);    pf$run(m)
set.seed(0);   Cpf$run(m)

1
2
3



## making MCMCsuite work for Ryan using Stan MCMC

allModels <- c('blocker', 'bones', 'dyes', 'line', 'pump', 'rats')
library(nimble)
library(coda)
allModels <- 'blocker'
mcmcs=c('nimble','jags','noConj','stan')
x=list()
for (i in 1:length(allModels)){
    x[[i]]<-readBUGSmodel(model=allModels[i],
                          dir=getBUGSexampleDir(allModels[i]),
                          returnModelComponentsOnly=TRUE)
}
i <- 1
suite_output <- MCMCsuite(
    x[[i]]$model,
    constants = x[[i]]$data,
    inits = x[[i]]$inits,
    MCMCs = mcmcs,
    makePlot=F,
    savePlot=F,
    summaryStats=c('mean','median','sd','CI95_low','CI95_upp','effectiveSize'),
    MCMCdefs = list(noConj = quote({ configureMCMC(Rmodel, useConjugacy=FALSE) })),
    stan_model = paste('~/temp/', allModels[i], '.stan', sep='')
)


## learning about R's memory management and storage of objects
gcinfo(TRUE)
gc()

library(pryr)
?object_size
object_size(1:100)
object_size(mtcars)

x <- 0:50
s <- sapply(x, function(i) object_size(1:i))
plot(x, s, type='s', ylim=c(0,300))

obj <- complex()
object_size(obj)

mem_used
mem_used()

pryr:::node_size()
pryr:::show_bytes

mem_change(NA)

bytes(1)
bytes(1L)
bytes('ab')

address(2)
x <- 1:10
address(x)
refs(x)
y <- x
refs(x)
z <- x
x[1] <- 16L

tracemem(x)
x[1] <- .1

x <- 15
address(x)

system(paste0('~/temp/t ', address(x)))



## testing of mcmcplots package function mcmcplot()

rm(list=ls())
library(nimble)
model <- 'SSMcorrelated'
load(paste0('~/GitHub/autoBlock/data/model_', model, '.RData'))
Rmodel <- nimbleModel(code, constants, data, inits)

out <- MCMCsuite(code, constants, data, inits, monitors=c('a','b','p'), MCMCs=c('nimble','nimble_RW', 'nimble_slice', 'jags'), makePlot=FALSE, niter=10000)

library(coda)
samples <- out$samples
dim(samples)
dimnames(samples)
mcmcList <- list()
for(i in 1:dim(samples)[1])     mcmcList[[i]] <- coda::mcmc(t(samples[i,,]))
names(mcmcList) <- dimnames(samples)[[1]]
codaList <- coda::as.mcmc.list(mcmcList)

library(mcmcplots)
mcmcplot(codaList)
traplot(codaList)




## testing conjugacy in newNimbleModel branch

library(nimble)
code <- nimbleCode({
    for(i in 1:10) {
        a[i] ~ dnorm(0, 1)
        b[i] ~ dgamma(1, 1)
    }
    for(i in 1:5)   a_temp[i] <- 3*a[i]
    for(i in 6:10)  a_temp[i] <- a[i]^2
    for(i in 1:10)  x[i] ~ dnorm(a_temp[i], 1)
    for(i in 1:10) {
        y1[i] ~ dpois(2*b[i])
        y2[i] ~ dgamma(1, rate = b[i])
    }
})
Rmodel <- nimbleModel(code)

spec <- configureMCMC(Rmodel)

spec$getSamplers()
for(i in 1:200) spec$addSampler('RW', 'x[2]', print=F)
spec$addSampler('RW', 'x[2]')

spec$setSamplers(c(1:40, 1:40))
spec$setSamplers(c('a', 'y2'))
spec$removeSamplers('a')
spec$setSamplers()


## NEED TO RE-RUN THIS TEST, ONCE PERRY FIXES THE MODELVALUES COPYING ISSUE
## Dirichlet-multinomial conjugacy
## single multinomial
library(nimble)
set.seed(0)
n <- 100
alpha <- c(10, 30, 15, 60, 1)
K <- length(alpha)
p <- c(.12, .24, .09, .54, .01)
y <- rmulti(1, n, p)
code <- nimbleCode({
    y[1:K] ~ dmulti(p[1:K], n)
    p[1:K] ~ ddirch(alpha[1:K])
    for(i in 1:K) {
        alpha[i] ~ dgamma(.001, .001)
    }
})
constants <- list(n = n, K = K)
data <- list(y = y)
inits <- list(p = rep(1/K, K), alpha = rep(K, K))
Rmodel <- nimbleModel(code, constants=constants, data=data, inits=inits)

mv <- modelValues(Rmodel)
Rmodel$y
mv$y
nimCopy(from=Rmodel, to=mv, nodes = 'y', row = 1)
mv$y


spec <- configureMCMC(Rmodel, monitors = c('alpha', 'p'))
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

set.seed(0)
Rmcmc$run(10)

conjugate posterior density appears to be wrong, off by Inf
conjugate posterior density appears to be wrong, off by Inf
There were 50 or more warnings (use warnings() to see the first 50)

test_mcmc(model = code, data= data, seed = 0, numItsC = 10000,
          inits = inits,
          results = list(mean = list(p = p)),
          resultsTolerance = list(mean = list(p = rep(.06, K))))




## just a good old fashioned test that it's working
library(nimble)
code <- BUGScode({
    x ~ dgamma(1, 1)       # should satisfy 'gamma' conjugacy class
    a  ~ dnorm(0, x)     # should satisfy 'norm' conjugacy class
    a2 ~ dnorm(0, tau = 3*x+0)
    b  ~ dpois(0+5*x)
    b2 ~ dpois(1*x*1)
    c ~ dgamma(1, 7*x*5)
    for(i in 2:3) {
        jTau[i] <- 1
        jNorm[i] ~ dnorm(c * (a+3) - i, var = jTau[i])
        kTauSd[i] <- 2
        kLogNorm[i] ~ dlnorm(0 - a - 6*i, kTauSd[i])
    }
})

constants <- list()
data <- list()
inits <- list()
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$checkConjugacy()

spec <- configureMCMC(Rmodel, control = list(scale=0.01), monitors = c('x', 'c'))
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
set.seed(0)
Rmcmc$run(10)
samples <- as.matrix(Rmcmc$mvSamples)
samples


sampleVals = list(x = c(3.950556165467749, 1.556947815895538, 1.598959152023738, 2.223758981790340, 2.386291653164086, 3.266282048060261, 3.064019155073057, 3.229661999356182, 1.985990552839427, 2.057249437940977),
  c = c( 0.010341199485849559, 0.010341199485849559, 0.003846483017887228, 0.003846483017887228, 0.007257679932131476, 0.009680314740728335, 0.012594777095902964, 0.012594777095902964, 0.018179641351556003, 0.018179641351556003))

test_mcmc(model = code, exactSample = sampleVals, seed = 0, mcmcControl = list(scale=0.01))




## testing if a Ref Class can just return a smaller object (not the whole Ref Class object)
RC <- setRefClass(
    Class = 'RC',
    fields = list(a='ANY'),
    methods = list(
        initialize = function(a) {
            a <<- a
            return(a)
        }
    )
)

rc <- RC(1)
class(rc)
rc$a



## bunch of random stuff

fileToList <- function(file) {
    env <- new.env()
    source(fileName, local = env)
    lst <- list()
    for(name in ls(env))   lst[[name]] <- get(name, env)
    return(lst)
}

library(nimble)

NF <- nimbleFunction(
    setup = function() { },
    run = function(a = double(0)) {
        returnType(double(0))
        return(a)
    }
)

myNF <- NF()
myNF_C <- compileNimble(myNF)

> myNF$run
## function (a) 
##     return(a)

myNF_C$run
## function (a) 
## {
##     ans <- .Call('CALL_nfRefClass60_operator_', a, .basePtr)
##     ans <- ans[[2]]
##     ans
## }


### testing the new autoBlock
rm(list=ls())
library(nimble)
model <- 'litters'
load(paste0('~/GitHub/autoBlock/data/model_', model, '.RData'))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$getNodeNames()

ab <- autoBlock(Rmodel, run = runList)
ab
ab$spec$getSamplers()
Rmcmc <- buildMCMC(ab$spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project=Rmodel)


spec <- configureMCMC(Rmodel, autoBlock=TRUE)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project=Rmodel)

Rmcmc <- buildMCMC(Rmodel, autoBlock=TRUE)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project=Rmodel)







## March 2015
## timing comparisons for red-state-blue-state
library(nimble)
library(rjags)
rm(list=ls())
load('~/GitHub/autoBlock/data/model_redblue.RData')

## this version of red-state-blue-state code is 'jags friendly'
## reparametrized to use Precision matrix, replaced expit with logit
code <- nimbleCode({
    for (i in 1:2) {
        for (j in 1:2) {
            gamma[i, j] ~ dnorm(0, 1e-04)
        }
    }
    sigmaIntercept ~ dunif(0, 100)
    sigmaSlope ~ dunif(0, 100)
    rho ~ dunif(-1, 1)
    Precision[1, 1] <- 1/(1-rho^2) * 1/(sigmaIntercept^2)
    Precision[2, 2] <- 1/(1-rho^2) * 1/(sigmaSlope^2)
    Precision[1, 2] <- 1/(1-rho^2) * -rho/(sigmaIntercept*sigmaSlope)
    Precision[2, 1] <- 1/(1-rho^2) * -rho/(sigmaIntercept*sigmaSlope)
    for (i in 1:Nstates) {
        stateBetaMeans[i, 1] <- gamma[1, 1] + gamma[1, 2] * stateIncome[i]
        stateBetaMeans[i, 2] <- gamma[2, 1] + gamma[2, 2] * stateIncome[i]
        stateBetas[i, 1:2] ~ dmnorm(stateBetaMeans[i, 1:2], Precision[1:2, 1:2])
    }
    for (i in 1:N) {
        logit(p[i]) <- stateBetas[state[i], 1] + stateBetas[state[i], 2] * income[i]
        y[i] ~ dbern(p[i])
    }
})

niter <- 50000
monitorVars <- c('sigmaIntercept', 'sigmaSlope', 'rho', 'gamma')
constsAndData <- c(constants, data)
modelfile <- file.path(tempdir(), 'model.txt')
writeLines(paste0('model\n', paste0(deparse(code, width.cutoff=500L), collapse='\n')), con=modelfile)


system.time(modelDef <- nimbleModel(code=code, constants=constants, data=data, inits=inits, returnDef=TRUE))[3]/60
26

system.time(Rmodel <- modelDef$newModel(data=data, inits=inits))[3]/60
5

system.time(spec <- configureMCMC(Rmodel))
6 seconds

system.time(Rmcmc <- buildMCMC(spec))[3]
15 seconds

system.time(Cmodel <- compileNimble(Rmodel))[3]/60
11

system.time(Cmcmc <- compileNimble(Rmcmc, project = Rmodel))[3]/60
1

system.time(Cmcmc$run(niter))[3]/60
11 minutes for 50,000 iterations

system.time(jags_mod <- jags.model(file=modelfile, data=constsAndData, inits=inits, n.chains=1, quiet=FALSE))[3]/60
10 seconds

system.time(jags_out <- coda.samples(model=jags_mod, variable.names=monitorVars, n.iter=niter, thin=1))[3]/60
2.001833





## displaying calc and sim functions
code <- nimbleCode({ x ~ dnorm(3, sd = 5) })
model <- nimbleModel(code)
model$nodes$x$simulate
## function()
##     model$x <<- rnorm(1, mean = 3, sd = 5)
model$nodes$x$calculate
## function() {
##     model$logProb_x <<- dnorm(model$x, mean = 3, sd = 5, log = 1)
##     return(invisible(model$logProb_x))
## }
model$nodes$x$getLogProb
## function() 
##     return(model$logProb_x)


### March 2015
### trying to figure out new MCMCspec

library(nimble)


#These first few lines are nothing new
myModelCode <- nimbleCode({
	a[1] ~ dnorm(0,1)
	a[2] ~ dexp(a[1]^2)
	a[3] ~ dnorm(0,1)
})
myModel <- nimbleModel(myModelCode)
mySpec1 <- configureMCMC(myModel)
myMCMC1 <- buildMCMC(mySpec1)
cmod <- compileNimble(myModel)
cmcmc1 <- compileNimble(myMCMC1, project = myModel)


# Rebuilding the way we are supposed to: 
# building a new spec with configureMCMC 
# adding a sampler which has already been compiled 
# not altering the monitors
mySpec2 <- configureMCMC(oldSpec = mySpec1)
mySpec2$addSampler('RW', control = list(targetNode = 'a[3]'))
rmcmc2 <- buildMCMC(mySpec2)
cmcmc2 <- compileNimble(rmcmc2, project = myModel)




#### March 2015 working through a full compileNimble call

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
inits <- list(a = 1)
Rmodel <- nimbleModel(code, inits = inits)

debug(compileNimble)
Cmodel <- compileNimble(Rmodel)

debug(project$compileModel)
debug(modelCpp$buildAll)
debug(buildNodes)
debug(nimbleProject$compileNimbleFunctionMulti)
debug(compileNimbleFunction)
debug(buildNimbleFunctionCompilationInfo)


#### March 2015 testing of RW_llFunction or Carl B, submitted as test to testing suite

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
inits <- list(a = 0)
Rmodel <- nimbleModel(code=code, inits=inits)

llFunction <- nimbleFunction(
    setup = function(model) { },
    run = function() {
        ll <- dnorm(1, model$a, 1, log=1)
        returnType(double())
        return(ll)
    }
)
 
myLL <- llFunction(Rmodel)

spec <- configureMCMC(Rmodel, nodes=NULL)
spec$addSampler('RW_llFunction', list(targetNode='a', llFunction=myLL, includesTarget=FALSE))
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
unlist(Rmcmc$mvSamples[['a']])
## [1] -0.2849616  0.4851313  0.1241257  0.1241257  0.1241257 -0.3553077 0.4223091  0.4455515  0.4455515  0.4455515

set.seed(0)
Cmcmc$run(10)
unlist(Cmcmc$mvSamples[['a']])
## [1] -0.2849616  0.4851313  0.1241257  0.1241257  0.1241257 -0.3553077 0.4223091  0.4455515  0.4455515  0.4455515

Cmcmc$run(100000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)
## should be within some tolerance of 0.5


###### Feb 2015 example for Cal Poly interview talk

library(nimble)

code <- nimbleCode({
    for( i in 1 : N ) {
        b[i] ~ dnorm(mu, tau)
        r[i] ~ dbin(p[i], n[i])
        logit(p[i]) <- b[i]
    }
    pop.mean <- 1 / (1 + exp(-mu))
    mu ~ dnorm(0, 0.001)
    sigma <- 1 / sqrt(tau)
    tau ~ dgamma(0.001, 0.001)
})

constants <- list(N = 10, n = c(12,20,18,19,10,23,19,15,8,10))
data <- list(r = c(4,7,10,3,6,7,11,9,3,8))
inits <- list(mu = 0, tau = 1)

model <- nimbleModel(code=code, constants=constants, data=data, inits=inits)

spec <- configureMCMC(model)
spec$getSamplers()
spec$addSampler(â€˜sliceâ€™, list(targetNode = â€˜muâ€™))
spec$getSamplers()
mcmc <- buildMCMC(spec)

Cmodel <- compileNimble(model)
Cmcmc <- compileNimble(mcmc, project = model)

Cmcmc$run(10000)

samples <- as.matrix(Cmcmc$mvSamples)

apply(samples, 2, mean)





## testing new addition to NIMBLE: conf$addSampler('node', 'conjugate')
library(nimble)

code <- nimbleCode({
    x ~ dgamma(1, 1)       # should satisfy 'gamma' conjugacy class
    a  ~ dnorm(0, x)     # should satisfy 'norm' conjugacy class
    a2 ~ dnorm(0, tau = 3*x+0)
    b  ~ dpois(0+5*x)
    b2 ~ dpois(1*x*1)
    c ~ dgamma(1, 7*x*5)
    for(i in 2:3) {
        jTau[i] <- 1
        jNorm[i] ~ dnorm(c * (a+3) - i, var = jTau[i])
        kTauSd[i] <- 2
        kLogNorm[i] ~ dlnorm(0 - a - 6*i, kTauSd[i])
    }
})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)

conf$printSamplers()
##[1]  conjugate_dgamma_dnorm_dpois_dgamma sampler: x,  dep_dnorm: a, a2,  dep_dpois: b, b2,  dep_dgamma: c
##[2]  conjugate_dnorm_dnorm_dlnorm sampler: a,  dep_dnorm: jNorm[2], jNorm[3],  dep_dlnorm: kLogNorm[2], kLogNorm[3]
##[3]  posterior_predictive sampler: a2
##[4]  posterior_predictive sampler: b
##[5]  posterior_predictive sampler: b2
##[6]  RW sampler: c
##[7]  posterior_predictive sampler: kLogNorm[2]
##[8]  posterior_predictive sampler: kLogNorm[3]
##[9]  posterior_predictive sampler: jNorm[2]
##[10] posterior_predictive sampler: jNorm[3]

pr <- TRUE
pr <- FALSE
nd <- 'x'
nd <- 'a'
nd <- 'c'
nd <- 'a2'
nd <- 'kLogNorm[3]'
conf$addSampler(nd, 'RW',        print=pr)
conf$addSampler(nd, 'slice',     print=pr)
conf$addSampler(nd, 'conjugate', print=pr)

conf$printSamplers()

conf$addSampler(nd, print=TRUE)
conf$addSampler(nd, print=TRUE)
conf$addSampler(nd, print=TRUE)





par(mfrow=c(1,3))
ns <- c(50, 500, 5000)
for(n in ns) {
    x <- rbinom(n=10000, size=n, prob=0.8) / n
    hist(x, xlim=c(0,1))
}



library(nimble)
df <- read.csv('~/Downloads/UsedCars.csv')
code <- nimbleCode({
    b0  ~ dnorm(0, sd=10000)
    bage ~ dnorm(0, sd=10000)
    bhp ~ dnorm(0, sd=10000)
    btype ~ dnorm(0, sd=10000)
    sigma ~ dunif(0, 50000)
    for(i in 1:N) {
        y[i] ~ dnorm(mu[i], sd = sigma)
        mu[i] <- b0 + bage*age[i] + bhp*hp[i] + btype*type[i]
    }
})
constants <- list(N = dim(df)[1], age=df$Age, hp=df$HP, type=df$Type)
data <- list(y = df$Price)
inits <- list(b0=0, bage=0, bhp=0, btype=0, sigma=1)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

samples <- runMCMC(Cmcmc, 20000, nburnin=10000)
dim(samples)

samplesPlot(samples)
samplesPlot(samples, 'btype')

apply(samples, 2, effectiveSize)







1

a <- acf(samples)
a
a$acf
plot(a)
acfplot(as.mcmc(samples), thin=10)

?acf
acfplot)

colnames(samples)
quantile(samples[, 'bage'], c(0.025, 0.975))

dim(samples)
colnames(samples)
samplesPlot(samples)
samplesPlot(samples, var='btype', burnin=2000)
samplesPlot(samples, var='btype', ind=1:200)
samplesPlot(samples, var='btype', ind=2001:10000)
samplesPlot(samples, var=c('btype', 'bage'), ind=2001:10000)
samplesPlot(samples, var=c('btype', 'bage'), ind=2001:10000, densityplots=FALSE)

samplesPlot(samples, burnin=2000)
samplesPlot(samples, var=1:3, burnin=2000)
samplesPlot(samples, var=4, burnin=5000)
samplesPlot(samples, var=1, burnin=2000)

quantile(samples[-(1:2000), 1], c(0.025, 0.975))


1








## "seizures" example for STAT365

library(nimble)
load('~/Downloads/seizures.RData')
N <- dim(seizures$Counts)[1]

code <- nimbleCode({
    b0 ~ dnorm(0, sd=1000)
    bbase ~ dnorm(0, sd=1000)
    bage ~ dnorm(0, sd=1000)
    btrt ~ dnorm(0, sd=1000)
    sigma ~ dunif(0, 1000)
    sigma_patient ~ dunif(0, 1000)
    for(i in 1:N) {
        g[i] ~ dnorm(0, sd=sigma_patient)
        for(j in 1:4) {
            eps[i, j] ~ dnorm(0, sd=sigma)
            log(lambda[i,j]) <- b0 + bbase * log(baseline[i]) + bage * age[i] + btrt * treatment[i] + eps[i,j] + g[i]
            y[i,j] ~ dpois(lambda[i,j])
        }
    }
})
constants <- list(N = dim(seizures$Counts)[1], baseline=seizures$Baseline, age=seizures$Age, treatment=seizures$Treatment)
data <- list(y=seizures$Counts)
inits <- list(b0=1, bbase=0, bage=0, btrt=0, sigma=1, sigma_patient=1, g=rep(0,N), eps = array(0,c(N,4)))
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
##conf <- configureMCMC(Rmodel, onlySlice=TRUE)
conf$printSamplers()
##conf$removeSamplers('b0')
##conf$addSampler('b0', 'slice')
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
samplesList <- runMCMC(Cmcmc, 500000, nburnin=10000, nchains=3, returnCodaMCMC=TRUE)

samples <- samplesList[[1]]
samplesPlot(samples)
samplesPlot(samples, 'btrt')
samplesPlot(samples, 'b0')
##samplesPlot(samples, 'bbase')
library(coda)
apply(samples, 2, effectiveSize)
##dim(samples)
gelman.diag(samplesList)

apply(samples, 2, effectiveSize)
apply(samples, 2, mean)
sqrt(apply(samples, 2, var)) / sqrt(apply(samples, 2, effectiveSize))


codalist <- runMCMC(Cmcmc, 100000, nchains=3, returnCodaMCMC=TRUE)

gelman.diag(codalist)
geweke.diag(codalist)
geweke.plot(codalist)

apply(codalist[[1]], 2, effectiveSize)
apply(codalist[[1]], 2, var) / apply(codalist[[1]], 2, effectiveSize)


## "seeds" example for STAT365
write.csv(df, '~/Downloads/seeds.csv')


df <- read.csv('~/Downloads/seeds.csv')
df

cucumber <- as.numeric(df$plant) - 1
fertB <- as.numeric(df$fertilizer) - 1
y <- df$germinations
n <- df$seeds
N <- dim(df)[1]

library(nimble)

sds <- c(.1, .5, 1, 5, 10)
samps <- array(0, c(10000, length(sds)))

for(i in seq_along(sds)) {
    sdC <- sds[i]
    code <- nimbleCode({
        sigma ~ dunif(0, 100)
        b0 ~ dnorm(0, sd=1000)
        bCuc ~ dnorm(0, sd=sdC)
        bFertB ~ dnorm(0, sd=1000)
        for(i in 1:N) {
            mu[i] <- b0 + bCuc * cucumber[i] + bFertB * fertB[i]
            logit(p[i]) ~ dnorm(mu[i], sd=sigma)
            y[i] ~ dbinom(size=n[i], prob=p[i])
        }
    })
    constants <- list(N=N, cucumber=cucumber, fertB=fertB, n=n, sdC=sdC)
    data <- list(y=y)
    inits <- list(sigma=1, b0=1, bCuc=0, bFertB=0)
    Rmodel <- nimbleModel(code, constants, data, inits)
    Rmodel$mu
    calculate(Rmodel)
    conf <- configureMCMC(Rmodel)
    conf$printSamplers()
    Rmcmc <- buildMCMC(conf)
    Cmodel <- compileNimble(Rmodel)
    Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
    set.seed(0)
    samples <- runMCMC(Cmcmc, 100000)
    ##Cmcmc$run(10000)
    ##samples <- as.matrix(Cmcmc$mvSamples)
    samps[, i] <- samples[ 50001:60000, 'bCuc']
}

dim(samps)
dimnames(samps)
colnames(samps) <- paste0('sd', as.character(sds))
dimnames(samps)

samplesPlot(samples)

## prior sensitity analysis for different
## parametrizations of the 'sigma' term
niter <- 100000
samps <- array(0, c(niter, 3))
colnames(samps) <- c('sd', 'var', 'tau')


code <- nimbleCode({
    sigma ~ dunif(0, 1000)
    b0 ~ dnorm(0, sd=1000)
    bCuc ~ dnorm(0, sd=1000)
    bFertB ~ dnorm(0, sd=1000)
    for(i in 1:N) {
        mu[i] <- b0 + bCuc * cucumber[i] + bFertB * fertB[i]
        logit(p[i]) ~ dnorm(mu[i], sd=sigma)
        y[i] ~ dbinom(size=n[i], prob=p[i])
    }
})
constants <- list(N=N, cucumber=cucumber, fertB=fertB, n=n)
data <- list(y=y)
inits <- list(sigma=1, b0=1, bCuc=0, bFertB=0)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
samples <- runMCMC(Cmcmc, niter)
samps[, 1] <- samples[, 'sigma']
code <- nimbleCode({
    v ~ dunif(0, 1000)
    b0 ~ dnorm(0, sd=1000)
    bCuc ~ dnorm(0, sd=1000)
    bFertB ~ dnorm(0, sd=1000)
    for(i in 1:N) {
        mu[i] <- b0 + bCuc * cucumber[i] + bFertB * fertB[i]
        logit(p[i]) ~ dnorm(mu[i], var=v)
        y[i] ~ dbinom(size=n[i], prob=p[i])
    }
})
constants <- list(N=N, cucumber=cucumber, fertB=fertB, n=n)
data <- list(y=y)
inits <- list(v=1, b0=1, bCuc=0, bFertB=0)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
samples <- runMCMC(Cmcmc, niter)
samps[, 2] <- samples[, 'v']
code <- nimbleCode({
    tau ~ dgamma(0.001, 0.001)
    b0 ~ dnorm(0, sd=1000)
    bCuc ~ dnorm(0, sd=1000)
    bFertB ~ dnorm(0, sd=1000)
    for(i in 1:N) {
        mu[i] <- b0 + bCuc * cucumber[i] + bFertB * fertB[i]
        logit(p[i]) ~ dnorm(mu[i], tau=tau)
        y[i] ~ dbinom(size=n[i], prob=p[i])
    }
})
constants <- list(N=N, cucumber=cucumber, fertB=fertB, n=n)
data <- list(y=y)
inits <- list(tau=1, b0=1, bCuc=0, bFertB=0)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
samples <- runMCMC(Cmcmc, niter)
samps[, 3] <- samples[, 'tau']

samps[, 'var'] <- sqrt(samps[, 'var'])
samps[, 'tau'] <- 1/sqrt(samps[, 'tau'])


head(samps)

dim(samps)
dimnames(samps)

samplesPlot(samps, burnin=50000)







## "pairs"
## studying estimation of normal variance / covariance,
## from paired observations from a common variance
## (and possibly different mean)


## this shows that for pairs x1,x2 ~ N(mean=[changing], var= constant v)
## using the average of (x2-x1)^2 / 2
## gives an unbiased estimate of the common variance v
estimateV <- function(n, v) {
    pairs <- t(replicate(n, rnorm(2, mean=runif(1,0,100), sd=sqrt(v))))
    vest <- apply(pairs, 1, function(x) ((x[2]-x[1])^2)/2)
    mean(vest)
}
n <- 1000
v <- 2
n.average.over=1000
estimates <- replicate(n.average.over, estimateV(n=n, v=v))
mean(estimates)

## this shows that the above (vest = (x2-x1)^2 / 2)
## is *not* the same as a traditional sd(x), if the x are used to
## generate a vector of n consecutive "pairs"
n <- 10
v <- 2
xs <- rnorm(n, sd=sqrt(v))
sd(xs)^2
pairs <- array(NA, c(n,2))
for(i in 1:(n-1)) pairs[i,] <- xs[i:(i+1)]
pairs[n,] <- xs[c(n,1)]
xs
pairs
vest <- apply(pairs, 1, function(x) ((x[2]-x[1])^2)/2)
sum(vest) / (n-1)
sd(xs)^2


## this shows that for multivariate-normal pairs
## x1,x2 ~ MVN(mean=[changing], Sigma = constant V)
## using the average of dif=x2-x1, dif %*% t(dif) / 2
## gives an unbiased estimate of the common covariance matrix V
estimateV <- function(n, V) {
    pairs <- lapply(1:n, function(x) {
        mus <- runif(2,0,100)
        list(rmvnorm(1, mus, sigma=V)[1,],
             rmvnorm(1, mus, sigma=V)[1,])
    })
    vest <- lapply(pairs, function(x) {
        dif <- x[[2]] - x[[1]]
        (dif %*% t(dif)) / 2
    })
    apply(array(unlist(vest), dim=c(2,2,n)), c(1,2), sum) / n
}
library(mvtnorm)
n <- 200
v1 <- 2
v2 <- 5
rho <- 0.6
V <- array(c(v1^2, rho*v1*v2, rho*v1*v2, v2^2), c(2,2))
n.average.over <- 200
estimates <- replicate(n.average.over, estimateV(n=n, V=V))
V
apply(estimates, c(1,2), mean)









x <- rgamma(10000, 0.001, 0.001)
curve(dgamma(x, 0.001, 0.001), col='blue')
hist(x, prob=TRUE, breaks=50000, add=TRUE)
curve(dgamma(x, 0.001, 0.001), col='blue', add=TRUE)


## testing the new adaptive properties for covariance in RW_block sampler,
## not adapting until acceptance rate >= 0.15 at least once
## uses scaleHistory and propCovHistory

library(nimble)
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)

code <- nimbleCode({
    a ~ dnorm(0, sd=100)
    b ~ dnorm(0, sd=100)
    c ~ dnorm(a/2, sd=.1)
    for(i in 1:n1) {
        y1[i] ~ dnorm(a, sd=0.1)
    }
    for(i in 1:n2) {
        y2[i] ~ dnorm(b, sd=0.1)
    }
})
n1 <- 10
n2 <- n1
y1 <- rnorm(n1, 3, 0.1)
y2 <- rnorm(n2, y1+5, 0.01)
constants <- list(n1=n1, n2=n2)
data <- list(y1=y1, y2=y2)
inits <- list(a = 0, b=0, c=0)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel, nodes=NULL)
##conf$addSampler('a', 'RW')
conf$addSampler(c('a', 'b', 'c'), 'RW_block')
conf$printSamplers()
conf$addMonitors(c('a', 'b', 'c'))
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(4000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)

samplesPlot(samples)

Cmcmc$samplerFunctions$contentsList[[1]]$scaleHistory
a <- Cmcmc$samplerFunctions$contentsList[[1]]$propCovHistory
dim(a)
for(i in 1:dim(a)[1]) print(a[i,,])


samplesPlot(samples)

Cmcmc$samplerFunctions$contentsList[[1]]$scaleHistory
a <- Cmcmc$samplerFunctions$contentsList[[1]]$propCovHistory
dim(a)
for(i in 1:dim(a)[1]) print(a[i,,])








## getting model running for Colin Lewis-Beck at Iowa State

## Create Constant W, G and F matrices to pass to NIMBLE
library(nimble)
y1 <- rep(0,2190)
W<-diag(2)
F1<-matrix(c(1,0), nrow = 1, ncol = 2)
G<-matrix(c(cos(2*pi/365),-sin(2*pi/365),sin(2*pi/365),cos(2*pi/365)),nrow=2,ncol=2)
smosCode <- nimbleCode({
    ## Initial Values for States
    c0[1]<-0
    c0[2]<-0
    m0[1:2,1:2]<-sigmaSquaredInvw*W[1:2,1:2]   ## CHANGE 1: add full indexing to m0[...]
    x0[1:2]~dmnorm(c0[1:2],m0[1:2,1:2])        ## CHANGE 2: add full indexing to x0[...]
    ## initial values
    m[1:2,1]<-G[1:2,1:2] %*% x0[1:2]           ## CHANGE 3: add indexing to m[...], inprod() only returns a scalar (not a vector or matrix) so use %*%
    var0[1:2,1:2]<-sigmaSquaredInvw*W[1:2,1:2] ## CHANGE 4: add full indexing to var0[...]
    x[1:2,1]~dmnorm(m[1:2,1],var0[1:2,1:2])
    y[1]~dnorm(inprod(F1[1,1:2],x[1:2,1]),sigmaSquaredInvv)
    ## Model
    for (t in 2:T){
        mu[1:2, t] <- G[1:2,1:2] %*% x[1:2,t-1]     ## CHANGE 5: something was funny with your mu[] declaration.  In addition to not having
                                                    ## the required indexing, it should should be indexed by t
        sigs[1:2,1:2]<-sigmaSquaredInvw*W[1:2,1:2]  ## CHANGE 6: add full indexing to sigs[...]
        x[1:2,t] ~ dmnorm(mu[1:2,t],sigs[1:2,1:2])  ## CHANGE 7: added appropriate 't' indexing to mu[...]
        y[t] ~ dnorm(inprod(F1[1,1:2],x[1:2,t]),sigmaSquaredInvv)
    }
    ## Priors
    sigmaSquaredInvv~dgamma(5,20)
    sigmaSquaredInvw~dgamma(5,200)
})
smosModel<-nimbleModel(code=smosCode,name='2harm',constants=list(T=2190,pi=pi,W=W,G=G,F1=F1),data=list(y=y1), inits=list(sigmaSquaredInvv=1,sigmaSquaredInvw=1))
Cmodel <- compileNimble(smosModel)




## doing the Surgeries example for STAT 365 homework

library(nimble)
df <- read.csv('~/Downloads/Surgeries.csv')
df

code <- nimbleCode({
    for(i in 1:N) {
        p[i] ~ dbeta(1, 1)
        y[i] ~ dbinom(size = n[i], prob = p[i])
    }
})
constants <- list(N = dim(df)[1], n = df$Surgeries)
data <- list(y = df$Mortalities)
inits <- list(p = rep(0.5, dim(df)[1]))

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

apply(samples, 2, mean)
colnames(samples)
samplesPlot(samples)
samplesPlot(samples, var=1)

mean(samples[,1])
median(samples[,1])
sd(samples[,1])
quantile(samples[,1], probs=c(0.025, 0.975))



code <- nimbleCode({
    mu ~ dnorm(0, 0.0001)
    sigma ~ dunif(0, 1000)
    for(i in 1:N) {
        logit(p[i]) ~ dnorm(mu, sd=sigma)
        y[i] ~ dbinom(size = n[i], prob = p[i])
    }
})
constants <- list(N = dim(df)[1], n = df$Surgeries)
data <- list(y = df$Mortalities)
inits <- list(mu=0, sigma=1)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

apply(samples, 2, mean)
colnames(samples)
samplesPlot(samples)
samplesPlot(samples, var=1)

mean(samples[,1])
quantile(samples[,1], probs=c(0.025, 0.975))

expit(mean(samples[,1]))
expit(quantile(samples[,1], probs=c(0.025, 0.975)))

sum(df$Mortalities)/sum(df$Surgeries)


code <- nimbleCode({
    b0 ~ dnorm(0, 0.0001)
    b1 ~ dnorm(0, 0.0001)
    sigma ~ dunif(0, 1000)
    for(i in 1:N) {
        logit(p[i]) ~ dnorm(b0 + b1*x[i], sd=sigma)
        y[i] ~ dbinom(size = n[i], prob = p[i])
    }
    old_procedure <- expit(b0)
    new_procedure <- expit(b0 + b1)
    mortality_difference <- new_procedure - old_procedure
})
constants <- list(N = dim(df)[1], n = df$Surgeries)
data <- list(y = df$Mortalities)
inits <- list(b0=0, b1=0, sigma=1, x=df$Procedure)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$addMonitors(c('old_procedure', 'new_procedure', 'mortality_difference'))
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
samples <- runMCMC(Cmcmc, 10000)

apply(samples, 2, mean)
colnames(samples)
samplesPlot(samples)
samplesPlot(samples, var=3:5, burnin=500)

mean(samples[,3])
quantile(samples[,3], probs=c(0.025, 0.975))

expit(mean(samples[,1]))
expit(quantile(samples[,1], probs=c(0.025, 0.975)))

sum(df$Mortalities)/sum(df$Surgeries)


## spatial NIMBLE model example from Abhirup Datta
library(nimble)
library(mvtnorm)
nimbleOptions(showCompilerOutput=TRUE)

## model code ###
gpCode <- nimbleCode({
    for(i in 1:n){ y[i] ~ dnorm(w[i],sd=tau)}
    Vw[,] <- sigma*chol(G[,])
    w[] ~ dmnorm(muw[], cholesky=Vw[,], prec_param=0)
    sigma ~ dunif(0, 100)
    tau ~ dunif(0, 10)
})

## data ###
set.seed(1)
n=100
sigs=4
w=as.vector(rmvnorm(1,rep(0,n),sigs*0.5^as.matrix(dist(1:n))))
Y=w+sqrt(0.5*sigs)*rnorm(n)

### setting up nimble model ####
gpModel <- nimbleModel(gpCode, constants = list(n = n, G=0.5^as.matrix(dist(1:n)), muw=rep(0,n)), 
    dimensions=list(y=n, Vw=c(n, n), w=n, Vy=c(n, n), G = c(n, n)), check=FALSE)

gpModel$setData(list(y = Y))

gpModel$sigma <- 1
gpModel$tau <- 0.1
gpModel$w <- rep(0,n)

CgpModel <- compileNimble(gpModel)

### MCMC ####

gpconf <- configureMCMC(CgpModel, print=TRUE)


gpconf$printSamplers()
gpconf$removeSamplers('w')
gpconf$printSamplers()
for(node in gpModel$expandNodeNames('w', returnScalarComponents=TRUE)) {
    gpconf$addSampler(node, 'RW')
}
gpconf$printSamplers()



gpconf$addMonitors(c('w'))	

gpMCMC <- buildMCMC(gpconf)

CgpMCMC <- compileNimble(gpMCMC, project = CgpModel)

##Nmcmc=100000
Nmcmc=1000
set.seed(1)
CgpMCMC$run(Nmcmc)

gpMCMCsamples <- as.matrix(CgpMCMC$mvSamples)

plot(gpMCMCsamples[,"w[1]"])

dimnames(gpMCMCsamples)

samplesPlot(gpMCMCsamples, var=3:10)


## implementing RJMCMC for a simple 2 models, logistic regression
## yi ~ bernoulli(logit(p) = a + bx)
set.seed(0)
n <- 50
x <- runif(n, -10, 10)
a <- .3
b <- 0.1
p <- 1/(1 + exp(-1*(a + b*x)))
y <- rbinom(n, prob=p, size=1)
##plot(x,y)
m1 <- glm(y~1, family=binomial())
m1
m2 <- glm(y~x, family=binomial())
m2
aic1 <- AIC(m1)  ## for this AIC function, "lower values indicate better fit"
aic2 <- AIC(m2)
aics <- c(aic1, aic2)
aic_min <- min(aics)
daics <- aics - aic_min
w1_aic <- exp(-daics[1]/2) / sum(exp(-daics/2))
w2_aic <- exp(-daics[2]/2) / sum(exp(-daics/2))
bic1 <- BIC(m1)  ## for this AIC function, "lower values indicate better fit"
bic2 <- BIC(m2)
bics <- c(bic1, bic2)
bic_min <- min(bics)
dbics <- bics - bic_min
w1_bic <- exp(-dbics[1]/2) / sum(exp(-dbics/2))
w2_bic <- exp(-dbics[2]/2) / sum(exp(-dbics/2))
w1_aic
w1_bic

## NEED TO KEEP WORKING ON LOGISTIC EXAMPLE FROM HERE
prior_sd <- 10
rj_proposal_sd <- 10
prior_m1 <- 0.5
{
    decide <- function(lMHR) {
        if(is.nan(lMHR)) return(FALSE)
        if(log(runif(1,0,1)) < lMHR) return(TRUE) else return(FALSE)
    }
    prior <- function(x) return(dnorm(x, 0, sd=prior_sd, log=TRUE))
    ll <- function(m, a, b, y, x) {
        if(m==1) return(sum(dnorm(y, a, 1, log=TRUE)))
        if(m==2) return(sum(dnorm(y, a+b*x, 1, log=TRUE)))
    }
    updateM <- function(mab, y, x) {
        ##browser()
        if(mab[1] == 1) {
            bprop <- rnorm(1, 0, rj_proposal_sd)   ## proposal for new slope 'b'
            lMHR <- prior(bprop) + ll(2,mab[2],bprop,y,x)  -  (ll(1,mab[2],0,y,x) + dnorm(bprop,0,rj_proposal_sd,log=TRUE)) + log((1-prior_m1)/prior_m1)
            if(decide(lMHR)) return(c(2,mab[2],bprop)) else return(mab)
        }
        if(mab[1] == 2) {
            lMHR <- ll(1,mab[2],0,y,x) + dnorm(mab[3],0,rj_proposal_sd,log=TRUE)  -  (prior(mab[3]) + ll(2,mab[2],mab[3],y,x)) - log((1-prior_m1)/prior_m1)
            if(decide(lMHR)) return(c(1,mab[2],0)) else return(mab)
        }
    }
    updateAB <- function(mab, y, x) {
        ## update a
        sd.aprop <- 0.1
        aprop <- rnorm(1,mab[2],sd.aprop)
        lMHR <- prior(aprop) + ll(mab[1],aprop,mab[3],y,x) - prior(mab[2]) - ll(mab[1],mab[2],mab[3],y,x)
        if(decide(lMHR)) mab <- c(mab[1],aprop,mab[3])
        ## update b
        if(mab[1] == 2) {
            sd.bprop <- 0.1
            bprop <- rnorm(1,mab[3],sd.bprop)
            lMHR <- prior(bprop) + ll(mab[1],mab[2],bprop,y,x) - prior(mab[3]) - ll(mab[1],mab[2],mab[3],y,x)
            if(decide(lMHR)) mab <- c(mab[1],mab[2],bprop) else mab
        }
        return(mab)
    }
}

set.seed(0)
iter <- 100000
mab <- c(1, 0, 0)  ## c(1, 0, 0)
##samp <- data.frame(a=rep(NA,iter), b=NA, c=NA)
samp <- array(NA, c(iter,3))
colnames(samp) <- c('m', 'a', 'b')
for(i in 1:iter) {
    mab <- updateM(mab, y, x)
    mab <- updateAB(mab, y, x)
    samp[i,] <- mab
}
df <- as.data.frame(samp)

mean(df$m==1)
##head(df,10)

c(mean(df$a[df$m==1]), m1$coef[1])
c(mean(df$a[df$m==2]), m2$coef[1])
c(mean(df$b[df$m==2]), m2$coef[2])

##plot(df$a, type='l')
##plot(1:iter, df$b)

prod(dnorm(y, 0, sd=sqrt(prior_sd^2 + 1^2)/10))   ## wrong for p(y|m1), don't know why

pym1 <- mean(replicate(500000, prod(dnorm(y, rnorm(1,0,prior_sd), 1))))
pym2 <- mean(replicate(500000, prod(dnorm(y, rnorm(1,0,prior_sd) + rnorm(1,0,prior_sd)*x, 1))))
pym1
pym2
ppm1 <- prior_m1*pym1 / (prior_m1*pym1 + (1-prior_m1)*pym2)
ppm2 <- (1-prior_m1)*pym2 / (prior_m1*pym1 + (1-prior_m1)*pym2)
ppm1
ppm2

w1_aic
w1_bic
mean(df$m==1)
ppm1

library(BMA)
bma <- bic.glm(x=data.frame(x=x), y=y, glm.family='gaussian')
##class(bma)
##str(bma)
bma$postprob[1]
bma$deviance
bma$label
bma$size
bma$which
bma$probne0
bma$postmean
bma$condpostmean
bma$mle





## implementing RJMCMC for a simple 2 models
## yi ~ N(a + bx, 1)
set.seed(0)
n <- 10
x <- runif(n, -1, 1)
a <- .3
b <- 0.1
y <- rnorm(n, a+b*x, 1)
##plot(x,y)
m1 <- lm(y~1)
m1
m2 <- lm(y~x)
m2
aic1 <- AIC(m1)  ## for this AIC function, "lower values indicate better fit"
aic2 <- AIC(m2)
aics <- c(aic1, aic2)
aic_min <- min(aics)
daics <- aics - aic_min
w1_aic <- exp(-daics[1]/2) / sum(exp(-daics/2))
w2_aic <- exp(-daics[2]/2) / sum(exp(-daics/2))
bic1 <- BIC(m1)  ## for this AIC function, "lower values indicate better fit"
bic2 <- BIC(m2)
bics <- c(bic1, bic2)
bic_min <- min(bics)
dbics <- bics - bic_min
w1_bic <- exp(-dbics[1]/2) / sum(exp(-dbics/2))
w2_bic <- exp(-dbics[2]/2) / sum(exp(-dbics/2))
w1_aic
w1_bic

prior_sd <- 10
rj_proposal_sd <- 10
prior_m1 <- 0.5
{
    decide <- function(lMHR) {
        if(is.nan(lMHR)) return(FALSE)
        if(log(runif(1,0,1)) < lMHR) return(TRUE) else return(FALSE)
    }
    prior <- function(x) return(dnorm(x, 0, sd=prior_sd, log=TRUE))
    ll <- function(m, a, b, y, x) {
        if(m==1) return(sum(dnorm(y, a, 1, log=TRUE)))
        if(m==2) return(sum(dnorm(y, a+b*x, 1, log=TRUE)))
    }
    updateM <- function(mab, y, x) {
        ##browser()
        if(mab[1] == 1) {
            bprop <- rnorm(1, 0, rj_proposal_sd)   ## proposal for new slope 'b'
            lMHR <- prior(bprop) + ll(2,mab[2],bprop,y,x)  -  (ll(1,mab[2],0,y,x) + dnorm(bprop,0,rj_proposal_sd,log=TRUE)) + log((1-prior_m1)/prior_m1)
            if(decide(lMHR)) return(c(2,mab[2],bprop)) else return(mab)
        }
        if(mab[1] == 2) {
            lMHR <- ll(1,mab[2],0,y,x) + dnorm(mab[3],0,rj_proposal_sd,log=TRUE)  -  (prior(mab[3]) + ll(2,mab[2],mab[3],y,x)) - log((1-prior_m1)/prior_m1)
            if(decide(lMHR)) return(c(1,mab[2],0)) else return(mab)
        }
    }
    updateAB <- function(mab, y, x) {
        ## update a
        sd.aprop <- 0.1
        aprop <- rnorm(1,mab[2],sd.aprop)
        lMHR <- prior(aprop) + ll(mab[1],aprop,mab[3],y,x) - prior(mab[2]) - ll(mab[1],mab[2],mab[3],y,x)
        if(decide(lMHR)) mab <- c(mab[1],aprop,mab[3])
        ## update b
        if(mab[1] == 2) {
            sd.bprop <- 0.1
            bprop <- rnorm(1,mab[3],sd.bprop)
            lMHR <- prior(bprop) + ll(mab[1],mab[2],bprop,y,x) - prior(mab[3]) - ll(mab[1],mab[2],mab[3],y,x)
            if(decide(lMHR)) mab <- c(mab[1],mab[2],bprop) else mab
        }
        return(mab)
    }
}

set.seed(0)
iter <- 100000
mab <- c(1, 0, 0)  ## c(1, 0, 0)
##samp <- data.frame(a=rep(NA,iter), b=NA, c=NA)
samp <- array(NA, c(iter,3))
colnames(samp) <- c('m', 'a', 'b')
for(i in 1:iter) {
    mab <- updateM(mab, y, x)
    mab <- updateAB(mab, y, x)
    samp[i,] <- mab
}
df <- as.data.frame(samp)

mean(df$m==1)
##head(df,10)

c(mean(df$a[df$m==1]), m1$coef[1])
c(mean(df$a[df$m==2]), m2$coef[1])
c(mean(df$b[df$m==2]), m2$coef[2])

##plot(df$a, type='l')
##plot(1:iter, df$b)

prod(dnorm(y, 0, sd=sqrt(prior_sd^2 + 1^2)/10))   ## wrong for p(y|m1), don't know why

pym1 <- mean(replicate(500000, prod(dnorm(y, rnorm(1,0,prior_sd), 1))))
pym2 <- mean(replicate(500000, prod(dnorm(y, rnorm(1,0,prior_sd) + rnorm(1,0,prior_sd)*x, 1))))
pym1
pym2
ppm1 <- prior_m1*pym1 / (prior_m1*pym1 + (1-prior_m1)*pym2)
ppm2 <- (1-prior_m1)*pym2 / (prior_m1*pym1 + (1-prior_m1)*pym2)
ppm1
ppm2

w1_aic
w1_bic
mean(df$m==1)
ppm1

library(BMA)
bma <- bic.glm(x=data.frame(x=x), y=y, glm.family='gaussian')
##class(bma)
##str(bma)
bma$postprob[1]
bma$deviance
bma$label
bma$size
bma$which
bma$probne0
bma$postmean
bma$condpostmean
bma$mle





## testing samplers used by JAGS for multivariate normal (dmnorm) nodes
library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    aa <- a*a
    b ~ dnorm(0, aa)
    ab <- a*b
    d ~ dnorm(ab, 1)
    C[1,1] <- 1
    C[1,2] <- 0
    C[2,1] <- 0
    C[2,2] <- 1
    mu[1] <- 0
    mu[2] <- 0
    y[1:2] ~ dmnorm(mu[1:2], C[1:2, 1:2])
    ymu[1] <- exp(y[1])
    ymu[2] <- exp(y[2])
    y2[1:2] ~ dmnorm(ymu[1:2], C[1:2, 1:2])
})
constants <- list()
data <- list()
inits <- list(y = c(0,0), y2=c(0,0), a=1, b=1, d=1)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()


niter <- 10000
monitorVars <- c('y', 'y2')

constsAndData <- c(constants, data)
modelfile <- file.path(tempdir(), 'model.txt')
writeLines(paste0('model\n', paste0(deparse(code, width.cutoff=500L), collapse='\n')), con=modelfile)

library(rjags)
jags_mod <- jags.model(file=modelfile, data=constsAndData, inits=inits, n.chains=1, quiet=FALSE)

list.samplers(jags_mod)

list.factories(jags_mod)
list.samplers
class(jags_mod)


dimnames(jags_out[[1]])
means <- apply(jags_out[[1]][,], 2, mean)
means
sds <- apply(jags_out[[1]][,], 2, sd)
sds





## doing the midterm question about normal mean hypothesis test
## "weights of sheep raised on a farm"
## now also with predictive distribution

library(nimble)

y <- c(78, 81, 77, 76, 75, 74, 78, 75, 77, 75)
n <- length(y)
np <- 5

code <- nimbleCode({
    mu ~ dnorm(75, sd=10)
    for(i in 1:n) {
        y[i] ~ dnorm(mu, sd=3)
    }
    for(i in 1:np) {
        p[i] ~ dnorm(mu, sd=3)
    }
    pmean <- mean(p[1:np])
})
constants <- list(n=n, np=np)
data <- list(y=y)
inits <- list(mu = 75, p = rep(0,np))

Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$addMonitors(c('mu', 'p', 'pmean'))
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel, resetFunctions=TRUE)

set.seed(0)
Cmcmc$run(1000000)
samples <- as.matrix(Cmcmc$mvSamples)
colnames(samples)
dim(samples)
samples[1:20,]
apply(samples, 2, mean)
apply(samples, 2, sd)
apply(samples, 2, var)
1/apply(samples, 2, var)

tau0 <- 1/100
mu0 <- 75
ybar <- mean(y)
tau <- 1/9
tau_n <- tau0 + n*tau
mu_n <- (mu0*tau0 + ybar*n*tau)/(tau0+n*tau)
mu_n
tau_n
1/tau_n
var(samples[,'mu'])

v <- 1/tau
v_n <- 1/tau_n
v_p1 <- v_n + v
v_p1
apply(samples, 2, var)

v_p1/5
var(samples[,'pmean'])
v_n/5 + v
v_n + v/5

var(apply(matrix(sample(as.numeric(samples[,2:6])), ncol=5), 1, mean))




## working on used cars regression example for Bayes course 365
df <- read.csv('~/Downloads/UsedCars.csv')
str(df)
hist(df$Age)
hist(df$HP)
dim(df)
head(df)

m <- lm(Price ~ Age + HP | Type, df=df)
summary(m)

summary(lm(Price ~ Age + HP, df=subset(df, Type==0)))
summary(lm(Price ~ Age + HP, df=subset(df, Type==1)))
summary(lm(Price ~ Age + HP + Type, df=df))


sd(residuals(lm(Price ~ Age + HP, df=subset(df, Type==0))))
sd(residuals(lm(Price ~ Age + HP, df=subset(df, Type==1))))




## recreating and fixing Dao's issue with the correlated SSM,
## where 'a' and 'b' don't mix until after 150,000 iterations
library(nimble)
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
##
code <- nimbleCode({
    a ~ dunif(-0.9999, 0.9999)
    b ~ dnorm(0, sd = 1000)
    sigPN ~ dunif(1e-04, 1)
    sigOE ~ dunif(1e-04, 1)
    x[1] ~ dnorm(b/(1 - a), sd = sqrt(sigPN^2 + sigOE^2))
    y[1] ~ dnorm(x[1], sd = sigOE)
    for (i in 2:t) {
        x[i] ~ dnorm(x[i - 1] * a + b, sd = sigPN)
        y[i] ~ dnorm(x[i], sd = sigOE)
    }
})
constants <- list(t = 100)
data <- list(y = c(20.24405,20.57693,20.49357,20.34159,20.45759,20.43326,20.20554,20.12860,20.14756,20.20781,20.23022,20.26766,20.22984,20.37703,20.13641,20.05309,19.95709,20.19303,20.30562,20.54443,20.91010,20.70580,20.42344,20.19795,20.28816,20.31894,20.76939,20.77023,20.83486,20.29335,20.40990,20.19601,20.04083,19.76056,19.80810,19.83129,19.69174,19.90069,19.87623,19.63371,19.62360,19.72630,19.64450,19.86779,20.17104,20.34797,20.32968,20.48027,20.46694,20.47006,20.51676,20.40695,20.18715,19.97552,19.88331,19.67831,19.74702,19.47502,19.24408,19.37179,19.38277,19.15034,19.08723,19.37051,19.14274,19.46433,19.62459,19.77971,19.54194,19.39081,19.61621,19.51307,19.34745,19.17019,19.26829,19.58943,19.77143,19.83582,19.71198,19.67746,19.75053,20.40197,20.49363,20.37079,20.19005,20.55862,20.48523,20.33071,19.97069,19.79758,19.83811,19.79728,19.86277,19.86836,19.92481,19.88095,20.24899,20.55165,20.22707,20.11235))
inits <- list(a = 0.95, b=1, sigPN = 0.2, sigOE=0.05, x = c(20.26036,20.51331,20.57057,20.35633,20.33736,20.47321,20.22002,20.14917,20.19216,20.26969,20.21135,20.22745,20.20466,20.41158,20.13408,20.08023,19.98956,20.13543,20.32709,20.55840,20.88206,20.74740,20.47671,20.14012,20.29953,20.33778,20.80916,20.75773,20.84349,20.35654,20.41045,20.20180,20.02872,19.74226,19.80483,19.81842,19.69770,19.84564,19.88211,19.70559,19.56090,19.73728,19.66545,19.88158,20.13870,20.39163,20.37372,20.47429,20.39414,20.42024,20.55560,20.40462,20.15831,19.89425,19.79939,19.72692,19.74565,19.42233,19.22730,19.36489,19.37289,19.19050,19.00823,19.35738,19.14293,19.48812,19.67329,19.82750,19.58979,19.43634,19.61278,19.56739,19.38584,19.19260,19.32732,19.65500,19.65295,19.84843,19.68285,19.69620,19.77497,20.31795,20.45797,20.32650,20.24045,20.60507,20.51597,20.30076,19.98100,19.86709,19.85965,19.74822,19.86730,19.90523,19.86970,19.87286,20.28417,20.46212,20.22618,20.13689))
##
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$calculate()   ## [1] 183.3436
##

conf <- configureMCMC(Rmodel, nodes = NULL)

conf$addSampler(c('a', 'b'), 'RW_block')
##conf$addSampler(c('a', 'b'), 'RW_block', control=list(adaptInterval=100))
##conf$addSampler(c('a', 'b'), 'RW_block', control=list(propCov=array(c(1,-.99,-0.99,1), c(2,2))))
##conf$addSampler(c('a', 'b'), 'RW_block', control=list(propCov=array(c(1,-.99,-0.99,1), c(2,2)), scale=0.01))
##conf$addSampler(c('a', 'b'), 'RW_block', control=list(propCov=array(c(0.001709168, -0.0341986, -0.0341986, 0.6844844), c(2,2))))


conf$printSamplers(c('a','b'))

conf$addSampler('sigOE', 'RW')
conf$addSampler('sigPN', 'RW')
for(node in Rmodel$expandNodeNames('x'))
    conf$addSampler(node, 'RW')
conf$resetMonitors()
conf$addMonitors(c('a', 'b', 'sigOE', 'sigPN'))
##conf$getMonitors()
##
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmodel$calculate()   ## [1] 183.3436
nimbleOptions(showCompilerOutput = TRUE)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
##
niter <- 500000
system.time(Cmcmc$run(niter))
##
samples <- as.matrix(Cmcmc$mvSamples)
dim(samples)
dimnames(samples)
##

## Plot1
dev.new(width=8, height=5)
par(mfrow=c(1,2))
plot(samples[1:300000,'a'], type='l', ylab='a')
plot(samples[1:300000,'b'], type='l', ylab='b')
par(mfrow=c(1,1))
getwd()
dev.copy2pdf(file='~/Downloads/plot1.pdf')

samplesPlot(samples, col=c('b','a'), ind=1:300000)

##samplesPlot(samples, col=c('sigOE','sigPN'))

##xs <- Rmodel$expandNodeNames('x')
##samplesPlot(samples, col=xs[ 1:10])
##samplesPlot(samples, col=xs[11:20])
##samplesPlot(samples, col=xs[21:30])
##samplesPlot(samples, col=xs[31:40])

##i <-  4:13   ## x[ 1]...x[10]
##i <- 14:23   ## x[11]...x[20]
##i <-  1:8    ## (a,b), sigOE, sigPN, x[1]...x[5]
##i <-  2:3    ## sigOE, sigPN

## all the latent state scales follow sigOE scale
##i <-  c(2,4:8)    ## sigOE, x[1]...x[5]
##nms <- sapply(conf$samplerConfs[i], function(x) x$target)
##nms
##ar <- do.call(cbind, lapply(Cmcmc$samplerFunctions$contentsList[i], function(x) x$scaleHistory))
##colnames(ar) <- nms
##samplesPlot(ar)
##dim(ar)
##samplesPlot(ar, burnin=500)

block_scales <- Cmcmc$samplerFunctions$contentsList[[1]]$scaleHistory
block_propCovHistory <- Cmcmc$samplerFunctions$contentsList[[1]]$propCovHistory
## create block_propCovScale
block_propCovScale <- block_propCovHistory
for(i in 1:length(block_scales))   block_propCovScale[i,,] <- block_scales[i] * block_propCovHistory[i,,]
##dim(block_propCovScale)
block_scale_a <- apply(block_propCovScale, 1, function(x) sqrt(x[1,1]))
block_scale_b <- apply(block_propCovScale, 1, function(x) sqrt(x[2,2]))
block_cors <- apply(block_propCovHistory, 1, function(x) cov2cor(x)[1,2])
ar <- cbind(block_scales, block_scale_a, block_scale_b, block_cors)
colnames(ar) <- c('scale', 'sig_a', 'sig_b', 'cor')
samplesPlot(ar)

## final constant that scale approaches:
block_scales[length(block_scales)]
##propCov adapts very nicely to true covariance between 'a' and 'b'
cov(samples[(dim(samples)[1]/2):(dim(samples)[1]), c('a','b')])
block_propCovHistory[dim(ar)[1],,]
## final adapted (and scaled) proposal corrleation is very accurate:
cor(samples[(dim(samples)[1]/2):(dim(samples)[1]), c('a','b')])
cov2cor(block_scales[length(block_scales)] * block_propCovHistory[dim(ar)[1],,])
## final adapted (and scaled) proposal standard deviations for 'a' and 'b':
sqrt((block_scales[length(block_scales)] * block_propCovHistory[dim(ar)[1],,])[1,1])
sqrt((block_scales[length(block_scales)] * block_propCovHistory[dim(ar)[1],,])[2,2])

## expand cor, sig_a, and sig_b by adaptInterval:
length(block_scale_a)
aI <- 200
block_scale_a_ex <- rep(block_scale_a, each=aI)
block_scale_b_ex <- rep(block_scale_b, each=aI)
block_cors_ex    <- rep(block_cors,    each=aI)
block_scales_ex  <- rep(block_scales,  each=aI)
samples_block_info <- cbind(samples[,'a'], samples[,'b'], block_scale_a_ex, block_scale_b_ex, block_cors_ex, block_scales_ex)
dimnames(samples_block_info)[[2]] <- c('a', 'b', 'sig_a', 'sig_b', 'cor', 'scale')

##samplesPlot(samples_block_info, ind=1:300000, col=c('b', 'a'))
##samplesPlot(samples_block_info, ind=1:300000, col=c('sig_a', 'sig_b', 'cor', 'scale'))

## this one is best:
## artificially trim 'b' samples:
samples_block_info_trim <- samples_block_info
samples_block_info_trim[,'b'] <- pmin(samples_block_info_trim[,'b'], 2)
samplesPlot(samples_block_info_trim, ind=1:300000, col=c('b', 'a', 'sig_a', 'sig_b', 'cor', 'scale'))
dev.copy2pdf(file='~/Downloads/plot2.pdf')

## same thing, on the "early" time scale
samplesPlot(samples_block_info_trim, ind=1:2000, col=c('b', 'a', 'sig_a', 'sig_b', 'cor', 'scale'), densityplot=FALSE)
dev.copy2pdf(file='~/Downloads/plot3.pdf')

samplesPlot(samples_block_info_trim, ind=1:20000, col=c('b', 'a', 'sig_a', 'sig_b', 'cor', 'scale'), densityplot=FALSE)
dev.copy2pdf(file='~/Downloads/plot4.pdf')







## testing addition of scaleHistory and propCovHistory
## back into RW and RW_block samplers


library(nimble)
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0, b=1)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel, nodes=NULL)
conf$addSampler('a', 'RW')
conf$addSampler(c('a', 'b'), 'RW_block')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)

Cmcmc$samplerFunctions$contentsList[[1]]$scaleHistory
Cmcmc$samplerFunctions$contentsList[[2]]$scaleHistory
a <- Cmcmc$samplerFunctions$contentsList[[2]]$propCovHistory
dim(a)
for(i in 1:dim(a)[1]) print(a[i,,])

set.seed(0)
Cmcmc$run(1000)

Cmcmc$samplerFunctions$contentsList[[1]]$scaleHistory
Cmcmc$samplerFunctions$contentsList[[2]]$scaleHistory
Cmcmc$samplerFunctions$contentsList[[2]]$propCovHistory
a <- Cmcmc$samplerFunctions$contentsList[[2]]$propCovHistory
dim(a)
for(i in 1:dim(a)[1]) print(a[i,,])







## playing with colorRamp and colorRampPalette
## to get gradients of colors in plots

FLcrime <- read.csv("~/github/courses/stat201/data/fl_crime.csv")
head(FLcrime[, 1:4])
FLcrime <- FLcrime[, 2:4]
colnames(FLcrime) <- c("Crime", "Education", "Urbanization")
head(FLcrime)
pairs(FLcrime, panel=panel.smooth)
pairs(FLcrime)
plot(FLcrime$Education, FLcrime$Crime)
hist(FLcrime$Urbanization)
boxplot(FLcrime$Urbanization)
quantile(FLcrime$Urbanization)

plot(FLcrime$Education, FLcrime$Crime, pch=19)
with(subset(FLcrime, Urbanization<20), points(Education, Crime, col='green', pch=19))
with(subset(FLcrime, Urbanization>80), points(Education, Crime, col='red', pch=19))


pal <- colorRampPalette(c('blue', 'red'))
cols <- pal(nrow(FLcrime))[cut(FLcrime$Urbanization, nrow(FLcrime))]
plot(FLcrime$Education, FLcrime$Crime, col=cols, pch=19)
with(subset(FLcrime, Urbanization<20), points(Education, Crime, col='green', pch=19))
with(subset(FLcrime, Urbanization>80), points(Education, Crime, col='red', pch=19))


x <- runif(100)
dat <- data.frame(x = x,y = x^2 + 1)

?colorRampPalette
rbPal <- colorRampPalette(c('red','blue'))

cc <- rbPal(100)[cut(dat$x, 100)]
plot(dat$x, dat$y, col = cc)

cc <- colorRampPalette(1:2)
plot(1:10, 1:10, col=cc(1:10/10))


plot(FLcrime$Education, FLcrime$Crime)

## practice problem 6 for STAT 365

## likelihood: yi ~ Normal(mu, sigma)
n <- 4
y <- c(100, 110, 112, 118)
sigma <- 10

## prior: mu ~ Normal(mu0, sigma0)
mu0 <- 100
sigma0 <- 10

## derive precisions
tau0 <- sigma0^-2
tau <- sigma^-2

## posterior distribution
tau_n <- tau0 + n*tau
mu_n <- (tau0*mu0 + n*tau*mean(y)) / tau_n

sigma_n <- 1/sqrt(tau_n)

mu_n
tau_n
sigma_n

## prior, likelihood, posterior plots

## 95% BCI

## one-sided hypothesis test
## H0: mu|y <= 100
## Ha: mu|y >  100

## two-sided hypothesis test
## H0: mu|y  = 100
## Ha: mu|y != 100

## samples from posterior

## 95% BCI

## one-sided hypothesis test
## H0: mu|y <= 100
## Ha: mu|y >  100


## two-sided hypothesis test
## H0: mu|y  = 100
## Ha: mu|y != 100


.





file <- 'HurricaneDamage.csv'
data <- read.csv(paste0('~/github/courses/stat201/data/', file))
Year <- data$Year
Damage <- data$Damage
Year2 <- Year[-1]
Damage2 <- Damage[-1]
m2 <- lm(Damage2~Year2)

res <- residuals(m2)

par(mfrow=c(1,1))
hist(res, breaks=12)

par(mfrow=c(2,1))
plot(Year2, res)
plot(fitted(m2), res)






## modifying hurricanes.csv data file into Hurricane_Damage.csv,
## for use in the STAT201 miderm
file <- 'hurricanes.csv'
data <- read.csv(paste0('~/Downloads/', file))
str(data)
names(data)
names(data)[1] <- 'Rank'
names(data)[2] <- 'Tropical.Cyclone'
names(data)[3] <- 'Year'
names(data)
dim(data)
data <- data[-(2:5),]
dim(data)
mtemp <- lm(data$Damage ~ data$Year)
coef(mtemp)
mtemp <- lm(data$Damage[-1] ~ data$Year[-1])
coef(mtemp)
fitted(mtemp)
newY <- c(data$Damage[1], 0.4*data$Damage[-1] + 0.6*fitted(mtemp))
data$Damage <- newY
write.csv(data, file='~/Downloads/HurricaneDamage.csv', row.names=FALSE)


file <- 'HurricaneDamage.csv'
data <- read.csv(paste0('~/Downloads/', file))
str(data)
x <- data$Year
y <- data$Damage
m <- lm(y~x)
summary(m)
plot(x, y)
abline(m, col='red')
coef(m)

sort(y)
which(y>40000)
i <- which(y>40000)
x2 <- x[-i]
y2 <- y[-i]
m2 <- lm(y2~x2)
summary(m2)
plot(x2, y2)
abline(m2, col='red')
coef(m2)
cor(x2,y2)
cor(x2,y2)^2





## doing LAX flight departures problem
## from the STAT201 midterm

x <- 3648
y <- 25843
w <- 3407
z <- 26134
N <- x+y+w+z

year <- c(rep(2014, x+y), rep(2015, w+z))
delay <- c(rep('ayes',x), rep('no',y), rep('ayes',w), rep('no',z))
tab <- table(year, delay)
tab

## (a) display contingency table
rbind(cbind(tab, margin.table(tab,1)), c(margin.table(tab,2), N))
##     ayes    no      
##2014 3648 25843 29491
##2015 3183 26284 29467
##     6831 52127 58958

## (b) percentage in 2014?
(x+y)/N * 100
prop.table(margin.table(tab, 1))[1]
## 50.02035 %

## (c) percentage of delayed departures in 2015?
w/(w+x) * 100
## 46.5964 %

## (d) diff. in prop, delays in 2015 relative to 2014?
ptab <- prop.table(tab, 1)
ptab[2,1] - ptab[1,1]
## -0.01567962 = -1.567962 %

## (e) interpret this diff. in prop.
## The fraction of LAX December 2015 departures that were delayed was 1.57 *percentage points* lower than the fraction of LAX December 2016 departures that were delayed.

iter <- 1000
data <- data.frame(year=year, delay=delay)
nr <- nrow(data)
ratio <- numeric(iter)
for(i in 1:iter) {
    ind <- sample(1:nr)
    tab <- table(data$year, data$delay[ind])
    cond.tab <- prop.table(tab, 1)
    ratio[i] <- cond.tab[2,1] - cond.tab[1,1]
}

hist(ratio, breaks=15)
tab <- table(data$year, data$delay)
cond.tab <- prop.table(tab, 1)
observed_ratio <- cond.tab[2,1] - cond.tab[1,1]
abline(v = observed_ratio, col = 'red', lwd = 2)



## demo of coin flips and LLN

n <- 30
n

flips <- rbinom(n, 1, 0.5)
flips

cumsum(flips)

seq_along(flips)

cumsum(flips) / seq_along(flips)

running_avg <- cumsum(flips) / seq_along(flips)

running_avg

par(mfrow=c(1,1))
plot(running_avg, type='l')
abline(h=0.5, col='red', lty=3)

ks <- 2:4
par(mfrow=c(length(ks),1), mar=c(2,2,2,2))

for(i in ks) {
    n <- 10^i
    flips <- rbinom(n, 1, 0.5)
    running_avg <- cumsum(flips) / seq_along(flips)
    plot(running_avg, type='l')
    abline(h=0.5, col='red', lty=3)
}





## testing seq_along in NIMBLE run code
library(nimble)

nfDef <- nimbleFunction(
    setup = function() {
        a <- 1:10
    },
    run = function() {
        for(i in seq_along(a)) {
            print(i)
        }
    }
)

Rnf <- nfDef()

Rnf$run()

Cnf <- compileNimble(Rnf)

Cnf$run()




## trying out MCMC with two Gibbs samplers for Normal

y <- c(4.3, 2.5, 3.2, 3.8, 2.9, 3.1, 4.2, 4.0)
n <- length(y)
n
y
mean(y)
sd(y)

## mu ~ dnorm(mu0=0, tau0=0.001)
## tau ~ dgamma(r0=0.001, v0=0.001)
## yi ~ dnorm(mu, tau)

mu0 <- 0
tau0 <- 0.001
r0 <- 0.001
v0 <- 0.001

## iter sampling iterations to run
iter <- 100000
samp <- cbind(mu = rep(NA,iter), tau = rep(NA,iter))

## inits: mu=0, tau=1
mu <- 0
tau <- 1

set.seed(0)
for(i in 1:iter) {
    mu <- rnorm(1, (tau0*mu0+tau*sum(y))/(tau0+n*tau), (tau0+n*tau)^-0.5)
    tau <- rgamma(1, r0+n/2, v0+0.5*sum((y-mu)^2))
    samp[i, 1] <- mu
    samp[i, 2] <- tau
}


1/2 * sum((y-mu)^2)
n/2 * (mean(y)-mu)^2


head(samp)

samp <- cbind(samp, sd = samp[,'tau']^-0.5)

apply(samp, 2, mean)
apply(samp, 2, median)
mean(y)
sd(y)
sd(y)^-2

samplesPlot(samp)

library(nimble)

code <- nimbleCode({
    mu ~ dnorm(0, 0.001)
    tau ~ dgamma(0.001, 0.001)
    for(i in 1:n) {
        y[i] ~ dnorm(mu, tau)
    }
})
constants <- list(n=n)
data <- list(y=y)
inits <- list(mu=0, tau=1)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)

set.seed(0)
Cmcmc$run(10)

Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)

head(Rsamples, 10)
head(Csamples, 10)
head(samp, 10)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)




## fixing bug in conjugacy

library(nimble)
set.seed(0)
n <- 10
##y <- c(rnorm(n,0,sd=1), rnorm(n,10,sd=1))
y <- c(rnorm(n,0,sd=1), rnorm(n,10,sd=2), rnorm(n,20,sd=3))

code <- nimbleCode({
    for(i in 1:3) {
        ##sig[i] ~ dunif(0, 100)
        tau[i] ~ dgamma(0.001, 0.001)   ## using TAU
    }
    mu[1] ~ dnorm(0, sd=1000)
    mu[2] <- mu[1] + delta1
    delta1 ~ T(dnorm(0, sd=1000), 0, 1000)
    mu[3] <- mu[2] + delta2
    delta2 ~ T(dnorm(0, sd=1000), 0, 1000)
    for(i in 1:N) {
        ##means[i] <- equals(z[i],1)*mu[1] + equals(z[i],2)*mu[2]
        means[i] <- equals(z[i],1)*mu[1] + equals(z[i],2)*mu[2] + equals(z[i],3)*mu[3]
        ##sigmas[i] <- equals(z[i],1)*sig[1] + equals(z[i],2)*sig[2] + equals(z[i],3)*sig[3]
        taus[i] <- equals(z[i],1)*tau[1] + equals(z[i],2)*tau[2] + equals(z[i],3)*tau[3]   ## using TAU
        ##y[i] ~ dnorm(means[i], sd=sigmas[i])
        y[i] ~ dnorm(means[i], taus[i])   ## using TAU
        z[i] ~ dcat(pi[1:3])
    }
    for(i in 1:3) {
        pi0[i] ~ dgamma(1, 1)
        pi[i] <- pi0[i] / (pi0[1] + pi0[2] + pi0[3])
    }
})

N <- length(y)
constants <- list(N=N)
data <- list(y=y)
##inits <- list(mu=c(1,2,3), delta1=1, delta2=1, pi0=c(1,1,1), sig=c(1,1,1), z=rep(1:3, each=n))
inits <- list(mu=c(1,2,3), delta1=1, delta2=1, pi0=c(1,1,1), tau=c(1,1,1), z=rep(1:3, each=n))  ## using TAU
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$pi0
Rmodel$pi
Rmodel$z
Rmodel$mu
Rmodel$means
Rmodel$sigmas
Rmodel$taus

##undebug(Rmodel$checkConjugacy)
##Rmodel$checkConjugacy('mu[1]')
## 
##undebug(Rmodel$checkConjugacy2)
##Rmodel$checkConjugacy2('mu[1]')

conf <- configureMCMC(Rmodel)
##conf <- configureMCMC(Rmodel, useConjugacy=FALSE)
conf$resetMonitors()
##conf$addMonitors(c('mu', 'z', 'sig'))
conf$addMonitors(c('mu', 'z', 'tau'))
conf$printSamplers('mu')
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
calculate(Rmodel)
calculate(Cmodel)

##iter <- 20
## 
##set.seed(0)
##Rmcmc$run(iter)
## 
##set.seed(0)
##Cmcmc$run(iter)
## 
##Rsamples <- as.matrix(Rmcmc$mvSamples)
##Csamples <- as.matrix(Cmcmc$mvSamples)
## 
##sampNames <- colnames(Rsamples)
## 
##Rsamples[, sampNames]
##Csamples[, sampNames]
##Rsamples[, sampNames] - Csamples[, sampNames]

set.seed(0)
Cmcmc$run(50000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)

mus <- Rmodel$expandNodeNames(c('mu'))
sigs <- Rmodel$expandNodeNames(c('sig'))
taus <- Rmodel$expandNodeNames(c('tau'))
zs <- Rmodel$expandNodeNames(c('z'))

ind <- 1:dim(samples)[1]
ind <- 1:5000
samplesPlot(samples, col = mus, ind = ind)
samplesPlot(samples, col = sigs, ind = ind)
samplesPlot(samples, col = taus, ind = ind)
samplesPlot(samples, col = c('mu[3]','sig[3]'), ind = ind)
samplesPlot(samples, col = c('mu[3]','tau[3]'), ind = ind)
samplesPlot(samples, col = c(        'tau[3]'), ind = ind)

samplesPlot(samples, col = zs, ind = ind)

samplez <- samples[, zs]
samplezsum <- cbind(samplez, z1=apply(samplez, 1, function(x) sum(x==1)), z2=apply(samplez, 1, function(x) sum(x==2)), z3=apply(samplez, 1, function(x) sum(x==3)))
dim(samplez)
dim(samplezsum)

samplesPlot(samplezsum, col = c('z1','z2','z3'), ind=1:5000)
samplesPlot(samplezsum, col = c('z1','z2','z3'), ind=1:15000)

mean(y[1:n])
mean(y[(n+1):(2*n)])
mean(y[(2*n+1):(3*n)])



## doing the midterm question about normal mean hypothesis test
## "weights of sheep raised on a farm"

tau0 <- 1/100
mu0 <- 75

y <- c(78, 81, 77, 76, 75, 74, 78, 75, 77, 75)
n <- length(y)
ybar <- mean(y)
ybar
tau <- 1/9

taup <- tau0 + n*tau
mup <- (mu0*tau0 + ybar*n*tau)/(tau0+n*tau)

mup
taup
pnorm(75, mup, 1/sqrt(taup))


library(nimble)

code <- nimbleCode({
    mu ~ dnorm(75, sd=10)
    for(i in 1:n) {
        y[i] ~ dnorm(mu, sd=3)
    }
})
constants <- list(n=n)
data <- list(y=y)
inits <- list(mu = 75)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(500000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)
mup
mean(samples[,1] < 75)
pnorm(75, mup, 1/sqrt(taup))






## inplementing a finite mixture model in NIMBLE

library(nimble)

set.seed(0)
n <- 500
y <- c(rnorm(n,0,sd=1), rnorm(n,10,sd=2), rnorm(n,20,sd=3))
##hist(y, breaks=30)

code <- nimbleCode({
    for(i in 1:3) {
        sig[i] ~ dunif(0, 100)
    }
    mu[1] ~ dnorm(0, sd=1000)
    mu[2] <- mu[1] + delta1
    delta1 ~ T(dnorm(0, sd=1000), 0, 1000)
    mu[3] <- mu[2] + delta2
    delta2 ~ T(dnorm(0, sd=1000), 0, 1000)
    for(i in 1:N) {
        means[i] <- equals(z[i],1)*mu[1] + equals(z[i],2)*mu[2] + equals(z[i],3)*mu[3]
        sigmas[i] <- equals(z[i],1)*sig[1] + equals(z[i],2)*sig[2] + equals(z[i],3)*sig[3]
        y[i] ~ dnorm(means[i], sd=sigmas[i])
        z[i] ~ dcat(pi[1:3])
    }
    ##pi[1:3] ~ ddirch(alpha[1:3])
    for(i in 1:3) {
        pi0[i] ~ dgamma(1, 1)
        pi[i] <- pi0[i] / (pi0[1] + pi0[2] + pi0[3])
    }
})

N <- length(y)
constants <- list(N=N)
data <- list(y=y)
inits <- list(sig=rep(1,3), mu=c(1,2,3), delta1=1, delta2=1, z=rep(1,N), pi0=c(1,1,1))

Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$pi0
Rmodel$pi
Rmodel$z
Rmodel$mu
Rmodel$means
Rmodel$sigmas

conf <- configureMCMC(Rmodel)
##conf <- configureMCMC(Rmodel, useConjugacy = FALSE)

conf$printSamplers()
conf$printSamplers('mu')
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

calculate(Rmodel)
calculate(Cmodel)

##set.seed(0)
##Rmcmc$run(2)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)


                   

## bootstrapping the slope 

data <- read.csv('~/Downloads/Animals\ (1).csv')
names(data)
head(data)

plot(data$gestation, data$longevity)

m <- lm(data$longevity ~ data$gestation)
m
summary(m)

s <- numeric(10000)
n <- dim(data)[1]
for(i in 1:10000) {
    ind <- sample(1:n, replace = TRUE)
    dat <- data[ind,]
    m <- lm(dat$longevity ~ dat$gestation)
    s[i] <- unname(m$coef[2])
}

hist(s)
m <- lm(data$longevity ~ data$gestation)
abline(v = unname(m$coef[2]), col='red')

              



## doing the bigmac problem

data <- read.delim('~/Downloads/bigmac.txt')
names(data)
head(data)
with(data, plot(EngSal, BigMac))
rownames(data)
colnames(data)
dimnames(data)

data <- read.delim('~/Downloads/bigmac.txt')
with(data, plot(EngSal, BigMac))
with(data, text(EngSal, BigMac, labels=rownames(data), cex=0.5, pos=4))

i <- -c(17,21)

m <- lm(data$EngSal[i] ~ data$BigMac[i])
coef(m)
summary(m)

plot(data$EngSal[i], residuals(m))

dim(data)
length(data$EngSal)
length(unname(residuals(m)))
names(m)
unname(residuals(m))
data$EngSal
data$BigMac

library(MASS)
boxcox
boxcox(m)

plot(data$EngSal[i], (data$BigMac[i])^.01)
plot(data$EngSal[i], log(data$BigMac[i]))

## likelihood: y ~ Normal(mu, sigma)
## what prior for sigma?
## sigma ~ Uniform(0, 1000)

sigma <- runif(100000, 0, 1000)

hist(sigma)



## verifying the change of variable formula

a <- runif(100000, 0, 1000)

par(mfrow=c(2,1))
plot(density(a))
plot(density(sqrt(a)))
curve(2*x/1000, col='red', add=TRUE)



## STAT 365 Monte Carlo Exercise 9.1
## comparing Bayesian and Frequentist estimators of pi

n <- 10
msef <- numeric()
mseb <- numeric()
biasf <- numeric()
biasb <- numeric()
varf <- numeric()
varb <- numeric()
pis <- c(1:5/100, 1:9/10, 95:99/100)
for(i in 1:length(pis)) {
    pi <- pis[i]
    samp <- rbinom(10000, size=n, prob=pi)
    pihatf <- samp / n
    pihatb <- (samp+1)/(n+2)
    biaspif <- mean(pihatf) - pi
    biaspib <- mean(pihatb) - pi
    varpif <- var(pihatf)
    varpib <- var(pihatb)
    msef[i] <- biaspif^2 + varpif
    mseb[i] <- biaspib^2 + varpib
    biasf[i] <- biaspif
    biasb[i] <- biaspib
    varf[i] <- varpif
    varb[i] <- varpib
}
par(mfrow = c(3,1))
plot(pis, biasf, main = 'Bias', col='red', type = 'b', ylim = range(c(biasf,biasb)))
lines(pis, biasb, col='blue', type = 'b')
plot(pis, varf, main = 'Variance', col='red', type = 'b', ylim = range(c(varf,varb)))
lines(pis, varb, col='blue', type = 'b')
plot(pis, msef, main = 'MSE', col='red', type = 'b', ylim = range(c(msef,mseb)))
lines(pis, mseb, col='blue', type = 'b')



## trying HW for STAT201

data <- read.delim('~/Downloads/Saratoga.txt')
names(data)
str(data)
attach(data)
boxplot(Price)
hist(Price)
boxplot(Living.Area)
hist(Living.Area)
plot(Living.Area, Price)

data <- read.delim('~/Downloads/Mauna-Loa-and-DJIA.txt')
attach(data)
ind <- Year>=1982
plot(DJIA, CO2.Avg)
plot(DJIA[ind], CO2.Avg[ind])
cor(DJIA[ind], CO2.Avg[ind])
str(data)

pairs(data, panel = panel.smooth)
?pairs

data <- read.delim('~/Downloads/Kentucky_Derby_2014.txt')
names(data)
str(data)
attach(data)
plot(data$Year, data$Speed..mph.)


## pulling together pieces of not having to recompile models and MCMCs, for Dao

library(nimble)
nimbleOptions(showCompilerOutput = TRUE) ### DELETE THIS later
code <- nimbleCode({
    a ~ dbern(0.5)
    b ~ dnorm(0, 1)
    c ~ dnorm(0, 1)
})
Rmodel <- nimbleModel(code, inits = list(a=0, b=0, c=0))
conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

model_orig <- Cmodel

## recover the R (uncompiled) model object
if(inherits(model_orig, 'CmodelBaseClass'))
    model_orig <- model_orig$Rmodel


##md <<- Rmodel_orig$modelDef
Rmodel <- model_orig$newModel(replicate = TRUE, check = FALSE)

conf_initial <- configureMCMC(Rmodel)

monitorsVector <- Rmodel$getNodeNames(stochOnly=TRUE, includeData=FALSE)
conf_initial$addMonitors(monitorsVector, print=FALSE)

scalarNodeVector <- Rmodel$getNodeNames(stochOnly=TRUE, includeData=FALSE, returnScalarComponents=TRUE)
discreteInd <- sapply(scalarNodeVector, function(n) Rmodel$isDiscrete(n), USE.NAMES=FALSE)
scalarNodeVectorContinuous <<- scalarNodeVector[!discreteInd]
scalarNodeVectorContinuous

firstScalarNode <- scalarNodeVectorContinuous[1]
firstScalarNode

conf_initial$printSamplers()

samplersWeMightUse <- c('RW', 'slice', 'RW_block')
for(sampler in samplersWeMightUse)
    conf_initial$addSampler(target = firstScalarNode, type = sampler)

conf_initial$printSamplers()

Rmcmc_initial <- buildMCMC(conf_initial)
Cmodel <- compileNimble(Rmodel)
Cmcmc_initial <- compileNimble(Rmcmc_initial, project = Rmodel)

conf_new <- configureMCMC(oldConf = conf_initial)

conf_new$setSamplers()  ## remove all samplers
conf_new$printSamplers()

nodes <- c('a', 'b', 'c')
for(node in nodes)
    conf_new$addSampler(target = node, type = 'slice')
conf_new$printSamplers()

Rmcmc_new <- buildMCMC(conf_new)
Cmcmc_new <- compileNimble(Rmcmc_new, project = Rmodel)


nimCopy(from = model_orig, to = Cmodel, logProb = TRUE)
calculate(Cmodel)


## doing the random drug abuse survey using NIMBLE MCMC

library(nimble)

N <- 29
y <- 14


code <- nimbleCode({
    theta ~ dunif(0, 1)
    y ~ dbinom(size = N, prob = 0.25 + 0.5*theta)
})

constants <- list(N = N)
data <- list(y = y)
inits <- list(theta = 0.5)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(100000)
samples <- as.matrix(Cmcmc$mvSamples)

samplesPlot(samples, col='theta')
theta <- samples[,'theta']
mean(theta)
median(theta)
quantile(theta, c(0.25, 0.5, 0.75))


## testing random drug abuse survey strategy, the variance of the results

f <- function(n, iter) {
    res <- rbinom(n=iter, size=n, prob=0.25)
    var(res)
}

## expect variance is 3*n/16
iter <- 100000

n <- 100
f(n, iter) - 3*n/16


g <- function(n, iter) {
    res <- numeric(iter)
    for(i in 1:iter) {
        a <- rbinom(n=1, size=n, prob=1/2)
        res[i] <- rbinom(n=1, size=a, prob=1/2)
    }
    var(res)
}

g(n, iter) - 3*n/16



test <- function(iter, n, theta) {
    res <- numeric()
    c <- numeric()
    d <- numeric()
    for(i in 1:iter) {
        a <- rbinom(1, size=n, prob=1/2)
        b <- n - a
        c[i] <- rbinom(1, size=a, prob=1/2)
        d[i] <- rbinom(1, size=b, prob=theta)
        res[i] <- c[i] + d[i]
    }
    var(d)
}

iter <- 10000
n <- 100
theta <- .2
test(iter, n, theta)

3*n/16

theta*n*(2-theta)/4



## solving 5 statisticians problem

one <- function() {
    cur <- 1  ## 1, 2, 3, 4, 5
    while(TRUE) {
        n <- runif(1)  # stay, +1, -1
        if(n < 1/3) {
            return(cur)
        } else if(n < 2/3) {
            cur <- ((cur - 1) - 1) %% 5 + 1
        } else cur <- ((cur + 1) - 1) %% 5 + 1
    }
}

many <- function(n) {
    ret <- numeric(n)
    for(i in 1:n)
        ret[i] <- one()
    return(ret)
}

prop.table(table(many(1000000))) * 11

##        1        2        3        4        5 
## 5.002679 2.000020 0.996589 0.999779 2.000933 
##     5/11,    2/11,    1/11,    1/11,    2/11


## playihg with Rmd

setwd('~/temp/lecTEMP')
getwd()
library(methods)
library(knitr)
library(rmarkdown)

list.files()

render('Lecture1Slides.rmd')

## testing Nick Michaud's question about nimbleFunctionLists
library(nimble)

bigFunction <- nimbleFunction(
    setup = function(N) {
        functions <- nimbleFunctionList(littleFunction_virtual)  ## CHANGE
        for(n in 1:N)
            functions[[n]] <-littleFunction(n)
    },
    run = function() {
        returnType(integer(0))
        sum_N <- 0
        for(n in 1:N) 
            sum_N <- sum_N + functions[[n]]$run()
        return(sum_N)
    }
)

## NEW
littleFunction_virtual <- nimbleFunctionVirtual(
    run = function() {
        returnType(integer(0))
    }
)

littleFunction <- nimbleFunction(
    contains = littleFunction_virtual,
    setup = function(n){},
    run = function(){
        returnType(integer(0))
        return(n)
    }
)

testFunction <- bigFunction(5)
testFunction$run()
CtestFunction <- compileNimble(testFunction)
CtestFunction$run()


## make configureMCMC respect dconstraint()
library(nimble)

code <- nimbleCode({
    for(j in 1:J) {
        for(i in 1:n[j]) {
            y[j,i] ~ dconstraint(w[j,i] > 0)
            w[j,i] ~ dnorm(theta[j], 1)
        }
        theta[j] ~ dnorm(mu, itau2)
    }
    itau2 ~ dgamma(a, b)
    mu ~ dnorm(0, .00001)
})

J <- 3
n <- rep(2, J)

y <- matrix( sample(c(0,1), sum(n), replace = TRUE), nrow = J)
m <- nimbleModel(code, constants = list(n = n, J = J), data = list(y = y))

conf <- configureMCMC(m)
conf$printSamplers()


## nested sampler function wrapper for Dao
library(nimble)

sampler_record_wrapperNEW <- nimbleFunction(
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control){
      numSamples <- 0
      before <- c(0, 0)
      after <- c(0, 0)
      samplerFunctionList <- nimbleFunctionList(sampler_BASE)
    ###### make sure to provide *named* arguments to this function
    ###### shouldn't require anything in control$control, if you don't want
    controlListForNestedSampler <- mcmc_generateControlListArgument(samplerFunction = control$sampler_function, control = control$control)
    samplerFunctionList[[1]] <- eval(call( control$sampler_function, model = model, mvSaved = mvSaved, target = target, control =  controlListForNestedSampler))}, 
    run = function() {
      ## these lines are new:
      numSamples <<- numSamples + 1
      setSize(before, numSamples)
      setSize(after, numSamples)
      before[numSamples] <<- model[[target]]
      ## back to the original sampler function code:
      samplerFunctionList[[1]]$run()
      ## this line new:
      after[numSamples] <<- model[[target]]
    },
    methods = list(
        reset = function() {samplerFunctionList[[1]]$reset()}
    ))

code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:10) {
        x[i] ~ dnorm(mu*mu, sd = sigma)
    }
})
Rmodel <- nimbleModel(code)

conf <- configureMCMC(Rmodel)
conf$printSamplers()

conf$removeSamplers('sigma')
conf$printSamplers()

## SEE CHANGES HERE
conf$addSampler(target = 'sigma', type = sampler_record_wrapperNEW, control = list(sampler_function = 'sampler_slice', control=list()))
conf$printSamplers()

## SEE CHANGES HERE
conf$addSampler(target = 'sigma', type = 'sampler_record_wrapperNEW', control = list(sampler_function = 'sampler_RW', control = list()))
conf$printSamplers()

Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)



## trying out a recursive nimble function
library(nimble)

Rnf <- nimbleFunction(
    run = function(x = double()) {
        if(x == 0 || x == 1) {
            return(1)
        } else {
            a <- 1
        }
    })



## testing the new NIMBLE function: runMCMC()
library(nimble)
code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:10) {
        x[i] ~ dnorm(mu*mu, sd = sigma)
    }
})
Rmodel <- nimbleModel(code)
Rmodel$setData(list(x = c(2, 5, 3, 4, 1, 0, 1, 3, 5, 3)))
conf <- configureMCMC(Rmodel)
conf$getMonitors()
conf$setThin(10)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)


## testing new functions for addSampler(), 'name', 'libraryTag', etc...
library(nimble)
code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:10) {
        x[i] ~ dnorm(mu*mu, sd = sigma)
    }
})
Rmodel <- nimbleModel(code)
conf <- configureMCMC(Rmodel)

debug(conf$addSampler)
undebug(conf$addSampler)
conf$printSamplers()
conf$removeSamplers('sigma')
conf$printSamplers()
conf$addSampler(target = 'sigma', type = sampler_slice, name='slice1')
conf$addSampler(target = 'sigma', type = 'slice', name='slice2')
conf$addSampler(target = 'sigma', type = sampler_slice)
conf$addSampler(target = 'sigma', type = 'slice')
conf$addSampler(target = 'sigma', type = sampler_RW, name='slice1')
conf$addSampler(target = 'sigma', type = 'RW', name='slice2')
conf$addSampler(target = 'sigma', type = sampler_RW)
conf$addSampler(target = 'sigma', type = 'RW')
conf$printSamplers()
conf$controlNamesLibrary



##debug(buildMCMC)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)



            

mc <- Cmcmc
mc <- Rmcmc

pb <- TRUE
pb <- FALSE

si <- TRUE
si <- FALSE

ni <- 10
ni <- 100

nb <- 5
nb <- 9



inits <- function() list(mu = rnorm(1,0,1000), sigma = runif(1,0,10))
##inits <- function() list(mu = 1:2, sigma = runif(1,0,10), x = 3)

initsList <- list(inits(), inits(), inits())
initsList <- list(inits(), inits())
initsList <- list(inits())

debug(runMCMC)
undebug(runMCMC)

debug(mcmc$run)

runMCMC(Rmcmc, niter = 3, nchains = 3, inits = inits())
runMCMC(Cmcmc, niter = 3, nchains = 3, inits = inits())

runMCMC(Rmcmc, niter = 3, nchains = 3, inits = initsList)
runMCMC(Cmcmc, niter = 3, nchains = 3, inits = initsList)

runMCMC(Rmcmc, niter = 300, nchains = 3, inits = inits())
runMCMC(Cmcmc, niter = 300, nchains = 3, inits = ii, nburnin=10)
a <- runMCMC(Rmcmc, niter = 300, nburnin=10, nchains = 4)

runMCMC(Rmcmc, niter = 300, nchains = 3, inits = inits(), nburnin=295)
runMCMC(Cmcmc, niter = 30000, nchains = 3, inits = inits, nburnin=29995)

runMCMC(Cmcmc, niter = 30000, nchains = 3, inits = inits, nburnin=29995, progressBar=TRUE, silent=TRUE, returnCodaMCMC = TRUE)

runMCMC(Cmcmc, niter = 30000, inits = inits, nburnin=29995, progressBar=TRUE, silent=TRUE, returnCodaMCMC = TRUE)

runMCMC(Cmcmc, niter = 30000, inits = inits, nburnin=29995, progressBar=TRUE, silent=TRUE)

runMCMC(Cmcmc, niter = 30000, nchains = 3, inits = inits, nburnin=29999, progressBar=TRUE, silent=TRUE, setSeed=TRUE)

runMCMC(Rmcmc, niter = 30, nchains = 3, inits = inits, nburnin=29, silent=TRUE, setSeed=TRUE)

runMCMC(Cmcmc, niter = 30000, nchains = 3, nburnin=29999, progressBar=TRUE, silent=TRUE, setSeed=TRUE)

runMCMC(Rmcmc, niter = 30, nchains = 3, nburnin=29, silent=TRUE, setSeed=TRUE)



## testing inconsistancy in dgamma() between R and C

library(nimble)

Rnf <- nimbleFunction(
    run = function(x = double(), a = double(), b = double()) {
        lp <- dgamma(x, a, b, log = 1)
        returnType(double())
        return(lp)
    }
)

Cnf <- compileNimble(Rnf)

x <- 6e-100
a <- 0.001
b <- 1.0
Rnf(x, a, b)
Cnf(x, a, b)




## making utility function for MCMC sample traceplots and density histograms

library(nimble)   ## get samples from birats2 model
Rmodel <- readBUGSmodel('birats2.bug', dir = getBUGSexampleDir('birats'), data = 'birats-data.R', inits = 'birats-inits.R')
Rmcmc <- buildMCMC(Rmodel)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
Cmcmc$run(20000)
samples <- as.matrix(Cmcmc$mvSamples)

library(nimble)   ## get samples from Dave Pleydel's multinomial test model
codeTest <- nimbleCode ({
    X[1:nGroups] ~ dmultinom(size=N, prob=pVecX[1:nGroups])
    Y[1:nGroups] ~ dmultinom(size=N, prob=pVecY[1:nGroups])
    for (ii in 1:nGroups)
        Z[ii] ~ dbeta(1 + X[ii], 1 + Y[ii]) })
nGroups   <- 5
N         <- 1E6
pVecX     <- rdirch(1, rep(1, nGroups))
pVecY     <- rdirch(1, rep(1, nGroups))
X         <- rmultinom(1, N, pVecX)[,1]
Y         <- rmultinom(1, N, pVecY)[,1]
Z         <- rbeta(nGroups, 1+X, 1+Y)
Xini      <- rmultinom(1, N, sample(pVecX))[,1]
Yini      <- rmultinom(1, N, sample(pVecY))[,1]
Constants <- list(nGroups=nGroups)
Inits     <- list(X=Xini, Y=Yini, pVecX=pVecX, pVecY=pVecY, N=N)
Data      <- list(Z=Z)
modelTest <- nimbleModel(codeTest, constants=Constants, inits=Inits, data=Data)
mcmcTest  <- buildMCMC(modelTest) 
cModelTest <- compileNimble(modelTest)
cMcmcTest <- compileNimble(mcmcTest, project=modelTest)
cModelTest$N     <- N <- 1E3
cModelTest$pVecX <- sort(rdirch(1, rep(1, nGroups)))
cModelTest$pVecY <- sort(rdirch(1, rep(1, nGroups)))
simulate(cModelTest, c('X','Y','Z'), includeData=TRUE)
niter  <- 1E4
cMcmcTest$run(niter)
samples <- as.matrix(cMcmcTest$mvSamples)

samplesPlot <- function(samples, ind=1:ncol(samples), burnin=NULL, width=7, height=4, legend=TRUE, legend.location='topright') {
    ## device window and plotting parameters
    dev.new(height=height, width=width)
    par(mfrow=c(1,2), cex=0.7, cex.main=1.5, lab=c(3,3,7), mgp=c(0,0.6,0), mar=c(2,1,2,1), oma=c(0,0,0,0), tcl=-0.3, yaxt='n', bty='l')
    ## process samples
    samples <- samples[, ind, drop=FALSE]
    if(!is.null(burnin))
        samples <- samples[(burnin+1):dim(samples)[1], , drop=FALSE]
    nparam <- ncol(samples)
    rng <- range(samples)
    ## traceplots
    plot(1:nrow(samples), ylim=rng, type='n', main='Traceplots', xlab='', ylab='')
    for(i in 1:nparam)
        lines(samples[,i], col=rainbow(nparam, alpha=0.75)[i])
    ## posterior densities
    xMin <- xMax <- yMax <- NULL
    for(i in 1:nparam) {
        d <- density(samples[,i])
        xMin <- min(xMin,d$x); xMax <- max(xMax,d$x); yMax <- max(yMax, d$y) }
    plot(1, xlim=c(xMin,xMax), ylim=c(0,yMax), type='n', main='Posterior Densities', xlab='', ylab='')
    alpha_density <- 0.2
    for(i in 1:nparam)
        polygon(density(samples[,i]), col=rainbow(nparam, alpha=alpha_density)[i], border=rainbow(nparam, alpha=alpha_density)[i])
    if(legend & !is.null(dimnames(samples)) & is.character(dimnames(samples)[[2]]))
        legend(legend=dimnames(samples)[[2]], fill=rainbow(nparam, alpha=0.5), bty='n', x=legend.location)
}


dim(samples)
dimnames(samples)
samplesPlot(samples)

apply(samples, 2, mean)

samplesPlot(samples, ind=c(1,2,3,7), burnin=1000, legend.location='topleft')
samplesPlot(samples, ind=c(4,6), burnin=1000)
samplesPlot(samples, ind=c(5), burnin=1000)

## better to just use the plotting functions in coda package!!!
library(coda)
mcmcSamples <- as.mcmc(samples)
acfplot(mcmcSamples)
plot(mcmcSamples)



## playing with plot(density(x))
x <- rnorm(10000)
plot(density(x))
d <- density(x)
class(d)
ls(d)
length(d$x)
length(d$y)
plot(d$x, d$y, type='l')  ## this creates the standard plot(density(x)) plot

      plot(prior ~ param.x, ylim = yLims, type = "l", xlim = range(param.x), 
            xlab = "", ylab = "", main = "", axes = FALSE, ...)
        polygon(param.x, prior, col = "red")
        box()
        r = legend("topleft", legend = "Prior", lty = 1, bty = "n", 
            plot = FALSE)$text
        text(r$x, r$y, "Prior", adj = 0)
        plot(likelihood ~ param.x, type = "l", xlab = "", ylab = "", 
            main = "", axes = FALSE, ...)
        polygon(param.x, likelihood, col = "green")
        box()
        r = legend("topleft", legend = "Prior", lty = 1, bty = "n", 
            plot = FALSE)$text
        text(r$x, r$y, "Likelihood", adj = 0)
        plot(posterior ~ param.x, ylim = yLims, type = "l", xlab = "", 
            ylab = "", main = "", axes = F, ...)
        polygon(param.x, posterior, col = "blue")




## testing funny behavior of DSL round() and nimRound()
library(nimble)

Rnf <- nimbleFunction(
    run = function() {
        for(i in 0:50) {
            x <- i/10
            xRound <- round(x)
            print('x: ', x, ',   round(x): ', xRound)
        }
    }
)

Cnf <- compileNimble(Rnf)

Rnf()
Cnf()



## testing making my own progress bar in NIMBLE DSL nimbleFunction

library(nimble)

rfun <- nimbleFunction(
    run = function(pb = logical(default=TRUE)) {
        ##print('|')
        ##for(i in 1:20)   { a <- i %% 7; print(a) }
        ##print('|')
        ##cat('|')
        ##for(i in 1:3)   cat('-', i)
        ##cat('|')
        a <- 1
        pb <- pb & 0
        print(pb)
    }
)

##rfun()
cfun <- compileNimble(rfun)
##cfun()
cfun()


library(nimble)
code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)


library(nimble)
Rmodel <- readBUGSmodel('birats2.bug', dir = getBUGSexampleDir('birats'), data = 'birats-data.R', inits = 'birats-inits.R')
Rmcmc <- buildMCMC(Rmodel)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Cmcmc$run(20000)
Cmcmc$run(30007, reset = FALSE)         ## continue previous run
Cmcmc$run(10000, progressBar = FALSE)   ## turn off progress bar
Cmcmc$run(100003, reset = FALSE)        ## sort of slow run
Cmcmc$run(5000)     ## faster
Cmcmc$run(2000)     ## faster still
Cmcmc$run(1000)     ## ...
Cmcmc$run(500)
Cmcmc$run(100)
Cmcmc$run(40)  ## no bar when too few iterations



## playing with R progress bars: txtProgressBar

f <- function(n = 1e6, frac = 0.01) {
    pb <- txtProgressBar(style = 3, char = '-')
    nupdate <- floor(frac * n) 
    for(i in 1:n) {
        a <- rnorm(1)
        if(i %% nupdate == 0) {
            setTxtProgressBar(pb, i/n)
        }
    }
    setTxtProgressBar(pb, 1)
    close(pb)
}

f(n=2e6, frac = .01)



library(Bolstad)

## datasets in library(Bolstad):
## bears (exercise in chapter 3)
## slug  (exercise in chapter 14)

## functions that I might possibly use:
## decomp: makes plots of prior, likelihood, posterior, but only for class=Bolstad.
##         maybe steal a bunch of the code, make one that works for general samples?
##         BUT WAIT -- this only works for sorted (x,y) pairs -- not for samples.


## getting MCMC samples for logProbs of variables

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)

conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$getMonitors()
conf$addMonitors('logProb_a')
conf$getMonitors()
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

##paste0('logProb_', letters)

set.seed(0)
Cmcmc$run(10)
samples <- as.matrix(Cmcmc$mvSamples)
samples
apply(samples, 2, mean)


## tracking down error in conjugacy checking system,
## this model was submitted by: Eduardo Martins <egmartins@gmail.com>
## during the ISEC 2016 NIMBLE workshop
library(nimble)
load('~/Downloads/data_inits.RData')
fec_code <- nimbleCode({
    ## Likelihood
    ## Fecundity
    for (i in 1:ny.fec) {
        y.fec[i] ~ dlnorm(log(z.fec[i]), tau.y.fec) # observation model
        z.fec[i] ~ dnegbin(p.fec[i], size.fec) # sampling model
        p.fec[i] <- size.fec / (size.fec + mu.fec[i])
        log(mu.fec[i]) <- a.fec.raw + b.fec * z.sl[i] + nu.fec.k.raw[idx.k[i]] + nu.fec.l.raw[idx.l[i]]
    }
    a.fec <- a.fec.raw + mean(nu.fec.k.raw[1:nk]) + mean(nu.fec.l.raw[1:nl])
    for (k in 1:nk){
        nu.fec.k.raw[k] ~ dnorm(mu.nu.fec.k, tau.nu.fec.k)
        nu.fec.k[k] <- nu.fec.k.raw[k] - mean(nu.fec.k.raw[1:nk])
    }
    for (l in 1:nl){
        nu.fec.l.raw[l] ~ dnorm(mu.nu.fec.l, tau.nu.fec.l)
        nu.fec.l[l] <- nu.fec.l.raw[l] - mean(nu.fec.l.raw[1:nl])
    }
    ## Length
    for(i in 1:ny.fec) {
        z.sl[i] ~ dnorm(mu.sl[i], tau.sl.i)
        mu.sl[i] <- a.sl.raw + b.sl * z.ocr[i] + nu.sl.k.raw[idx.k[i]] + nu.sl.l.raw[idx.l[i]]
    }
    a.sl <- a.sl.raw + mean(nu.sl.k.raw[1:nk]) + mean(nu.sl.l.raw[1:nl])
    for (k in 1:nk){
        nu.sl.k.raw[k] ~ dnorm(mu.nu.sl.k, tau.nu.sl.k)
        nu.sl.k[k] <- nu.sl.k.raw[k] - mean(nu.sl.k.raw[1:nk])
    }
    for (l in 1:nl){
        nu.sl.l.raw[l] ~ dnorm(mu.nu.sl.l[l], tau.nu.sl.l)
        mu.nu.sl.l[l] <- b.nu.sl.l[1] * sst6.std[l] + b.nu.sl.l[2] * pdo6.std[l] + b.nu.sl.l[3] * nsal.std[l]
        nu.sl.l[l] <- nu.sl.l.raw[l] - mean(nu.sl.l.raw[1:nl])
    }
    ## Years of ocean residence
    for(i in 1:ny.fec){
        z.ocr[i] ~ dbern(p.z.ocr[i])
        logit(p.z.ocr[i]) <- a.ocr.raw + nu.ocr.k.raw[idx.k[i]] + nu.ocr.l.raw[idx.l[i]]
    }
    a.ocr <- a.ocr.raw + mean(nu.ocr.k.raw[1:nk]) + mean(nu.ocr.l.raw[1:nl])
    for (k in 1:nk){
        nu.ocr.k.raw[k] ~ dnorm(mu.nu.ocr.k, tau.nu.ocr.k)
        nu.ocr.k[k] <- nu.ocr.k.raw[k] - mean(nu.ocr.k.raw[1:nk])
    }
    for (l in 1:nl){
        nu.ocr.l.raw[l] ~ dnorm(mu.nu.ocr.l, tau.nu.ocr.l)
        nu.ocr.l[l] <- nu.ocr.l.raw[l] - mean(nu.ocr.l.raw[1:nl])
    }
    ## Priors
    ## Fecundity
    tau.y.fec <- pow(sig.y.fec, -2)
    sig.y.fec ~ dunif(0, 100)
    num.size.fec ~ dnorm(0, 0.0016)
    den.size.fec ~ dnorm(0, 1)
    size.fec <- abs(num.size.fec / den.size.fec)
    mu.nu.fec.k ~ dnorm(0, 0.0001)
    tau.nu.fec.k <- pow(sig.nu.fec.k, -2)
    sig.nu.fec.k ~ dunif(0, 100)
    mu.nu.fec.l ~ dnorm(0, 0.0001)
    tau.nu.fec.l <- pow(sig.nu.fec.l, -2)
    sig.nu.fec.l ~ dunif(0, 100)
    a.fec.raw ~ dnorm(mean.z.fec, tau.z.fec)
    b.fec ~ dnorm(0, 0.0001)
    ## Length
    tau.sl.i <- pow(sig.sl.i, -2)
    sig.sl.i ~ dunif(0, 100)
    mu.nu.sl.k ~ dnorm(0, 0.0001)
    tau.nu.sl.k <- pow(sig.nu.sl.k, -2)
    sig.nu.sl.k ~ dunif(0, 100)
    tau.nu.sl.l <- pow(sig.nu.sl.l, -2)
    sig.nu.sl.l ~ dunif(0, 100)
    a.sl.raw ~ dnorm(0, 0.0001)
    b.sl ~ dnorm(0, 0.0001)
    for (b in 1:nb.nu.sl.l) {
        b.nu.sl.l[b] ~ dnorm(0, 0.0001)
    }
    ## Years of ocean residence
    mu.nu.ocr.k ~ dnorm(0, 0.0001)     
    tau.nu.ocr.k <- pow(sig.nu.ocr.k, -2)
    sig.nu.ocr.k ~ dunif(0, 100)
    mu.nu.ocr.l ~ dnorm(0, 0.0001)
    tau.nu.ocr.l <- pow(sig.nu.ocr.l, -2)
    sig.nu.ocr.l ~ dunif(0, 100)
    a.ocr.raw ~ dnorm(0, 0.0001)
    ## Residuals and metrics for model assessment
    ## Fecundity
    for(i in 1:ny.fec){
        res.y.fec[i] <- y.fec[i] - z.fec[i]
        new.y.fec[i] ~ dlnorm(log(z.fec[i]), tau.y.fec)
    }
    for(i in 1:ny.fec){
        pres.z.fec[i] <- (z.fec[i] - mu.fec[i])/sqrt(mu.fec[i] + pow(mu.fec[i], 2) / size.fec)
        new.z.fec[i] ~ dnegbin(p.fec[i], size.fec)
        pres.new.z.fec[i] <- (new.z.fec[i] - mu.fec[i])/sqrt(mu.fec[i] + pow(mu.fec[i], 2) / size.fec)
        d.z.fec[i] <- pow(pres.z.fec[i], 2)
        d.new.z.fec[i] <- pow(pres.new.z.fec[i], 2)
    }
    fit.z.fec <- sum(d.z.fec[1:ny.fec])
    fit.new.z.fec <- sum(d.new.z.fec[1:ny.fec])
    ## Length
    for (i in 1:ny.fec) {
        res.z.sl[i] <- z.sl[i] - mu.sl[i]
        new.z.sl[i] ~ dnorm(mu.sl[i], tau.sl.i)
    }
    for (l in 1:nl){
        res.nu.sl.l[l] <- nu.sl.l[l] - mu.nu.sl.l[l]
        new.nu.sl.l[l] ~ dnorm(mu.nu.sl.l[l], tau.nu.sl.l)
    }
})
fec_mod <- nimbleModel(fec_code, constants = dat, inits = inits())
fec_cmod <- compileNimble(fec_mod)

##options(error = recover)

## next line errors out in conjugacy check:
## Error in x[[1]] : subscript out of bounds
mcmcConf <- configureMCMC(fec_mod)

mcmcConf$printSamplers()
mcmcConf$getMonitors()
fecMCMC <- buildMCMC(mcmcConf)
cfecMCMC <- compileNimble(fecMCMC, project = fec_mod)

cfecMCMC$run(10)

## testing of new setData() functionality:
## taking a character vector of variable names

library(nimble)
Rmodel <- readBUGSmodel('birats2.bug', dir = getBUGSexampleDir('birats'), data = 'birats-data.R', inits = 'birats-inits.R')

Rmodel$getNodeNames(includeData = FALSE)

modelLs <- readBUGSmodel('birats2.bug', dir = getBUGSexampleDir('birats'), data = 'birats-data.R', inits = 'birats-inits.R', returnModelComponents = TRUE)
modelLs$model

Rmodel$beta
Rmodel$tau.c
Rmodel$r

Rmodel$getNodeNames(dataOnly = TRUE)
Rmodel$setData(c('beta', 'tau.c', 'r'))
Rmodel$getNodeNames(dataOnly = TRUE)
Rmodel$resetData()
Rmodel$getNodeNames(dataOnly = TRUE)
Rmodel$setData(c('beta', 'tau.c', 'r'))
Rmodel$getNodeNames(dataOnly = TRUE)
Rmodel$setData(c('Y'))





## testing new MCMC runtime argument: time
library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a, 1)
    c ~ dnorm(b + b^2, 1)
})
constants <- list()
data <- list(b=0, c=0)
inits <- list(a=0)

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$printSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(100000, time = TRUE)
Cmcmc$getTimes()
Cmcmc$run(100000, time = TRUE, reset = FALSE)
Cmcmc$getTimes()



## error from Perry about not finding nimArray
## but I can't reproduce this error

library(nimble)

Rmodel <- readBUGSmodel('birats2.bug', dir = getBUGSexampleDir('birats'), data = 'birats-data.R', inits = 'birats-inits.R')
##spec <- configureMCMC(Rmodel)
##spec$printSamplers()
##spec$getSamplerDefinition(1)
##spec$getSamplerDefinition(4)
##mcmc <- buildMCMC(spec)
mcmc <- buildMCMC(Rmodel)
mcmc$run(1)
##Error in samplerFunctions[[i]]$run() : could not find function "nimArray"

source(system.file(file.path('tests', 'test_utils.R'), package = 'nimble'))
test_mcmc('birats', model = 'birats2.bug', inits = 'birats-inits.R', data = 'birats-data.R', numItsC = 1000, resampleData = TRUE)




## Soledad's compilation error

library(nimble)
x <- c(2, 2, 3)

## works
test1 <- nimbleFunction ( run = function (indx = double(1) ) {
    if(  length(indx)==2 & indx[1]==indx[2] ) {
        aa <- indx[1] 
    } else {
        newindx <- indx
        aa         <- length(newindx)
    }
    output <- 1 + aa
    returnType(double(0))
    return(output)
})

## doesn't work
test1 <- nimbleFunction(run = function(indx = double(1)) {
    if(  length(indx)==2 & indx[1]==indx[2] ) {
        newindx <- indx[1]
        aa          <- newindx     ## THIS IS THE MAIN CHANGE IN THIS FUNCTION
    } else {
        newindx <- indx
        aa          <- length(newindx)
    }
    output <- 1 + aa
    returnType(double(0))
    return(output)
})

Ctest1 <- compileNimble(test1)


test1(x)
Ctest1(x)





## using a custom distribution to have flexible length data
## for floriane plard population models

library(nimble)


dxxx <- nimbleFunction(
    run = function(x = double(1), mu = double(1), sigma = double(1), length = integer(), log.p = double()) {
        ll <- 0
        for(i in 1:length) {
            ll <- ll + dnorm(x[i], mu[i], sd=sigma[i], log=TRUE)
        }
        returnType(double())
        if(log.p) return(ll) else return(exp(ll))
    }
)

rxxx <- nimbleFunction(
    run = function(n = integer(), mu = double(1), sigma = double(1), length = integer()) {
        print('this should never run')
        x <- numeric(length)
        return(x)
    }
)

registerDistributions(list(
    dxxx = list(
        BUGSdist = 'dxxx(mu, sigma, length)',
        types    = c('value = double(1)', 'mu = double(1)', 'sigma = double(1)', 'length = integer()')
    )
))

code <- nimbleCode({
    y[1:N] ~ dxxx(mu[1:N], sigma[1:N], length)
})
constants <- list(N = 10)
data <- list()
inits <- list(length = 10)

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$length
Rmodel$mu
Rmodel$sigma
Rmodel$y
Rmodel$getNodeNames(dataOnly = TRUE)

length <- 10
length
mu <- ((1:10)/2)[1:length]
mu
sigma <- (1+(1:10)/10)[1:length]
sigma
y <- c(1,0,1,3,2,5,4,6,7,5)[1:length]
y
yaug <- c(y, rep(as.numeric(NA), 10-length))
yaug

Rmodel$length <- length
Rmodel$length
Rmodel$mu[1:length] <- mu
Rmodel$mu
Rmodel$sigma[1:length] <- sigma
Rmodel$sigma
Rmodel$resetData()
Rmodel$getNodeNames(dataOnly = TRUE)
Rmodel$setData(list(y=yaug))
Rmodel$getNodeNames(dataOnly = TRUE)
Rmodel$y

sum(dnorm(y, mu, sd=sigma, log=TRUE))
calculate(Rmodel)


debug(exprClasses_setSizes)



## testing MCEM for pump model
library(nimble)

pumpCode <- nimbleCode({ 
  for (i in 1:N){
      theta[i] ~ dgamma(alpha,beta) 
      lambda[i] <- theta[i]*t[i]
      x[i] ~ dpois(lambda[i]) 
  }
  alpha ~ dexp(1.0) 
  beta ~ dgamma(0.1,1.0) 
}) 
pumpConsts <- list(N = 10,
                   t = c(94.3, 15.7, 62.9, 126, 5.24,
                       31.4, 1.05, 1.05, 2.1, 10.5)) 
pumpData <- list(x = c(5, 1, 5, 14, 3, 19, 1, 1, 4, 22)) 
pumpInits <- list(alpha = 1, beta = 1,
                  theta = rep(0.1, pumpConsts$N)) 
newPump <- nimbleModel(code = pumpCode, name = 'pump', constants = pumpConsts,
                    data = pumpData, inits = pumpInits) 

pumpMCEM <- buildMCEM(model = newPump,
                      latentNodes = 'theta',
                      burnIn = 100,
                      mcmcControl = list(adaptInterval = 20),
                      boxConstraints = list( list( c('alpha', 'beta'), 
                          limits = c(0, Inf) ) ), 
                      buffer = 1e-6)

pumpMCEM(maxit = 20, m1 = 250, m2 = 500)
pumpMCEM(maxit = 50, m1 = 1000, m2 = 5000)


## testing adding numeric(), integer(), array(), matrix()
library(nimble)
Rfun <- nimbleFunction(run = function() {
    ##ans <- numeric(10, value = 2)
    ##
    #x <- 100 
    #ans <- integer(x) 
    #for(i in 1:x) {
    #    ans[i] <- i 
    #}
    ##
    ##ans <- matrix(1, nrow = 10, ncol = 1)
    ##y <- 3
    ##ans <- numeric(10, value = y) 
    ##ans <- array(y, dim = 10) 
    ##ans <- array(y, dim = c(10))
    ##z <- numeric(10)
    ##z[5] <- 3
    ##x <- 20
    ##y <- 30
    ##ans <- integer(z[5], value = x + y) 
    ##ans <- array(x+y, dim = z[5], type = 'integer')
    ##ans <- array(x+y, dim = c(z[5]), type = 'integer')
    ##
    ##x <- array(0, c(4,5))
    ##ans <- matrix(0, nrow = dim(x)[1], ncol = dim(x)[2]) 
    ##ans <- array(0, dim = c(dim(x)[1], dim(x)[2]))
    ##
    x <- 1
    y <- 2
    z <- 3
    ans <- array(0, dim = c(x, y, z))
    ##returnType(double(1))
    ##returnType(integer(1))
    ##returnType(double(2))
    returnType(double(3))
    return(ans)
})
Cfun <- compileNimble(Rfun)

Rfun()
Cfun()
class(Rfun()[1])
class(Cfun()[1])
class(Rfun()[1,1])
class(Cfun()[1,1])
class(Rfun()[1,1,1])
class(Cfun()[1,1,1])

## creates a length-100 integer vector, containing 1, 2, ..., 100 

## creates a 10x1 ones-matrix 

## the following three lines are equivalent 
## each creates a length-10 vector, with all elements equal to y 

## the following two lines are equivalent 
## each creates an integer vector of length z[5], with all elements equal to x+y 

## the following two lines are equivalent 
## each one creates a matrix of 0's of the same size as matrix x 

## the following creates a 3-dimensional array of 0's 



## testing adding numeric(), integer(), array(), matrix()
library(nimble)
library(testthat)
##source(system.file(file.path('tests', 'test_utils.R'), package = 'nimble'))

expected <- numeric(10)
Rfun <- nimbleFunction(run = function() {
    ans <- numeric(10)
    returnType(double(1))
    return(ans)
})
Cfun <- compileNimble(Rfun)
test_that('numeric', expect_equal(Rfun(), expected))
test_that('numeric', expect_equal(Cfun(), expected))
test_that('numeric', expect_identical(class(Rfun()[1]), 'numeric'))
test_that('numeric', expect_identical(class(Cfun()[1]), 'numeric'))


expected <- rep(3, length = 2)
Rfun <- nimbleFunction(run = function() {
    ans <- numeric(value = 3, length = 2)
    returnType(double(1))
    return(ans)
})
Cfun <- compileNimble(Rfun)
test_that('numeric', expect_equal(Rfun(), expected))
test_that('numeric', expect_equal(Cfun(), expected))
test_that('numeric', expect_identical(class(Rfun()[1]), 'numeric'))
test_that('numeric', expect_identical(class(Cfun()[1]), 'numeric'))

expected <- rep(9, 3)
Rfun <- nimbleFunction(run = function() {
    x <- numeric(10, value = 3)
    ans <- integer(x[2], x[2]+x[3]*2)
    returnType(integer(1))
    return(ans)
})
Cfun <- compileNimble(Rfun)
test_that('integer', expect_equal(Rfun(), expected))
test_that('integer', expect_equal(Cfun(), expected))
test_that('integer', expect_identical(class(Rfun()[1]), 'integer'))
test_that('integer', expect_identical(class(Cfun()[1]), 'integer'))

expected <- array(4, c(10,11))
Rfun <- nimbleFunction(run = function() {
    ans <- array(4, c(10,11))
    returnType(double(2))
    return(ans)
})
Cfun <- compileNimble(Rfun)
test_that('integer', expect_equal(Rfun(), expected))
test_that('integer', expect_equal(Cfun(), expected))
test_that('integer', expect_identical(class(Rfun()[1]), 'numeric'))
test_that('integer', expect_identical(class(Cfun()[1]), 'numeric'))

expected <- matrix(as.integer(0), nrow=4, ncol=5)
Rfun <- nimbleFunction(run = function() {
    x <- 4
    y <- 5
    ans <- matrix(init=FALSE, nrow=x, ncol=y, type='integer')
    returnType(integer(2))
    return(ans)
})
Cfun <- compileNimble(Rfun)
test_that('integer', expect_equal(Rfun(), expected))
test_that('integer', expect_equal(Cfun(), expected))
test_that('integer', expect_identical(class(Rfun()[1,1]), 'integer'))
test_that('integer', expect_identical(class(Cfun()[1,1]), 'integer'))




expected
Rfun()
Cfun()


## testing adding numeric(), integer(), array(), matrix()
library(nimble)
nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        x <- 2
        bbb <- array(3, x+10, type='integer')
        ##print(bbb)
        bbb2 <- array(value=3, dim=c(3,4), type='double')
        ##print(bbb2)
        ccc <- array(value=3, dim=c(3,4), type='double', init=FALSE)
        ##print(ccc)
        x <- 5
        y <- matrix(2, ncol = 3, nrow = x)
        ##print(y)
        zxc <- array(2, c(x, dim(y)[2], y[2,2]))
        zxc[1,1,1] <- 98
        ##returnType(double(3));  return(zxc)
        zz <- numeric(4)
        zz[1] <- 99
        ##print(zz)
        z <- numeric(value=.5, 4)
        ##print(z)
        zz[2:4] <- z[1:3] + numeric(3, 10)
        ##print(zz)
        mat <- matrix(0, 5, 5)
        mat[1, 1] <- 97
        ##print(mat)
        arr <- array(4, type = 'integer', init = FALSE, dim=c(3,4))
        ##print(arr)
        ar2 <- array(dim = dim(z)[1])
        ##print(ar2)
        ar3 <- array(dim = c(dim(z)[1]))
        ##print(ar3)
        ar4 <- array(dim = c(dim(arr)[1], dim(arr)[2]), type = 'integer')
        ##print(ar4)
    }
)
Rnf <- nfDef()
##Rnf$run
Cnf <- compileNimble(Rnf, dirName)

Rnf$run()

Cnf$run()



tempdir()

##Cnf$run()



## ???

library(nimble)
sampler_dt <- nimbleFunction(
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        browser()
        1
        2
        3
        ###  node list generation  ###
        calcNodes  <- model$getDependencies(target)
    },
    run = function() {
        simulate(model, target)
        calculate(model, calcNodes)
        nimCopy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)
    },
    methods = list(
        reset = function() { }
    ), where = getLoadingNamespace()
)
pumpCode <- nimbleCode({
	for(i in 1:N){
		theta[i] ~ dgamma(alpha,beta)
		lambda[i] <- theta[i]*t[i]
		x[i] ~ dpois(lambda[i])
	}
	alpha ~ dexp(1.0)
        alpha2 ~ dnorm(alpha, 1)
	beta ~ dpois(lambda=3)  # Make beta discrete valued
})
pumpConsts <- list(N=10,t=c(94.3,15.7,62.9,126,5.24,31.4,1.05,1.05,2.1,10.5))
pumpData <- list(x=c(5,1,5,14,3,19,1,1,4,22))
pumpInits <- list(alpha=1,beta=1,theta=rep(0.1,pumpConsts$N))
Rmodel <- nimbleModel(code=pumpCode,name='pump',constants=pumpConsts,data=pumpData,inits=pumpInits)

spec <- configureMCMC(Rmodel)
spec$getMonitors()
spec$addMonitors(c('theta', 'alpha2'))
spec$getMonitors()
spec$printSamplers()
spec$addSampler(type = 'dt', target = 'alpha')
spec$printSamplers()

Rmcmc <- buildMCMC(spec)

## testing new function in modelBaseClass: model$getNodeFunctions()
library(nimble)
pumpCode <- nimbleCode({
	for(i in 1:N){
		theta[i] ~ dgamma(alpha,beta)
		lambda[i] <- theta[i]*t[i]
		x[i] ~ dpois(lambda[i])
	}
	alpha ~ dexp(1.0)
        alpha2 ~ dnorm(alpha, 1)
	beta ~ dpois(lambda=3)  # Make beta discrete valued
})
pumpConsts <- list(N=10,t=c(94.3,15.7,62.9,126,5.24,31.4,1.05,1.05,2.1,10.5))
pumpData <- list(x=c(5,1,5,14,3,19,1,1,4,22))
pumpInits <- list(alpha=1,beta=1,theta=rep(0.1,pumpConsts$N))
Rmodel <- nimbleModel(code=pumpCode,name='pump',constants=pumpConsts,data=pumpData,inits=pumpInits)
Cmodel <- compileNimble(Rmodel)

nodes <- c('theta[3]')
nodes <- c('theta[3]', 'x[10]')
nodes <- c('theta[3]', 'x[9:10]')
Rmodel$getNodeFunctions(nodes)$calculate
Cmodel$getNodeFunctions(nodes)[[1]]$calculate

Rmodel$getNodeNames()
modelDef <- Rmodel$getModelDef()
gids <- mdef$nodeName2GraphIDs(nodes)
gids
mdef$graphIDs2indexedNodeInfo(gids)
Rmodel$nodeFunctions[[2]]$calculate


## making dmnorm conjugate sampler work with dependent
## nodes of different size from target node
library(nimble)
source(system.file(file.path('tests', 'test_utils.R'), package = 'nimble'))

code <- nimbleCode({
    x[1:3] ~ dmnorm(mu0[1:3], prec = ident[1:3,1:3])
    mu_y2[1:2] <- asCol(a[1:2]) + B[1:2,1:3] %*% asCol(x[1:3])
    mu_y3[1:3] <- asCol(a[1:3]) + B[1:3,1:3] %*% asCol(x[1:3])
    mu_y5[1:5] <- asCol(a[1:5]) + B[1:5,1:3] %*% asCol(x[1:3])
    y2[1:2] ~ dmnorm(mu_y2[1:2], prec = prec_y[1:2,1:2])
    y3[1:3] ~ dmnorm(mu_y3[1:3], prec = prec_y[1:3,1:3])
    y5[1:5] ~ dmnorm(mu_y5[1:5], prec = prec_y[1:5,1:5])
})

mu0 <- rep(0,3)
ident <- diag(3)
a <- 11:15
B <- matrix(1:15, nrow=5, ncol=3, byrow=TRUE)
prec_y <- diag(1:5)

constants <- list(mu0=mu0, ident=ident, a=a, B=B, prec_y=prec_y)
data <- list(y2=1:2, y3=1:3, y5=1:5)
inits <- list(x=rep(0,3))

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
##spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)

set.seed(0)
Cmcmc$run(10)

Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rsamples - Csamples

Rsamples[c(1,10),]
Csamples[c(1,10),]
##         x[1]       x[2]      x[3]
##[1,] 4.966869 -1.1252473 -3.922769
##[2,] 4.972376 -0.5288582 -4.511282


##node <- quote(mu0[1:3])
##node <- quote(prec_y[1:2, 1:2])
##node <- quote(prec_y[1:3, 1:3])
##node <- quote(prec_y[1:5, 1:5])
##nimble:::cc_expandDetermNodesInExpr(Rmodel, node)
##debug(nimble:::cc_expandDetermNodesInExpr)
##undebug(nimble:::cc_expandDetermNodesInExpr)
##debug(Rmcmc$samplerFunctions$contentsList[[1]]$run)
##a <- spec$getSamplerDefinition(1)
##createNamedObjectsFromList(a, writeToFile = '~/temp/del.R')







## ???
library(nimble)

d <- read.csv("http://personal.bgsu.edu/~albert/data/gateway.csv")
library(dplyr)
d$month <- as.numeric(d$Month)
d$year <- d$Year - 2002

worshipCode <- nimbleCode({ 
  for (i in 1:N){
  y[i] ~ dpois(lambda[i])
  log(lambda[i]) <- mu + epsilon[i] + alpha[month[i]]
  epsilon[i] ~ dnorm(0, sd=sigma.y)
  }
  for (j in 1:J){
  alpha[j] ~ dnorm(0, sd=sigma.month)
}
mu ~ dnorm(0, sd=1000)
sigma.y ~ dunif(0, 100)
sigma.month ~ dunif(0, 100)
})

worshipConsts <- list(N=484,
                      J=12,
                      K=10,
                      month=d$month)
worshipData <- list(y=d$Count)
worshipInits <- list(mu=5, sigma.y=.4, sigma.month=.4,
                     alpha=rep(0, 12))

worship <- nimbleModel(code = worshipCode, name = 'worship', 
                    constants = worshipConsts,
                    data = worshipData,
                    inits=worshipInits)

Rmodel <- worship

spec <- configureMCMC(Rmodel)
spec$printSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)


## error produced in model$check() when using equals()
library(nimble)
code <- nimbleCode({
    for (i in 1:5) {
        x[i] ~ dbern(0.5)
        ##y[i] <- equals(x[i], 0)
    }
    sss <- sum(x[1:5])
    none <- equals(sss, 0)
})
constants <- list()
data <- list()
inits <- list(x = c(0,0,1,1,1))

Rmodel <- nimbleModel(code, constants, data, inits)

Rmodel$x
Rmodel$sss
Rmodel$none

## testing new log=TRUE option of RW sampler

library(nimble)
code <- nimbleCode({
    sigma ~ dunif(0,10)
    a ~ dnorm(0, sd=sigma)
    b ~ dnorm(sigma, tau=3)
})
constants <- list()
data <- list(a=1, b=4)
inits <- list(sigma=1)

Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)
spec <- configureMCMC(Rmodel, nodes=NULL)

spec$addSampler('sigma', 'RW_log')
### --- or ---
spec$addSampler('sigma', 'RW', control=list(log=TRUE))

Rmcmc <- buildMCMC(spec)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
as.matrix(Rmcmc$mvSamples)

set.seed(0)
Cmcmc$run(10)
as.matrix(Cmcmc$mvSamples)

##         sigma
## [1,] 3.535852
## [2,] 3.535852
## [3,] 3.535852
## [4,] 5.352671
## [5,] 5.352671
## [6,] 3.986347
## [7,] 3.963423
## [8,] 3.963423
## [9,] 3.963423
##[10,] 3.963423




## Github issue #107
## initializeModel issues false warning about RHSonly variable not initialized
## reduced case
library(nimble)

code <- nimbleCode({
    for (n in 1:N){
        a0[n,1:3] ~ dmnorm(mu[1:3], Sigma[1:3,1:3])
        ##lambda[n,1:3] <- exp(a0[n,1:3]) ## alternative, which shifts the problem to lambda
        for (k in 1:K){
            lambda[n,k] <- exp(a0[n,k])
            y[n,k]~dpois(lambda[n,k])
        }
    }
})

constants <- list(N = 3, K = 3)
data <- list(y = matrix(rpois(9, 3), nrow=3))
inits <- list(mu = rep(0,3), Sigma = diag(3))

Rmodel <- nimbleModel(code, constants, data, inits)

Cmodel <- compileNimble(Rmodel)

Rmcmc <- buildMCMC(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Rmcmc$run(1)
Cmcmc$run(1)



## testing new binary sampler
library(nimble)

code <- nimbleCode({
    a ~ dbern(0.5)
    b ~ dbern(0.6)
    c ~ dbern(0.05)
    d ~ dbin(prob=0.2, size=1)
    e ~ dbinom(prob=0.9, size=1)
    f ~ dbern(0.5)
    g ~ dbern(0.5)
    h ~ dbern(0.5)
    for(i in 1:10)
        yf[i] ~ dnorm(f, sd = 1)
    for(i in 1:10)
        yg[i] ~ dnorm(g, sd = 1)
    for(i in 1:10)
        yh[i] ~ dnorm(h, sd = 1)
    ##x ~ dnorm(0,1)
    ##y ~ dnorm(x*x, 1)
    ##z ~ dnorm(x*y, 1)
    ##zz ~ dnorm(z + z, 1)
})
constants <- list()
data <- list(yf = c(rep(0,2), rep(1,8)), yg = c(rep(0,8), rep(1,2)), yh = c(rep(0,5), rep(1,5)))
inits <- list(a=0, b=0, c=0, d=0, e=0, f=0, g=0, h=0)
Rmodel <- nimbleModel(code, constants, data, inits)

##spec <- configureMCMC(Rmodel, autoBlock = TRUE)
##spec$printSamplers()

Rmodel$isBinary('a')
Rmodel$isBinary('b')
Rmodel$isBinary('c')
Rmodel$isBinary('d')
Rmodel$isBinary('e')
Rmodel$isBinary('f')
Rmodel$isBinary('g')
Rmodel$isBinary('h')

spec <- configureMCMC(Rmodel, nodes = NULL)
spec$addSampler('a', 'binary', print=FALSE)
spec$addSampler('b', 'binary', print=FALSE)
spec$addSampler('c', 'binary', print=FALSE)
spec$addSampler('d', 'binary', print=FALSE)
spec$addSampler('e', 'binary', print=FALSE)
spec$addSampler('f', 'binary', print=FALSE)
spec$addSampler('g', 'binary', print=FALSE)
spec$addSampler('h', 'binary', print=FALSE)
spec$printSamplers()
Rmcmc <- buildMCMC(spec)
##Rmcmc$run(3)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(100000)
samples <- as.matrix(Cmcmc$mvSamples)
means <- apply(samples, 2, mean)
means


# Slice sampler name conflict with library(bbmle) 'slice' function
library(bbmle)
library(nimble)  # No messages about slice are generated

pumpCode <- nimbleCode({
	for(i in 1:N){
		theta[i] ~ dgamma(alpha,beta)
		lambda[i] <- theta[i]*t[i]
		x[i] ~ dpois(lambda[i])
	}
	alpha ~ dexp(1.0)
        alpha2 ~ dnorm(alpha, 1)
	beta ~ dpois(lambda=3)  # Make beta discrete valued
})

pumpConsts <- list(N=10,t=c(94.3,15.7,62.9,126,5.24,31.4,1.05,1.05,2.1,10.5))
pumpData <- list(x=c(5,1,5,14,3,19,1,1,4,22))
pumpInits <- list(alpha=1,beta=1,theta=rep(0.1,pumpConsts$N))
Rmodel <- nimbleModel(code=pumpCode,name='pump',constants=pumpConsts,data=pumpData,inits=pumpInits)

spec <- configureMCMC(Rmodel)

spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(2000000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)


## testing passing a compiled model as a setup argument to a NF

library(nimble)

code <- nimbleCode({
     a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)

nfDef <- nimbleFunction(
    setup = function(model, node) {},
    run = function() {
        simulate(model, node)
    }
)

Rnf <- nfDef(Rmodel, 'a')
Rnf <- nfDef(Cmodel, 'a')

Cnf <- compileNimble(Rnf, project = Rmodel)

set.seed(0)
Rmodel$a
Rnf$run()
Rmodel$a

set.seed(0)
Cmodel$a
Cnf$run()
Cmodel$a



## testing passing a compiled model object to configureMCMC(), or buildMCMC()

library(nimble)

code <- nimbleCode({
     a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)

###
Rmcmc <- buildMCMC(Cmodel)

###
spec <- configureMCMC(Cmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
as.matrix(Rmcmc$mvSamples)

set.seed(0)
Cmcmc$run(10)
as.matrix(Cmcmc$mvSamples)


## testing problem with covariance matrix in BUGS model causing a chol() error at time of model checking

library(nimble)

code <- nimbleCode({
    C[1,1] <- 1
    C[1,2] <- 0
    C[2,1] <- 0
    C[2,2] <- 1
    mu[1] <- 0
    mu[2] <- 0
    y[1:2] ~ dmnorm(mu[1:2], prec = C[1:2, 1:2])
})
constants <- list()
data <- list(y = c(0,0))
inits <- list()

Rmodel <- nimbleModel(code, constants, data, inits)





##If you're interested, here's the ecological/scientific background to the project: the potato psyllid spreads a disease called Zebra Chip Disease that is damaging potato and tomato crops in California and western US and Mexico. The disease is new, having only been described about 20 years ago, but the potato psyllid is native to the area. So what's changed to cause the disease outbreaks? I'm using data from where and when potato psyllid museum specimens were collected in California to test if psyllid populations have increased over the last century and if climate change may play a role. If so, this could explain the disease outbreaks. It's complicated though because museum data is messy, specifically, the museum specimens give me presence-only information. So I'm using lists of related species collected at the same time as the psyllids to infer psyllid absences, based on an assumed correlation between number of species collected and collecting effort. This is the "list_length" covariate that's in the model.

##BLOCK SAMPLE PARAMS THAT GO IN THE SAME LINEAR PREDICTOR,
##i.e. all the betapN terms in this one

{
    ## Priors
    ## For the site random effect
    for(j in 1:nsite) { 
        alpha[j] ~ dnorm(mu.alpha, tau.alpha) 
    }
    mu.alpha ~ dnorm(0, 0.001)
    tau.alpha <- 1 / (sigma.alpha * sigma.alpha)
    sigma.alpha ~ dunif(0, 5)
    
    ## Grand mean
    muq ~ dnorm(0, 0.001)
    
    ## For the fixed effect coefficients
    betaq ~ dnorm(0, 0.001)
    betap1 ~ dnorm(0, 0.001)
    betap2 ~ dnorm(0, 0.001)
    betap3 ~ dnorm(0, 0.001)
    betap4 ~ dnorm(0, 0.001)
    betap5 ~ dnorm(0, 0.001)
    betap6 ~ dnorm(0, 0.001)
    betap7 ~ dnorm(0, 0.001)
    betap8 ~ dnorm(0, 0.001)

    ## Likelihood
    for (i in 1:nlist){ # i = events (year-months)
        for(j in 1:nsite) { # j = sites

            logit(p[i,j]) <- betap1*year[i,j] + betap2*pow(year[i,j],2) + # Year quadratic effects
                betap2*month[i,j] + betap3*pow(month[i,j],2) + # month quadratic effects
                    betap4*aet[i,j] + betap5*cwd[i,j] + betap6*tmn[i,j] + betap7*tmx[i,j] + # Climate effects
                        alpha[j] # Random effects

            Y[i,j] ~ dbern(p[i,j]) # Occupancy probability

            logit(q[i,j]) <- Y[i,j] + muq + betaq*list_length[i,j] # Logistic regression for detection

            detectionMatrix[i,j] ~ dbern(q[i,j])  # Distribution for random part; observed presences relating to detection probability

        } #j
    } #i
}



## bug with forwardsolve in nimbleFunctions

library(nimble)
A <- matrix(c(2,1,0,3), nrow = 2)
b <- c(1,5)

nf <- nimbleFunction(
    run = function(A = double(2), b = double(1)) {
        declare(x, double(1))
        setSize(x, 2)
        x[1:2] <- forwardsolve(A[1:2, 1:2], b[1:2])
        ##x <- forwardsolve(A[1:2,1:2], b[1:2])
        x[1:2,1:2] <- chol(A[1:2,1:2])
        returnType(double(1))
        ##declare(x, double(2))
        ##setSize(x, 2, 2)
        ##returnType(double(2))
        return(x)
    })

cnf <- compileNimble(nf, dirName = '.')

nf(A,b)
cnf(A,b)



## bug with forwardsolve in BUGS models

library(nimble)

code <- nimbleCode({
    A[1,1] <- 2
    A[2,1] <- 1
    A[1,2] <- 0
    A[2,2] <- 3
    b[1] <- 1
    b[2] <- 5
    x[1:2] <- forwardsolve(A[1:2, 1:2], b[1:2])
})

Rmodel <- nimbleModel(code)
Cmodel <- compileNimble(Rmodel)

Cmodel$A
Cmodel$b
Cmodel$x
calculate(Cmodel)
## Process R floating point exception: 8 at Thu Apr  7 14:38:02 2016



## testing using forwardsolve and backsolve in a user-defined function
## in a BUGS model

library(nimble)
A <- array(NA, c(2,2))
A[1,1] <- 2
A[2,1] <- 1
A[1,2] <- 0
A[2,2] <- 3
b <- c(1,5)

myFS <- nimbleFunction(
    run = function(A = double(2), b = double(1)) {
        returnType(double(1))
        ret <- forwardsolve(A, b)
        return(ret)
    }
)

myChol <- nimbleFunction(
    run = function(A = double(2)) {
        returnType(double(2))
        ret <- chol(A)
        return(ret)
    }
)

myInverse <- nimbleFunction(
    run = function(A = double(2)) {
        returnType(double(2))
        ret <- inverse(A)
        return(ret)
    }
)

code <- nimbleCode({
    x1[1:2] <- forwardsolve(A[1:2, 1:2], b[1:2])  ## forwardsolve() directly here does NOT work
    x2[1:2] <- myFS(A = A[1:2, 1:2], b = b[1:2])
    C1[1:2,1:2] <- chol(A[1:2, 1:2])
    C2[1:2,1:2] <- myChol(A[1:2, 1:2])
    I1[1:2,1:2] <- inverse(A[1:2, 1:2])
    I2[1:2,1:2] <- myInverse(A[1:2, 1:2])
    xx1[1:2,1:2] <- chol(A[1:2, 1:2]) + 2 * inverse(A[1:2, 1:2])
    xx2[1:2,1:2] <- myInverse(A[1:2, 1:2]) * 2 + myChol(A[1:2, 1:2])
})

Rmodel <- nimbleModel(code, constants = list(A=A, b=b))
Cmodel <- compileNimble(Rmodel)

m <- Rmodel
m <- Cmodel

calculate(m)
m$x1; m$x2
m$C1; m$C2
m$I1; m$I2
m$xx1; m$xx2

## another example of MVN conjugate sampler, for test-mcmc.R
## using both cov and prec parametrizaions of MVN,
## and various linear links

library(nimble)

set.seed(0)
mu0 <- rep(0,5)
ident <- diag(5)
a <- array(rnorm(20), c(4,5))
B <- array(NA, c(4, 5, 5))
for(i in c(1,3)) {
    A <- array(rnorm(25,i), c(5,5))
    A <- A + t(A) + 5*i*diag(5)
    B[i,,] <- A
}
B[2,,] <- diag(5)
B[4,,] <- diag(5)
M_y <- array(NA, c(4, 5, 5))
for(i in 1:4)
    M_y[i,,] <- i * diag(5)
x <- array(0, c(4,5))
y <- array(0, c(4,5))

code <- nimbleCode({
    for(i in 1:2)
        x[i,1:5] ~ dmnorm(mu0[1:5], prec = ident[1:5,1:5])
    for(i in 3:4)
        x[i,1:5] ~ dmnorm(mu0[1:5], cov  = ident[1:5,1:5])
    for(i in 1:4)
        mu_y[i,1:5] <- asCol(a[i,1:5]) + B[i,1:5,1:5] %*% asCol(x[i,1:5])
    y[1,1:5] ~ dmnorm(mu_y[1,1:5], prec = M_y[1,1:5,1:5])
    y[2,1:5] ~ dmnorm(mu_y[2,1:5], cov  = M_y[2,1:5,1:5])
    y[3,1:5] ~ dmnorm(mu_y[3,1:5], prec = M_y[3,1:5,1:5])
    y[4,1:5] ~ dmnorm(mu_y[4,1:5], cov  = M_y[4,1:5,1:5])
})
constants <- list(mu0=mu0, ident=ident, a=a, B=B, M_y=M_y)
data <- list(y=y)
inits <- list(x=x)
Rmodel <- nimbleModel(code, constants, data, inits)
spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
Rsamples <- as.matrix(Rmcmc$mvSamples)

set.seed(0)
Cmcmc$run(10)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rsamples - Csamples

Rsamples[c(1,10),]
Csamples[c(1,10),]
##        x[1, 1]     x[2, 1]     x[3, 1]    x[4, 1]     x[1, 2]   x[2, 2]
##[1,] -0.7098220 -0.92579244  0.02859492 -0.9271968 -0.03440439 0.1133790
##[2,] -0.6642612  0.09915454 -0.13079585  0.6298495 -0.08432850 0.8806305
##        x[3, 2]    x[4, 2]     x[1, 3]   x[2, 3]     x[3, 3]    x[4, 3]
##[1,] 0.09213596  0.3257091  0.27465408 -1.410726 -0.12937422 -0.2311789
##[2,] 0.01777368 -0.6051182 -0.09322287 -1.723685  0.07759651  0.4119755
##       x[1, 4]     x[2, 4]     x[3, 4]     x[4, 4]   x[1, 5]  x[2, 5]
##[1,] 0.5071505 0.140652780  0.01778143  0.51383003 0.3639871 2.146307
##[2,] 0.4781541 0.002361055 -0.02517465 -0.06995162 0.4202336 1.225317
##         x[3, 5]    x[4, 5]
##[1,] -0.02913630 -0.2563026
##[2,]  0.04303507  0.2716980




## Richard McElreath Statistical Rethinking package,
## might have lots of good data and hierarchical (Bayesian) modeling examples
install.packages('rethinking')
library(rethinking)



## tesing MVN conjugate sampler

library(nimble)

set.seed(0)
mu0 = 1:3
Q0 = matrix(c(1, .2, .8, .2, 2, 1, .8, 1, 2), nrow = 3)
Q = solve(matrix(c(3, 1.7, .9, 1.7, 2, .6, .9, .6, 1), nrow = 3))
a = c(-2, .5, 1)
B = matrix(rnorm(9), 3)

code <- nimbleCode({
  mu[1:3] ~ dmnorm(mu0[1:3], Q0[1:3, 1:3])
  y_mean[1:3] <- asCol(a[1:3]) + B[1:3, 1:3] %*% asCol(mu[1:3])
  y[1:3] ~ dmnorm(y_mean[1:3], Q[1:3, 1:3])
})
mu <- mu0 + chol(solve(Q0)) %*% rnorm(3)
y <- c(a + B%*%mu + chol(solve(Q)) %*% rnorm(3))
data = list(mu0 = mu0, Q0 = Q0, Q = Q, a = a, B = B, y = y)
muQtrue = t(B) %*% Q%*%B + Q0
muMeanTrue = c(solve(muQtrue, crossprod(B, Q%*%(y-a)) + Q0%*%mu0))

Rmodel <- nimbleModel(code, data= data)
spec <- configureMCMC(Rmodel)
spec$getSamplers()
##spec$getSamplerDefinition(1)
Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)

Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
Rsamples <- as.matrix(Rmcmc$mvSamples)

set.seed(0)
Cmcmc$run(10)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rsamples
Csamples
##         mu[1]    mu[2]    mu[3]
## [1,] 3.877574 2.201065 1.084173
## [2,] 2.537805 1.916830 2.084021
## [3,] 4.426193 2.326982 1.567054
## [4,] 2.456652 1.921560 1.892777
## [5,] 2.959890 2.133226 1.506502
## [6,] 3.084360 1.560040 1.941621
## [7,] 3.178667 2.072212 2.611881
## [8,] 2.996492 2.210513 2.795391
## [9,] 2.494759 1.539610 2.118232
##[10,] 2.880285 1.826356 1.805385

Rsamples - Csamples
##              mu[1] mu[2] mu[3]
## [1,] -8.881784e-16     0     0
## [2,]  0.000000e+00     0     0
## [3,] -8.881784e-16     0     0
## [4,] -4.440892e-16     0     0
## [5,] -4.440892e-16     0     0
## [6,]  0.000000e+00     0     0
## [7,] -4.440892e-16     0     0
## [8,] -4.440892e-16     0     0
## [9,]  0.000000e+00     0     0
##[10,] -4.440892e-16     0     0


## inconsistent behavior between R and C chol()

n <- 4
set.seed(0)
tmp <- array(rnorm(n^2), c(n,n)) + n*diag(n)
A <- tmp + t(tmp)
R <- chol(A)
L <- t(R)
B <- array(as.numeric(1:n^2), c(n,n))
b <- as.numeric(1:n)
mat <- A + L

sym <- function(mat, lower) {
    ind <- if(lower) lower.tri(mat) else upper.tri(mat)
    ans <- mat
    ans[!ind] <- 0
    ans <- ans + t(ans) + diag(diag(mat))
    ans
}

## R execution
chol(mat)
chol(sym(mat, FALSE))
chol(mat) - chol(sym(mat, FALSE)) ## R takes the UPPER part of the matrix

## Rnf execution
Rnf$chol_oPA_plus_L_cP() - chol(sym(mat, FALSE))  ## Rnf also UPPER part of matrix

## Cnf execution
Cnf$chol_oPA_plus_L_cP() - chol(sym(mat, FALSE))  ## Cnf now also uses UPPER part of matrix





## test case for forwardsolve, backsolve in the DSL
## A.triangularView<Eigen::Upper>().solve(b);
## could make a file called nimble_macros.h:
##define ......

library(nimble)
## inputs
n <- 4
tests <- c(
    'chol(A)',
    'forwardsolve(A, b)', 'forwardsolve(A, B)', 'forwardsolve(L, b)', 'forwardsolve(L, B)',
       'backsolve(A, b)',    'backsolve(A, B)',    'backsolve(R, b)',    'backsolve(R, B)',
           'solve(A, b)',        'solve(A, B)',        'solve(R, b)',        'solve(R, B)'
)
## derived objects
testNames <- as.character(sapply(tests, nimble:::Rname2CppName))
testDims <- sapply(grepl('_b_', testNames), function(x) if(x) 1 else 2)
methods <- mapply(function(test, dim) eval(substitute(function() { ans <- TEST; returnType(double(DIM)); return(ans) }, list(TEST = parse(text=test)[[1]], DIM = dim))), tests, testDims)
names(methods) <- testNames
set.seed(0)
tmp <- array(rnorm(n^2), c(n,n)) + n*diag(n)
A <- tmp + t(tmp)
R <- chol(A)
L <- t(R)
B <- array(as.numeric(1:n^2), c(n,n))
b <- as.numeric(1:n)
nfDef <- nimbleFunction(
    setup = function(A, B, b) {
        R <- chol(A)
        L <- t(R)
    },
    run = function() {},
    methods = methods
)
Rnf <- nfDef(A, B, b)

Cnf <- compileNimble(Rnf)

testOneCase <- function(test, testName, Rnf, Cnf) {
    Rans <- eval(parse(text=test)[[1]])
    Rnfans <- eval(substitute(Rnf$TEST(), list(TEST=as.name(testName))))
    Cnfans <- eval(substitute(Cnf$TEST(), list(TEST=as.name(testName))))
    dif <- max(abs(Rans - Rnfans))
    if(dif > 1E-15) return(1)
    dif <- max(abs(Rans - Cnfans))
    if(dif > 1E-15) return(1)
    return(0)
}
for(i in seq_along(tests)) {
    test <- tests[i]
    testName <- testNames[i]
    testResult <- testOneCase(test, testName, Rnf, Cnf)
    if(testResult != 0) message('failed test: ', test)
}




library(nimble)

n <- 3
set.seed(0)
tmp <- array(rnorm(n^2), c(n,n)) + diag(n)
A <- tmp + t(tmp)
B <- array(1:n^2, c(n,n))
b <- as.numeric(1:n)


Rnf <- nimbleFunction(
    run = function(A = double(2), b = double(1), B = double(2)) {
        ##ans2 <- forwardsolve(A, b)
        ##ans3 <- forwardsolve(A, B)
        ans4 <- chol(A + A)
    }
)

Cnf <- compileNimble(Rnf)   ## this works




nfDef <- nimbleFunction(
    setup = function(A, B, b) {},
    run = function() {
        ans2 <- forwardsolve(A, b)
        ##ans3 <- forwardsolve(A, B)
    }
)

Rnf <- nfDef(A, B, b)
Cnf <- compileNimble(Rnf)   ## FAIL


library(nimble)

n <- 3
set.seed(0)
tmp <- array(rnorm(n^2), c(n,n)) + diag(n)
A <- tmp + t(tmp)
B <- array(1:n^2, c(n,n))
b <- as.numeric(1:n)


Rnf <- nimbleFunction(
    run = function(A = double(2), b = double(1), B = double(2)) {
        ans1 <- chol(A)
        print('chol(A)')
        print(ans1)
        ##
        ##ans2 <- forwardsolve(A, b)
        ##print('forwardsolve(A, b)')
        ##print(ans2)
        ####
        ##ans3 <- forwardsolve(A, B)
        ##print('forwardsolve(A, B)')
        ##print(ans3)
        ####
        ##ans4 <- backsolve(A, b)
        ##print('backsolve(A, b)')
        ##print(ans4)
        ####
        ans5 <- backsolve(A, B)
        print('backsolve(A, B)')
        print(ans5)
    }
)

Cnf <- compileNimble(Rnf)


set.seed(0)
tmp <- array(rnorm(9), c(3,3)) + diag(3)
A <- tmp + t(tmp)
B <- array(1:9, c(3,3))
b <- 1:3

chol(A)
forwardsolve(A, b)
forwardsolve(A, B)
backsolve(A, b)
backsolve(A, B)

Rnf(A, b, B)
Cnf(A, b, B)

##debug(exprClasses_labelForEigenization)
##undebug(exprClasses_labelForEigenization)
##debug(exprClasses_eigenize)
##undebug(exprClasses_eigenize)
##writeCode(nimDeparse(code))
##writeCode(nimDeparse(compileInfo$nimExpr))


## using compareMCMCs, and MCMC comparisons

library(nimble)

code1 <- nimbleCode({ ## really toy model
    a ~ dnorm(0, 1)
    b ~ dnorm(a, 1)
    sigma ~ dunif(0.5, 1)
    for(i in 1:3) y[i] ~ dnorm(b, sd = sigma)
})

input1 <- list(code = code1, data = list(y = 1:3), inits = list(a = 0.5)) ## can also provide constants
results1 <- compareMCMCs(input1, MCMCs = c('nimble')) ## run a single case

results2 <- compareMCMCs(input1, MCMCs = c('nimble','noConj')) ## run two more cases
results2[[1]]$efficiency ## inspect
results2[[1]]$summary
results2[[1]]$timing
make_MCMC_comparison_pages(results2, 'model1b') ## could generate a comparison of these

## can combine results as follows
## If the same name (â€œnimbleâ€ in this case) was used multiple times one will have to be renamed:
results2[[1]] <- rename_MCMC_comparison_method('nimble', 'another nimble', results2[[1]])
results3 <- combine_MCMC_comparison_results(results1[[1]], results2[[1]], name = 'combined results')
make_MCMC_comparison_pages(results3, 'model1c')



## testing calc_dmnormConjugacyContributions

nimble:::identityMatrix(3)

calc_dmnormConjugacyContributions(diag(3), diag(3), 1)
calc_dmnormConjugacyContributions(diag(3), 7*diag(3), 1)
calc_dmnormConjugacyContributions(diag(3), 8*diag(3), 2)
calc_dmnormConjugacyContributions(3*diag(3), 1+diag(3), 1)
calc_dmnormConjugacyContributions(3*diag(3), 2+diag(3), 2)

Cnf <- compileNimble(calc_dmnormConjugacyContributions)

Cnf(diag(3), diag(3), 1)
Cnf(diag(3), 7*diag(3), 1)
Cnf(diag(3), 8*diag(3), 2)
Cnf(3*diag(3), 1+diag(3), 1)
Cnf(3*diag(3), 2+diag(3), 2)



## compiler lesson 101
library(nimble)

nf <- nimbleFunction(
    setup = function() {
        a <- 1
    },
    run = function(arg = double()) {
        b <- a + arg
        returnType(double())
        return(b)
    })

Rnf <- nf()

## nfProc object is created every time we compile a new NF generator
## nfCompileInfo data structure for compilation info RCfunctionCompileClass"
## RCfunProcessing class to manage compilation info for a *single* run/member function
proj <- nimble:::nimbleProjectClass()
nfProc <- nimble:::nfProcessing(Rnf, project = proj)
nfProc$process(control = list(debug = TRUE))

## place to define 1st arg switching at top of genCpp_sizeProcessing
## switch name to nimArr_xxx in genCpp_processSpecificCalls






## testing of new parsed posterior text in conjugacy system
posteriorText = '{a<-1;
b
c
dnorm(mean = (prior_mean*prior_tau + contribution_mean) / (prior_tau + contribution_tau),
                            sd   = (prior_tau + contribution_tau)^(-0.5))
}'

posteriorText = '{ R <- chol(prior_prec + contribution_prec)
                        A <- prior_prec %*% asCol(prior_mean) + asCol(contribution_mean)
                        mu <- backsolve(R, forwardsolve(t(R), A))[,1]
                        dmnorm_chol(mean = mu, cholesky = R, prec_param = 1) }'

parsedTotalPosterior <- parse(text = posteriorText)[[1]]
parsedTotalPosterior
if(parsedTotalPosterior[[1]] != '{') parsedTotalPosterior <- substitute({POST}, list(POST = parsedTotalPosterior))
parsedTotalPosterior
prePosteriorCodeBlock <- parsedTotalPosterior[-length(parsedTotalPosterior)]
prePosteriorCodeBlock
posteriorExpr <- parsedTotalPosterior[[length(parsedTotalPosterior)]]
posteriorExpr
rDistribution <- cc_makeRDistributionName(as.character(posteriorExpr[[1]]))
rDistribution
dDistribution <- as.character(posteriorExpr[[1]])
dDistribution
argumentExprs <- as.list(posteriorExpr)[-1]
argumentExprs
argumentNames <- names(argumentExprs)
argumentNames
rCallExpr <- as.call(c(as.name(rDistribution), 1, argumentExprs))
rCallExpr
dCallExpr <- as.call(c(as.name(dDistribution), quote(VALUE), argumentExprs, log = 1))
dCallExpr
posteriorVars <- all.vars(parsedTotalPosterior)
posteriorVars
neededPriorParams <- gsub('^prior_', '', posteriorVars[grepl('^prior_', posteriorVars)])
neededPriorParams
neededContributionNames <- posteriorVars[grepl('^contribution_', posteriorVars)]
neededContributionNames
neededContributionDims <- inferContributionTermDimensions(prior)


## cases of dmnorm() parametrizations, and possible getParam() calls
library(nimble)

code <- nimbleCode({ x ~ dnorm(mu, var = v) })
inits <- list(x=0, mu=0, v=1)

set.seed(0)
tmp <- array(rnorm(9), c(3,3)) + diag(3)
A <- tmp + t(tmp)
code <- nimbleCode({ x[1:3] ~ dmnorm(mu[1:3], prec = Q[1:3, 1:3]) })
inits <- list(x=rep(0,3), mu=rep(0,3), Q=A)

md <- nimbleModel(code, inits = inits, returnDef = TRUE, debug=TRUE)

Rmodel <- nimbleModel(code, inits = inits)

Rmodel$getNodeNames()
nn <- Rmodel$getNodeNames(stochOnly = TRUE)
lifted <- Rmodel$getNodeNames(determOnly = TRUE)
nn
lifted
Rmodel$nodes[[lifted]]$simulate  ## calculates chol(Q) into lifted node
Rmodel$nodes[[nn]]$simulate
Rmodel$nodes[[nn]]$get_var
Rmodel$nodes[[nn]]$get_sd
Rmodel$nodes[[nn]]$get_tau
Rmodel$nodes[[nn]]$get_cholesky
Rmodel$nodes[[nn]]$get_prec
Rmodel$nodes[[nn]]$get_cov


## cases of dmnorm() parametrizations, and possible getParam() calls
library(nimble)

set.seed(0)
tmp <- array(rnorm(9), c(3,3)) + diag(3)
A <- tmp + t(tmp)

## parametrize in terms of prec matrix (Q):
code <- nimbleCode({
    x[1:3] ~ dmnorm(mu[1:3], prec = Q[1:3, 1:3])
})
inits <- list(x=rep(0,3), mu=rep(0,3), Q=A)

Rmodel <- nimbleModel(code, inits = inits)
Cmodel <- compileNimble(Rmodel)

Rmodel$getNodeNames()
nn <- Rmodel$getNodeNames(stochOnly = TRUE)
lifted <- Rmodel$getNodeNames(determOnly = TRUE)
nn
lifted
Rmodel[[lifted]]

Q <- A
ch <- chol(Q)
V <- solve(Q)
I <- diag(3)
Q
ch
V
t(ch) %*% ch
Q
backsolve(ch, forwardsolve(t(ch), I))
V

Rmodel$nodes[[nn]]$get_mean
Rmodel$nodes[[nn]]$get_mean()
Rmodel$getParam(nn, 'mean')

Rmodel$nodes[[nn]]$get_cholesky
Rmodel$nodes[[nn]]$get_cholesky()
Rmodel$getParam(nn, 'cholesky')
ch

Rmodel$nodes[[nn]]$get_prec
Rmodel$nodes[[nn]]$get_prec()
Rmodel$getParam(nn, 'prec')
Q

Rmodel$nodes[[nn]]$get_cov
Rmodel$nodes[[nn]]$get_cov()
Rmodel$getParam(nn, 'cov')
V

xx <- forwardsolve(t(ch), I)
t(xx) %*% xx

xx <- backsolve(ch, I)  ## would this work too???
xx %*% t(xx)

t(ch) %*% ch %*% V
t(ch) %*% ch
ch %*% V
Q


nfDef <- nimbleFunction(
    setup = function(model, node) {},
    run = function() {
        print('chol:')
        chh <- model$getParam(node, 'cholesky')
        print(chh)
        print('prec:')
        q <- model$getParam(node, 'prec')
        print(q)
        print('cov:')
        vv <- model$getParam(node, 'cov')
        print(vv)
    }
)

Rnf <- nfDef(Rmodel, nn)
Cnf <- compileNimble(Rnf, project=Rmodel)


ch
Q
V
Rnf$run()
Cnf$run()

## store something into model variable 'Q'
Rmodel$Q <- Q

## calculate deterministic dependents
Rmodel$nodes[[lifted]]$simulate  ## calculates chol(Q) into lifted node

Rmodel$nodes[[nn]]$simulate
Rmodel$nodes[[nn]]$get_cholesky
Rmodel$nodes[[nn]]$get_cholesky()
ch

## request 'prec'
Rmodel$nodes[[nn]]$get_prec  ## direct: model$Q
Rmodel$nodes[[nn]]$get_prec()
Q

## request 'cov'
## here's what we want to get cov: backsolve(ch,  forwardsolve(t(ch), I)  )
Rmodel$nodes[[nn]]$get_cov
Rmodel$nodes[[nn]]$get_cov()
V







    


## figuring out R / nimble functions:
## inverse, solve, forwardsolve, backsolve

library(nimble)
set.seed(0)
A <- array(rnorm(9), c(3,3))
A <- t(A) %*% A

nfDef <- nimbleFunction(
    setup <- function(A) {
        D <- diag(3)
        b <- 1:3
    },
    methods = list(
        inv = function() {
            Ainv <- inverse(A)
            print(Ainv)
        },
        ch = function() {
            R <- chol(A)
            print(R)
        },
        fs = function() {
            R <- chol(A)
            L <- t(R)
            Linv <- forwardsolve(L, D)
            print(Linv)
        },
        bs = function() {
            R <- chol(A)
        }
    )
)

Rnf <- nfDef(A)
Cnf <- compileNimble(Rnf)

print('inverse of A')
solve(A)
Rnf$inv()
Cnf$inv()

print('chol of A')
chol(A)
Rnf$ch()
Cnf$ch()

R <- chol(A)
L <- t(R)

print('forwardward solve')
forwardsolve(L, diag(3))

print('backward solve')
backsolve(R, diag(3))



## example of using nf$getDefinition() and nf_getDefinition()
library(nimble)
nfDef <- nimbleFunction(
    setup = function(a) { },
    run = function(b = double()) {
        a <<- a + b
        returnType(double())
        return(a)
    },
    methods = list(
        nothing = function() {
            print(a)
        })
)
Rnf <- nfDef(3)
Cnf <- compileNimble(Rnf)
getDefinition(nfDef)
Rnf$getDefinition()
Cnf$getDefinition()

## example of spec$getSamplerDefinition(..)
library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dgamma(a, 1)
    for(i in 1:140)
        c[i] ~ dnorm(0,1)
})

Rmodel <- nimbleModel(code, check = FALSE)

spec <- configureMCMC(Rmodel, print = TRUE)
spec$printSamplers()
spec$getSamplerDefinition(1)
spec$getSamplerDefinition('b')

spec$getSamplers()
spec$printSamplers()
spec$addSampler(type='RW_block', target=c('a', 'b'))
spec$getSamplers()
spec$printSamplers()


## performance comparison of of new dynamic conjugate samplers, rats example
library(nimble)
rats <- readBUGSmodel('rats', dir = getBUGSexampleDir('rats'), returnModelComponentsOnly = TRUE)
Rmodel <- nimbleModel(rats$model, rats$data[c('N','T','x')], rats$data['Y'], rats$inits)
spec <- configureMCMC(Rmodel, print = TRUE)
out <- MCMCsuite(
    rats$model, rats$data[c('N','T','x')], rats$data['Y'], rats$inits,
    niter = 100000,
    MCMCs = c('staticConj', 'dynamicConj'),
    MCMCdefs = list(
        staticConj  = quote({ nimbleOptions(useDynamicConjugacy = FALSE)
                              configureMCMC(Rmodel) }),
        dynamicConj = quote({ nimbleOptions(useDynamicConjugacy = TRUE)
                              configureMCMC(Rmodel) })),
    makePlot = FALSE
)
sampdiff <- out$samples['staticConj',,] - out$samples['dynamicConj',,]
all(sampdiff == 0)
## [1] TRUE                      ## samples from each MCMC are identical
out$timing
## staticConj    dynamicConj
##     26.309         17.801     ## ~ 33% decrease in conjugate sampler runtime



## testing proper dmnorm conjugate calculations for Chris

## code = nimbleCode({
##    y[1:3]  ~ dmnorm(mu [1:3], cov = covy [1:3,1:3])
##    mu[1:3] ~ dmnorm(mu0[1:3], cov = covmu[1:3,1:3])
## })
## Concerned about some explicit inverses in conjugate update for mu,
## compared to use of already existing Cholesky factors.  In the R sampler,
## 1) we have: inverse(prior_prec + contribution_prec)
## 2) we use get_prec, which when we specify a dmnorm with a covariance
## has code like the following (in calc_dmnormAltParams()):
##  tmp <- t(cholesky) %*% cholesky
##  return(inverse(tmp))

library(nimble)

nimbleOptions('buildInterfacesForCompiledNestedNimbleFunctions')
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
nimbleOptions('buildInterfacesForCompiledNestedNimbleFunctions')
nimbleOptions()

code = nimbleCode({
   mu[1:3] ~ dmnorm(mu0[1:3], cov=covmu[1:3,1:3])
   y[1:3] ~ dmnorm(mu[1:3], cov = covy[1:3,1:3])
   a ~ dnorm(y[1], y[1])
   b ~ dgamma(a, y[1])
})

cov <- diag(3)
constants <- list(covy = cov, covmu = cov, mu0 = rep(0,3))
data <- list(y = 1:3)
inits <- list(mu = 4:6, a = 1, b = 1)

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()
spec$printSamplers()
spec$getSamplerDefinition('mu')
Rmcmc <- buildMCMC(spec)


Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

Rmcmc$samplerFunctions$contentsList[[2]]$scale
Cmcmc$samplerFunctions$contentsList[[2]]
Cmcmc$samplerFunctions$contentsList[[2]][['scale']]
Cmcmc$samplerFunctions$contentsList[[2]]$scale



set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)



## testing what works and what doesn't with nimbleFunctionLists, getParam, etc..

library(nimble)

getNimbleOption('checkModel')
nimbleOptions(checkModel = FALSE)
getNimbleOption('checkModel')

code <- quote({
    x ~ dgamma(1, 1)
    y[1] ~ dnorm(1, tau = 1*x)
    y[2] ~ dnorm(2, tau = 2*x)
    y[3] ~ dnorm(3, tau = 3*x)
})
Rmodel <- nimbleModel(code, data=list(), inits=list(x=1, y=11:13))

nfDef <- nimbleFunction(
    setup = function(model) {
        nl <- model$expandNodeNames('y')
        N <- length(nl)
        nfl <- nimbleFunctionList(node_stoch_dnorm)
        for (i in 1:N) nfl[[i]] <- model$nodeFunctions[[nl[i]]]
    },
    run = function() {
        for(i in 1:N) {
            print('i = ', i)
            val <- nfl[[i]]$get_mean()
            print('mean = ', val)
            val <- nfl[[i]]$get_tau()
            print('tau = ', val)
            val <- nfl[[i]]$get_var()
            print('var = ', val)
            val <- nfl[[i]]$get_value()
            print('value = ', val)
        }
        returnType(double(0))
        return(1)
    }
)
Rnf <- nfDef(Rmodel)
Rnf$run()
Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(Rnf, project = Rmodel)
Cnf$run()


## testing of new dynamic conjugate samplers
### demo2 of check conjugacy
library(nimble)
code <- BUGScode({
    x ~ dbeta(3, 13)
    y[1] ~ dbin(x, 10)
    y[2] ~ dbin(x, 20)
    z ~ dbeta(10, 10)
    zz[1] ~ dbin(z, 10)
    zz[2] ~ dbin(z, 20)
    aaa ~ dbeta(10, 10)
    bbb[1] ~ dbin(aaa, 10)
    bbb[2] ~ dbin(aaa, 20)
    bbb[3] ~ dbin(aaa, 20)
    d ~ dnorm(0, 1)
    dd ~ dnorm(0, 1)
    ddd ~ dnorm(0, 1)
    e ~ dnorm(d + dd + ddd, 1)
    f ~ dlnorm(d + dd + ddd, 1)
    ff ~ dlnorm(d + dd, 1)
    g ~ dnorm(0, 1)
    gg ~ dnorm(5+g, 1)
})
constants <- list()
inits <- list(x = 0.5, z=0.6, aaa=0.3, d=0, dd=0, ddd=0, g=0)
data = list(y = c(3,4), zz=c(5,5), bbb=c(3,3,3), ff=1, e=0, f=1, gg=0)
Rmodel <- nimbleModel(code, constants, data, inits)


spec <- configureMCMC(Rmodel)

spec$getSamplers()

Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
Rmcmc$run(3)
Rsamples <- as.matrix(Rmcmc$mvSamples)
set.seed(0)
Cmcmc$run(3)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rnames <- dimnames(Rsamples)[[2]]
Rsamples[, Rnames] - Csamples[, Rnames]
##     x z aaa d dd ddd g
##[1,] 0 0   0 0  0   0 0
##[2,] 0 0   0 0  0   0 0
##[3,] 0 0   0 0  0   0 0
Rsamples[, Rnames]
##             x         z       aaa         d         dd        ddd         g
##[1,] 0.3254630 0.3746455 0.3658422 0.6362147 -0.2698403 -1.1333402 -3.156596
##[2,] 0.1975665 0.3995488 0.3210974 0.3695457 -0.2843177 -0.2239394 -2.711577
##[3,] 0.1900000 0.4199454 0.2194685 0.5430496 -0.9140867  0.1178770 -2.233141


## testing of new dynamic conjugate samplers
### demo2 of check conjugacy
library(nimble)
code <- BUGScode({
    x ~ dbeta(3, 13)
    y[1] ~ dbin(x, 10)
    y[2] ~ dbin(x, 20)
})
data = list(y = c(3,4))
constants <- list()
inits <- list()

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
Rsamples <- as.matrix(Rmcmc$mvSamples)
set.seed(0)
Cmcmc$run(10)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rsamples - Csamples
as.numeric(Rsamples) - c(0.195510839527966, 0.332847482503424,0.247768152764931, 0.121748195439553, 0.157842271774841, 0.197566496350904, 0.216991517500577, 0.276609942874852, 0.165733872345582, 0.144695512780252)



## testing of new dynamic conjugate samplers
### checkConjugacy_demo3_run.R - various conjugacies
library(nimble)
code <- BUGScode({
    x ~ dgamma(1, 1)       # should satisfy 'gamma' conjugacy class
    a  ~ dnorm(0, x)     # should satisfy 'norm' conjugacy class
    a2 ~ dnorm(0, tau = 3*x+0)
    b  ~ dpois(0+5*x)
    b2 ~ dpois(1*x*1)
    c ~ dgamma(1, 7*x*5)
    for(i in 2:3) {
        jTau[i] <- 1
        jNorm[i] ~ dnorm(c * (a+3) - i, var = jTau[i])
        kTauSd[i] <- 2
        kLogNorm[i] ~ dlnorm(0 - a - 6*i, kTauSd[i])
    }
})
data = list()
constants <- list()
inits <- list()
Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel, monitors = c('x', 'c'), control = list(scale = 0.01))
spec$getSamplers()

Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
Rsamples <- as.matrix(Rmcmc$mvSamples)
set.seed(0)
Cmcmc$run(10)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rnames <- dimnames(Rsamples)[[2]]
Rsamples[, Rnames] - Csamples[, Rnames]

Rsamples[, Rnames] - cbind(c(3.950556165467749, 1.556947815895538, 1.598959152023738, 2.223758981790340, 2.386291653164086, 3.266282048060261, 3.064019155073057, 3.229661999356182, 1.985990552839427, 2.057249437940977), c( 0.010341199485849559, 0.010341199485849559, 0.003846483017887228, 0.003846483017887228, 0.007257679932131476, 0.009680314740728335, 0.012594777095902964, 0.012594777095902964, 0.018179641351556003, 0.018179641351556003))


## testing of new dynamic conjugate samplers
### Dirichlet-multinomial conjugacy
# as of v0.4, exact numerical results here have changed because
# ddirch now sometimes returns NaN rather than -Inf (when an
# alpha is proposed to be negative) -- this changes the RNG
# sequence because NaN values result in no runif() call in decide()
# single multinomial
library(nimble)
set.seed(0)
n <- 100
alpha <- c(10, 30, 15, 60, 1)
K <- length(alpha)
p <- c(.12, .24, .09, .54, .01)
y <- rmulti(1, n, p)
code <- quote({
    y[1:K] ~ dmulti(p[1:K], n);
    p[1:K] ~ ddirch(alpha[1:K]);
    for(i in 1:K) {
        alpha[i] ~ dgamma(.001, .001);
    }
               })
inits <- list(p = rep(1/K, K), alpha = rep(K, K))
constants <- list(n=n, K=K)
data <- list(y = y)
Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel, monitors = c('alpha', 'p'))
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)

apply(samples, 2, mean)[6:10]
p
apply(samples, 2, mean)[6:10] - p



## testing of new dynamic conjugate samplers
## rats example
library(nimble)
lst <- readBUGSmodel('rats', dir = getBUGSexampleDir('rats'), returnModelComponentsOnly = TRUE)
code <- lst$model
data <- lst$data[c('Y')]
constants <- lst$data[c('N', 'T', 'x')]
inits <- lst$inits

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()

Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0)
Cmcmc$run(5000)
samples <- as.matrix(Cmcmc$mvSamples)

a <- samples
b <- samples
a
b
a-b
any(a-b != 0)




## phase transition regression example for Mark.

## generate data
sigma <- 5
a <- 2
b <- 0.5
t0 <- 30
x <- 1:100
n <- length(x)
y <- rnorm(n, a + (x>t0) * (b*(x-t0)), sd=sigma)
plot(x, y)

## fit NIMBLE model
library(nimble)

code <- nimbleCode({
    a ~ dunif(-100, 100)
    b ~ dunif(-100, 100)
    t0 ~ dunif(0, 100)
    sigma ~ dunif(0, 100)
    for(i in 1:n) {
        phase[i] <- step(x[i] - t0)
        mu[i] <- a + phase[i] * b * (x[i]-t0)
        y[i] ~ dnorm(mu[i], sd = sigma)
    }
})
constants <- list(n=n, x=x)
data <- list(y=y)
inits <- list(sigma=1, a=0, b=1, t0=50)

## MCMC Suite will let us do a few MCMCs quickly, and look at results easily
out <- MCMCsuite(
    code, constants, data, inits,
    MCMCs = c('nimble', 'nimble_slice', 'bugs'),
    niter = 20000,
    burnin = 2000,
    makePlot = TRUE,
    savePlot = FALSE,
    calculateEfficiency = TRUE
)

out$summary
out$efficiency
dim(out$samples)
out$timing





## nimble problem with using 'sd' name for 'tau' parameter, e.g.
library(nimble)

code <- nimbleCode({
    sd ~ dunif(0, 1)
    y ~ dnorm(0, tau = sd)
    shape1 ~ dnorm(0, 1)
    f ~ dbeta(mean = y, sd = shape1)
})

m <- nimbleModel(code, inits = list(sd=.1))

m$sd

m$getModelDef()$printDI()

code <- nimbleCode({
    sd2 ~ dunif(0, 1)
    y ~ dnorm(0, tau = sd2)
})
m <- nimbleModel(code, debug=TRUE)

m$sd2






     
## Plot the fake MCMC output
denoverplot(fakemcmc, fakemcmc2)
     denoverplot(fakemcmc, fakemcmc2, style="plain",
                 col=mcmcplotsPalette(3, type="grayscale"),
                 ci=0.95, greek=TRUE)
     denoverplot(fakemcmc, fakemcmc2,
                 plot.title="Comparison of densities of fake data")
     denoverplot(fakemcmc, fakemcmc2,
                 plot.title="Comparison of densities of fake data", greek=TRUE)

setwd('/Users/dturek/GitHub/legacy/autoBlock/')
getwd()

source("autoBlock.R")
load(file.path("data", "model_test.RData"))
saveSamples <- TRUE
niter <- 10000
ab <- autoBlock(code, constants, data, inits, niter, runList, saveSamples = saveSamples)
dftest <- ab$summary
save(dftest, file = file.path("results_samples", "results_test.RData"))
if (saveSamples) {
    burnedSamplesList <- ab$samples
    for (i in 1:length(burnedSamplesList)) burnedSamplesList[[i]] <- burnedSamplesList[[i]][(floor(niter/2) + 1):niter, ]
    save(burnedSamplesList, niter, file = file.path("results_samples", "results_test_samples.RData"))
}

class(burnedSamplesList)
names(burnedSamplesList)

i <- 4
dim(burnedSamplesList[[i]])
dimnames(burnedSamplesList[[i]])

dimnames(ab$samples[[3]])


ret$summary

dftest <- autoBlock(code, constants, data, inits, 10000, runList)$summary
save(dftest, file = file.path("results_hclust_complete2", "results_test.RData"))


rm(list=ls())
getwd()
load('results_samples/results_test.RData')
ls()
dftest

rm(list=ls())
getwd()
load('results_samples/results_test_samples.RData')
ls()
niter
class(burnedSamplesList)
names(burnedSamplesList)
i <- 4
dim(burnedSamplesList[[i]])
dimnames(burnedSamplesList[[i]])







## renaming stupid data frame from the autoBlock results
setwd('~/GitHub/legacy/autoBlock')

rm(list=ls())
file <- 'results/results_samplingEfficiency.RData'
load(file)
ls()
dfsamplingEfficiency <- dfSamplingEfficiency
rm(dfSamplingEfficiency)
ls()
save(list='dfsamplingEfficiency', file = file)

rm(list=ls())
file <- 'results/results_computationalRequirement.RData'
load(file)
ls()
dfcomputationalRequirement <- dfComputationalRequirement
rm(dfComputationalRequirement)
ls()
save(list='dfcomputationalRequirement', file = file)





## working on the spatial capture-recapture (SCR) models

tr<-seq(15,85, length=10)

X<-cbind(rep(tr,each=length(tr)),rep(tr,times=length(tr))) # 100 coord. traps

plot(X, xlim=c(0,100), ylim=c(0,100), pch=3, cex=0.75)

set.seed(10)
xlim <- c(0,100); ylim <- c(0,100)      # Area 100*100=1e4
A <- (xlim[2]-xlim[1])*(ylim[2]-ylim[1])/10000
mu <- 50                 # Density
N <- rpois(1, mu*A) ;N   # Generate population


s <- cbind(runif(N, xlim[1], xlim[2]), runif(N, ylim[1], ylim[2]))
points(s, pch=16, col=2)

sigma <- 5
lambda0 <- 0.4
J <- nrow(X)
K <- 5
yy <- array(NA, c(N, J, K))
for(j in 1:J) {
    dist <- sqrt((X[j,1]-s[,1])^2 + (X[j,2]-s[,2])^2)
    lambda <- lambda0*exp(-dist^2/(2*sigma^2))
    for(k in 1:K) {
        yy[,j,k] <- rpois(N, lambda)
    }
}

n <- apply(yy, c(2,3), sum)

# Plot capture events
tot<-apply(n, 1,sum)
symbols(X, circles=tot, inches=F, bg="#00000022", add=T)
points(X, pch=3, cex=0.75); points(s, pch=16, col=2)


library(nimble)

## define the model
code <- nimbleCode({
    sigma ~ dunif(0,10)
    lam0 ~ dunif(0,5)
    psi ~ dbeta(1,1)
    for(i in 1:M) {
        z[i] ~ dbern(psi)
        s[i,1] ~ dunif(xlim[1], xlim[2])
        s[i,2] ~ dunif(ylim[1], ylim[2])
        for(j in 1:J) {# Number of traps
            dist[i,j] <- (s[i,1] - X[j,1])^2 + (s[i,2] - X[j,2])^2
            lam[i,j] <- lam0*exp(-dist[i,j]/(2*sigma^2))*z[i]
        }
    }
    for(j in 1:J){
        bigLambda[j] <- sum(lam[1:M,j])
        for(k in 1:K) {
            n[j,k] ~ dpois(bigLambda[j])
        }
    }
    N <- sum(z[1:M])
})


M<-200

constants <- list(M = M, K=K, J=J)
n1<-apply(n,1,sum)
data<-list(n=n, X=X, xlim=xlim, ylim=ylim)
s<-cbind(runif(M, xlim[1], xlim[2]), runif(M, ylim[1], ylim[2]))
z<-rep(1,M)
inits <- list (sigma=0.5, lam0=0.1, s=s, z=z)

Rmodel <- nimbleModel(code=code, constants=constants, data=data, inits=inits, check=FALSE) ## check=FALSE is faster

mcmcspec<-configureMCMC(Rmodel, print=TRUE, monitors = c("N", "lam0", "psi", "sigma"))
scrMCMC <- buildMCMC(mcmcspec)
Cmodel <- compileNimble(Rmodel) 
CscrMCMC <- compileNimble(scrMCMC, project = Rmodel)

## It's pretty slow to run so I just want an execution time first for a small sample
t1_100 <- system.time(CscrMCMC$run(100))

## And now I want a decent sample
t1_20k <- system.time(CscrMCMC$run(20000))
t1_20k_samples <- as.matrix(CscrMCMC$mvSamples)
save(t1_100, t1_20k, t1_20k_samples, file = "case1results.Rdata")


## checking dcat distribution for Chris

library(nimble)

code <- nimbleCode({
    y ~ dcat((p[1:2,1:2] %*% ones[1:2])[1:2,1])
})
constants <- list(ones = c(1,1))
data <- list()
inits <- list(y = 2, p = diag(c(.5,.5)))

Rmodel <- nimbleModel(code, constants, data, inits)




library(nimble)
Rmodel <- readBUGSmodel('rats', dir = '~/GitHub/nimble/nimble/packages/nimble/inst/classic-bugs/vol1/rats/')

customSpec <- configureMCMC(Rmodel)
customSpec$getSamplers()
customSpec$removeSamplers(1:65)
customSpec$getSamplers()
customSpec$addSampler(target = 'alpha.c', type = 'slice')
customSpec$addSampler(target = 'beta.c', type = 'slice')
customSpec$addSampler(target = 'tau.c', type = 'slice')
customSpec$addSampler(target = 'tau.alpha', type = 'slice')
customSpec$addSampler(target = 'tau.beta', type = 'slice')
customSpec$getSamplers()
alphaNames <- Rmodel$getDependencies('tau.alpha', self = FALSE, stochOnly = TRUE)
alphaNames
length(alphaNames)  ## 30
for(aN in alphaNames) customSpec$addSampler(target = aN, type = 'slice')
customSpec$getSamplers()
betaNames <- Rmodel$getDependencies('tau.beta', self = FALSE, stochOnly = TRUE)
betaNames
length(betaNames)  ## 30
for(bN in betaNames) customSpec$addSampler(target = bN, type = 'slice')
customSpec$getSamplers()
customSpec


## making a simple MCMC traceplot, and density plot

n <- 1000
x <- rnorm(n, 3, 1)
dev.new(width=4, height=3)
plot(1:n, x, type='l')
dev.copy2pdf(file = '~/Downloads/traceplot.pdf')
dev.new(width=4, height=3)
plot(density(x))
dev.copy2pdf(file = '~/Downloads/densityplot.pdf')


## make sure I get the empirical covariance right

x <- matrix(c(1,2,1,2,1,4,6,5,7,4), 5, 2)
timesRan <- 5
statSums <- apply(x, 2, sum)
statSums <- t(statSums)
statProds <- matrix(0, 2, 2)
for(i in 1:timesRan) statProds <- statProds + t(t(x[i,])) %*% t(x[i,])
statSums
statProds
(statProds - (t(statSums) %*% statSums)/timesRan) / (timesRan-1)
cov(x)
colMeans <- apply(x, 2, mean)
xCentered <- x - rep(colMeans, each=timesRan)
xCentered
(t(xCentered) %*% xCentered) / (timesRan-1)

set.seed(0)
library(nimble)
code <- nimbleCode({
    x[1:d] ~ dmnorm(mu[1:d], cov = Sigma[1:d, 1:d])
})
d <- 4
mu <- 1:d
ch <- array(rnorm(d^2), c(d,d))
Sigma <- ch %*% t(ch)
print(Sigma)
constants <- list(d=d, mu=mu, Sigma=Sigma)
data <- list()
inits <- list(x = rep(1, d))
Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel, nodes = NULL)
spec$addSampler('x', 'RW_block', print = FALSE)
spec$addSampler('x', 'RW_block_NEW', print = FALSE)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
Cmcmc$run(1000)
samples <- as.matrix(Cmcmc$mvSamples)
cov(samples)
           x[1]       x[2]     x[3]       x[4]
x[1]  2.0934846 -0.1465907 1.107342  1.1252451
x[2] -0.1465907  4.1696671 1.644427 -0.5950559
x[3]  1.1073420  1.6444267 2.581730  1.5097074
x[4]  1.1252451 -0.5950559 1.509707  1.8741393
samples[c(1,250,500,750,1000),]
           x[1]      x[2]       x[3]     x[4]
[1,]  1.0000000  1.000000  1.0000000 1.000000
[2,]  0.2162332  2.260594  1.9804354 2.913190
[3,]  1.8118014  3.966497  2.7563534 2.552825
[4,] -1.6015689 -1.004725 -0.9864974 1.065231
[5,]  0.8037007  2.496745  1.2659404 2.208456






## looking into RStudio hanging problem (once again... Nov 2015)

library(nimble)
code <- nimbleCode({
  a ~ dnorm(0, 1)
  sd ~ dunif(0, 100)
  for(i in 1:N) {
    b[i] ~ dnorm(a, sd = sd)
  }
})
N <- 100
constants <- list(N = N)
data <- list()
inits <- list(a = 0, sd = 1, b = rep(0, N))


Rmodel <- nimbleModel(code, constants, data, inits)


## NEED TO FIND THE PROBLEM IN MODEL CHECKING

## EXAMPLE 1:
rm(list = ls())
library(nimble)
classicDyesModel <- readBUGSmodel('dyes', dir = getBUGSexampleDir('dyes'))

## EXAMPLE 2:
set.seed(0)
library(nimble)
n_obs <- 50
### Parameters
mu <- 2
rho_a <- 1
rho_b <- 0.1
monitored_param <- c('mu', 'rho_a', 'rho_b', 'ngis')
SV =  rgamma(n=n_obs, 1, 1)
### Data generation
lambda <-  exp(mu)*SV
ngis  <-  rpois(n=n_obs, lambda=lambda )
Y <- rgamma(n=n_obs, ngis*rho_a, rho_b)
### Remove zero values from ngis
ngis <- ngis[which(ngis>0)]
abs <- which(Y==0)
n_abs <- length(abs)
pres <- which(Y>0)
n_pres <- length(pres)

# Install from source for OS X, Linux, or Windows:
#install.packages("nimble", repos = "http://r-nimble.org", type = "source")
M1_CPG_nimbleCode <- nimbleCode(
    {
        ## Covariates
        for (s in 1:n_obs){
            lambda[s] <- exp(mu)*SV[s]
        }
        ## Observation model
        ## Strictly positive Observation
        for ( s in 1:n_pres){
            ngis[s] ~ T(dpois(lambda[pres[s]]), 1, )  
            Y[pres[s]] ~ dgamma(rho_a * ngis[s], rho_b)
        }
        ## Zero Observation
        for ( s in 1:n_abs){
            proba[s] <- 1 - exp(-lambda[abs[s]])
            Y[abs[s]] ~ dbern(proba[s])
        }
        ## Prior
        rho_a ~ dgamma(0.01, 0.01)
        rho_b ~ dgamma(0.01, 0.01)
        mu ~ dnorm(0, 0.01)
    })

constants_list <- list(n_obs = length(Y), n_abs=n_abs, n_pres=n_pres, abs=abs, pres=pres)
nimble_data <- list(Y = Y, SV=SV)
## Initial values
inits_fc <- function(chain_id = 1){
  mu <- rnorm(1,0,1)
  rho_a <- runif(1,0,10)
  rho_b <- runif(1,0,10)
  ngis <- round(runif(n_pres, 2, 5), digits=0)
  list(mu=mu, rho_a=rho_a, rho_b=rho_b ,ngis=ngis)
}
init_ll <- lapply(1, function(id) inits_fc(chain_id = id))

Rmodel <- nimbleModel(code= M1_CPG_nimbleCode, name= 'M1_CPG_nimble', constants = constants_list, data =nimble_data, inits = init_ll[[1]], check = TRUE)

calculate(Rmodel, 'ngis[1]')

Rmodel$ngis

check <- TRUE
check <- FALSE

md <- nimbleModel(code= M1_CPG_nimbleCode, name= 'M1_CPG_nimble', constants = constants_list, data =nimble_data, inits = init_ll, returnDef = TRUE)

M1_CPG_nimble <- md$newModel(data =nimble_data, inits = init_ll, check = check)

debug(md$newModel)
md$newModel(data =nimble_data, inits = init_ll, check = TRUE)

debug(model$check)





## random stuff for CR paper....

rm(list=ls())
library(nimble)
##source('~/GitHub/legacy/dipper/dipperCode.R')

Rmodel <- nimbleModel(code_dipper, constants, data, inits)
lnodes <- Rmodel$expandNodeNames('x')
length(lnodes) - nind

rm(list=ls())
trunc <- FALSE
source('~/GitHub/userDistMCMC/defs.R')
source('~/GitHub/userDistMCMC/create_data.R')
Rmodel <- nimbleModel(orchidDHMM$code, orchidDHMM$constants, orchidDHMM$data, orchidDHMM$inits)
topN <- Rmodel$getNodeNames(topOnly = TRUE, stochOnly = TRUE)
topN
length(topN)

con <- orchidDHMM$constants
con$f
con$nind
#### actually 250 individuals in original dataset!!!

con$f
12 - con$f
sum(12 - con$f)

rm(list=ls())
load('~/GitHub/userDistMCMC/results.RData')
or <- results$df[results$df$model == 'orchid', ]
or
or[or$mcmc %in% c('jags', 'nimbleDHMM2'), ]
or1 <- or[or$param == 's[1]', ]
data.frame(mcmc = or1$mcmc, runtime_minutes = or1$timing/60)

rm(list=ls())
load('~/GitHub/userDistMCMC/results.RData')
go <- results$df[results$df$model == 'goose', ]
go
go1 <- go[go$param == 'p[1]', ]
data.frame(mcmc = go1$mcmc, runtime_minutes = go1$timing/60, runtime_hours = go1$timing/60/60)


rm(list=ls())
trunc <- FALSE
source('~/GitHub/userDistMCMC/defs.R')
source('~/GitHub/userDistMCMC/create_data.R')
Rmodel <- nimbleModel(gooseDHMM$code, gooseDHMM$constants, gooseDHMM$data, gooseDHMM$inits)
topN <- Rmodel$getNodeNames(topOnly = TRUE, stochOnly = TRUE)
topN
length(topN)



## testing the MVN conjugacy test from test-mcmc.R
library(nimble)
set.seed(0)
mu0 = 1:3
Q0 = matrix(c(1, .2, .8, .2, 2, 1, .8, 1, 2), nrow = 3)
Q = solve(matrix(c(3, 1.7, .9, 1.7, 2, .6, .9, .6, 1), nrow = 3))
a = c(-2, .5, 1)
B = matrix(rnorm(9), 3)

##### not currently working - see Perry's email of ~ 10/6/14
## code <- nimbleCode({
##   mu[1:3] ~ dmnorm(mu0[1:3], Q0[1:3, 1:3])
##   y[1:3] ~ dmnorm(asCol(a[1:3]) + B[1:3, 1:3] %*% asCol(mu[1:3]), Q[1:3, 1:3])
## })

code <- nimbleCode({
  mu[1:3] ~ dmnorm(mu0[1:3], Q0[1:3, 1:3])
  y_mean[1:3] <- asCol(a[1:3]) + B[1:3, 1:3] %*% asCol(mu[1:3])
  y[1:3] ~ dmnorm(y_mean[1:3], Q[1:3, 1:3])
})

## Simplest version of model w/o 'a' and 'B'
## a = rep(0,3)
## B = diag(rep(1,3))
## code <- nimbleCode({
##   mu[1:3] ~ dmnorm(mu0[1:3], Q0[1:3, 1:3])
##   y[1:3] ~ dmnorm(mu[1:3], Q[1:3, 1:3])
## })


mu <- mu0 + chol(solve(Q0)) %*% rnorm(3)
# make sure y is a vec not a 1-col matrix or get a dimensionality error
y <- c(a + B%*%mu + chol(solve(Q)) %*% rnorm(3))
data = list(mu0 = mu0, Q0 = Q0, Q = Q, a = a, B = B, y = y)

muQtrue = t(B) %*% Q%*%B + Q0
muMeanTrue = c(solve(muQtrue, crossprod(B, Q%*%(y-a)) + Q0%*%mu0))

constants <- list()
inits <- list()

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

##debug(Rmcmc$samplerFunctions$contentsList[[1]]$run)
set.seed(0)
Rmcmc$run(5)
Rsamples <- as.matrix(Rmcmc$mvSamples)
Rsamples

set.seed(0)
Cmcmc$run(5)
Csamples <- as.matrix(Cmcmc$mvSamples)
Csamples

Rsamples - Csamples

Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)
muMeanTrue

apply(samples, 2, mean) - muMeanTrue


source('~/GitHub/nimble/nimble/packages/nimble/inst/tests/test_utils.R')
##test_mcmc
test_mcmc(model = code, name = 'two-level multivariate normal', data = data, seed = 0, numItsC = 10000,
          results = list(mean = list(mu = muMeanTrue),
                           cov = list(mu = solve(muQtrue))),
          resultsTolerance = list(mean = list(mu = rep(.02,3)),
            cov = list(mu = matrix(.01, 3, 3))))



## working the basketball "streaks" problem emailed out by Aaron Strauss
p <- 0.1
ns <- 1
run <- function(p, ns) {
    out <- numeric()
    n <- 0
    while(n < ns) {
        nmade <- 1
        while(rbinom(1, 1, p) == 1) nmade <- nmade + 1
        out <- c(out, nmade)
        n <- length(out)
    }
    return(out)
}

pOfStreaks <- function(p, streaks, niter=1000) {
    pMissing <- missing(p)
    ns <- length(streaks)
    suc <- 0
    for(i in 1:niter) {
        if(pMissing) p <- runif(1)
        thisStreak <- run(p, ns)
        if(all(thisStreak == streaks)) suc <- suc+1
    }
    prob <- suc/niter
    return(prob)
}

pOfStreaks(p=0.2, streaks=4, niter=100000)
0.2^3*(.8)

p <- 0.7
n <- 4
m <- 7
pOfStreaks(p=p, streaks=c(n,m), niter=1000000)
p^(n+m-2)*(1-p)^2

n <- 2
m <- 1
pOfStreaks(streaks=c(n,m), niter=1000000)
1/12

pOfPlessthanPROB <- function(streaks, PROB, niter=1000) {
    ns <- length(streaks)
    suc <- 0
    ranSoFar <- 0
    while(ranSoFar < niter) {
        p <- runif(1)
        thisStreak <- run(p, ns)
        if(all(thisStreak == streaks)) {
            if(p < PROB) suc <- suc+1
            ranSoFar <- ranSoFar+1
        }
    }
    prob <- suc/niter
    return(prob)
}

pOfPlessthanPROB(streaks=c(2,1), PROB=0.5, niter=20000)
11/16

## continusing the "sreaks" basketball problem, now in NIMBLE

library(nimble)

code <- nimbleCode({
    ##p ~ dunif(0, 1)
    p ~ dbeta(1, 1)
    y[1] ~ dnegbin(prob=p, size=1)
    y[2] ~ dnegbin(prob=p, size=1)
})
constants <- list()
data <- list(y = 0:1)  ## the 'streak' history of {2,1} in the original problem
inits <- list(p = 0.5) ## translates into c(1,0) in terms of the negative-binomial distribution

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
#Rmcmc$samplerFunctions$contentsList[[1]]$run
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
debug(Rmcmc$run)
debug(samplerFunctions[[1]]$run)
Rmcmc$run(1)
set.seed(0)
Cmcmc$run(3)
Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)
Rsamples
Csamples
Rsamples - Csamples

apply(samples, 2, mean)

mean(samples[,1] > 0.5)
5/16

out <- MCMCsuite(
    code, constants, data, inits,
    MCMCs = c('nimble', 'nimble_noConj'),
    summaryStats = c('mean', 'median', 'sd', 'function(x) mean(x>0.5)'),
    calculateEfficiency = TRUE,
    makePlot = TRUE,
    savePlot = FALSE,
    niter = 100000
)

out$timing
out$summary
11/16


## creating a really small, blank figure, for use in captions in CR-MCMC paper
dev.new(width=3, height=2)
plot(1, 1)
dev.copy2pdf(file='~/GitHub/nimble/nimblePapers/CR-MCMC/blankFigure.pdf')

## testing the problem with naming a variable 'ans' in a nimbleFunction
library(nimble)

rcFunction1 <- nimbleFunction(
    run = function() {
        declare(ans, double(1, 5))
        for(i in 1:5) ans[i] <- i
        print(ans)
    })

rcFunction1()

rcFunction2 <- nimbleFunction(
    run = function() {
        declare(functionAsList, double(1, 5))
        for(i in 1:5) functionAsList[i] <- i
        print(functionAsList)
    })

rcFunction2()

code <- nimbleCode({
    mu ~ dnorm(0, 1)
    mu2 <- timesTwo(mu)
})

Rmodel <- nimbleModel(code, inits = list(mu=1), check=FALSE)

##undebug(calculate)
##debug(nimble:::rCalcNodes)
##Rmodel$nodes$mu2$calculate
##Rmodel$nodes$mu2$calculate()
##Rmodel$nodes$mu2$simulate
##Rmodel$nodes$mu2$simulate()

calculate(Rmodel)
Cmodel <- compileNimble(Rmodel)

Rmodel$mu
Rmodel$mu2
Cmodel$mu
Cmodel$mu2

Rmodel$mu <- 2
calculate(Rmodel)
Rmodel$mu2


## finding the posterior pairwise correlations of
## the blocked parameters for the orchid model
## (multistate CR-MCMC, userDistMCMC)

library(nimble)
rm(list=ls())
source('~/GitHub/userDistMCMC/defs.R')
load('~/GitHub/userDistMCMC/models.RData')

Rmodel <- nimbleModel(orchidDHMM$code, orchidDHMM$constants, orchidDHMM$data, orchidDHMM$inits)
spec <- configureMCMC(Rmodel)
spec$getSamplers()
spec$removeSamplers(numeric(0))
spec$removeSamplers(c('b[1:2]','c[1:2]'))
spec$addSampler(c('b[1]','b[2]'), 'RW_block')
spec$addSampler(c('c[1]','c[2]'), 'RW_block')
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
## n <- 10000
n <- 50000
system.time(Cmcmc$run(n)) / 60
samples <- as.matrix(Cmcmc$mvSamples)
samples2 <- samples[(n/2+1):n, ]   ## truncate to second half of samples
apply(samples2, 2, mean)
Cov <- cov(samples2)
Cov
Cor <- cov2cor(Cov)
Cor
sort(as.numeric(abs(Cor)))
Cor['b[1]', 'b[2]']
## [1] 0.9866091
Cor['c[1]', 'c[2]']
## [1] 0.9675666


## printing the numeric performance results for CR-MCMC paper, or userDistMCMC paper
load('~/GitHub/userDistMCMC/results.RData')
##load('~/GitHub/userDistMCMC/resultsNew.RData')
source('~/GitHub/userDistMCMC/defs.R')
options(digits = 3)
df <- results$df
for(mod in unique(df$model)) {
    dfmod <- df[df$model==mod, ]
    message('******************************************')
    message(mod, ' model')
    message('******************************************')
    for(mc in unique(dfmod$mcmc)) {
        dfmodmcmc <- dfmod[dfmod$mcmc==mc, ]
        message(mc, ' MCMC:')
        message('minimum: ',  min(dfmodmcmc$Efficiency))
        message('mean: ',    mean(dfmodmcmc$Efficiency))
    }
    message('******************************************')
}

df[df$model=='orchid',]
df[df$model=='goose',]

results$check()
results$quickplot()


## problem with multivariate conjugate samplers... ?
## fixed.  problem was in vector demoting in the conjugacy definition
library(nimble)
code <- nimbleCode({
    x[1:d] ~ dmnorm(mu[1:d], cov = Sigma[1:d, 1:d])
    y[1:d] ~ dmnorm(x[1:d], cov = Sigma[1:d, 1:d])
})
d <- 2
mu <- rep(0, d)
Sigma <- diag(d)
constants <- list(d=d, mu=mu, Sigma=Sigma)
Rmodel <- nimbleModel(code, constants, data=list(y=rep(0,d)), inits=list(x=rep(0, d)))
Cmodel <- compileNimble(Rmodel)

spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)
solve(cov(samples))

library(nimble)
nimble:::conjugateSamplerDefinitions$sampler_conjugate_dnorm
nimble:::conjugateSamplerDefinitions$sampler_conjugate_dbeta
nimble:::conjugateSamplerDefinitions$sampler_conjugate_dmnorm



## testing different priors for rho in package gpmanagement
library(gpmanagement)
library(ggplot2)
example_data <- function(p, x0, Tobs){
    f <- function (x, h){
        sapply(x, function(x) {
            x <- pmax(0, x - h)
            x * exp(p[1] * (1 - x/p[2]) * (x - p[3])/p[2])
        })
    }
    sigma_g <- 0.1
    pdfn <- function(x, mu, sigma = sigma_g){
        dlnorm(x, log(mu), sdlog = sigma)
    }
    z_g <- function() rlnorm(1, 0, sigma_g)
    x <- numeric(Tobs)
    x[1] <- x0
    for(t in 1:(Tobs-1))
        x[t+1] = z_g() * f(x[t], h=0)
    obs <- data.frame(x = c(0, 
                          pmax(rep(0,Tobs-1), x[1:(Tobs-1)])), 
                      y = c(0, 
                          x[2:Tobs]))
    obs
}
myfit <- function(xObs, yObs, priors = NULL, niter = 100000){    ## default value for priors
  xPred <- seq(0, 1.1 * max(xObs), length = 50)
  fit <- gp_setup(xObs, yObs, xPred, priors)     ## included priors argument
  Cmcmc <- fit$Cmcmc 
  Cpred <- fit$Cpred
  Cmodel <- fit$Cmodel
  system.time(Cmcmc$run(niter))
  samples <- as.matrix(Cmcmc$mvSamples)
  system.time(Cpred$run(samples))
  E <- Cpred$getE()
  V <- sqrt(diag(Cpred$getC()))
  list(samples = tidyr::gather(as.data.frame(samples)),
       pred = data.frame(x = xPred, y = E, ymin = E - V, ymax = E + V))
}

densPlot <- function(fit, paramName, paramPrior) {
        dev.new()
        samp <- fit$samples[fit$samples$key==paramName, 'value']
        plot(density(samp), main=paste0('black posterior, red prior: ', paramName, ' ~ ', deparse(paramPrior)))
        xs <- seq(range(samp)[1], range(samp)[2], length.out = 500)
        yCall <- as.call(c(list(as.name(paramPrior[[1]])), list(quote(xs)), as.list(paramPrior[-1])))
        ys <- eval(yCall, envir = environment())
        lines(xs, ys, col='red')
}

## Carrying capcacity at 10, critical point at 5, x0=6
## 100, 50, x0=60
plotPred <- function(ccap, crit, x0, Tobs=40, plotData=FALSE, rhoPrior, plotRhoDensity=TRUE, niter=100000, plotSigGPDensity=FALSE, plotSigOEDensity=FALSE, plotPredictionIntervals=TRUE) {
    set.seed(0)
    obs <- example_data(p=c(2,ccap,crit), x0=x0, Tobs=Tobs)
    if(plotData) {dev.new(); qplot(seq_along(obs$x[-1]), obs$x[-1]) + geom_line()}
    priors <- expression({
        rho ~ X
        sigGP ~ dunif(0, 1e5)
        sigOE ~ dunif(0, 1e5)
    })
    if(missing(rhoPrior)) stop('must specify a prior for rho')
    rhoPrior[-1] <- lapply(rhoPrior[-1], function(param)
        eval(eval(substitute(substitute(PARAM, list(xObs=obs$x[-1])), list(PARAM=param)))))
    print(rhoPrior)
    priors[[1]][[2]][[3]] <- rhoPrior
    fit <- myfit(obs$x, obs$y, priors = priors, niter=niter)
    if(plotRhoDensity) densPlot(fit, 'rho', rhoPrior)
    if(plotSigGPDensity) densPlot(fit, 'sigGP', priors[[1]][[3]][[3]])
    if(plotSigOEDensity) densPlot(fit, 'sigOE', priors[[1]][[4]][[3]])
    if(plotPredictionIntervals) {
        dev.new()
        print(ggplot2::ggplot(fit$pred) + 
                  geom_ribbon(aes(x = x, y = y, ymin = ymin, ymax = ymax), fill = "grey80") +
                      geom_line(aes(x = x, y = y), size = 1) + 
                          geom_point(data = obs, aes(x, y)) +
                              coord_cartesian(xlim = range(c(obs$x, fit$pred$x)), 
                                              ylim = range(c(obs$y, fit$pred$y))) +
                                                  ggtitle(paste0('rho ~ ', deparse(rhoPrior))))
    }
}

rp <- quote(dunif(0, diff(range(xObs))/sqrt(6)/2))

plotPred(rhoPrior = rp, ccap= 10, crit= 5, x0= 6)
plotPred(rhoPrior = rp, ccap=100, crit=50, x0=60)

plotPred(rhoPrior = rp, ccap=100, crit=50, x0=60, plotSigGPDensity=TRUE, plotSigOEDensity=TRUE)

plotPred(rhoPrior = quote(dunif(0,200)), ccap=10, crit=5, x0=6)

for(uMax in 5:6) {
    rp <- substitute(dunif(0, UMAX), list(UMAX = as.numeric(uMax)))
    plotPred(rhoPrior = rp, ccap=10, crit=5, x0=6, niter=10000)
}



## testing ggplot() inside a loop??
library(ggplot2)
df <- data.frame(a=1:2, b=3:4)
for(i in 1:3) {
    dev.new()
    ## need explict print(...) to make ggplot() plot, when inside a loop!!!
    print(ggplot(df, aes(a, b)) + geom_line() + ggtitle(i))
}



## testing use of substitute...
## which I used to be more comfortable with!

f <- function(a) {
    b <- substitute(a)
    print(b)
    print(class(b))
    vv <- eval(b)
    print(vv)
    print(class(vv))
}

f(dnorm(0,1))
f(4+5)

x <- f(quote(dnorm(0,1)))
x
class(x)


## testing NIMBLE compiler to handle pi
nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        a <- pi
        print(a)
        b <- 2 * pi
        print(b)
    }
)
Rnf <- nfDef()
Rnf$run()
Cnf <- compileNimble(Rnf)    ## ERROR
Cnf$run()


## example of trying to use NIMBLE functions in another package
## this uses my minimalist nimtest package; same thing I sent to Duncan
library(devtools)
install_github('danielturek/nimtest')
library(nimtest)
library(nimble)
## First Test:
## a nimbleFunction defined as a package function
nfDefinition
Rnf <- nfDefinition()
Cnf <- compileNimble(Rnf)
Rnf$run()
Cnf$run()
## Second Test:
## A package function which internally defines and returns a nimbleFunction definition
## This one gives ugly looking warnings
nextFunction
def <- nextFunction()        ## WARNINGS
Rnf <- def()
Cnf <- compileNimble(Rnf)    ## WARNINGS
Rnf$run()
Cnf$run()

## alternate.  from shell:
## R CMD BUILD nimtest
## R CMD install nimtest_0.0.0.9000.tar.gz
library(nimtest)
library(nimble)
nfDefinition    ## finds it -- it created the NIMBLE function at build time
Rnf <- nfDefinition()  ## can't use it
Cnf <- compileNimble(Rnf)
Rnf$run()
Cnf$run()
nextFunction
nextFunction()
def <- nextFunction()
Rnf <- def()  ## can't use it
Cnf <- compileNimble(Rnf)
Rnf$run()
Cnf$run()


## testing out the new elliptical slice sampler (ess)
library(nimble)
code <- nimbleCode({
    x[1:d] ~ dmnorm(mu[1:d], cov = Sigma[1:d, 1:d])
})
d <- 2
##mu <- rep(0, d)
mu <- 1:d
##Sigma <- diag(d);  print(Sigma)
ch <- array(c(1, 0.7, 0, 2), c(2,2));  Sigma <- ch %*% t(ch);  print(Sigma)
constants <- list(d=d, mu=mu, Sigma=Sigma)
data <- list()
inits <- list(x = rep(1, d))
Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)
spec <- configureMCMC(Rmodel, nodes = NULL)
spec$addSampler('x', 'ess', print=FALSE)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0); Cmcmc$run(20000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)
cov(samples)   ## empirical cov seems WRONG!  about half of what it should be?
Sigma


## conjugate MVN-MVN
set.seed(1)
library(nimble)
code <- nimbleCode({
    x[1:d] ~ dmnorm(mu_x[1:d], prec = prec_x[1:d, 1:d])
    y[1:d] ~ dmnorm(x[1:d], prec = prec_y[1:d, 1:d])
})
d <- 3
mu_x <- rnorm(d)
temp <- array(rnorm(d^2), c(d,d))
prec_x <- solve(temp %*% t(temp))
temp <- array(rnorm(d^2), c(d,d))
prec_y <- solve(temp %*% t(temp))
y <- rnorm(d)
constants <- list(d = d, mu_x = mu_x, prec_x = prec_x, prec_y = prec_y)
data <- list(y = y)
inits <- list(x = rep(0, d))
Rmodel <- nimbleModel(code, constants, data, inits)
Cmodel <- compileNimble(Rmodel)

## block sampling
spec <- configureMCMC(Rmodel, nodes = NULL)
spec$addSampler('x', 'RW_block', print = FALSE)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0); Cmcmc$run(100000)
samples <- as.matrix(Cmcmc$mvSamples)
solve(prec_x + prec_y)
cov(samples)   ## empirical cov seems WRONG!  about half of what it should be?
solve(prec_x + prec_y) %*% (prec_y %*% y + prec_x %*% mu_x)
apply(samples, 2, mean)

## ess sampling
spec <- configureMCMC(Rmodel, nodes = NULL)
spec$addSampler('x', 'ess', print = FALSE)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
set.seed(0); Cmcmc$run(100000)
samples <- as.matrix(Cmcmc$mvSamples)
solve(prec_x + prec_y)
cov(samples)   ## empirical cov seems WRONG!  about half of what it should be?
solve(prec_x + prec_y) %*% (prec_y %*% y + prec_x %*% mu_x)
apply(samples, 2, mean)



## testing elliptical slice sampler on 'spatial' model
library(nimble)
load('~/GitHub/legacy/autoBlock/data/model_spatial.RData')

out <- MCMCsuite(
    code, constants, data, inits,
    MCMCs = c('block', 'ess'),
    MCMCdefs = list(
        block = quote({
            spec <- configureMCMC(Rmodel, nodes = NULL)
            spec$addSampler('mu', 'RW', print=FALSE)
            spec$addSampler(c('sigma', 'rho'), 'RW_block', print=FALSE)
            spec$addSampler('g[1:148]', 'RW_block', print=FALSE)
            spec
        }),
        ess = quote({
            spec <- configureMCMC(Rmodel, nodes = NULL)
            spec$addSampler('mu', 'RW', print=FALSE)
            spec$addSampler(c('sigma', 'rho'), 'RW_block', print=FALSE)
            spec$addSampler('g[1:148]', 'ess', print=FALSE)
            spec
        })),
    niter = 10000,
    burnin = 2000,
    monitors = c('mu', 'sigma', 'rho', 'g'),
    makePlot = FALSE
)

out$timing
out$summary[,,1:5]    ## WRONG again...
cov(t(out$samples['block', -(1:3), ])) - cov(t(out$samples['ess', -(1:3), ]))






## Taken from Carl's lab notebook:
## Gpdp Via MDPtoolbox Cont; 30 Jul 2015
## http://www.carlboettiger.info/2015/07/30/gpdp-via-mdptoolbox-cont.html
## (Markov Decision Processes (MDP) toolbox)
##knitr::opts_chunk$set(eval=FALSE)
##devtools::install_github("cboettig/gpmanagement@b3b765cbceb51c9b0b8cb2724e395353ec365df9")
library("MDPtoolbox")
library("gpmanagement")
library("tidyr")
library("dplyr")
library("ggplot2")

## True model
p <- c(2, 100, 50)
f <- function (x, h){
    sapply(x, function(x) {
        x <- pmax(0, x - h)
        x * exp(p[1] * (1 - x/p[2]) * (x - p[3])/p[2])      #### Allen model
    })
}

sigma_g <- 0.1
pdfn <- function(x, mu, sigma = sigma_g){
    dlnorm(x, log(mu), sdlog = sigma)
}

z_g <- function() rlnorm(1, 0, sigma_g)
set.seed(0)
Tobs <- 40

x <- numeric(Tobs)
x[1] <- 60
for(t in 1:(Tobs-1)) x[t+1] = z_g() * f(x[t], h=0)
obs <- data.frame(x = c(0, pmax(rep(0,Tobs-1), x[1:(Tobs-1)])), 
                  y = c(0, x[2:Tobs]))
xObs <- obs$x
yObs <- obs$y
xPred <- seq(0, 1.1 * max(xObs), length = 50)
qplot(seq_along(x), x) + geom_line()

## Now the GP estimation from NIMBLE. Letâ€™s emphasize shorter length-scales with the prior to compare:

## having this work with output from `nimbleCode` might be more natural than `expression`
priors <- expression({
  rho ~ dgamma(1, 1)
  sigGP ~ dunif(0, 1e5)
  sigOE ~ dunif(0, 1e5)
})

fit <- gp_setup(xObs, yObs, xPred)

Cmcmc <- fit$Cmcmc 
Cpred <- fit$Cpred
Cmodel <- fit$Cmodel
system.time(Cmcmc$run(100000))
samples <- as.matrix(Cmcmc$mvSamples)
## basic sanity check
testthat::expect_identical(Cmodel$getNodeNames(topOnly = TRUE), colnames(samples))

## predict from GP model using posterior MCMC samples
system.time(Cpred$run(samples))

## Posteriors
samples <- as.data.frame(as.matrix(Cmcmc$mvSamples))
df <- tidyr::gather(samples)

ggplot(df) + 
  geom_density(aes(value)) + 
  facet_wrap(~key, scale='free')

## extract predictions: E and C
E <- Cpred$getE()
C <- Cpred$getC()

obs <- data.frame(x = xObs, y = yObs)
pred <- data.frame(x = xPred, y = E, ymin = E - sqrt(diag(C)), ymax = E + sqrt(diag(C)))

ggplot2::ggplot(pred) + 
  geom_ribbon(aes(x = x,y = y, ymin = ymin, ymax = ymax), fill = "grey80") +
  geom_line(aes(x = x, y = y), size=1) + 
  geom_point(data = obs, aes(x,y)) +
  coord_cartesian(xlim = range(c(xObs, xPred)), ylim = range(c(yObs,E))) +
  theme_bw()

## Decision theory
states <- xPred # Vector of all possible states
actions <- states # Vector of actions: harvest
## Letâ€™s consider a slight variation of the most trivial utility function:
## one which explicitly adds a cost to completely exhausting the stock
## (or reducing the stock by more than, say 95% in this case.)
## This should be somewhat similar to the impact of no discount rate.

## Utility function
discount = 0.99

#get_utility <- function(x,h) pmin(x,h)
#R <- outer(states, actions, get_utility)

R <- sapply(actions, function(h){
    sapply(states, function(x){
        if(h < x) h else - 1 * max(states)
    })
})

## Implementing policy
z <- function() rlnorm(1, meanlog = 0, sdlog = sigma_g)

simulate_policy <- function(states, actions, policy, f, z, s0, steps = 50, utility = function(s,a) NA, discount = 1){
    s <- numeric(steps)
    a <- numeric(steps)
    u <- numeric(steps)
    s[1] <- s0
    for(t in 1:(steps-1)){
        a[t] <- actions[policy[which.min(abs(states - s[t]))]]
        s[t+1] <- z() * f(s[t], a[t])
        u[t] <- utility(s[t], a[t]) * discount ^ t
    }
    ## Final action determined but not implemented
    a[steps] <- actions[policy[which.min(abs(states - s[t]))]]
    data.frame(time = 1:steps, state = s, action = a, utility = u)
}

## GP model
gp_matrix <- function(states, actions, E, C){
    transition <- array(0, dim = c(length(states), length(states), length(actions)))
    K <- length(states)
    sigmas <- sqrt(diag(C))
    for (k in 1:length(states)) {
        for (i in 1:length(actions)) {
            nextpop <- E[k] - actions[i]
            if(nextpop <= 0) {
                transition[k, , i] <- c(1, rep(0, K - 1))
            } else {
                transition[k, , i] <- dnorm(states, nextpop, sigmas[i]) / sum(dnorm(states, nextpop, sigmas[i]))
            }
        }
    }
    transition
}

P_gp <- gp_matrix(states, actions, E, C)
mdp_check(P = P_gp, R = R)

gp <- mdp_value_iteration(P_gp, R, discount = discount, epsilon = 0.00001, max_iter = 5e3, V0 = numeric(length(states)))

plot(states, states - actions[gp$policy],  xlab="Population size", ylab="Escapement")

data.frame(reps = 1:50) %>% 
    group_by(reps) %>% 
        do(simulate_policy(states, actions, gp$policy, f, z, s0 = 100, steps = 20, utility = pmin, discount = discount)[c("time", "state", "utility")]) -> sims 

mean(sims$utility)
ggplot(sims) + geom_line(aes(time, state, group = reps), alpha = 0.3, col = "darkblue")
## With this amount of data, the gp solution is too cautious, and avoids any exploitation.

## Simulate under the true model

data.frame(reps = 1:50) %>% 
    group_by(reps) %>% 
        do(simulate_policy(states, actions, gp$policy, f, z, s0 = 100, steps = 20, utility = pmin, discount = discount)[c("time", "state", "utility")]) -> sims 

mean(sims$utility)
## (Average utility is approximate here since it does not include penalty;
## since a function and not a matrix is requred by this function at this time.)

ggplot(sims) + geom_line(aes(time, state, group = reps), alpha = 0.3, col = "darkblue")

P <- transition_matrix(states, actions, f, pdfn)
mdp_check(P = P, R = R)

mdp <- mdp_value_iteration(P, R, discount = discount, epsilon = 0.001, max_iter = 5e3, V0 = numeric(length(states)))
plot(states, states - actions[mdp$policy],  xlab="Population size", ylab="Escapement")

## Note that the altered award structure has almost no effect on the optimal policy
## given the true model, other than to avoid harvesting directly to zero even when
## the stock cannot persist, due to the explicit penalty for doing so.

data.frame(reps = 1:50) %>% 
    group_by(reps) %>% 
        do(simulate_policy(states, actions, mdp$policy, f, z, s0 = 100, steps = 20, utility = pmin, discount = discount)[c("time", "state", "utility")]) -> sims 

mean(sims$utility)

ggplot(sims) + geom_line(aes(time, state, group = reps), alpha = 0.3, col = "darkblue")





## Taken from Carl's lab notebook:
## MDPtoolbox Allen Model; 29 Jul 2015
## http://www.carlboettiger.info/2015/07/29/mdptoolbox-allen-model.html
## (Markov Decision Processes (MDP) toolbox)
library("MDPtoolbox", quietly = TRUE)
library("ggplot2", quietly = TRUE)
K <- 10 # state space limit
states <- 0:K # Vector of all possible states
actions <- states # Vector of actions: harvest

sigma_g = 0.1
p <- c(2, 15, 5)

f <- function (x, h){
    sapply(x, function(x) {
        x <- pmax(0, x - h)
        x * exp(p[1] * (1 - x/p[2]) * (x - p[3])/p[2])      #### Allen model
    })
}

pdfn <- function(x, mu, sigma = sigma_g){
    dlnorm(x, log(mu), sdlog = sigma)
}

## Utility function
discount = 0.95
get_utility <- function(x,h) {
    pmin(x,h)
}

R <- outer(states, actions, get_utility)

transition_matrix <- function(states, actions, f, pdfn){
    ## Initialize
    transition <- array(0, dim = c(length(states), length(states), length(actions)))
    K <- length(states)
    for (k in 1:length(states)) {
        for (i in 1:length(actions)) {
            ## Calculate the transition state at the next step, given the 
            ## current state k and action i (harvest H[i])
            nextpop <- f(states[k], actions[i])
            ## Population always extinct if this is negative.
            ## since multiplicitive shock z_t * f(n) < 0 for all f(n) < 0
            if(nextpop <= 0)
                transition[k, , i] <- c(1, rep(0, length(states) - 1))
            ## Implement demographic stochasticity 
            else {
                ## Cts distributions need long-tailed denominator as normalizing factor:
                fine_states <- seq(min(states), 10 * max(states), by = states[2] - states[1])
                N <- sum(pdfn(fine_states, nextpop))  
                transition[k, , i] <-pdfn(states, nextpop) / N
                ## We need to correct this density for the final capping state ("Pile on boundary")
                ## (discrete or cts case)
                ## this can be a tiny but negative value due to floating-point errors.
                ## so we take max(v,0) to avoid
                transition[k, K, i] <- max(1 - sum(transition[k, -K, i]), 0)
            }
        } 
    }
    transition
}

P <- transition_matrix(states, actions, f, pdfn)
apply(P, c(1,3), sum)  ## double-check

## Using toolbox
mdp_check(P = P, R = R)
mdp <- mdp_value_iteration(P, R, discount = discount, epsilon = 0.001, max_iter = 5e3, V0 = numeric(length(states)))
plot(states, states - actions[mdp$policy],  xlab="Population size", ylab="Escapement")

## Compare to Reed
## From Reed (1979) we know that the optimal solution is a constant-escapement rule
## when the growth function in convex. Note that this condition is violated by the
## growth function with alternative stable states (Allen/Ricker-Allee model),
## resulting in a very different optimal policy:
## f^prime(s^star) = 1/discount
## For growth-rate function f, where discount is the discount factor and sâˆ— the stock size
## for the constant escapement. Analytic solutions are clearly possible for certain
## growth functions, but here Iâ€™ve just implemented a generic numerical solution.
fun <- function(x) - f(x,0) + x / discount
out <- optimize(f = fun, interval = c(0,K))
S_star <- out$minimum
exact_policy <- sapply(states, function(x) if(x < S_star) 0 else x - S_star)
D <- actions[mdp$policy]
# The difference between Bellman and the analytical solution is small:
plot(states, states - D,  xlab="Population size", ylab="Escapement", ylim = c(0, 1.2*max(states-D)))
lines(states, states - exact_policy)




## Carl's examples using MDPtoolbox
## MDPtoolbox Ex 2; 14 Jul 2015
## (Markov Decision Processes (MDP) toolbox)
## http://www.carlboettiger.info/2015/07/14/mdptoolbox-ex-2.html
## Adapted from Marescot et al. appendix 5, to Reed optimal control problem, including direct comparison against (semi) analytic optimum.

## step 1: define objectives
## This is a conceptual step which does not require coding
## step 2: define states
##K <- 150 # state space limit
K <- 10 # state space limit
states <- 0:K # Vector of all possible states

## step 3: define control actions
## Vector of actions: harvest
H <- states

## step 4: define dynamic model (with demographic parameters)
##p <- c(6,0.05)
p <- c(3, 0.2)
f <- function(x, h){
  A <- p[1] 
  B <- p[2] 
  s <- pmax(x-h, 0)
  A * s/(1 + B * s)   #### Bellman equation
}
sigma_g = 0.1

## step 5: define utility
## Utility function
get_utility <- function(x,h) {
    pmin(x,h)
}

## step 6: solve bellman equation with value iteration
## Initialize transition matrix
transition <- array(0, dim = c(length(states), length(states), length(H)))
## Initialize utility matrix
utility <- array(0, dim = c(length(states), length(H)))
## Fill in the transition and utility matrix
## Loop on all states
for (k in 0:K) {
    ## Loop on all actions
    for (i in 1:length(H)) {
        ## Calculate the transition state at the next step, given the 
        ## current state k and the harvest H[i]
        nextpop <- f(k, H[i])
        if(nextpop <= 0)
            transition[k+1, , i] <- c(1, rep(0, length(states) - 1))
        ## Implement demographic stochasticity by drawing probability from a density function
        else {
            ## We need to correct this density for the final capping state ("Pile on boundary").
            ## For discrete probability distribution,
            ## this is easy if `states` includes all possible
            ## discrete states below the capping state.
            ## (e.g. all non-negative integers less than K).  
            ## For a continuous distribution, this is more problematic
            ## as we have to first normalize the densities.
            ## EDIT: this can be negative, due to floating-point errors.
            ## so we take max(v,0) to avoid.
            ## Get long-tailed denominator as normalizing factor (continuous distributions only):
            fine_states <- seq(min(states), 10 * max(states), by = states[2]-states[1])
            N <- sum(dlnorm(fine_states, log(nextpop), sdlog = sigma_g))
            transition[k+1, , i] <- dlnorm(states, log(nextpop), sdlog = sigma_g) / N
            ## We need to correct this density for the final capping state ("Pile on boundary")
            transition[k+1, K+1, i] <- max(1 - sum(transition[k+1, -(K+1), i]), 0)
        }
        ## Compute utility
        utility[k+1, i] <- get_utility(k, H[i])
    } # end of action loop
} # end of state loop

## Solution calculated explicitly:
## The backward iteration consists in storing action values in the vector
## Vt which is the maximum of utility plus the future action values for all
## possible next states. Knowing the final action values, we can then backwardly
## reset the next action value Vtplus to the new value Vt.  We start The backward
## iteration at time T-1 since we already defined the action value at Tmax.

## Discount factor
discount <- 0.95
## Action value vector at tmax
Vtmax <- numeric(length(states))
## Action value vector at t and t+1
Vt <- numeric(length(states))
Vtplus <- numeric(length(states))
## Optimal policy vector
D <- numeric(length(states))
## Time horizon
##Tmax <- 150
Tmax <- 4

for (t in (Tmax - 1):1) {
    ## We define a matrix Q that stores the updated action values for 
    ## all states (rows)
    ## actions (columns)
    Q <- array(0, dim = c(length(states), length(H)))
    for (i in 1:length(H)) {
        ## For each harvest rate we fill for all states values (row) 
        ## the ith column (Action) of matrix Q
        ## The utility of the ith action recorded for all states is 
        ## added to the product of the transition matrix of the ith 
        ## action by the action value of all states 
        Q[,i] <- utility[, i] + discount * (transition[,,i] %*% Vtplus)
    } # end of the harvest loop
    ## Find the optimal action value at time t is the maximum of Q
    Vt <- apply(Q, 1, max)
    ## After filling vector Vt of the action values at all states, we 
    ## update the vector Vt+1 to Vt and we go to the next step standing 
    ## for previous time t-1, since we iterate backward
    Vtplus <- Vt
} # end of the time loop

## Find optimal action for each state
for (k in 0:K) {
    ## We look for each state which column of Q corresponds to the 
    ## maximum of the last updated value 
    ## of Vt (the one at time t + 1). If the index vector is longer than 1 
    ## (if there is more than one optimal value we chose the minimum 
    ## harvest rate)
    D[k + 1] <- H[(min(which(Q[k + 1, ] == Vt[k + 1])))]
}

## plot solution
plot(states, states - D, xlab="Population size", ylab="Escapement")

## proof of optimality: compare with analytical solution
fun <- function(x) - f(x,0) + x / discount
out <- optimize(f = fun, interval = c(0,K))
S_star <- out$minimum
exact_policy <- sapply(states, function(x) if(x < S_star) 0 else x - S_star)

## The difference between Bellman equation solution and the analytical solution is small:
plot(states, states - D, xlab="Population size", ylab="Escapement", ylim = c(0, 1.2*max(states-D)))
lines(states, states - exact_policy)

## Using MDPtoolbox
library('MDPtoolbox')
mdp_check(P = transition, R = utility)
out <- mdp_value_iteration(transition, utility, discount = discount, epsilon = 0.001, max_iter = 5e3, V0 = Vtmax)
plot(states, states - D, xlab="Population size", ylab="Escapement")
lines(states, states - H[out$policy], col="red", lty=2)




## testing using new NIMBLE option: verbose
nimbleOptions('verbose')
nimbleOptions(verbose = FALSE)
nimbleOptions('verbose')

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)

Rmodel <- nimbleModel(code, constants, data, inits)


## testing the new includeEfficiency option for MCMCsuite
## calculates N, ESS, and Eff

library(nimble)
load('~/GitHub/legacy/autoBlock/data/model_litters.RData')
load('~/GitHub/legacy/autoBlock/data/model_ice.RData')

out <- MCMCsuite(code, constants, data, inits,
##                 MCMCs = c('nimble', 'nimble_slice', 'autoBlock'),
                 MCMCs = c('nimble', 'nimble_slice'),
                 makePlot = FALSE,
                 calculateEfficiency = TRUE
                 )

out$timing

out$summary

apply(out$summary[, 'efficiency', ], 1, min)
apply(out$summary[, 'efficiency', ], 1, mean)

out$efficiency


## running OpenBUGS installation from OSX (using wine)

##library(R2OpenBUGS)
library(R2WinBUGS)
library(BRugs)

## schools data in the R2OpenBUGS library
data(schools)
schools

## define the model
nummodel <- function() {
    for (j in 1:J) {
        y[j] ~ dnorm(theta[j], tau.y[j])
        theta[j] ~ dnorm(mu.theta, tau.theta)
        tau.y[j] <- pow(sigma.y[j], -2)
    }
    mu.theta ~ dnorm(0, 1E-6)
    tau.theta <- pow(sigma.theta, -2)
    sigma.theta ~ dunif(0, 1000)
}

## write the model code out to a file
write.model(nummodel, 'nummodel.txt')
model.file1 = paste(getwd(), 'nummodel.txt', sep='/')
## and let's take a look:
file.show('nummodel.txt')

## prepare the data for input into OpenBUGS
J <- nrow(schools)
y <- schools$estimate
sigma.y <- schools$sd
data <- list(J = J, y = y, sigma.y = sigma.y)

## initialization of variables
inits <- function() {
    list(theta = rnorm(J, 0, 100), mu.theta = rnorm(1, 0, 100), sigma.theta = runif(1, 0, 100))
}

## set the WINE working directory and the directory to OpenBUGS
## change the OpenBUGS.exe location as necessary
WINE <- '/opt/local/bin/wine'
WINEPATH <- '/opt/local/bin/winepath'
##OpenBUGS.pgm <- '/Users/dturek/.wine/drive_c/Program Files/OpenBUGS/OpenBUGS323/OpenBUGS.exe'
bugs.directory <- '/Users/dturek/.wine/drive_c/Program Files/OpenBUGS/OpenBUGS323'

## these are the parameters to save
parameters = c('theta', 'mu.theta', 'sigma.theta')

## run the model
schools.sim <- bugs(data, inits, model.file = model.file1, parameters=parameters,
                    n.chains = 3, n.iter = 1000,
                    ##OpenBUGS.pgm = OpenBUGS.pgm,   ## syntax for R2OpenBUGS::bugs
                    program = 'OpenBUGS',
                    bugs.directory = bugs.directory,
                    WINE = WINE, WINEPATH = WINEPATH, useWINE = TRUE)

## R will pause
## When model is complete a prompt will reappear
print(schools.sim)


## debugging commands in R
## n: next
## c: continue
## s: step into function call
## f: step out of current function call



library(nimble)
modelName <- 'rats'
rats <- readBUGSmodel(model = modelName, dir = getBUGSexampleDir(modelName), returnModelComponentsOnly = TRUE)

code <- rats$model
constants <- rats$data[c('N','T','x')]
data <- rats$data[c('Y')]
inits <- rats$inits

Rmodel <- nimbleModel(code, constants, data, inits, dimensions = rats$dims)

spec <- configureMCMC(Rmodel)
spec$getSamplers()




## creating dendogram figure of autoBlocking for JSM presentation
library(nimble)
load('~/GitHub/legacy/autoBlock/data/model_litters.RData')
Rmodel <- nimbleModel(code, constants, data, inits)
Rmcmc <- buildMCMC(Rmodel, autoBlock = TRUE, makePlots = TRUE)




## working on the conjugacy system, to infer dimensions of 'contributions'
library(nimble)
getDistributionsInfo('dnorm')



## let's manually run all NIMBLE testing
## (to find what's crashing, easier than using travis)
library(nimble)
library(testthat)
setwd('~/GitHub/nimble/nimble/packages/nimble/inst/tests/')
source('test_utils.R')

testFiles <- list.files()
print(testFiles)

##### this listing is out of date!
test_package('nimble', 'copy')      ## pass
test_package('nimble', 'dsl_dists') ## pass
test_package('nimble', 'math')      ## pass
test_package('nimble', 'mcmc')      ## errors/warnings copied below
test_package('nimble', 'meta')      ## pass
test_package('nimble', 'models')    ## 
test_package('nimble', 'trunc')     ## 
test_package('nimble', 'user')      ## 



## fully rebuild NIMBLE user manual

setwd('~/GitHub/nimble/nimble-docs/UserManual/')
library(knitr) 
knit2pdf('NimbleUserManual.Rnw') 
system('open NimbleUserManual.pdf')


## investigating the bug in R M-H sampler

library(nimble)
code <- nimbleCode({
    a ~ dnorm(0, 1)
})
constants <- list()
data <- list()
inits <- list(a = 0)
Rmodel <- nimbleModel(code, constants, data, inits)
spec <- configureMCMC(Rmodel, nodes = NULL)
spec$addSampler('a', 'RW')
Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
niter <- 1000

set.seed(0); Rmcmc$run(niter)
set.seed(0); Cmcmc$run(niter)

Rsamples <- as.matrix(Rmcmc$mvSamples)
Csamples <- as.matrix(Cmcmc$mvSamples)

Rsamples
Csamples

d <- Rsamples - Csamples
all(d == 0)



Rmodel$nodes[['a']]$calculateDiff

Rmodel$logProb_a
Cmodel$logProb_a

calculateDiff(Rmodel, 'a')
calculateDiff(Cmodel, 'a')


nimble:::rCalcDiffNodes(Rmodel, 'a')
nimble:::rCalcDiffNodes(Cmodel, 'a')


## testing nimbleOptions() system
## and also the new nimble option: MCMCcontrolDefaultList

library(nimble)

getNimbleOption('MCMCcontrolDefaultList')

environment(sampler_conjugate_dnorm)$nfRefClassDef

getNimbleOption('verifyConjugatePosteriors')
nimble:::setNimbleOption('verifyConjugatePosteriors', TRUE)
getNimbleOption('verifyConjugatePosteriors')
environment(sampler_conjugate_dnorm)$nfRefClassDef
buildConjugateSamplerFunctions()
environment(sampler_conjugate_dnorm)$nfRefClassDef


code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dgamma(a, 2)
    c ~ dbin(b, 10)
    d ~ dnorm(c, 1)
})
constants <- list()
data <- list()
inits <- list(a = 1, b = .5, c = 4, d = 1)

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()

lst <- getNimbleOption('MCMCcontrolDefaultList')
lst
lst$sliceWidth <- 99
lst$scale <- 9999
lst
nimbleOptions(MCMCcontrolDefaultList = lst)
getNimbleOption('MCMCcontrolDefaultList')

spec <- configureMCMC(Rmodel)
spec$getSamplers()


## complete example of using RW_llFunction sampler,
## for the User Manual

library(nimble)

code <- nimbleCode({
    p ~ dunif(0, 1)
    y ~ dbin(p, n)
})

Rmodel <- nimbleModel(code, data = list(y = 3),
                      inits = list(p = 0.5, n = 10))

llFun <- nimbleFunction(
    setup = function(model) { },
    run = function() {
        y <- model$y
        p <- model$p
        n <- model$n
        ll <- lfactorial(n) - lfactorial(y) - lfactorial(n-y)
                 + y*log(p) + (n-y)*log(1-p)
        returnType(double())
        return(ll)
    }
)

RllFun <- llFun(Rmodel)

mcmcspec <- configureMCMC(Rmodel, nodes = NULL)

mcmcspec$addSampler(target = 'p', type = 'RW_llFunction',
    control = list(llFunction = RllFun, includesTarget = FALSE))

Rmcmc <- buildMCMC(spec)





## testing new nimble options system

library(nimble)
nimbleOptions()
ls(nimble:::.nimbleOptions)

code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dgamma(a, 1)
    c ~ dbin(b, 10)
    d ~ dnorm(c, 1)
})
constants <- list()
data <- list()
inits <- list(a = 1, b=.5, c=1, d=0)

Rmodel <- nimbleModel(code, constants, data, inits)

spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)



## working on dipper models

rm(list=ls())
library(nimble)
trunc <- TRUE
load('~/GitHub/userDistMCMC/dipperData.RData')
## optionally truncate data:
last <- apply(y, 1, function(hist) max(which(hist==1)))
yDHMM <- 2 - y
onesMatrix = matrix(1,nind,k)  ## 'ones' matrix for survival
onesVector = rep(1,nind)       ## 'ones' vector for chi
if(trunc) { ind <- 1:3;   nind<-length(ind);   first<-first[ind];   last<-last[ind];   y<-y[ind,,drop=FALSE];   yDHMM<-yDHMM[ind,,drop=FALSE];   x_init<-x_init[ind,,drop=FALSE];   onesMatrix<-onesMatrix[ind,,drop=FALSE];   onesVector<-onesVector[ind] }
code <- quote({
    phi ~ dunif(0, 1)
    p ~ dunif(0, 1)
    for(i in 1:nind) {
        for(t in (first[i] + 1):last[i]) {
            y[i,t] ~ dbin(p, 1)
            ##onesMatrix[i,t] ~ dbin(phi, 1)
        }
        ##onesVector[i] ~ dbin(chi[last[i]], 1)
    }
    ##chi[k] <- 1
    ##for(t in 1:(k-1)) {
    ##chi[k-t] <- (1-phi) + phi * (1-p) * chi[k-t+1]
    ##}
})
constants <- list(k=k, nind=nind, first=first, last=last)
data      <- list(y=y)##, onesMatrix=onesMatrix, onesVector=onesVector)
inits     <- list(phi=0.6, p=0.9)

md <- nimbleModel(code, constants, data, inits, returnDef = TRUE)


Rmodel <- nimbleModel(code, constants, data, inits)



## reproducible example of problem in new igraph package
## emailed this example to Gabor


remove.packages('igraph')
install.packages('igraph')

library(nimble)
code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ dnorm(a, 1)
})
constants <- list()
data <- list()
inits <- list(a=0, b=0)

Rmodel <- nimbleModel(code, constants, data, inits)


library(igraph)
graph <- graph.empty()
graph <- add.vertices(graph, 2, name = c('a', 'b'))
graph <- add.edges(graph, c(1, 2))
toposortReturn <- topological.sort(graph, mode = 'out')
print(toposortReturn)
class(toposortReturn)



oldGraphID_2_newGraphID <- sort(newGraphID_2_oldGraphID, index = TRUE)$ix
graph <<- permute.vertices(graph, oldGraphID_2_newGraphID)  # re-label vertices in the graph



## writing stan model file for RSBS
##source('http://mc-stan.org/rstan/install.R', echo = TRUE, max.deparse.length = 2000)
##install_rstan()

load('~/GitHub/legacy/automated-blocking-examples/data/model_redblue.RData')
constantsAndData <- c(constants, data)

library(rstan)

stan_mod <- stan_model(file = '~/temp/redblue.stan')

stan_out <- sampling(stan_mod, data=constantsAndData, chains=1, iter=10000, thin=1, init=list(inits))

tempArray <- extract(stan_out, permuted = FALSE, inc_warmup = TRUE)[, 1, ]
dimnames(tempArray)[[2]] <- gsub('_', '.', dimnames(tempArray)[[2]])
if(!all(monitorNodesBUGS %in% dimnames(tempArray)[[2]])) {
    missingNames <- setdiff(monitorNodesBUGS, dimnames(tempArray)[[2]])
    warning(paste0('Stan output is missing values for: ', paste0(missingNames,collapse=', ')))
}
samplesArray <- array(0, dim = c(nkeep, length(monitorNodesBUGS)))
dimnames(samplesArray)[[2]] <- monitorNodesBUGS
monitorsWeHave <- intersect(monitorNodesBUGS, dimnames(tempArray)[[2]])
samplesArray[, monitorsWeHave] <- tempArray[(burnin+1):floor(niter/thin), monitorsWeHave, drop=FALSE]
addToOutput('stan', samplesArray, timeResult)

## playing around to build, install, run, and test gpmanagement package

q('no')
library(devtools)
setwd('~/GitHub/forks/gpmanagement')
remove.packages('gpmanagement')
install_github('danielturek/gpmanagement')
##document('gpmanagement')
system('R CMD build gpmanagement')
system('R CMD INSTALL .')

library(nimble)
library(testthat)
library(gpmanagement)

check('.')  ## this gives me the errors 

gp_setup

test_package('gpmanagement')

source('~/GitHub/forks/gpmanagement/tests/testthat/test_gp.R')

##setwd('~/GitHub/forks/gpmanagement/tests/')
##test_check('gpmanagement')

nf <- gpmanagement:::nfd(3)
nf$run()

set.seed(0)
x <- 1:100
y <- sin(x/5) + rnorm(100, 0.1)
ind <- sort(sample(1:100, 40))
xObs <- x[ind]                ## input
yObs <- y[ind]                ## input
xPred <- c(x, 101:120)        ## input

fit <- gp_setup(xObs, yObs, xPred)


## testing of 3-D array arguments, and their automatic demotion to smaller dim arrays

library(nimble)

nf1 <- nimbleFunction(
    run = function(a = double(2), b = double(3)) {
        print('a11: ', a[1,1])
        print('b111: ', b[1,1,1])
        print(dim(a)[1])
        print(dim(a)[2])
        print(dim(b)[1])
        print(dim(b)[2])
        print(dim(b)[3])
    }
)

nf <- nimbleFunction(
    run = function() {
        a <- nimArray(0, 3, 3)
        declare(b, double(3, c(3,3,3)))
        for(i in 1:3)
            for(j in 1:3) {
                a[i,j] <- 10*i + j
                for(k in 1:3)
                    b[i,j,k] <- 100*i+10*j+k
            }
        print(a)
        ##print(b)
        nf1(a[2,1:2], b[,,])
    }
)

nf()
Cnf <- compileNimble(nf)
Cnf()


## testing distribution dDHMM on a multistate example

CdDHMM <- compileNimble(dDHMM)
CrDHMM <- compileNimble(rDHMM)

checkD <- function(ind) {
    y <- yDHMM
    print(y[ind,])
    x <- y[ind, first[ind]:k]
    length <- length(x)
    phi <- 0.6
    p <- 0.9
    T <- array(c(phi, 1-phi, 0, 1), c(2,2))
    Z <- array(c(p,   1-p,   0, 1), c(2,2))
    pi <- c(1, 0)
    condition <- c(1, 0)
    print(dDHMM(x, length, pi, Z, T, condition, 0))
    print(CdDHMM(x, length, pi, Z, T, condition, 0))
    print(exp(dDHMM(x, length, pi, Z, T, condition, 1)))
    print(exp(CdDHMM(x, length, pi, Z, T, condition, 1)))
}
checkD(225)   ## 0.2484
checkD(202)   ## 0.017496
checkD(167)   ## 0.1246882

checkR <- function(length) {
    phi <- 0.6
    p <- 0.9
    T <- array(c(phi, 1-phi, 0, 1), c(2,2))
    Z <- array(c(p,   1-p,   0, 1), c(2,2))
    pi <- c(1, 0)
    condition <- c(1, 0)
    print(rDHMM(1, length, pi, Z, T, condition))
    print(CrDHMM(1, length, pi, Z, T, condition))
}
checkR(1)
checkR(2)
checkR(3)
checkR(10)


Z <- array(c(.9,.05,.05,.1,.8,.1,0,1/2,1/2), c(3,3))
Z
T <- array(c(0,0,1,0,0,1,0,0,1), c(3,3))
T
pi <- c(1, 1, 1)
condition <- c(1, 1, 1)

y <- c(1,1)
length <- length(y)
dDHMM( y, length, pi, Z, T, condition, 0)
CdDHMM(y, length, pi, Z, T, condition, 0)


## testing new DSL functions:
## nimVector()
## nimArray()

library(nimble)

Rnf <- nimbleFunction(
    run = function(val = double(), len = double(), r = double(), c = double()) {
        onesVec <- nimVector(1, 5)
        print('onesVec:');   print(onesVec)
        zerosVec <- nimVector(0, 5)
        print('zerosVec:');   print(zerosVec)
        otherVec <- nimVector(val, len)
        print('otherVec:');   print(otherVec)
        sumVec <- zerosVec + onesVec + otherVec[1:5]
        print('sumVec:');   print(sumVec)
        arr <- nimArray(.4, r, c)
        print('arr:');   print(arr)
        print('sum of otherVec: ', sum(otherVec))
        print('sum of arrL: ', sum(arr))
    }
)

Cnf <- compileNimble(Rnf)

Rnf(10, 7, 3, 4)
Cnf(10, 7, 3, 4)

## trying nimVector() and nimArray() in BUGS code ?!?
## it works!!
code <- nimbleCode({
    a[1:4] <- nimVector(2, 4)
    phi ~ dnorm(0, 1)
    b[2:10] <- nimVector(10, 9)
    b[1] <- phi
})

Rmodel <- nimbleModel(code)
simulate(Rmodel, 'phi')
calculate(Rmodel)

Cmodel <- compileNimble(Rmodel)

model <- Rmodel
model <- Cmodel

model$a
model$phi
model$b

simulate(model)
calculate(model)

## re-creating NA's data example error
## 11 June 2015:
## Perry said he might get to this by July
## running tests/test-copy.R also gives this same error

library(nimble)

code <- nimbleCode({
    y[2:4] ~ dmnorm(mu3[1:3], cov = cov3[1:3, 1:3])
})
constants <- list(mu3=rep(0,3), cov3=diag(3))
data <- list(y = c(NA,0,0,0))

Rmodel <- nimbleModel(code, constants, data)

calculate(Rmodel)
simulate(Rmodel)

Cmodel <- compileNimble(Rmodel)
calculate(Rmodel)
simulate(Rmodel)


## same error from test-copy.R
library(testthat)
library(nimble)
test_package('nimble', 'copy')




## Zhenglei's example, trying MCMC suite on it

library(lme4)
library(ggplot2)
library(scales)
library(nimble)
library(coda)

I <- 25
J <- 5
beta <- c(-0.2, 0.1)
theta <- 1
sigma <-0.5

simfun <- function(substance=letters[1:10],experiment=factor(1:5),log10HQ=seq(-1,3,by=0.2),theta=1,sigma=0.1,beta=c(-0.2, 0.5)){
    expdat <- expand.grid(experiment = experiment,log10HQ=log10HQ,substance=substance)
    expdat$HQ <- 10^expdat$log10HQ
    expdat$obs <- factor(seq(nrow(expdat)))
    n <- nrow(expdat)
    nsub <- length(substance)
    re <- rnorm(nsub,0,theta)
    names(re) <- substance
    fixed <- beta[1]+beta[2]*expdat$log10HQ
    expdat$y <- re[expdat$substance]+rnorm(n,0,sigma)+fixed
    expdat$effect <- gtools::inv.logit(expdat$y)
    return(expdat)
}

expdat <- simfun(substance=factor(paste0('S',1:I)),experiment = factor(1:J),log10HQ=seq(-1,3,by=0.2),theta=0.5,beta=c(-0.2,0.5),sigma=0.3)

code <- nimbleCode({
    alpha ~ dnorm(0, 0.001)
    beta ~ dnorm( 0, 0.001 )
    tau.y ~ dgamma(0.001, 0.001)  
    tau.re ~ dgamma(0.001, 0.001)
    for(j in 1:M){
        ranef.v[j]~dnorm(0, tau.re)
    }
    for (i in 1:N){
        y[i] ~ dnorm ( mu.y[i], tau.y )
        mu.y[i] <- alpha+beta*x[i] + ranef.v[unit[i]]
    }
})

constants <- list(N = nrow(expdat),M=nlevels(expdat$substance),x = expdat$log10HQ,unit=as.numeric(expdat$substance))
##data <- data.frame(y = expdat$y)
data <- list(y = expdat$y)
inits <- list(alpha = 0, beta = 0.5, tau.y=1,tau.re=1)

out <- MCMCsuite(
    code, constants, data, inits,
    MCMCs = c('nimble', 'jags', 'noConj'),
    niter = 20000,
    makePlot = TRUE,
    savePlot = TRUE,
    summaryStats = c('mean', 'median', 'sd', 'CI95_low', 'CI95_upp', 'effectiveSize'),
    MCMCdefs = list(noConj = quote({ configureMCMC(Rmodel, useConjugacy=FALSE) }))
)

out$summary[, 'effectiveSize', ] / out$timing[c('nimble', 'jags', 'noConj')]


## helping Nick get started with using node$get_mean() function for particle filter

library(nimble)

code <- nimbleCode({
    x ~ dnorm(1, 1)
    y ~ dnorm(x, 1)
})

Rmodel <- nimbleModel(code)
simulate(Rmodel)
Rmodel$x
Rmodel$y
Rmodel$nodeFunctions[['x']]$get_mean()
Rmodel$nodeFunctions[['y']]$get_mean()

## only necessary for the 'higherLevel' example
virtual_NF <- nimbleFunctionVirtual(
    ## don't need to supply prototpye for run(),
    ## since the default is no args, and returnType void
    methods = list(  ## definitely need prototype for return_mean() function
        return_mean = function() {
            returnType(double())
        }
    )
)

nfDef <- nimbleFunction(
    contains = virtual_NF,  ## only necessary for the 'higherLevel' example
    setup = function(model, node) {
        nfList <- nimbleFunctionList(node_stoch_dnorm)
        nfList[[1]] <- model$nodeFunctions[[node]]
    },
    run = function() {
        print('node value is: ', model[[node]])
        print('(assumed to be dnorm) node mean is: ', nfList[[1]]$get_mean())
    },
    methods = list(                        ## only necessary for the 'higherLevel' example
        return_mean = function() {         ##
            returnType(double())           ##
            return(nfList[[1]]$get_mean()) ##
        }                                  ##
    )                                      ##
)

nfX <- nfDef(Rmodel, 'x')
nfY <- nfDef(Rmodel, 'y')
nfX$run()
nfY$run()

compiledList <- compileNimble(list(Rmodel, nfX, nfY))
Cmodel <- compiledList[[1]]
CnfX <- compiledList[[2]]
CnfY <- compiledList[[3]]

CnfX$run()
CnfY$run()

CnfX$return_mean()
CnfY$return_mean()

## this is the 'higherLevel' example
nfDefHigherLevel <- nimbleFunction(
    setup = function(model, nodes) {
        nfList <- nimbleFunctionList(virtual_NF)
        for(i in seq_along(nodes))
            nfList[[i]] <- nfDef(model, nodes[i])
    },
    run = function() {
        declare(meansVector, double(1, length(nfList)))  ## declares a vector of doubles, with length equal to the length of nfList
        for(i in seq_along(nfList)) {
            nfList[[i]]$run()
            meansVector[i] <- nfList[[i]]$return_mean()
        }
        print('vector of the means is: ', meansVector)
    }
)

nfHigherLevel <- nfDefHigherLevel(Rmodel, c('x', 'y'))

nfHigherLevel$run()

CnfHigherLevel <- compileNimble(nfHigherLevel, project = Rmodel)

CnfHigherLevel$run()


## fixing values(model, nodes) <- value reference class '<<-' warning


library(nimble)

nfdef <- nimbleFunction(
    setup = function(model) {},
    run = function(val = double(1)) {
        values(model, 'xxx') <<- val
    }
)

code <- nimbleCode({
    xxx ~ dnorm(0, 1)
})

Rmodel <- nimbleModel(code)

nf <- nfdef(Rmodel)

Cmodel <- compileNimble(Rmodel)
Cnf <- compileNimble(nf, project = Rmodel)

Rmodel$xxx
nf$run(3)
Rmodel$xxx

Cmodel$xxx
Cnf$run(4)
Cmodel$xxx


## testing / trying to find some error in conjugacy system(??)  May 2015

library(nimble)
source('~/GitHub/nimble/packages/nimble/inst/tests/test_utils.R')

## run entire MCMC testing system
source('~/GitHub/nimble/packages/nimble/inst/tests/test-mcmc.R')

set.seed(0)
mu0 = 1:3
Q0 = matrix(c(1, .2, .8, .2, 2, 1, .8, 1, 2), nrow = 3)
Q = solve(matrix(c(3, 1.7, .9, 1.7, 2, .6, .9, .6, 1), nrow = 3))
a = c(-2, .5, 1)
B = matrix(rnorm(9), 3)
code <- nimbleCode({
  mu[1:3] ~ dmnorm(mu0[1:3], Q0[1:3, 1:3])
  y_mean[1:3] <- asCol(a[1:3]) + B[1:3, 1:3] %*% asCol(mu[1:3])
  y[1:3] ~ dmnorm(y_mean[1:3], Q[1:3, 1:3])
})
mu <- mu0 + chol(solve(Q0)) %*% rnorm(3)
# make sure y is a vec not a 1-col matrix or get a dimensionality error
y <- c(a + B%*%mu + chol(solve(Q)) %*% rnorm(3))
data = list(mu0 = mu0, Q0 = Q0, Q = Q, a = a, B = B, y = y)
muQtrue = t(B) %*% Q%*%B + Q0
muMeanTrue = c(solve(muQtrue, crossprod(B, Q%*%(y-a)) + Q0%*%mu0))

constants <- list(mu0 = mu0, Q0 = Q0, Q = Q, B = B, a=a)
data <- list(y=y)
m <- nimbleModel(code, constants = constants, data=data)

m <- nimbleModel(code, constants = data)

spec <- configureMCMC(m)
Rmcmc <- buildMCMC(spec)
spec$getSamplers()

options(error = recover)

test_mcmc(model = code, data = data, seed = 0, numItsC = 10000,
          results = list(mean = list(mu = muMeanTrue),
                           cov = list(mu = solve(muQtrue))),
          resultsTolerance = list(mean = list(mu = rep(.02,3)),
            cov = list(mu = matrix(.01, 3, 3))))


## testing distribution of particle filter likelihood estimates

ll <- log(sapply(muvec, function(mu) mean(dnorm(y, rnorm(n, mu, sigx), sigy))))

plot(muvec, varll)
varest <- 1/2*sigx^4/sigy^4 + muvec^2*sigx^2/sigy^4
lines(muvec, varest, col='red')



## library(nimble)
## Rpf <- buildPF(Rmodel, 'x')
## Cmodel <- compileNimble(Rmodel)
## Cpf <- compileNimble(Rpf, project = Rmodel)
## Rpf$run(10)
## Cpf$run(10)
## ll <- numeric()
## for(i in seq_along(muvec)) {
##     mu <- muvec[i]
##     Cmodel$mu <- mu
##     ll[i] <- Cpf$run(10000)
## }
## plot(muvec, ll)


library(nimble)
source('~/GitHub/pfLL/pfLL.R')
m <- 1000
rep <- 1000
n <- m*rep
muvec <- seq(-5, 5, by=0.2)
sigx <- 1
sigy <- 1
taux <- 1/sigx^2; tauy <- 1/sigy^2
y <- 0   ## observation
constants <- list()
inits <- list(mu = y, sigx = sigx, sigy = sigy)
data <- list(y = y)
code <- quote({
    mu ~ dnorm(0, 0.00001)  ## these should not be necessary
    sigx ~ dunif(0, 1000)  ## these should not be necessary
    sigy ~ dunif(0, 1000)  ## these should not be necessary
    x ~ dnorm(mu, sigx)
    y ~ dnorm(x, sigy)
})
Rmodel <- nimbleModel(code, constants, data, inits)

out <- pfLL(Rmodel, 'x', param = data.frame(mu=muvec), m=m, rep=rep, makePlot=FALSE)

ll <- apply(out$ll, 1, mean)
##ll <- log(sapply(muvec, function(mu) mean(dnorm(y, rnorm(n, mu, sigx), sigy))))
plot(muvec, ll)
ll.pred <- -1/2 * muvec^2 / (sigx^2+sigy^2) + log(1/sqrt(2*pi*(sigx^2+sigy^2)))
lines(muvec, ll.pred, col='red')


varll <- apply(out$ll, 1, var)
##varll <- varll - min(varll)  ### TEMPORARY
plot(muvec, varll)
var.pred <- exp(-8.8 + 1.2592*abs(muvec))
lines(muvec, var.pred, col='red')

1



n <- 100000
sigx <- 1
muvec <- seq(-3, 3, by=0.1)
nmu <- length(muvec)
X <- array(NA, c(n, nmu))
for(i in 1:nmu) X[,i] <- rnorm(n, muvec[i], sigx)

Y <- X^2
var <- apply(Y, 2, var)
pred <- muvec^2 + sigx^2
plot(muvec, var, type='p')
lines(muvec, pred, col='red')



## figuring out 3D plotting in R
rm(list=ls())
df <- expand.grid(list(x=1:10, y=1:10))
rep <- 1
ll <- array(NA, c(dim(df)[1], rep))
for(i in 1:dim(df)[1]) {
    for(j in 1:rep) {
        x <- df[i, 'x']
        y <- df[i, 'y']
        dist <- sqrt((x-4)^2 + (y-7)^2)
        ll[i,j] <- rnorm(1, 100-dist^2, sd = dist/10)
    }
}

library(plot3D)
?scatter3D

x <- rep(df$x, rep)
y <- rep(df$y, rep)
z <- as.numeric(ll)

scatter3D(x, y, z)


## minimally reproducible example of getDependencies error for Perry
library(nimble)
code <- nimbleCode({
    x[1] ~ dnorm(0, 1)
    x[2] ~ dnorm(0, 1)
    y[1] ~ dnorm(x[1], 1)
    y[2] ~ dnorm(x[2], 1)
})

Rmodel <- nimbleModel(code)

Rmodel$getDependencies('x[1]')   ## should be x[1], y[1]
## [1] 'x[1]' 'y[1]' 'y[2]'

Rmodel$getDependencies('x[2]')   ## should be x[2], y[2]
## [1] 'x[2]'



## trying to figure out conjugate sampling in 'equiv' BUGS example model
library(nimble)
code <- nimbleCode({
    tau[1] ~ dgamma(0.001, 0.001)
    tau[2] ~ dgamma(0.001, 0.001)
    pi ~ dnorm(0, 1.0E-06)
    phi ~ dnorm(0, 1.0E-06)
    mu ~ dnorm(0, 1.0E-06)
    for(i in 1:N) { 
        d[i] ~ dnorm(0,tau[2])  # Subject random effect
        for (k in 1:2) {
            Treat[i,k] <- group[i]*(k-1.5) + 1.5  # treatment given
            m[i,k] <- mu + pow(-1, Treat[i,k]-1)* phi /2 + pow(-1, k-1)* pi /2 + d[i]
            Y[i,k] ~ dnorm(m[i,k], tau[1])
        }
    }
    ##theta <- exp(phi)
    ##equivalence <- step(theta - 0.8) - step(theta - 1.2)
})
Y <- structure(c(1.4, 1.64, 1.44, 1.36, 1.65, 1.08, 1.09, 1.25, 1.25, 1.3, 1.65, 1.57, 1.58, 1.68, 1.69, 1.31, 1.43, 1.44, 1.39, 1.52), .Dim = c(10, 2))
group <- c(1, 1, -1, -1, -1, 1, 1, 1, -1, -1)
N <- 10
##T <- 2
constants <- list(N=N, group=group)
data <- list(Y=Y)
inits <- list(mu=0, phi=0, pi=0, tau=c(1,1))
Rmodel <- nimbleModel(code, constants, data, inits)
spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

niter <- 10

set.seed(0); Rmcmc$run(niter)
Rsamples <- as.matrix(Rmcmc$mvSamples)
Rsamples

set.seed(0); Cmcmc$run(niter)
Csamples <- as.matrix(Cmcmc$mvSamples)
Csamples <- Csamples[, dimnames(Rsamples)[[2]]]
Csamples

Rsamples-Csamples


## test equiv model
library(nimble)
source('~/GitHub/nimble/packages/nimble/inst/tests/test_utils.R')
test_mcmc('equiv', numItsC = 1000, resampleData = TRUE)


1

## trying Kalman Filter and calculating likelihood for a linear SSM

# Say state process is
# X(t+1) = r * X(t) + b + nu(t)
# nu(t) ~ dnorm(0, sig2nu)
# 
# Obs process is
# Y(t) = X(t) + eps(t)
# eps(t) ~ dnorm(0, sig2eps)
# 
# Time series goes from t = 1:T
# 
# This is the simplest possible model for us.
# 
# Direct mvn approach:
#   1. Stationary variance of states:
#   V[x] = r^2 V[x] + sig2nu
# V[x] = sig2nu / (1-r^2)
# 
# 2. Deriving entries in covariance matrix of Y (from 1:T)
# Cov[X(t), X(t+1)] = r V[x]
# Cov[X(t), X(t+d)] = r^d V[x]
# Therefore Cov[Y(t), Y(t+d)] = r^d V[x]
# 
# And on the diagonal:
#   V[y] = V[x] + sig2eps
# 
# And mu(t) = E[Y] = E[X] = b/(1-r) for all t
# 
# So with that mean vector and covariance matrix in hand 
# you can directly calculate mvnorm(Y, mu, Cov(Y))
# 
# Kalman filter approach and direct LL are shown in code below.  
# Sorry  I didn't think of this.  I had this from the early 
# explorations that led to Knape, Besbeas and de Valpine in 
# Ecology last year (or was it this year?).  
# 
# In this setup, you want numSites = 1 (we were considering time-series 
# with data from multiple sites).  The LL functions use the meanData 
# (mean over all the sites, which we were comparing to a more complete 
# site-specific model), which for you will be the same as the fullData.



simData <- function(mu=10, a=0.8, sigPN=0.2, sigOE=0.4, t=20) {
    b <- mu*(1-a)
    X <- mu
    for(i in 1:50) X <- a*X + b + rnorm(1,0,sigPN) ## get to stationary
    states <- numeric(t)
    states[1] <- X
    for(i in 2:t) states[i] <- a*states[i-1]+b + rnorm(1,0,sigPN)
    obsStates <- rnorm(t, states, sigOE)
    list(x = states, y = obsStates, mu=mu, a=a, b=b, sigOE=sigOE, sigPN=sigPN, t=t)
}

mvn_ll <- function(d) {
    require(mvtnorm)
    mu <- d$mu
    a <- d$a
    b <- mu*(1-a)
    sigPN <- d$sigPN
    sigPN2 <- sigPN*sigPN
    sigOE <- d$sigOE
    sigOE2 <- sigOE*sigOE
    y <- d$y
    t <- d$t
    mu_vec <- rep( b/(1-a), t)
    cov_mat <- matrix(nrow = t, ncol = t)
    var_x <- sigPN2 / (1-a^2)
    var_y <- var_x + sigOE2
    diag(cov_mat) <- var_y
    for(i in 2:t) {
        for(j in 1:(i-1)) {
            cov_mat[i,j] <- (a^(i-j))*var_x
            cov_mat[j,i] <- cov_mat[i,j]
        }
    }
    dmvnorm(y, mu_vec, cov_mat, log = TRUE)
}

KF_ll <- function(d) {
    mu <- d$mu
    a <- d$a
    b <- mu*(1-a)
    sigPN <- d$sigPN
    sigPN2 <- sigPN*sigPN
    sigOE <- d$sigOE
    sigOE2 <- sigOE*sigOE
    y <- d$y
    t <- d$t
    mu_x <- b/(1-a)
    var_x <- sigPN2 / (1-a^2)
    cov_xy <- var_x
    var_y <- var_x + sigOE2
    ll <- dnorm(mu_x, y[1], sqrt(var_y), log = TRUE)
    mu_x <- mu_x + (cov_xy / var_y) * (y[1] - mu_x)
    var_x <- var_x - cov_xy*cov_xy / var_y
    for(i in 2:t) {
        mu_x <- a*mu_x + b
        var_x <- a^2 * var_x + sigPN2
        if(!is.na(y[i])) {
            cov_xy <- var_x
            var_y <- var_x + sigOE2
            ll <- ll + dnorm(mu_x, y[i], sqrt(var_y), log = TRUE)
            mu_x <- mu_x + (cov_xy / var_y) * (y[i] - mu_x)
            var_x <- var_x - cov_xy*cov_xy / var_y
        }
    }
    ll
}

code <- quote({
    x[1] ~ dnorm(mu, sd = sqrt((sigPN^2) / (1 - a^2)))
    y[1] ~ dnorm(x[1], sd = sigOE)
    for(i in 2:t){
        x[i] ~ dnorm(x[i-1] * a + b, sd = sigPN)
        y[i] ~ dnorm(x[i], sd = sigOE)
    }
})

d <- simData()

library(nimble)
constants <- list(a=d$a, b=d$b, t=d$t, sigOE=d$sigOE, sigPN=d$sigPN, mu=d$mu)
data <- list(y = d$y)
inits <- list(x = d$x)
Rmodel <- nimbleModel(code = code, constants = constants, data=data, inits=inits)
calculate(Rmodel)
pf <- buildPF(Rmodel, 'x')
Cmodel <- compileNimble(Rmodel)
Cpf <- compileNimble(pf, project = Rmodel)

Cpf$run(100000)

mvn_ll(d)

KF_ll(d)



## exploring how truncation works in our system

library(nimble)
code <- nimbleCode({
    a ~ dnorm(0, 1)
    b ~ T(dnorm(a, 1), 1.5, 1.6)
    c ~ T(dnorm(b, 1), 3, Inf)
    d ~ dnorm(c, 1)
    e ~ dnorm(d, 1)
})
constants <- list()
data <- list()
inits <- list()

md <- nimbleModel(code, constants, data, inits, returnDef = TRUE)
Rmodel <- md$newModel(data=data, inits=inits)

Rmodel$modelDef$printDI()

node <- 'c'
Rmodel$nodes[[node]]$calculate
Rmodel$nodes[[node]]$simulate
Rmodel$nodes[[node]]$getLogProb

lapply(Rmodel$modelDef$declInfo, function(di) di$truncation)
for(node in c('a', 'b', 'c')) print(Rmodel$isTruncated(node))
for(node in c('a', 'b', 'c')) print(Rmodel$getBounds(node))

spec <- configureMCMC(Rmodel)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)



## using rankSample

library(nimble)

##Cmodel <- compileNimble(Rmodel = nimbleModel(quote({a ~ dnorm(0, 1)})))

nfDef <- nimbleFunction(
    setup = function() {},
    run = function(wts = double(1), m = integer(), silent = logical()) {
        ##print('in nimbleFunction rankSample')
        declare(samp, integer(1))
        rankSample(wts, m, samp, silent)
        print(samp)
    }
)
nf <- nfDef()
Cnf <- compileNimble(nf)

nfDefOmit <- nimbleFunction(
    setup = function() {},
    run = function(wts = double(1), m = integer()) {
        ##print('in nimbleFunction rankSample')
        declare(samp, integer(1))
        rankSample(wts, m, samp)
        print(samp)
    }
)
nfOmit <- nfDefOmit()
CnfOmit <- compileNimble(nfOmit)
x <- 1:10

silent <- FALSE
silent <- TRUE

set.seed(1);   .Call('rankSample', c(.5,.4,.1), 10L, x, silent)
set.seed(1);            rankSample(c(.5,.4,.1), 10L, x, silent);    print(x)
set.seed(1);                nf$run(c(.5,.4,.1), 10L,    silent)
set.seed(1);               Cnf$run(c(.5,.4,.1), 10L,    silent)

set.seed(1);   .Call('rankSample', c( 0, 0, 0), 10L, x, silent)
set.seed(1);            rankSample(c( 0, 0, 0), 10L, x, silent);    print(x)
set.seed(1);                nf$run(c( 0, 0, 0), 10L,    silent)
set.seed(1);               Cnf$run(c( 0, 0, 0), 10L,    silent)

set.seed(1);   .Call('rankSample', c(.5,.4,-1), 10L, x, silent)
set.seed(1);            rankSample(c(.5,.4,-1), 10L, x, silent);    print(x)
set.seed(1);                nf$run(c(.5,.4,-1), 10L,    silent)
set.seed(1);               Cnf$run(c(.5,.4,-1), 10L,    silent)

set.seed(1);   .Call('rankSample', c(.5,.4,-1), 10L, x)
set.seed(1);            rankSample(c(.5,.4,-1), 10L, x);            print(x)
set.seed(1);            nfOmit$run(c(.5,.4,-1), 10L)
set.seed(1);           CnfOmit$run(c(.5,.4,-1), 10L)



## difference in rank sample
library(nimble)
nfDef <- nimbleFunction(
    setup = function() {},
    run = function() {
        declare(wts, double(1, 1))
        wts[1] <- 1
        m <- 1L
        declare(samp, integer(1, 1))
        rand <- rnorm(1, 0, 1)
        print('random number before rankSample: ', rand)
        rankSample(wts, m, samp)
        rand <- rnorm(1, 0, 1)
        print('random number after rankSample: ', rand)
    }
)

nf <- nfDef()
Cnf <- compileNimble(nf)
set.seed(0)
nf$run()
set.seed(0)
Cnf$run()



## testing buildPF() particle filter (pf) algorithm
library(nimble)
code <- quote({
    mu ~ dnorm(0, sd = 1000)
    b ~ dnorm(0, sd = 1000)
    sigPN ~ dunif(0.0001, 1)
    sigOE ~ dunif(0.0001, 1)
    x[1] ~ dnorm(mu, sd = sqrt(sigPN^2 + sigOE^2))
    y[1] ~ dnorm(x[1], sd = sigOE)
    a <- 1-(b/mu)
    for(i in 2:t){
        x[i] ~ dnorm(x[i-1] * a + b, sd = sigPN)
        y[i] ~ dnorm(x[i], sd = sigOE)
    }
})
t <- 5
constants <- list(t = t)
Rmodel <- nimbleModel(code = code, constants = constants)
Rmodel$mu <- 1/(1-.95)
Rmodel$b <- 1
Rmodel$sigPN <- .2
Rmodel$sigOE <- .05
set.seed(0)
calculate(Rmodel, Rmodel$getDependencies(c('mu','b','sigPN','sigOE'), determOnly = TRUE))
simulate(Rmodel, Rmodel$getDependencies(c('x', 'y')))
data <- list(y = Rmodel$y)
inits <- list(mu = Rmodel$mu, b = Rmodel$b, sigPN = Rmodel$sigPN, sigOE = Rmodel$sigOE, x = Rmodel$x)
rm(Rmodel)
Rmodel <- nimbleModel(code, constants, data, inits)
pf <- buildPF(Rmodel, 'x')
Cmodel <- compileNimble(Rmodel)
Cpf <- compileNimble(pf, project = Rmodel)

m <- 100
set.seed(0);    pf$run(m)
set.seed(0);   Cpf$run(m)

1
2
3



## making MCMCsuite work for Ryan using Stan MCMC

allModels <- c('blocker', 'bones', 'dyes', 'line', 'pump', 'rats')
library(nimble)
library(coda)
allModels <- 'blocker'
mcmcs=c('nimble','jags','noConj','stan')
x=list()
for (i in 1:length(allModels)){
    x[[i]]<-readBUGSmodel(model=allModels[i],
                          dir=getBUGSexampleDir(allModels[i]),
                          returnModelComponentsOnly=TRUE)
}
i <- 1
suite_output <- MCMCsuite(
    x[[i]]$model,
    constants = x[[i]]$data,
    inits = x[[i]]$inits,
    MCMCs = mcmcs,
    makePlot=F,
    savePlot=F,
    summaryStats=c('mean','median','sd','CI95_low','CI95_upp','effectiveSize'),
    MCMCdefs = list(noConj = quote({ configureMCMC(Rmodel, useConjugacy=FALSE) })),
    stan_model = paste('~/temp/', allModels[i], '.stan', sep='')
)


## learning about R's memory management and storage of objects
gcinfo(TRUE)
gc()

library(pryr)
?object_size
object_size(1:100)
object_size(mtcars)

x <- 0:50
s <- sapply(x, function(i) object_size(1:i))
plot(x, s, type='s', ylim=c(0,300))

obj <- complex()
object_size(obj)

mem_used
mem_used()

pryr:::node_size()
pryr:::show_bytes

mem_change(NA)

bytes(1)
bytes(1L)
bytes('ab')

address(2)
x <- 1:10
address(x)
refs(x)
y <- x
refs(x)
z <- x
x[1] <- 16L

tracemem(x)
x[1] <- .1

x <- 15
address(x)

system(paste0('~/temp/t ', address(x)))



## testing of mcmcplots package function mcmcplot()

rm(list=ls())
library(nimble)
model <- 'SSMcorrelated'
load(paste0('~/GitHub/autoBlock/data/model_', model, '.RData'))
Rmodel <- nimbleModel(code, constants, data, inits)

out <- MCMCsuite(code, constants, data, inits, monitors=c('a','b','p'), MCMCs=c('nimble','nimble_RW', 'nimble_slice', 'jags'), makePlot=FALSE, niter=10000)

library(coda)
samples <- out$samples
dim(samples)
dimnames(samples)
mcmcList <- list()
for(i in 1:dim(samples)[1])     mcmcList[[i]] <- coda::mcmc(t(samples[i,,]))
names(mcmcList) <- dimnames(samples)[[1]]
codaList <- coda::as.mcmc.list(mcmcList)

library(mcmcplots)
mcmcplot(codaList)
traplot(codaList)




## testing conjugacy in newNimbleModel branch

library(nimble)
code <- nimbleCode({
    for(i in 1:10) {
        a[i] ~ dnorm(0, 1)
        b[i] ~ dgamma(1, 1)
    }
    for(i in 1:5)   a_temp[i] <- 3*a[i]
    for(i in 6:10)  a_temp[i] <- a[i]^2
    for(i in 1:10)  x[i] ~ dnorm(a_temp[i], 1)
    for(i in 1:10) {
        y1[i] ~ dpois(2*b[i])
        y2[i] ~ dgamma(1, rate = b[i])
    }
})
Rmodel <- nimbleModel(code)

spec <- configureMCMC(Rmodel)

spec$getSamplers()
for(i in 1:200) spec$addSampler('RW', 'x[2]', print=F)
spec$addSampler('RW', 'x[2]')

spec$setSamplers(c(1:40, 1:40))
spec$setSamplers(c('a', 'y2'))
spec$removeSamplers('a')
spec$setSamplers()


## NEED TO RE-RUN THIS TEST, ONCE PERRY FIXES THE MODELVALUES COPYING ISSUE
## Dirichlet-multinomial conjugacy
## single multinomial
library(nimble)
set.seed(0)
n <- 100
alpha <- c(10, 30, 15, 60, 1)
K <- length(alpha)
p <- c(.12, .24, .09, .54, .01)
y <- rmulti(1, n, p)
code <- nimbleCode({
    y[1:K] ~ dmulti(p[1:K], n)
    p[1:K] ~ ddirch(alpha[1:K])
    for(i in 1:K) {
        alpha[i] ~ dgamma(.001, .001)
    }
})
constants <- list(n = n, K = K)
data <- list(y = y)
inits <- list(p = rep(1/K, K), alpha = rep(K, K))
Rmodel <- nimbleModel(code, constants=constants, data=data, inits=inits)

mv <- modelValues(Rmodel)
Rmodel$y
mv$y
nimCopy(from=Rmodel, to=mv, nodes = 'y', row = 1)
mv$y


spec <- configureMCMC(Rmodel, monitors = c('alpha', 'p'))
spec$getSamplers()
Rmcmc <- buildMCMC(spec)

set.seed(0)
Rmcmc$run(10)

conjugate posterior density appears to be wrong, off by Inf
conjugate posterior density appears to be wrong, off by Inf
There were 50 or more warnings (use warnings() to see the first 50)

test_mcmc(model = code, data= data, seed = 0, numItsC = 10000,
          inits = inits,
          results = list(mean = list(p = p)),
          resultsTolerance = list(mean = list(p = rep(.06, K))))




## just a good old fashioned test that it's working
library(nimble)
code <- BUGScode({
    x ~ dgamma(1, 1)       # should satisfy 'gamma' conjugacy class
    a  ~ dnorm(0, x)     # should satisfy 'norm' conjugacy class
    a2 ~ dnorm(0, tau = 3*x+0)
    b  ~ dpois(0+5*x)
    b2 ~ dpois(1*x*1)
    c ~ dgamma(1, 7*x*5)
    for(i in 2:3) {
        jTau[i] <- 1
        jNorm[i] ~ dnorm(c * (a+3) - i, var = jTau[i])
        kTauSd[i] <- 2
        kLogNorm[i] ~ dlnorm(0 - a - 6*i, kTauSd[i])
    }
})

constants <- list()
data <- list()
inits <- list()
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$checkConjugacy()

spec <- configureMCMC(Rmodel, control = list(scale=0.01), monitors = c('x', 'c'))
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
set.seed(0)
Rmcmc$run(10)
samples <- as.matrix(Rmcmc$mvSamples)
samples


sampleVals = list(x = c(3.950556165467749, 1.556947815895538, 1.598959152023738, 2.223758981790340, 2.386291653164086, 3.266282048060261, 3.064019155073057, 3.229661999356182, 1.985990552839427, 2.057249437940977),
  c = c( 0.010341199485849559, 0.010341199485849559, 0.003846483017887228, 0.003846483017887228, 0.007257679932131476, 0.009680314740728335, 0.012594777095902964, 0.012594777095902964, 0.018179641351556003, 0.018179641351556003))

test_mcmc(model = code, exactSample = sampleVals, seed = 0, mcmcControl = list(scale=0.01))




## testing if a Ref Class can just return a smaller object (not the whole Ref Class object)
RC <- setRefClass(
    Class = 'RC',
    fields = list(a='ANY'),
    methods = list(
        initialize = function(a) {
            a <<- a
            return(a)
        }
    )
)

rc <- RC(1)
class(rc)
rc$a



## bunch of random stuff

fileToList <- function(file) {
    env <- new.env()
    source(fileName, local = env)
    lst <- list()
    for(name in ls(env))   lst[[name]] <- get(name, env)
    return(lst)
}

library(nimble)

NF <- nimbleFunction(
    setup = function() { },
    run = function(a = double(0)) {
        returnType(double(0))
        return(a)
    }
)

myNF <- NF()
myNF_C <- compileNimble(myNF)

> myNF$run
## function (a) 
##     return(a)

myNF_C$run
## function (a) 
## {
##     ans <- .Call('CALL_nfRefClass60_operator_', a, .basePtr)
##     ans <- ans[[2]]
##     ans
## }


### testing the new autoBlock
rm(list=ls())
library(nimble)
model <- 'litters'
load(paste0('~/GitHub/autoBlock/data/model_', model, '.RData'))
Rmodel <- nimbleModel(code, constants, data, inits)
Rmodel$getNodeNames()

ab <- autoBlock(Rmodel, run = runList)
ab
ab$spec$getSamplers()
Rmcmc <- buildMCMC(ab$spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project=Rmodel)


spec <- configureMCMC(Rmodel, autoBlock=TRUE)
spec$getSamplers()
Rmcmc <- buildMCMC(spec)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project=Rmodel)

Rmcmc <- buildMCMC(Rmodel, autoBlock=TRUE)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project=Rmodel)







## March 2015
## timing comparisons for red-state-blue-state
library(nimble)
library(rjags)
rm(list=ls())
load('~/GitHub/autoBlock/data/model_redblue.RData')

## this version of red-state-blue-state code is 'jags friendly'
## reparametrized to use Precision matrix, replaced expit with logit
code <- nimbleCode({
    for (i in 1:2) {
        for (j in 1:2) {
            gamma[i, j] ~ dnorm(0, 1e-04)
        }
    }
    sigmaIntercept ~ dunif(0, 100)
    sigmaSlope ~ dunif(0, 100)
    rho ~ dunif(-1, 1)
    Precision[1, 1] <- 1/(1-rho^2) * 1/(sigmaIntercept^2)
    Precision[2, 2] <- 1/(1-rho^2) * 1/(sigmaSlope^2)
    Precision[1, 2] <- 1/(1-rho^2) * -rho/(sigmaIntercept*sigmaSlope)
    Precision[2, 1] <- 1/(1-rho^2) * -rho/(sigmaIntercept*sigmaSlope)
    for (i in 1:Nstates) {
        stateBetaMeans[i, 1] <- gamma[1, 1] + gamma[1, 2] * stateIncome[i]
        stateBetaMeans[i, 2] <- gamma[2, 1] + gamma[2, 2] * stateIncome[i]
        stateBetas[i, 1:2] ~ dmnorm(stateBetaMeans[i, 1:2], Precision[1:2, 1:2])
    }
    for (i in 1:N) {
        logit(p[i]) <- stateBetas[state[i], 1] + stateBetas[state[i], 2] * income[i]
        y[i] ~ dbern(p[i])
    }
})

niter <- 50000
monitorVars <- c('sigmaIntercept', 'sigmaSlope', 'rho', 'gamma')
constsAndData <- c(constants, data)
modelfile <- file.path(tempdir(), 'model.txt')
writeLines(paste0('model\n', paste0(deparse(code, width.cutoff=500L), collapse='\n')), con=modelfile)


system.time(modelDef <- nimbleModel(code=code, constants=constants, data=data, inits=inits, returnDef=TRUE))[3]/60
26

system.time(Rmodel <- modelDef$newModel(data=data, inits=inits))[3]/60
5

system.time(spec <- configureMCMC(Rmodel))
6 seconds

system.time(Rmcmc <- buildMCMC(spec))[3]
15 seconds

system.time(Cmodel <- compileNimble(Rmodel))[3]/60
11

system.time(Cmcmc <- compileNimble(Rmcmc, project = Rmodel))[3]/60
1

system.time(Cmcmc$run(niter))[3]/60
11 minutes for 50,000 iterations

system.time(jags_mod <- jags.model(file=modelfile, data=constsAndData, inits=inits, n.chains=1, quiet=FALSE))[3]/60
10 seconds

system.time(jags_out <- coda.samples(model=jags_mod, variable.names=monitorVars, n.iter=niter, thin=1))[3]/60
2.001833





## displaying calc and sim functions
code <- nimbleCode({ x ~ dnorm(3, sd = 5) })
model <- nimbleModel(code)
model$nodes$x$simulate
## function()
##     model$x <<- rnorm(1, mean = 3, sd = 5)
model$nodes$x$calculate
## function() {
##     model$logProb_x <<- dnorm(model$x, mean = 3, sd = 5, log = 1)
##     return(invisible(model$logProb_x))
## }
model$nodes$x$getLogProb
## function() 
##     return(model$logProb_x)


### March 2015
### trying to figure out new MCMCspec

library(nimble)


#These first few lines are nothing new
myModelCode <- nimbleCode({
	a[1] ~ dnorm(0,1)
	a[2] ~ dexp(a[1]^2)
	a[3] ~ dnorm(0,1)
})
myModel <- nimbleModel(myModelCode)
mySpec1 <- configureMCMC(myModel)
myMCMC1 <- buildMCMC(mySpec1)
cmod <- compileNimble(myModel)
cmcmc1 <- compileNimble(myMCMC1, project = myModel)


# Rebuilding the way we are supposed to: 
# building a new spec with configureMCMC 
# adding a sampler which has already been compiled 
# not altering the monitors
mySpec2 <- configureMCMC(oldSpec = mySpec1)
mySpec2$addSampler('RW', control = list(targetNode = 'a[3]'))
rmcmc2 <- buildMCMC(mySpec2)
cmcmc2 <- compileNimble(rmcmc2, project = myModel)




#### March 2015 working through a full compileNimble call

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
inits <- list(a = 1)
Rmodel <- nimbleModel(code, inits = inits)

debug(compileNimble)
Cmodel <- compileNimble(Rmodel)

debug(project$compileModel)
debug(modelCpp$buildAll)
debug(buildNodes)
debug(nimbleProject$compileNimbleFunctionMulti)
debug(compileNimbleFunction)
debug(buildNimbleFunctionCompilationInfo)


#### March 2015 testing of RW_llFunction or Carl B, submitted as test to testing suite

library(nimble)

code <- nimbleCode({
    a ~ dnorm(0, 1)
})
inits <- list(a = 0)
Rmodel <- nimbleModel(code=code, inits=inits)

llFunction <- nimbleFunction(
    setup = function(model) { },
    run = function() {
        ll <- dnorm(1, model$a, 1, log=1)
        returnType(double())
        return(ll)
    }
)
 
myLL <- llFunction(Rmodel)

spec <- configureMCMC(Rmodel, nodes=NULL)
spec$addSampler('RW_llFunction', list(targetNode='a', llFunction=myLL, includesTarget=FALSE))
Rmcmc <- buildMCMC(spec)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

set.seed(0)
Rmcmc$run(10)
unlist(Rmcmc$mvSamples[['a']])
## [1] -0.2849616  0.4851313  0.1241257  0.1241257  0.1241257 -0.3553077 0.4223091  0.4455515  0.4455515  0.4455515

set.seed(0)
Cmcmc$run(10)
unlist(Cmcmc$mvSamples[['a']])
## [1] -0.2849616  0.4851313  0.1241257  0.1241257  0.1241257 -0.3553077 0.4223091  0.4455515  0.4455515  0.4455515

Cmcmc$run(100000)
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)
## should be within some tolerance of 0.5


###### Feb 2015 example for Cal Poly interview talk

library(nimble)

code <- nimbleCode({
    for( i in 1 : N ) {
        b[i] ~ dnorm(mu, tau)
        r[i] ~ dbin(p[i], n[i])
        logit(p[i]) <- b[i]
    }
    pop.mean <- 1 / (1 + exp(-mu))
    mu ~ dnorm(0, 0.001)
    sigma <- 1 / sqrt(tau)
    tau ~ dgamma(0.001, 0.001)
})
constants <- list(N = 10, n = c(12,20,18,19,10,23,19,15,8,10))
data <- list(r = c(4,7,10,3,6,7,11,9,3,8))
inits <- list(mu = 0, tau = 1)

model <- nimbleModel(code=code, constants=constants, data=data, inits=inits)

spec <- configureMCMC(model)
spec$getSamplers()
spec$addSampler(â€˜sliceâ€™, list(targetNode = â€˜muâ€™))
spec$getSamplers()
mcmc <- buildMCMC(spec)

Cmodel <- compileNimble(model)
Cmcmc <- compileNimble(mcmc, project = model)

Cmcmc$run(10000)

samples <- as.matrix(Cmcmc$mvSamples)

apply(samples, 2, mean)

